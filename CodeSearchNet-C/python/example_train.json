{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "split_phylogeny", "original_string": "def split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level+\"__\"\n    result = p.split(level)\n    return result[0]+level+result[1].split(\";\")[0]", "language": "python", "code": "def split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level+\"__\"\n    result = p.split(level)\n    return result[0]+level+result[1].split(\";\")[0]", "code_tokens": ["def", "split_phylogeny", "(", "p", ",", "level", "=", "\"s\"", ")", ":", "level", "=", "level", "+", "\"__\"", "result", "=", "p", ".", "split", "(", "level", ")", "return", "result", "[", "0", "]", "+", "level", "+", "result", "[", "1", "]", ".", "split", "(", "\";\"", ")", "[", "0", "]"], "docstring": "Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.", "docstring_tokens": ["Return", "either", "the", "full", "or", "truncated", "version", "of", "a", "QIIME", "-", "formatted", "taxonomy", "string", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L159-L177", "partition": "train", "up_fun_num": 5, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def ensure_dir(d):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "ensure_dir", "original_string": "def ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\")\n                return msg.format(d)\n            else:\n                msg = twdd(\"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\")\n                return msg.format(d, oe.strerror)", "language": "python", "code": "def ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\")\n                return msg.format(d)\n            else:\n                msg = twdd(\"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\")\n                return msg.format(d, oe.strerror)", "code_tokens": ["def", "ensure_dir", "(", "d", ")", ":", "if", "not", "os", ".", "path", ".", "exists", "(", "d", ")", ":", "try", ":", "os", ".", "makedirs", "(", "d", ")", "except", "OSError", "as", "oe", ":", "# should not happen with os.makedirs", "# ENOENT: No such file or directory", "if", "os", ".", "errno", "==", "errno", ".", "ENOENT", ":", "msg", "=", "twdd", "(", "\"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"", ")", "return", "msg", ".", "format", "(", "d", ")", "else", ":", "msg", "=", "twdd", "(", "\"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"", ")", "return", "msg", ".", "format", "(", "d", ",", "oe", ".", "strerror", ")"], "docstring": "Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.", "docstring_tokens": ["Check", "to", "make", "sure", "the", "supplied", "directory", "path", "does", "not", "exist", "if", "so", "create", "it", ".", "The", "method", "catches", "OSError", "exceptions", "and", "returns", "a", "descriptive", "message", "instead", "of", "re", "-", "raising", "the", "error", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L180-L206", "partition": "train", "up_fun_num": 6, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "file_handle", "original_string": "def file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle", "language": "python", "code": "def file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle", "code_tokens": ["def", "file_handle", "(", "fnh", ",", "mode", "=", "\"rU\"", ")", ":", "handle", "=", "None", "if", "isinstance", "(", "fnh", ",", "file", ")", ":", "if", "fnh", ".", "closed", ":", "raise", "ValueError", "(", "\"Input file is closed.\"", ")", "handle", "=", "fnh", "elif", "isinstance", "(", "fnh", ",", "str", ")", ":", "handle", "=", "open", "(", "fnh", ",", "mode", ")", "return", "handle"], "docstring": "Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.", "docstring_tokens": ["Takes", "either", "a", "file", "path", "or", "an", "open", "file", "handle", "checks", "validity", "and", "returns", "an", "open", "file", "handle", "or", "raises", "an", "appropriate", "Exception", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L209-L231", "partition": "train", "up_fun_num": 7, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def ensure_dir(d):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "gather_categories", "original_string": "def gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [header.index(cat)\n               for cat in categories if cat in header and \"=\" not in cat]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    #If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table", "language": "python", "code": "def gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [header.index(cat)\n               for cat in categories if cat in header and \"=\" not in cat]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    #If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table", "code_tokens": ["def", "gather_categories", "(", "imap", ",", "header", ",", "categories", "=", "None", ")", ":", "# If no categories provided, return all SampleIDs", "if", "categories", "is", "None", ":", "return", "{", "\"default\"", ":", "DataCategory", "(", "set", "(", "imap", ".", "keys", "(", ")", ")", ",", "{", "}", ")", "}", "cat_ids", "=", "[", "header", ".", "index", "(", "cat", ")", "for", "cat", "in", "categories", "if", "cat", "in", "header", "and", "\"=\"", "not", "in", "cat", "]", "table", "=", "OrderedDict", "(", ")", "conditions", "=", "defaultdict", "(", "set", ")", "for", "i", ",", "cat", "in", "enumerate", "(", "categories", ")", ":", "if", "\"=\"", "in", "cat", "and", "cat", ".", "split", "(", "\"=\"", ")", "[", "0", "]", "in", "header", ":", "cat_name", "=", "header", "[", "header", ".", "index", "(", "cat", ".", "split", "(", "\"=\"", ")", "[", "0", "]", ")", "]", "conditions", "[", "cat_name", "]", ".", "add", "(", "cat", ".", "split", "(", "\"=\"", ")", "[", "1", "]", ")", "# If invalid categories or conditions identified, return all SampleIDs", "if", "not", "cat_ids", "and", "not", "conditions", ":", "return", "{", "\"default\"", ":", "DataCategory", "(", "set", "(", "imap", ".", "keys", "(", ")", ")", ",", "{", "}", ")", "}", "#If only category column given, return column-wise SampleIDs", "if", "cat_ids", "and", "not", "conditions", ":", "for", "sid", ",", "row", "in", "imap", ".", "items", "(", ")", ":", "cat_name", "=", "\"_\"", ".", "join", "(", "[", "row", "[", "cid", "]", "for", "cid", "in", "cat_ids", "]", ")", "if", "cat_name", "not", "in", "table", ":", "table", "[", "cat_name", "]", "=", "DataCategory", "(", "set", "(", ")", ",", "{", "}", ")", "table", "[", "cat_name", "]", ".", "sids", ".", "add", "(", "sid", ")", "return", "table", "# Collect all condition names", "cond_ids", "=", "set", "(", ")", "for", "k", "in", "conditions", ":", "try", ":", "cond_ids", ".", "add", "(", "header", ".", "index", "(", "k", ")", ")", "except", "ValueError", ":", "continue", "idx_to_test", "=", "set", "(", "cat_ids", ")", ".", "union", "(", "cond_ids", ")", "# If column name and condition given, return overlapping SampleIDs of column and", "# condition combinations", "for", "sid", ",", "row", "in", "imap", ".", "items", "(", ")", ":", "if", "all", "(", "[", "row", "[", "header", ".", "index", "(", "c", ")", "]", "in", "conditions", "[", "c", "]", "for", "c", "in", "conditions", "]", ")", ":", "key", "=", "\"_\"", ".", "join", "(", "[", "row", "[", "idx", "]", "for", "idx", "in", "idx_to_test", "]", ")", "try", ":", "assert", "key", "in", "table", ".", "keys", "(", ")", "except", "AssertionError", ":", "table", "[", "key", "]", "=", "DataCategory", "(", "set", "(", ")", ",", "{", "}", ")", "table", "[", "key", "]", ".", "sids", ".", "add", "(", "sid", ")", "try", ":", "assert", "len", "(", "table", ")", ">", "0", "except", "AssertionError", ":", "return", "{", "\"default\"", ":", "DataCategory", "(", "set", "(", "imap", ".", "keys", "(", ")", ")", ",", "{", "}", ")", "}", "else", ":", "return", "table"], "docstring": "Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned", "docstring_tokens": ["Find", "the", "user", "specified", "categories", "in", "the", "map", "and", "create", "a", "dictionary", "to", "contain", "the", "relevant", "data", "for", "each", "type", "within", "the", "categories", ".", "Multiple", "categories", "will", "have", "their", "types", "combined", "such", "that", "each", "possible", "combination", "will", "have", "its", "own", "entry", "in", "the", "dictionary", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L238-L309", "partition": "train", "up_fun_num": 8, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def ensure_dir(d):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "parse_unifrac", "original_string": "def parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\"File format not supported/recognized. Please check input \"\n                         \"unifrac file.\")", "language": "python", "code": "def parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\"File format not supported/recognized. Please check input \"\n                         \"unifrac file.\")", "code_tokens": ["def", "parse_unifrac", "(", "unifracFN", ")", ":", "with", "open", "(", "unifracFN", ",", "\"rU\"", ")", "as", "uF", ":", "first", "=", "uF", ".", "next", "(", ")", ".", "split", "(", "\"\\t\"", ")", "lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "uF", "]", "unifrac", "=", "{", "\"pcd\"", ":", "OrderedDict", "(", ")", ",", "\"eigvals\"", ":", "[", "]", ",", "\"varexp\"", ":", "[", "]", "}", "if", "first", "[", "0", "]", "==", "\"pc vector number\"", ":", "return", "parse_unifrac_v1_8", "(", "unifrac", ",", "lines", ")", "elif", "first", "[", "0", "]", "==", "\"Eigvals\"", ":", "return", "parse_unifrac_v1_9", "(", "unifrac", ",", "lines", ")", "else", ":", "raise", "ValueError", "(", "\"File format not supported/recognized. Please check input \"", "\"unifrac file.\"", ")"], "docstring": "Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)", "docstring_tokens": ["Parses", "the", "unifrac", "results", "file", "into", "a", "dictionary"], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L311-L334", "partition": "train", "up_fun_num": 9, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def ensure_dir(d):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "parse_unifrac_v1_8", "original_string": "def parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac", "language": "python", "code": "def parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac", "code_tokens": ["def", "parse_unifrac_v1_8", "(", "unifrac", ",", "file_data", ")", ":", "for", "line", "in", "file_data", ":", "if", "line", "==", "\"\"", ":", "break", "line", "=", "line", ".", "split", "(", "\"\\t\"", ")", "unifrac", "[", "\"pcd\"", "]", "[", "line", "[", "0", "]", "]", "=", "[", "float", "(", "e", ")", "for", "e", "in", "line", "[", "1", ":", "]", "]", "unifrac", "[", "\"eigvals\"", "]", "=", "[", "float", "(", "entry", ")", "for", "entry", "in", "file_data", "[", "-", "2", "]", ".", "split", "(", "\"\\t\"", ")", "[", "1", ":", "]", "]", "unifrac", "[", "\"varexp\"", "]", "=", "[", "float", "(", "entry", ")", "for", "entry", "in", "file_data", "[", "-", "1", "]", ".", "split", "(", "\"\\t\"", ")", "[", "1", ":", "]", "]", "return", "unifrac"], "docstring": "Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.", "docstring_tokens": ["Function", "to", "parse", "data", "from", "older", "version", "of", "unifrac", "file", "obtained", "from", "Qiime", "version", "1", ".", "8", "and", "earlier", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L337-L356", "partition": "train", "up_fun_num": 10, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def ensure_dir(d):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "parse_unifrac_v1_9", "original_string": "def parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry)*100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac", "language": "python", "code": "def parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry)*100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac", "code_tokens": ["def", "parse_unifrac_v1_9", "(", "unifrac", ",", "file_data", ")", ":", "unifrac", "[", "\"eigvals\"", "]", "=", "[", "float", "(", "entry", ")", "for", "entry", "in", "file_data", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", "]", "unifrac", "[", "\"varexp\"", "]", "=", "[", "float", "(", "entry", ")", "*", "100", "for", "entry", "in", "file_data", "[", "3", "]", ".", "split", "(", "\"\\t\"", ")", "]", "for", "line", "in", "file_data", "[", "8", ":", "]", ":", "if", "line", "==", "\"\"", ":", "break", "line", "=", "line", ".", "split", "(", "\"\\t\"", ")", "unifrac", "[", "\"pcd\"", "]", "[", "line", "[", "0", "]", "]", "=", "[", "float", "(", "e", ")", "for", "e", "in", "line", "[", "1", ":", "]", "]", "return", "unifrac"], "docstring": "Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.", "docstring_tokens": ["Function", "to", "parse", "data", "from", "newer", "version", "of", "unifrac", "file", "obtained", "from", "Qiime", "version", "1", ".", "9", "and", "later", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L359-L378", "partition": "train", "up_fun_num": 11, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def ensure_dir(d):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def color_mapping(sample_map, header, group_column, color_column=None):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/util.py", "func_name": "color_mapping", "original_string": "def color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors", "language": "python", "code": "def color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors", "code_tokens": ["def", "color_mapping", "(", "sample_map", ",", "header", ",", "group_column", ",", "color_column", "=", "None", ")", ":", "group_colors", "=", "OrderedDict", "(", ")", "group_gather", "=", "gather_categories", "(", "sample_map", ",", "header", ",", "[", "group_column", "]", ")", "if", "color_column", "is", "not", "None", ":", "color_gather", "=", "gather_categories", "(", "sample_map", ",", "header", ",", "[", "color_column", "]", ")", "# match sample IDs between color_gather and group_gather", "for", "group", "in", "group_gather", ":", "for", "color", "in", "color_gather", ":", "# allow incomplete assignment of colors, if group sids overlap at", "# all with the color sids, consider it a match", "if", "group_gather", "[", "group", "]", ".", "sids", ".", "intersection", "(", "color_gather", "[", "color", "]", ".", "sids", ")", ":", "group_colors", "[", "group", "]", "=", "color", "else", ":", "bcolors", "=", "itertools", ".", "cycle", "(", "Set3_12", ".", "hex_colors", ")", "for", "group", "in", "group_gather", ":", "group_colors", "[", "group", "]", "=", "bcolors", ".", "next", "(", ")", "return", "group_colors"], "docstring": "Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}", "docstring_tokens": ["Determine", "color", "-", "category", "mapping", ".", "If", "color_column", "was", "specified", "then", "map", "the", "category", "names", "to", "color", "values", ".", "Otherwise", "use", "the", "palettable", "colors", "to", "automatically", "generate", "a", "set", "of", "colors", "for", "the", "group", "values", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L380-L419", "partition": "train", "up_fun_num": 12, "context": "\"\"\"\n:Date: Created on Feb 2, 2013\n:Author: Shareef Dabdoub\n\"\"\"\nimport errno\nimport itertools\nimport os\nimport sys\nfrom textwrap import dedent as twdd\nfrom collections import namedtuple, OrderedDict, defaultdict\n\ntry:\n    from palettable.colorbrewer.qualitative import Set3_12\nexcept ImportError as ie:\n    sys.exit(\"No module named palettable\")\n\n\nFASTARecord = namedtuple(\"FASTA_Record\", \"id descr data\")\n\n\ndef storeFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file by first reading the entire file into memory.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records. Expects the\n                   input to resolve to a collection that can be iterated through, such as\n                   an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    fasta = file_handle(fastaFNH).read()\n    return [\n        FASTARecord(rec[0].split()[0], rec[0].split(None, 1)[1], \"\".join(rec[1:]))\n        for rec in (x.strip().split(\"\\n\") for x in fasta.split(\">\")[1:])\n    ]\n\n\ndef parseFASTA(fastaFNH):\n    \"\"\"\n    Parse the records in a FASTA-format file keeping the file open, and reading through\n    one line at a time.\n\n    :type source: path to FAST file or open file handle\n    :param source: The data source from which to parse the FASTA records.\n                   Expects the input to resolve to a collection that can be iterated\n                   through, such as an open file handle.\n\n    :rtype: tuple\n    :return: FASTA records containing entries for id, description and data.\n    \"\"\"\n    recs = []\n    seq = []\n    seqID = \"\"\n    descr = \"\"\n\n    for line in file_handle(fastaFNH):\n        line = line.strip()\n        if line[0] == \";\":\n            continue\n        if line[0] == \">\":\n            # conclude previous record\n            if seq:\n                recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n                seq = []\n            # start new record\n            line = line[1:].split(None, 1)\n            seqID, descr = line[0], line[1]\n        else:\n            seq.append(line)\n\n    # catch last seq in file\n    if seq:\n        recs.append(FASTARecord(seqID, descr, \"\".join(seq)))\n    return recs\n\n\ndef parse_map_file(mapFNH):\n    \"\"\"\n    Opens a QIIME mapping file and stores the contents in a dictionary keyed on SampleID\n    (default) or a user-supplied one. The only required fields are SampleID,\n    BarcodeSequence, LinkerPrimerSequence (in that order), and Description\n    (which must be the final field).\n\n    :type mapFNH: str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :rtype: tuple, dict\n    :return: A tuple of header line for mapping file and a map associating each line of\n             the mapping file with the appropriate sample ID (each value of the map also\n             contains the sample ID). An OrderedDict is used for mapping so the returned\n             map is guaranteed to have the same order as the input file.\n\n    Example data:\n    #SampleID BarcodeSequence LinkerPrimerSequence State   Description\n    11.V13    ACGCTCGACA      GTTTGATCCTGGCTCAG    Disease Rat_Oral\n    \"\"\"\n    m = OrderedDict()\n    map_header = None\n\n    with file_handle(mapFNH) as mapF:\n        for line in mapF:\n            if line.startswith(\"#SampleID\"):\n                map_header = line.strip().split(\"\\t\")\n            if line.startswith(\"#\") or not line:\n                continue\n            line = line.strip().split(\"\\t\")\n            m[line[0]] = line\n\n    return map_header, m\n\n\ndef write_map_file(mapFNH, items, header):\n    \"\"\"\n    Given a list of mapping items (in the form described by the parse_mapping_file method)\n    and a header line, write each row to the given input file with fields separated by tabs.\n\n    :type mapFNH: file or str\n    :param mapFNH: Either the full path to the map file or an open file handle\n\n    :type items: list\n    :param item: The list of row entries to be written to the mapping file\n\n    :type header: list or str\n    :param header: The descriptive column names that are required as the first line of\n                   the mapping file\n\n    :rtype: None\n    \"\"\"\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row) + \"\\n\")\n\n\ndef parse_taxonomy_table(idtaxFNH):\n    \"\"\"\n    Greengenes provides a file each OTU a full taxonomic designation. This\n    method parses that file into a map with (key,val) = (OTU, taxonomy).\n\n    :type idtaxFNH: file or str\n    :param idtaxFNH: Either the full path to the map file or an open file handle\n\n    :rtype: dict\n    :return: A map associating each OTU ID with the taxonomic specifier. An OrderedDict\n             is used so the returned map is guaranteed to have the same order as the input\n             file.\n    \"\"\"\n    idtax = OrderedDict()\n    with file_handle(idtaxFNH) as idtxF:\n        for line in idtxF:\n            ID, tax = line.strip().split(\"\\t\")[:2]\n            idtax[ID] = tax\n\n    return idtax\n\n\ndef split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level + \"__\"\n    result = p.split(level)\n    return result[0] + level + result[1].split(\";\")[0]\n\n\ndef ensure_dir(d):\n    \"\"\"\n    Check to make sure the supplied directory path does not exist, if so, create it. The\n    method catches OSError exceptions and returns a descriptive message instead of\n    re-raising the error.\n\n    :type d: str\n    :param d: It is the full path to a directory.\n\n    :return: Does not return anything, but creates a directory path if it doesn't exist\n             already.\n    \"\"\"\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\n                    \"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\"\n                )\n                return msg.format(d)\n            else:\n                msg = twdd(\n                    \"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\"\n                )\n                return msg.format(d, oe.strerror)\n\n\ndef file_handle(fnh, mode=\"rU\"):\n    \"\"\"\n    Takes either a file path or an open file handle, checks validity and returns an open\n    file handle or raises an appropriate Exception.\n\n    :type fnh: str\n    :param fnh: It is the full path to a file, or open file handle\n\n    :type mode: str\n    :param mode: The way in which this file will be used, for example to read or write or\n                 both. By default, file will be opened in rU mode.\n\n    :return: Returns an opened file for appropriate usage.\n    \"\"\"\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle\n\n\n# Meant to contain all the data necessary for calculating a single column of\n# an iTol data table\nDataCategory = namedtuple(\"DataCategory\", \"sids results\")\n\n\ndef gather_categories(imap, header, categories=None):\n    \"\"\"\n    Find the user specified categories in the map and create a dictionary to contain the\n    relevant data for each type within the categories. Multiple categories will have their\n    types combined such that each possible combination will have its own entry in the\n    dictionary.\n\n    :type imap: dict\n    :param imap: The input mapping file data keyed by SampleID\n    :type header: list\n    :param header: The header line from the input mapping file. This will be searched for\n                   the user-specified categories\n    :type categories: list\n    :param categories: The list of user-specified category column name from mapping file\n    :rtype: dict\n    :return: A sorted dictionary keyed on the combinations of all the types found within\n             the user-specified categories. Each entry will contain an empty DataCategory\n             namedtuple. If no categories are specified, a single entry with the key\n             'default' will be returned\n    \"\"\"\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [\n        header.index(cat) for cat in categories if cat in header and \"=\" not in cat\n    ]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    # If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table\n\n\ndef parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\n            \"File format not supported/recognized. Please check input \" \"unifrac file.\"\n        )\n\n\ndef parse_unifrac_v1_8(unifrac, file_data):\n    \"\"\"\n    Function to parse data from older version of unifrac file obtained from Qiime version\n    1.8 and earlier.\n\n    :type unifrac: dict\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac\n\n\ndef parse_unifrac_v1_9(unifrac, file_data):\n    \"\"\"\n    Function to parse data from newer version of unifrac file obtained from Qiime version\n    1.9 and later.\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :type file_data: list\n    :param file_data: Unifrac data lines after stripping whitespace characters.\n    \"\"\"\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry) * 100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import errno", "import itertools", "import os", "import sys", "from textwrap import dedent as twdd", "from collections import namedtuple, OrderedDict, defaultdict", "from palettable.colorbrewer.qualitative import Set3_12"], "function": ["def storeFASTA(fastaFNH):\n", "def parseFASTA(fastaFNH):\n", "def parse_map_file(mapFNH):\n", "def write_map_file(mapFNH, items, header):\n", "def parse_taxonomy_table(idtaxFNH):\n", "def split_phylogeny(p, level=\"s\"):\n", "def ensure_dir(d):\n", "def file_handle(fnh, mode=\"rU\"):\n", "def gather_categories(imap, header, categories=None):\n", "def parse_unifrac(unifracFN):\n", "def parse_unifrac_v1_8(unifrac, file_data):\n", "def parse_unifrac_v1_9(unifrac, file_data):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/shuffle_genome.py", "func_name": "rev_c", "original_string": "def rev_c(read):\n    \"\"\"\n    return reverse completment of read\n    \"\"\"\n    rc = []\n    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}\n    for base in read:\n        rc.extend(rc_nucs[base.upper()])\n    return rc[::-1]", "language": "python", "code": "def rev_c(read):\n    \"\"\"\n    return reverse completment of read\n    \"\"\"\n    rc = []\n    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}\n    for base in read:\n        rc.extend(rc_nucs[base.upper()])\n    return rc[::-1]", "code_tokens": ["def", "rev_c", "(", "read", ")", ":", "rc", "=", "[", "]", "rc_nucs", "=", "{", "'A'", ":", "'T'", ",", "'T'", ":", "'A'", ",", "'G'", ":", "'C'", ",", "'C'", ":", "'G'", ",", "'N'", ":", "'N'", "}", "for", "base", "in", "read", ":", "rc", ".", "extend", "(", "rc_nucs", "[", "base", ".", "upper", "(", ")", "]", ")", "return", "rc", "[", ":", ":", "-", "1", "]"], "docstring": "return reverse completment of read", "docstring_tokens": ["return", "reverse", "completment", "of", "read"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/shuffle_genome.py#L27-L35", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for randomly messing up a genome\n\"\"\"\n\nimport os\nimport sys\nimport random\nimport argparse\nimport numpy as np\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef plot_dist_normal(s, mu, sigma):\n    \"\"\"\n    plot distribution\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    count, bins, ignored = plt.hist(s, 30, normed=True)\n    plt.plot(\n        bins,\n        1\n        / (sigma * np.sqrt(2 * np.pi))\n        * np.exp(-((bins - mu) ** 2) / (2 * sigma ** 2)),\n        linewidth=2,\n        color=\"r\",\n    )\n    plt.show()\n\n\ndef shuffle_genome(\n    genome,\n    cat,\n    fraction=float(100),\n    plot=True,\n    alpha=0.1,\n    beta=100000,\n    min_length=1000,\n    max_length=200000,\n):\n    \"\"\"\n    randomly shuffle genome\n    \"\"\"\n    header = \">randomized_%s\" % (genome.name)\n    sequence = list(\"\".join([i[1] for i in parse_fasta(genome)]))\n    length = len(sequence)\n    shuffled = []\n    # break genome into pieces\n    while sequence is not False:\n        s = int(random.gammavariate(alpha, beta))\n        if s <= min_length or s >= max_length:\n            continue\n        if len(sequence) < s:\n            seq = sequence[0:]\n        else:\n            seq = sequence[0:s]\n        sequence = sequence[s:]\n        #        if bool(random.getrandbits(1)) is True:\n        #            seq = rev_c(seq)\n        #            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)\n        #        else:\n        #            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)\n        shuffled.append(\"\".join(seq))\n        if sequence == []:\n            break\n    # shuffle pieces\n    random.shuffle(shuffled)\n    # subset fragments\n    if fraction == float(100):\n        subset = shuffled\n    else:\n        max_pieces = int(length * fraction / 100)\n        subset, total = [], 0\n        for fragment in shuffled:\n            length = len(fragment)\n            if total + length <= max_pieces:\n                subset.append(fragment)\n                total += length\n            else:\n                diff = max_pieces - total\n                subset.append(fragment[0:diff])\n                break\n    # combine sequences, if requested\n    if cat is True:\n        yield [header, \"\".join(subset)]\n    else:\n        for i, seq in enumerate(subset):\n            yield [\"%s fragment:%s\" % (header, i), seq]\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# randomly re-arrange genome\")\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fasta(s)\")\n    parser.add_argument(\n        \"-p\",\n        type=float,\n        default=100,\n        help=\"percent of genome to return (default = 100)\",\n    )\n    parser.add_argument(\n        \"--cat\", action=\"store_true\", help=\"concatenate random fragments\"\n    )\n    args = vars(parser.parse_args())\n    for genome in args[\"f\"]:\n        if genome == \"-\":\n            genome = sys.stdin\n        else:\n            genome = open(genome)\n        for seq in shuffle_genome(genome, args[\"cat\"], fraction=args[\"p\"]):\n            print(\"\\n\".join(seq))\n", "levels": [0], "package": ["import os", "import sys", "import random", "import argparse", "import numpy as np", "from ctbBio.fasta import iterate_fasta as parse_fasta", "import matplotlib.pyplot as plt"], "function": ["def plot_dist_normal(s, mu, sigma):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/shuffle_genome.py", "func_name": "shuffle_genome", "original_string": "def shuffle_genome(genome, cat, fraction = float(100), plot = True, \\\n        alpha = 0.1, beta = 100000, \\\n        min_length = 1000, max_length = 200000):\n    \"\"\"\n    randomly shuffle genome\n    \"\"\"\n    header = '>randomized_%s' % (genome.name)\n    sequence = list(''.join([i[1] for i in parse_fasta(genome)]))\n    length = len(sequence)\n    shuffled = []\n    # break genome into pieces\n    while sequence is not False:\n        s = int(random.gammavariate(alpha, beta))\n        if s <= min_length or s >= max_length:\n            continue\n        if len(sequence) < s:\n            seq = sequence[0:]\n        else:\n            seq = sequence[0:s]\n        sequence = sequence[s:]\n#        if bool(random.getrandbits(1)) is True:\n#            seq = rev_c(seq)\n#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)\n#        else:\n#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)\n        shuffled.append(''.join(seq))\n        if sequence == []:\n            break\n    # shuffle pieces\n    random.shuffle(shuffled)\n    # subset fragments\n    if fraction == float(100):\n        subset = shuffled\n    else:\n        max_pieces = int(length * fraction/100)\n        subset, total = [], 0\n        for fragment in shuffled:\n            length = len(fragment)\n            if total + length <= max_pieces:\n                subset.append(fragment)\n                total += length\n            else:\n                diff = max_pieces - total\n                subset.append(fragment[0:diff])\n                break\n    # combine sequences, if requested\n    if cat is True:\n        yield [header, ''.join(subset)]\n    else:\n        for i, seq in enumerate(subset):\n            yield ['%s fragment:%s' % (header, i), seq]", "language": "python", "code": "def shuffle_genome(genome, cat, fraction = float(100), plot = True, \\\n        alpha = 0.1, beta = 100000, \\\n        min_length = 1000, max_length = 200000):\n    \"\"\"\n    randomly shuffle genome\n    \"\"\"\n    header = '>randomized_%s' % (genome.name)\n    sequence = list(''.join([i[1] for i in parse_fasta(genome)]))\n    length = len(sequence)\n    shuffled = []\n    # break genome into pieces\n    while sequence is not False:\n        s = int(random.gammavariate(alpha, beta))\n        if s <= min_length or s >= max_length:\n            continue\n        if len(sequence) < s:\n            seq = sequence[0:]\n        else:\n            seq = sequence[0:s]\n        sequence = sequence[s:]\n#        if bool(random.getrandbits(1)) is True:\n#            seq = rev_c(seq)\n#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)\n#        else:\n#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)\n        shuffled.append(''.join(seq))\n        if sequence == []:\n            break\n    # shuffle pieces\n    random.shuffle(shuffled)\n    # subset fragments\n    if fraction == float(100):\n        subset = shuffled\n    else:\n        max_pieces = int(length * fraction/100)\n        subset, total = [], 0\n        for fragment in shuffled:\n            length = len(fragment)\n            if total + length <= max_pieces:\n                subset.append(fragment)\n                total += length\n            else:\n                diff = max_pieces - total\n                subset.append(fragment[0:diff])\n                break\n    # combine sequences, if requested\n    if cat is True:\n        yield [header, ''.join(subset)]\n    else:\n        for i, seq in enumerate(subset):\n            yield ['%s fragment:%s' % (header, i), seq]", "code_tokens": ["def", "shuffle_genome", "(", "genome", ",", "cat", ",", "fraction", "=", "float", "(", "100", ")", ",", "plot", "=", "True", ",", "alpha", "=", "0.1", ",", "beta", "=", "100000", ",", "min_length", "=", "1000", ",", "max_length", "=", "200000", ")", ":", "header", "=", "'>randomized_%s'", "%", "(", "genome", ".", "name", ")", "sequence", "=", "list", "(", "''", ".", "join", "(", "[", "i", "[", "1", "]", "for", "i", "in", "parse_fasta", "(", "genome", ")", "]", ")", ")", "length", "=", "len", "(", "sequence", ")", "shuffled", "=", "[", "]", "# break genome into pieces", "while", "sequence", "is", "not", "False", ":", "s", "=", "int", "(", "random", ".", "gammavariate", "(", "alpha", ",", "beta", ")", ")", "if", "s", "<=", "min_length", "or", "s", ">=", "max_length", ":", "continue", "if", "len", "(", "sequence", ")", "<", "s", ":", "seq", "=", "sequence", "[", "0", ":", "]", "else", ":", "seq", "=", "sequence", "[", "0", ":", "s", "]", "sequence", "=", "sequence", "[", "s", ":", "]", "#        if bool(random.getrandbits(1)) is True:", "#            seq = rev_c(seq)", "#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)", "#        else:", "#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)", "shuffled", ".", "append", "(", "''", ".", "join", "(", "seq", ")", ")", "if", "sequence", "==", "[", "]", ":", "break", "# shuffle pieces", "random", ".", "shuffle", "(", "shuffled", ")", "# subset fragments", "if", "fraction", "==", "float", "(", "100", ")", ":", "subset", "=", "shuffled", "else", ":", "max_pieces", "=", "int", "(", "length", "*", "fraction", "/", "100", ")", "subset", ",", "total", "=", "[", "]", ",", "0", "for", "fragment", "in", "shuffled", ":", "length", "=", "len", "(", "fragment", ")", "if", "total", "+", "length", "<=", "max_pieces", ":", "subset", ".", "append", "(", "fragment", ")", "total", "+=", "length", "else", ":", "diff", "=", "max_pieces", "-", "total", "subset", ".", "append", "(", "fragment", "[", "0", ":", "diff", "]", ")", "break", "# combine sequences, if requested", "if", "cat", "is", "True", ":", "yield", "[", "header", ",", "''", ".", "join", "(", "subset", ")", "]", "else", ":", "for", "i", ",", "seq", "in", "enumerate", "(", "subset", ")", ":", "yield", "[", "'%s fragment:%s'", "%", "(", "header", ",", "i", ")", ",", "seq", "]"], "docstring": "randomly shuffle genome", "docstring_tokens": ["randomly", "shuffle", "genome"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/shuffle_genome.py#L37-L87", "partition": "train", "up_fun_num": 2, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for randomly messing up a genome\n\"\"\"\n\nimport os\nimport sys\nimport random\nimport argparse\nimport numpy as np\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef plot_dist_normal(s, mu, sigma):\n    \"\"\"\n    plot distribution\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    count, bins, ignored = plt.hist(s, 30, normed=True)\n    plt.plot(\n        bins,\n        1\n        / (sigma * np.sqrt(2 * np.pi))\n        * np.exp(-((bins - mu) ** 2) / (2 * sigma ** 2)),\n        linewidth=2,\n        color=\"r\",\n    )\n    plt.show()\n\n\ndef rev_c(read):\n    \"\"\"\n    return reverse completment of read\n    \"\"\"\n    rc = []\n    rc_nucs = {\"A\": \"T\", \"T\": \"A\", \"G\": \"C\", \"C\": \"G\", \"N\": \"N\"}\n    for base in read:\n        rc.extend(rc_nucs[base.upper()])\n    return rc[::-1]\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# randomly re-arrange genome\")\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fasta(s)\")\n    parser.add_argument(\n        \"-p\",\n        type=float,\n        default=100,\n        help=\"percent of genome to return (default = 100)\",\n    )\n    parser.add_argument(\n        \"--cat\", action=\"store_true\", help=\"concatenate random fragments\"\n    )\n    args = vars(parser.parse_args())\n    for genome in args[\"f\"]:\n        if genome == \"-\":\n            genome = sys.stdin\n        else:\n            genome = open(genome)\n        for seq in shuffle_genome(genome, args[\"cat\"], fraction=args[\"p\"]):\n            print(\"\\n\".join(seq))\n", "levels": [0, 0], "package": ["import os", "import sys", "import random", "import argparse", "import numpy as np", "from ctbBio.fasta import iterate_fasta as parse_fasta", "import matplotlib.pyplot as plt"], "function": ["def plot_dist_normal(s, mu, sigma):\n", "def rev_c(read):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/regression.py", "func_name": "MultiVarLinReg._prune", "original_string": "def _prune(self, fit, p_max):\n        \"\"\"\n        If the fit contains statistically insignificant parameters, remove them.\n        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max\n\n        Parameters\n        ----------\n        fit: fm.ols fit object\n            Can contain insignificant parameters\n        p_max : float\n            Maximum allowed probability of the t-statistic\n\n        Returns\n        -------\n        fit: fm.ols fit object\n            Won't contain any insignificant parameters\n\n        \"\"\"\n\n        def remove_from_model_desc(x, model_desc):\n            \"\"\"\n            Return a model_desc without x\n            \"\"\"\n\n            rhs_termlist = []\n            for t in model_desc.rhs_termlist:\n                if not t.factors:\n                    # intercept, add anyway\n                    rhs_termlist.append(t)\n                elif not x == t.factors[0]._varname:\n                    # this is not the term with x\n                    rhs_termlist.append(t)\n\n            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)\n            return md\n\n        corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])\n        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n        try:\n            pars_to_prune.remove('Intercept')\n        except:\n            pass\n        while pars_to_prune:\n            corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)\n            fit = fm.ols(corrected_model_desc, data=self.df).fit()\n            pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n            try:\n                pars_to_prune.remove('Intercept')\n            except:\n                pass\n        return fit", "language": "python", "code": "def _prune(self, fit, p_max):\n        \"\"\"\n        If the fit contains statistically insignificant parameters, remove them.\n        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max\n\n        Parameters\n        ----------\n        fit: fm.ols fit object\n            Can contain insignificant parameters\n        p_max : float\n            Maximum allowed probability of the t-statistic\n\n        Returns\n        -------\n        fit: fm.ols fit object\n            Won't contain any insignificant parameters\n\n        \"\"\"\n\n        def remove_from_model_desc(x, model_desc):\n            \"\"\"\n            Return a model_desc without x\n            \"\"\"\n\n            rhs_termlist = []\n            for t in model_desc.rhs_termlist:\n                if not t.factors:\n                    # intercept, add anyway\n                    rhs_termlist.append(t)\n                elif not x == t.factors[0]._varname:\n                    # this is not the term with x\n                    rhs_termlist.append(t)\n\n            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)\n            return md\n\n        corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])\n        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n        try:\n            pars_to_prune.remove('Intercept')\n        except:\n            pass\n        while pars_to_prune:\n            corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)\n            fit = fm.ols(corrected_model_desc, data=self.df).fit()\n            pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n            try:\n                pars_to_prune.remove('Intercept')\n            except:\n                pass\n        return fit", "code_tokens": ["def", "_prune", "(", "self", ",", "fit", ",", "p_max", ")", ":", "def", "remove_from_model_desc", "(", "x", ",", "model_desc", ")", ":", "\"\"\"\n            Return a model_desc without x\n            \"\"\"", "rhs_termlist", "=", "[", "]", "for", "t", "in", "model_desc", ".", "rhs_termlist", ":", "if", "not", "t", ".", "factors", ":", "# intercept, add anyway", "rhs_termlist", ".", "append", "(", "t", ")", "elif", "not", "x", "==", "t", ".", "factors", "[", "0", "]", ".", "_varname", ":", "# this is not the term with x", "rhs_termlist", ".", "append", "(", "t", ")", "md", "=", "ModelDesc", "(", "model_desc", ".", "lhs_termlist", ",", "rhs_termlist", ")", "return", "md", "corrected_model_desc", "=", "ModelDesc", "(", "fit", ".", "model", ".", "formula", ".", "lhs_termlist", "[", ":", "]", ",", "fit", ".", "model", ".", "formula", ".", "rhs_termlist", "[", ":", "]", ")", "pars_to_prune", "=", "fit", ".", "pvalues", ".", "where", "(", "fit", ".", "pvalues", ">", "p_max", ")", ".", "dropna", "(", ")", ".", "index", ".", "tolist", "(", ")", "try", ":", "pars_to_prune", ".", "remove", "(", "'Intercept'", ")", "except", ":", "pass", "while", "pars_to_prune", ":", "corrected_model_desc", "=", "remove_from_model_desc", "(", "pars_to_prune", "[", "0", "]", ",", "corrected_model_desc", ")", "fit", "=", "fm", ".", "ols", "(", "corrected_model_desc", ",", "data", "=", "self", ".", "df", ")", ".", "fit", "(", ")", "pars_to_prune", "=", "fit", ".", "pvalues", ".", "where", "(", "fit", ".", "pvalues", ">", "p_max", ")", ".", "dropna", "(", ")", ".", "index", ".", "tolist", "(", ")", "try", ":", "pars_to_prune", ".", "remove", "(", "'Intercept'", ")", "except", ":", "pass", "return", "fit"], "docstring": "If the fit contains statistically insignificant parameters, remove them.\n        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max\n\n        Parameters\n        ----------\n        fit: fm.ols fit object\n            Can contain insignificant parameters\n        p_max : float\n            Maximum allowed probability of the t-statistic\n\n        Returns\n        -------\n        fit: fm.ols fit object\n            Won't contain any insignificant parameters", "docstring_tokens": ["If", "the", "fit", "contains", "statistically", "insignificant", "parameters", "remove", "them", ".", "Returns", "a", "pruned", "fit", "where", "all", "parameters", "have", "p", "-", "values", "of", "the", "t", "-", "statistic", "below", "p_max"], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/regression.py#L222-L272", "partition": "train", "up_fun_num": 6, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\nimport datetime as dt\nimport pandas as pd\n\nfrom .plotting import plot_style\n\nplt = plot_style()\nimport numpy as np\nimport statsmodels.formula.api as fm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom patsy import ModelDesc, Term, LookupFactor\nfrom copy import deepcopy\nimport re\n\nfrom .analysis import Analysis\n\n\nclass MultiVarLinReg(Analysis):\n    \"\"\"\n    Multi-variable linear regression based on statsmodels and Ordinary Least Squares (ols)\n\n    Pass a dataframe with the variable to be modelled y (dependent variable) and the possible independent variables x.\n    Specify as string the name of the dependent variable, and optionally pass a list with names of\n    independent variables to try (by default all other columns will be tried as independent variables).\n\n    The analysis is based on a forward-selection approach: starting from a simple model, the model is iteratively\n    refined and verified until no statistical relevant improvements can be obtained.  Each model in the iteration loop\n    is stored in the attribute self.list_of_fits.  The selected model is self.fit (=pointer to the last element of\n    self.list_of_fits).\n\n    The dataframe can contain daily, weekly, monthly, yearly ... values.  Each row is an instance.\n\n\n    Examples\n    --------\n\n    >> mvlr = MultiVarLinReg(df, 'gas', p_max=0.04)\n    >> mvlr = MultiVarLinReg(df, 'gas', list_of_x=['heatingDegreeDays14', 'GlobalHorizontalIrradiance', 'WindSpeed'])\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        df,\n        y,\n        p_max=0.05,\n        list_of_x=None,\n        confint=0.95,\n        cross_validation=False,\n        allow_negative_predictions=False,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Datetimeindex and both independent variables (x) and dependent variable (y) as columns\n        y : str\n            Name of the dependent (endogeneous) variable to model\n        p_max : float (default=0.05)\n            Acceptable p-value of the t-statistic for estimated parameters\n        list_of_x : list of str (default=None)\n            If None (default), try to build a model with all columns in the dataframe\n            If a list with column names is given, only try these columns as independent variables\n        confint : float, default=0.95\n            Two-sided confidence interval for predictions.\n        cross_validation : bool, default=False\n            If True, compute the model based on cross-validation (leave one out)\n            Only possible if the df has less than 15 entries.\n            Note : this will take much longer computation times!\n        allow_negative_predictions : bool, default=False\n            If True, allow predictions to be negative.\n            For gas consumption or PV production, this is not physical so allow_negative_predictions should be False\n        \"\"\"\n        self.df = df.copy()  # type: pd.DataFrame\n        assert (\n            y in self.df.columns\n        ), \"The dependent variable {} is not a column in the dataframe\".format(y)\n        self.y = y\n\n        self.p_max = p_max\n        self.list_of_x = list_of_x or self.df.columns.tolist()\n        self.confint = confint\n        self.cross_validation = cross_validation\n        self.allow_negative_predictions = allow_negative_predictions\n        try:\n            self.list_of_x.remove(self.y)\n        except ValueError:\n            pass\n        self._fit = None\n        self._list_of_fits = []\n\n    @property\n    def fit(self):\n        if self._fit is None:\n            raise UnboundLocalError(\n                'Run \"do_analysis()\" first to fit a model to the data.'\n            )\n        else:\n            return self._fit\n\n    @property\n    def list_of_fits(self):\n        if not self._list_of_fits:\n            raise UnboundLocalError(\n                'Run \"do_analysis()\" first to fit a model to the data.'\n            )\n        else:\n            return self._list_of_fits\n\n    def do_analysis(self):\n        \"\"\"\n        Find the best model (fit) and create self.list_of_fits and self.fit\n\n        \"\"\"\n        if self.cross_validation:\n            return self._do_analysis_cross_validation()\n        else:\n            return self._do_analysis_no_cross_validation()\n\n    def _do_analysis_no_cross_validation(self):\n        \"\"\"\n        Find the best model (fit) and create self.list_of_fits and self.fit\n        \"\"\"\n\n        # first model is just the mean\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        # ...then add another term for each candidate\n        # model_terms += [Term([LookupFactor(c)]) for c in candidates]\n        model_desc = ModelDesc(response_term, model_terms)\n        self._list_of_fits.append(fm.ols(model_desc, data=self.df).fit())\n        # try to improve the model until no improvements can be found\n\n        while all_model_terms_dict:\n            # try each x and overwrite the best_fit if we find a better one\n            # the first best_fit is the one from the previous round\n            ref_fit = self._list_of_fits[-1]\n            best_fit = self._list_of_fits[-1]\n            best_bic = best_fit.bic\n            for x, term in all_model_terms_dict.items():\n                # make new_fit, compare with best found so far\n                model_desc = ModelDesc(\n                    response_term, ref_fit.model.formula.rhs_termlist + [term]\n                )\n                fit = fm.ols(model_desc, data=self.df).fit()\n                if fit.bic < best_bic:\n                    best_bic = fit.bic\n                    best_fit = fit\n                    best_x = x\n            # Sometimes, the obtained fit may be better, but contains unsignificant parameters.\n            # Correct the fit by removing the unsignificant parameters and estimate again\n            best_fit = self._prune(best_fit, p_max=self.p_max)\n\n            # if best_fit does not contain more variables than ref fit, exit\n            if len(best_fit.model.formula.rhs_termlist) == len(\n                ref_fit.model.formula.rhs_termlist\n            ):\n                break\n            else:\n                self._list_of_fits.append(best_fit)\n                all_model_terms_dict.pop(best_x)\n        self._fit = self._list_of_fits[-1]\n\n    def _do_analysis_cross_validation(self):\n        \"\"\"\n        Find the best model (fit) based on cross-valiation (leave one out)\n\n        \"\"\"\n        assert (\n            len(self.df) < 15\n        ), \"Cross-validation is not implemented if your sample contains more than 15 datapoints\"\n\n        # initialization: first model is the mean, but compute cv correctly.\n        errors = []\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        model_desc = ModelDesc(response_term, model_terms)\n        for i in self.df.index:\n            # make new_fit, compute cross-validation and store error\n            df_ = self.df.drop(i, axis=0)\n            fit = fm.ols(model_desc, data=df_).fit()\n            cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n            errors.append(cross_prediction[\"predicted\"] - cross_prediction[self.y])\n\n        self._list_of_fits = [fm.ols(model_desc, data=self.df).fit()]\n        self.list_of_cverrors = [np.mean(np.abs(np.array(errors)))]\n\n        # try to improve the model until no improvements can be found\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        while all_model_terms_dict:\n            # import pdb;pdb.set_trace()\n            # try each x in all_exog and overwrite if we find a better one\n            # at the end of iteration (and not earlier), save the best of the iteration\n            better_model_found = False\n            best = dict(fit=self._list_of_fits[-1], cverror=self.list_of_cverrors[-1])\n            for x, term in all_model_terms_dict.items():\n                model_desc = ModelDesc(\n                    response_term,\n                    self._list_of_fits[-1].model.formula.rhs_termlist + [term],\n                )\n                # cross_validation, currently only implemented for monthly data\n                # compute the mean error for a given formula based on leave-one-out.\n                errors = []\n                for i in self.df.index:\n                    # make new_fit, compute cross-validation and store error\n                    df_ = self.df.drop(i, axis=0)\n                    fit = fm.ols(model_desc, data=df_).fit()\n                    cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n                    errors.append(\n                        cross_prediction[\"predicted\"] - cross_prediction[self.y]\n                    )\n                cverror = np.mean(np.abs(np.array(errors)))\n                # compare the model with the current fit\n                if cverror < best[\"cverror\"]:\n                    # better model, keep it\n                    # first, reidentify using all the datapoints\n                    best[\"fit\"] = fm.ols(model_desc, data=self.df).fit()\n                    best[\"cverror\"] = cverror\n                    better_model_found = True\n                    best_x = x\n\n            if better_model_found:\n                self._list_of_fits.append(best[\"fit\"])\n                self.list_of_cverrors.append(best[\"cverror\"])\n\n            else:\n                # if we did not find a better model, exit\n                break\n\n            # next iteration with the found exog removed\n            all_model_terms_dict.pop(best_x)\n\n        self._fit = self._list_of_fits[-1]\n\n    @staticmethod\n    def find_best_rsquared(list_of_fits):\n        \"\"\"Return the best fit, based on rsquared\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.rsquared)\n        return res[-1]\n\n    @staticmethod\n    def find_best_akaike(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.aic)\n        return res[0]\n\n    @staticmethod\n    def find_best_bic(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.bic)\n        return res[0]\n\n    def _predict(self, fit, df):\n        \"\"\"\n        Return a df with predictions and confidence interval\n\n        Notes\n        -----\n        The df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        The result will depend on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Parameters\n        ----------\n        fit : Statsmodels fit\n        df : pandas DataFrame or None (default)\n            If None, use self.df\n\n\n        Returns\n        -------\n        df_res : pandas DataFrame\n            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'\n        \"\"\"\n\n        # Add model results to data as column 'predictions'\n        df_res = df.copy()\n        if \"Intercept\" in fit.model.exog_names:\n            df_res[\"Intercept\"] = 1.0\n        df_res[\"predicted\"] = fit.predict(df_res)\n        if not self.allow_negative_predictions:\n            df_res.loc[df_res[\"predicted\"] < 0, \"predicted\"] = 0\n\n        prstd, interval_l, interval_u = wls_prediction_std(\n            fit, df_res[fit.model.exog_names], alpha=1 - self.confint\n        )\n        df_res[\"interval_l\"] = interval_l\n        df_res[\"interval_u\"] = interval_u\n\n        if \"Intercept\" in df_res:\n            df_res.drop(labels=[\"Intercept\"], axis=1, inplace=True)\n\n        return df_res\n\n    def add_prediction(self):\n        \"\"\"\n        Add predictions and confidence interval to self.df\n        self.df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        Parameters\n        ----------\n        None, but the result depends on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Returns\n        -------\n        Nothing, adds columns to self.df\n        \"\"\"\n        self.df = self._predict(fit=self.fit, df=self.df)\n\n    def plot(self, model=True, bar_chart=True, **kwargs):\n        \"\"\"\n        Plot measurements and predictions.\n\n        By default, use self._fit and self.df, but both can be overruled by the arguments df and fit\n        This function will detect if the data has been used for the modelling or not and will\n        visualize them differently.\n\n        Parameters\n        ----------\n        model : boolean, default=True\n            If True, show the modified energy signature\n        bar_chart : boolean, default=True\n            If True, make a bar chart with predicted and measured data\n\n        Other Parameters\n        ----------------\n        df : pandas Dataframe, default=None\n            The data to be plotted.  If None, use self.df\n            If the dataframe does not have a column 'predicted', a prediction will be made\n        fit : statsmodels fit, default=None\n            The model to be used.  if None, use self._fit\n\n        Returns\n        -------\n        figures : List of plt.figure objects.\n\n        \"\"\"\n        plot_style()\n        figures = []\n        fit = kwargs.get(\"fit\", self.fit)\n        df = kwargs.get(\"df\", self.df)\n\n        if not \"predicted\" in df.columns:\n            df = self._predict(fit=fit, df=df)\n        # split the df in the auto-validation and prognosis part\n        df_auto = df.loc[self.df.index[0] : self.df.index[-1]]\n        if df_auto.empty:\n            df_prog = df\n        else:\n            df_prog = df.loc[df_auto.index[-1] :].iloc[1:]\n\n        if model:\n            # The first variable in the formula is the most significant.  Use it as abcis for the plot\n            try:\n                exog1 = fit.model.exog_names[1]\n            except IndexError:\n                exog1 = self.list_of_x[0]\n\n            # plot model as an adjusted trendline\n            # get sorted model values\n            dfmodel = df[[exog1, \"predicted\", \"interval_u\", \"interval_l\"]]\n            dfmodel.index = dfmodel[exog1]\n            dfmodel = dfmodel.sort_index()\n            plt.plot(dfmodel.index, dfmodel[\"predicted\"], \"--\", color=\"royalblue\")\n            plt.plot(dfmodel.index, dfmodel[\"interval_l\"], \":\", color=\"royalblue\")\n            plt.plot(dfmodel.index, dfmodel[\"interval_u\"], \":\", color=\"royalblue\")\n            # plot dots for the measurements\n            if len(df_auto) > 0:\n                plt.plot(\n                    df_auto[exog1],\n                    df_auto[self.y],\n                    \"o\",\n                    mfc=\"orangered\",\n                    mec=\"orangered\",\n                    ms=8,\n                    label=\"Data used for model fitting\",\n                )\n            if len(df_prog) > 0:\n                plt.plot(\n                    df_prog[exog1],\n                    df_prog[self.y],\n                    \"o\",\n                    mfc=\"seagreen\",\n                    mec=\"seagreen\",\n                    ms=8,\n                    label=\"Data not used for model fitting\",\n                )\n            plt.title(\"rsquared={:.2f} - BIC={:.1f}\".format(fit.rsquared, fit.bic))\n            plt.xlabel(exog1)\n            figures.append(plt.gcf())\n\n        if bar_chart:\n            ind = np.arange(len(df.index))  # the x locations for the groups\n            width = 0.35  # the width of the bars\n\n            fig, ax = plt.subplots()\n            title = \"Measured\"  # will be appended based on the available data\n            if len(df_auto) > 0:\n                model = ax.bar(\n                    ind[: len(df_auto)],\n                    df_auto[\"predicted\"],\n                    width * 2,\n                    color=\"#FDD787\",\n                    ecolor=\"#FDD787\",\n                    yerr=df_auto[\"interval_u\"] - df_auto[\"predicted\"],\n                    label=self.y + \" modelled\",\n                )\n                title = title + \", modelled\"\n            if len(df_prog) > 0:\n                prog = ax.bar(\n                    ind[len(df_auto) :],\n                    df_prog[\"predicted\"],\n                    width * 2,\n                    color=\"#6CD5A1\",\n                    ecolor=\"#6CD5A1\",\n                    yerr=df_prog[\"interval_u\"] - df_prog[\"predicted\"],\n                    label=self.y + \" expected\",\n                )\n                title = title + \" and predicted\"\n\n            meas = ax.bar(\n                ind, df[self.y], width, label=self.y + \" measured\", color=\"#D5756C\"\n            )\n            # add some text for labels, title and axes ticks\n            ax.set_title(\"{} {}\".format(title, self.y))\n            ax.set_xticks(ind)\n            ax.set_xticklabels(\n                [x.strftime(\"%d-%m-%Y\") for x in df.index], rotation=\"vertical\"\n            )\n            ax.yaxis.grid(True)\n            ax.xaxis.grid(False)\n\n            plt.legend(ncol=3, loc=\"upper center\")\n            figures.append(plt.gcf())\n\n        plt.show()\n\n        return figures\n\n    def _modeldesc_to_dict(self, md):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        d = {\"lhs_termlist\": [md.lhs_termlist[0].factors[0].name()]}\n        rhs_termlist = []\n\n        # add other terms, if any\n        for term in md.rhs_termlist[:]:\n            if len(term.factors) == 0:\n                # intercept, represent by empty string\n                rhs_termlist.append(\"\")\n            else:\n                rhs_termlist.append(term.factors[0].name())\n\n        d[\"rhs_termlist\"] = rhs_termlist\n        return d\n\n    def _modeldesc_from_dict(self, d):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        lhs_termlist = [Term([LookupFactor(d[\"lhs_termlist\"][0])])]\n        rhs_termlist = []\n        for name in d[\"rhs_termlist\"]:\n            if name == \"\":\n                rhs_termlist.append(Term([]))\n            else:\n                rhs_termlist.append(Term([LookupFactor(name)]))\n\n        md = ModelDesc(lhs_termlist, rhs_termlist)\n        return md\n\n    def __getstate__(self):\n        \"\"\"\n        Remove attributes that cannot be pickled and store as dict.\n\n        Each fit has a model.formula which is a patsy ModelDesc and this cannot be pickled.\n        We use our knowledge of this ModelDesc (as we build it up manually in the do_analysis() method)\n        and decompose it into a dictionary.  This dictionary is stored in the list 'formulas',\n        one dict per fit.\n\n        Finally we have to remove each fit entirely (not just the formula), it is built-up again\n        from self.formulas in the __setstate__ method.\n        \"\"\"\n        d = self.__dict__\n        d[\"formulas\"] = []\n        for fit in self._list_of_fits:\n            d[\"formulas\"].append(self._modeldesc_to_dict(fit.model.formula))\n            # delattr(fit.model, 'formula')\n        d.pop(\"_list_of_fits\")\n        d.pop(\"_fit\")\n\n        print(\n            \"Pickling...  Removing the 'formula' from each fit.model.\\n\\\n             You have to unpickle your object or run __setstate__(self.__dict__) to restore them.\".format(\n                d\n            )\n        )\n        return d\n\n    def __setstate__(self, state):\n        \"\"\"Restore the attributes that cannot be pickled\"\"\"\n        for k, v in state.items():\n            if k is not \"formulas\":\n                setattr(self, k, v)\n        self._list_of_fits = []\n        for formula in state[\"formulas\"]:\n            self._list_of_fits.append(\n                fm.ols(self._modeldesc_from_dict(formula), data=self.df).fit()\n            )\n        self._fit = self._list_of_fits[-1]\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import datetime as dt", "import pandas as pd", "from .plotting import plot_style", "import numpy as np", "import statsmodels.formula.api as fm", "from statsmodels.sandbox.regression.predstd import wls_prediction_std", "from patsy import ModelDesc, Term, LookupFactor", "from copy import deepcopy", "import re", "from .analysis import Analysis"], "function": ["class MultiVarLinReg(Analysis):\n", "    def fit(self):\n", "    def list_of_fits(self):\n", "    def do_analysis(self):\n", "    def _do_analysis_no_cross_validation(self):\n", "    def _do_analysis_cross_validation(self):\n", "    def find_best_rsquared(list_of_fits):\n", "    def find_best_akaike(list_of_fits):\n", "    def find_best_bic(list_of_fits):\n", "    def _predict(self, fit, df):\n", "    def add_prediction(self):\n", "    def plot(self, model=True, bar_chart=True, **kwargs):\n", "    def _modeldesc_to_dict(self, md):\n", "    def _modeldesc_from_dict(self, d):\n", "    def __getstate__(self):\n", "    def __setstate__(self, state):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/regression.py", "func_name": "MultiVarLinReg.find_best_rsquared", "original_string": "def find_best_rsquared(list_of_fits):\n        \"\"\"Return the best fit, based on rsquared\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.rsquared)\n        return res[-1]", "language": "python", "code": "def find_best_rsquared(list_of_fits):\n        \"\"\"Return the best fit, based on rsquared\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.rsquared)\n        return res[-1]", "code_tokens": ["def", "find_best_rsquared", "(", "list_of_fits", ")", ":", "res", "=", "sorted", "(", "list_of_fits", ",", "key", "=", "lambda", "x", ":", "x", ".", "rsquared", ")", "return", "res", "[", "-", "1", "]"], "docstring": "Return the best fit, based on rsquared", "docstring_tokens": ["Return", "the", "best", "fit", "based", "on", "rsquared"], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/regression.py#L275-L278", "partition": "train", "up_fun_num": 8, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\nimport datetime as dt\nimport pandas as pd\n\nfrom .plotting import plot_style\n\nplt = plot_style()\nimport numpy as np\nimport statsmodels.formula.api as fm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom patsy import ModelDesc, Term, LookupFactor\nfrom copy import deepcopy\nimport re\n\nfrom .analysis import Analysis\n\n\nclass MultiVarLinReg(Analysis):\n    \"\"\"\n    Multi-variable linear regression based on statsmodels and Ordinary Least Squares (ols)\n\n    Pass a dataframe with the variable to be modelled y (dependent variable) and the possible independent variables x.\n    Specify as string the name of the dependent variable, and optionally pass a list with names of\n    independent variables to try (by default all other columns will be tried as independent variables).\n\n    The analysis is based on a forward-selection approach: starting from a simple model, the model is iteratively\n    refined and verified until no statistical relevant improvements can be obtained.  Each model in the iteration loop\n    is stored in the attribute self.list_of_fits.  The selected model is self.fit (=pointer to the last element of\n    self.list_of_fits).\n\n    The dataframe can contain daily, weekly, monthly, yearly ... values.  Each row is an instance.\n\n\n    Examples\n    --------\n\n    >> mvlr = MultiVarLinReg(df, 'gas', p_max=0.04)\n    >> mvlr = MultiVarLinReg(df, 'gas', list_of_x=['heatingDegreeDays14', 'GlobalHorizontalIrradiance', 'WindSpeed'])\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        df,\n        y,\n        p_max=0.05,\n        list_of_x=None,\n        confint=0.95,\n        cross_validation=False,\n        allow_negative_predictions=False,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Datetimeindex and both independent variables (x) and dependent variable (y) as columns\n        y : str\n            Name of the dependent (endogeneous) variable to model\n        p_max : float (default=0.05)\n            Acceptable p-value of the t-statistic for estimated parameters\n        list_of_x : list of str (default=None)\n            If None (default), try to build a model with all columns in the dataframe\n            If a list with column names is given, only try these columns as independent variables\n        confint : float, default=0.95\n            Two-sided confidence interval for predictions.\n        cross_validation : bool, default=False\n            If True, compute the model based on cross-validation (leave one out)\n            Only possible if the df has less than 15 entries.\n            Note : this will take much longer computation times!\n        allow_negative_predictions : bool, default=False\n            If True, allow predictions to be negative.\n            For gas consumption or PV production, this is not physical so allow_negative_predictions should be False\n        \"\"\"\n        self.df = df.copy()  # type: pd.DataFrame\n        assert (\n            y in self.df.columns\n        ), \"The dependent variable {} is not a column in the dataframe\".format(y)\n        self.y = y\n\n        self.p_max = p_max\n        self.list_of_x = list_of_x or self.df.columns.tolist()\n        self.confint = confint\n        self.cross_validation = cross_validation\n        self.allow_negative_predictions = allow_negative_predictions\n        try:\n            self.list_of_x.remove(self.y)\n        except ValueError:\n            pass\n        self._fit = None\n        self._list_of_fits = []\n\n    @property\n    def fit(self):\n        if self._fit is None:\n            raise UnboundLocalError(\n                'Run \"do_analysis()\" first to fit a model to the data.'\n            )\n        else:\n            return self._fit\n\n    @property\n    def list_of_fits(self):\n        if not self._list_of_fits:\n            raise UnboundLocalError(\n                'Run \"do_analysis()\" first to fit a model to the data.'\n            )\n        else:\n            return self._list_of_fits\n\n    def do_analysis(self):\n        \"\"\"\n        Find the best model (fit) and create self.list_of_fits and self.fit\n\n        \"\"\"\n        if self.cross_validation:\n            return self._do_analysis_cross_validation()\n        else:\n            return self._do_analysis_no_cross_validation()\n\n    def _do_analysis_no_cross_validation(self):\n        \"\"\"\n        Find the best model (fit) and create self.list_of_fits and self.fit\n        \"\"\"\n\n        # first model is just the mean\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        # ...then add another term for each candidate\n        # model_terms += [Term([LookupFactor(c)]) for c in candidates]\n        model_desc = ModelDesc(response_term, model_terms)\n        self._list_of_fits.append(fm.ols(model_desc, data=self.df).fit())\n        # try to improve the model until no improvements can be found\n\n        while all_model_terms_dict:\n            # try each x and overwrite the best_fit if we find a better one\n            # the first best_fit is the one from the previous round\n            ref_fit = self._list_of_fits[-1]\n            best_fit = self._list_of_fits[-1]\n            best_bic = best_fit.bic\n            for x, term in all_model_terms_dict.items():\n                # make new_fit, compare with best found so far\n                model_desc = ModelDesc(\n                    response_term, ref_fit.model.formula.rhs_termlist + [term]\n                )\n                fit = fm.ols(model_desc, data=self.df).fit()\n                if fit.bic < best_bic:\n                    best_bic = fit.bic\n                    best_fit = fit\n                    best_x = x\n            # Sometimes, the obtained fit may be better, but contains unsignificant parameters.\n            # Correct the fit by removing the unsignificant parameters and estimate again\n            best_fit = self._prune(best_fit, p_max=self.p_max)\n\n            # if best_fit does not contain more variables than ref fit, exit\n            if len(best_fit.model.formula.rhs_termlist) == len(\n                ref_fit.model.formula.rhs_termlist\n            ):\n                break\n            else:\n                self._list_of_fits.append(best_fit)\n                all_model_terms_dict.pop(best_x)\n        self._fit = self._list_of_fits[-1]\n\n    def _do_analysis_cross_validation(self):\n        \"\"\"\n        Find the best model (fit) based on cross-valiation (leave one out)\n\n        \"\"\"\n        assert (\n            len(self.df) < 15\n        ), \"Cross-validation is not implemented if your sample contains more than 15 datapoints\"\n\n        # initialization: first model is the mean, but compute cv correctly.\n        errors = []\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        model_desc = ModelDesc(response_term, model_terms)\n        for i in self.df.index:\n            # make new_fit, compute cross-validation and store error\n            df_ = self.df.drop(i, axis=0)\n            fit = fm.ols(model_desc, data=df_).fit()\n            cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n            errors.append(cross_prediction[\"predicted\"] - cross_prediction[self.y])\n\n        self._list_of_fits = [fm.ols(model_desc, data=self.df).fit()]\n        self.list_of_cverrors = [np.mean(np.abs(np.array(errors)))]\n\n        # try to improve the model until no improvements can be found\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        while all_model_terms_dict:\n            # import pdb;pdb.set_trace()\n            # try each x in all_exog and overwrite if we find a better one\n            # at the end of iteration (and not earlier), save the best of the iteration\n            better_model_found = False\n            best = dict(fit=self._list_of_fits[-1], cverror=self.list_of_cverrors[-1])\n            for x, term in all_model_terms_dict.items():\n                model_desc = ModelDesc(\n                    response_term,\n                    self._list_of_fits[-1].model.formula.rhs_termlist + [term],\n                )\n                # cross_validation, currently only implemented for monthly data\n                # compute the mean error for a given formula based on leave-one-out.\n                errors = []\n                for i in self.df.index:\n                    # make new_fit, compute cross-validation and store error\n                    df_ = self.df.drop(i, axis=0)\n                    fit = fm.ols(model_desc, data=df_).fit()\n                    cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n                    errors.append(\n                        cross_prediction[\"predicted\"] - cross_prediction[self.y]\n                    )\n                cverror = np.mean(np.abs(np.array(errors)))\n                # compare the model with the current fit\n                if cverror < best[\"cverror\"]:\n                    # better model, keep it\n                    # first, reidentify using all the datapoints\n                    best[\"fit\"] = fm.ols(model_desc, data=self.df).fit()\n                    best[\"cverror\"] = cverror\n                    better_model_found = True\n                    best_x = x\n\n            if better_model_found:\n                self._list_of_fits.append(best[\"fit\"])\n                self.list_of_cverrors.append(best[\"cverror\"])\n\n            else:\n                # if we did not find a better model, exit\n                break\n\n            # next iteration with the found exog removed\n            all_model_terms_dict.pop(best_x)\n\n        self._fit = self._list_of_fits[-1]\n\n    def _prune(self, fit, p_max):\n        \"\"\"\n        If the fit contains statistically insignificant parameters, remove them.\n        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max\n\n        Parameters\n        ----------\n        fit: fm.ols fit object\n            Can contain insignificant parameters\n        p_max : float\n            Maximum allowed probability of the t-statistic\n\n        Returns\n        -------\n        fit: fm.ols fit object\n            Won't contain any insignificant parameters\n\n        \"\"\"\n\n        def remove_from_model_desc(x, model_desc):\n            \"\"\"\n            Return a model_desc without x\n            \"\"\"\n\n            rhs_termlist = []\n            for t in model_desc.rhs_termlist:\n                if not t.factors:\n                    # intercept, add anyway\n                    rhs_termlist.append(t)\n                elif not x == t.factors[0]._varname:\n                    # this is not the term with x\n                    rhs_termlist.append(t)\n\n            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)\n            return md\n\n        corrected_model_desc = ModelDesc(\n            fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:]\n        )\n        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n        try:\n            pars_to_prune.remove(\"Intercept\")\n        except:\n            pass\n        while pars_to_prune:\n            corrected_model_desc = remove_from_model_desc(\n                pars_to_prune[0], corrected_model_desc\n            )\n            fit = fm.ols(corrected_model_desc, data=self.df).fit()\n            pars_to_prune = (\n                fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n            )\n            try:\n                pars_to_prune.remove(\"Intercept\")\n            except:\n                pass\n        return fit\n\n    @staticmethod\n    @staticmethod\n    def find_best_akaike(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.aic)\n        return res[0]\n\n    @staticmethod\n    def find_best_bic(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.bic)\n        return res[0]\n\n    def _predict(self, fit, df):\n        \"\"\"\n        Return a df with predictions and confidence interval\n\n        Notes\n        -----\n        The df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        The result will depend on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Parameters\n        ----------\n        fit : Statsmodels fit\n        df : pandas DataFrame or None (default)\n            If None, use self.df\n\n\n        Returns\n        -------\n        df_res : pandas DataFrame\n            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'\n        \"\"\"\n\n        # Add model results to data as column 'predictions'\n        df_res = df.copy()\n        if \"Intercept\" in fit.model.exog_names:\n            df_res[\"Intercept\"] = 1.0\n        df_res[\"predicted\"] = fit.predict(df_res)\n        if not self.allow_negative_predictions:\n            df_res.loc[df_res[\"predicted\"] < 0, \"predicted\"] = 0\n\n        prstd, interval_l, interval_u = wls_prediction_std(\n            fit, df_res[fit.model.exog_names], alpha=1 - self.confint\n        )\n        df_res[\"interval_l\"] = interval_l\n        df_res[\"interval_u\"] = interval_u\n\n        if \"Intercept\" in df_res:\n            df_res.drop(labels=[\"Intercept\"], axis=1, inplace=True)\n\n        return df_res\n\n    def add_prediction(self):\n        \"\"\"\n        Add predictions and confidence interval to self.df\n        self.df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        Parameters\n        ----------\n        None, but the result depends on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Returns\n        -------\n        Nothing, adds columns to self.df\n        \"\"\"\n        self.df = self._predict(fit=self.fit, df=self.df)\n\n    def plot(self, model=True, bar_chart=True, **kwargs):\n        \"\"\"\n        Plot measurements and predictions.\n\n        By default, use self._fit and self.df, but both can be overruled by the arguments df and fit\n        This function will detect if the data has been used for the modelling or not and will\n        visualize them differently.\n\n        Parameters\n        ----------\n        model : boolean, default=True\n            If True, show the modified energy signature\n        bar_chart : boolean, default=True\n            If True, make a bar chart with predicted and measured data\n\n        Other Parameters\n        ----------------\n        df : pandas Dataframe, default=None\n            The data to be plotted.  If None, use self.df\n            If the dataframe does not have a column 'predicted', a prediction will be made\n        fit : statsmodels fit, default=None\n            The model to be used.  if None, use self._fit\n\n        Returns\n        -------\n        figures : List of plt.figure objects.\n\n        \"\"\"\n        plot_style()\n        figures = []\n        fit = kwargs.get(\"fit\", self.fit)\n        df = kwargs.get(\"df\", self.df)\n\n        if not \"predicted\" in df.columns:\n            df = self._predict(fit=fit, df=df)\n        # split the df in the auto-validation and prognosis part\n        df_auto = df.loc[self.df.index[0] : self.df.index[-1]]\n        if df_auto.empty:\n            df_prog = df\n        else:\n            df_prog = df.loc[df_auto.index[-1] :].iloc[1:]\n\n        if model:\n            # The first variable in the formula is the most significant.  Use it as abcis for the plot\n            try:\n                exog1 = fit.model.exog_names[1]\n            except IndexError:\n                exog1 = self.list_of_x[0]\n\n            # plot model as an adjusted trendline\n            # get sorted model values\n            dfmodel = df[[exog1, \"predicted\", \"interval_u\", \"interval_l\"]]\n            dfmodel.index = dfmodel[exog1]\n            dfmodel = dfmodel.sort_index()\n            plt.plot(dfmodel.index, dfmodel[\"predicted\"], \"--\", color=\"royalblue\")\n            plt.plot(dfmodel.index, dfmodel[\"interval_l\"], \":\", color=\"royalblue\")\n            plt.plot(dfmodel.index, dfmodel[\"interval_u\"], \":\", color=\"royalblue\")\n            # plot dots for the measurements\n            if len(df_auto) > 0:\n                plt.plot(\n                    df_auto[exog1],\n                    df_auto[self.y],\n                    \"o\",\n                    mfc=\"orangered\",\n                    mec=\"orangered\",\n                    ms=8,\n                    label=\"Data used for model fitting\",\n                )\n            if len(df_prog) > 0:\n                plt.plot(\n                    df_prog[exog1],\n                    df_prog[self.y],\n                    \"o\",\n                    mfc=\"seagreen\",\n                    mec=\"seagreen\",\n                    ms=8,\n                    label=\"Data not used for model fitting\",\n                )\n            plt.title(\"rsquared={:.2f} - BIC={:.1f}\".format(fit.rsquared, fit.bic))\n            plt.xlabel(exog1)\n            figures.append(plt.gcf())\n\n        if bar_chart:\n            ind = np.arange(len(df.index))  # the x locations for the groups\n            width = 0.35  # the width of the bars\n\n            fig, ax = plt.subplots()\n            title = \"Measured\"  # will be appended based on the available data\n            if len(df_auto) > 0:\n                model = ax.bar(\n                    ind[: len(df_auto)],\n                    df_auto[\"predicted\"],\n                    width * 2,\n                    color=\"#FDD787\",\n                    ecolor=\"#FDD787\",\n                    yerr=df_auto[\"interval_u\"] - df_auto[\"predicted\"],\n                    label=self.y + \" modelled\",\n                )\n                title = title + \", modelled\"\n            if len(df_prog) > 0:\n                prog = ax.bar(\n                    ind[len(df_auto) :],\n                    df_prog[\"predicted\"],\n                    width * 2,\n                    color=\"#6CD5A1\",\n                    ecolor=\"#6CD5A1\",\n                    yerr=df_prog[\"interval_u\"] - df_prog[\"predicted\"],\n                    label=self.y + \" expected\",\n                )\n                title = title + \" and predicted\"\n\n            meas = ax.bar(\n                ind, df[self.y], width, label=self.y + \" measured\", color=\"#D5756C\"\n            )\n            # add some text for labels, title and axes ticks\n            ax.set_title(\"{} {}\".format(title, self.y))\n            ax.set_xticks(ind)\n            ax.set_xticklabels(\n                [x.strftime(\"%d-%m-%Y\") for x in df.index], rotation=\"vertical\"\n            )\n            ax.yaxis.grid(True)\n            ax.xaxis.grid(False)\n\n            plt.legend(ncol=3, loc=\"upper center\")\n            figures.append(plt.gcf())\n\n        plt.show()\n\n        return figures\n\n    def _modeldesc_to_dict(self, md):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        d = {\"lhs_termlist\": [md.lhs_termlist[0].factors[0].name()]}\n        rhs_termlist = []\n\n        # add other terms, if any\n        for term in md.rhs_termlist[:]:\n            if len(term.factors) == 0:\n                # intercept, represent by empty string\n                rhs_termlist.append(\"\")\n            else:\n                rhs_termlist.append(term.factors[0].name())\n\n        d[\"rhs_termlist\"] = rhs_termlist\n        return d\n\n    def _modeldesc_from_dict(self, d):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        lhs_termlist = [Term([LookupFactor(d[\"lhs_termlist\"][0])])]\n        rhs_termlist = []\n        for name in d[\"rhs_termlist\"]:\n            if name == \"\":\n                rhs_termlist.append(Term([]))\n            else:\n                rhs_termlist.append(Term([LookupFactor(name)]))\n\n        md = ModelDesc(lhs_termlist, rhs_termlist)\n        return md\n\n    def __getstate__(self):\n        \"\"\"\n        Remove attributes that cannot be pickled and store as dict.\n\n        Each fit has a model.formula which is a patsy ModelDesc and this cannot be pickled.\n        We use our knowledge of this ModelDesc (as we build it up manually in the do_analysis() method)\n        and decompose it into a dictionary.  This dictionary is stored in the list 'formulas',\n        one dict per fit.\n\n        Finally we have to remove each fit entirely (not just the formula), it is built-up again\n        from self.formulas in the __setstate__ method.\n        \"\"\"\n        d = self.__dict__\n        d[\"formulas\"] = []\n        for fit in self._list_of_fits:\n            d[\"formulas\"].append(self._modeldesc_to_dict(fit.model.formula))\n            # delattr(fit.model, 'formula')\n        d.pop(\"_list_of_fits\")\n        d.pop(\"_fit\")\n\n        print(\n            \"Pickling...  Removing the 'formula' from each fit.model.\\n\\\n             You have to unpickle your object or run __setstate__(self.__dict__) to restore them.\".format(\n                d\n            )\n        )\n        return d\n\n    def __setstate__(self, state):\n        \"\"\"Restore the attributes that cannot be pickled\"\"\"\n        for k, v in state.items():\n            if k is not \"formulas\":\n                setattr(self, k, v)\n        self._list_of_fits = []\n        for formula in state[\"formulas\"]:\n            self._list_of_fits.append(\n                fm.ols(self._modeldesc_from_dict(formula), data=self.df).fit()\n            )\n        self._fit = self._list_of_fits[-1]\n", "levels": [0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import datetime as dt", "import pandas as pd", "from .plotting import plot_style", "import numpy as np", "import statsmodels.formula.api as fm", "from statsmodels.sandbox.regression.predstd import wls_prediction_std", "from patsy import ModelDesc, Term, LookupFactor", "from copy import deepcopy", "import re", "from .analysis import Analysis"], "function": ["class MultiVarLinReg(Analysis):\n", "    def fit(self):\n", "    def list_of_fits(self):\n", "    def do_analysis(self):\n", "    def _do_analysis_no_cross_validation(self):\n", "    def _do_analysis_cross_validation(self):\n", "    def _prune(self, fit, p_max):\n", "        def remove_from_model_desc(x, model_desc):\n", "    def find_best_akaike(list_of_fits):\n", "    def find_best_bic(list_of_fits):\n", "    def _predict(self, fit, df):\n", "    def add_prediction(self):\n", "    def plot(self, model=True, bar_chart=True, **kwargs):\n", "    def _modeldesc_to_dict(self, md):\n", "    def _modeldesc_from_dict(self, d):\n", "    def __getstate__(self):\n", "    def __setstate__(self, state):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/regression.py", "func_name": "MultiVarLinReg._predict", "original_string": "def _predict(self, fit, df):\n        \"\"\"\n        Return a df with predictions and confidence interval\n\n        Notes\n        -----\n        The df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        The result will depend on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Parameters\n        ----------\n        fit : Statsmodels fit\n        df : pandas DataFrame or None (default)\n            If None, use self.df\n\n\n        Returns\n        -------\n        df_res : pandas DataFrame\n            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'\n        \"\"\"\n\n        # Add model results to data as column 'predictions'\n        df_res = df.copy()\n        if 'Intercept' in fit.model.exog_names:\n            df_res['Intercept'] = 1.0\n        df_res['predicted'] = fit.predict(df_res)\n        if not self.allow_negative_predictions:\n            df_res.loc[df_res['predicted'] < 0, 'predicted'] = 0\n\n        prstd, interval_l, interval_u = wls_prediction_std(fit,\n                                                           df_res[fit.model.exog_names],\n                                                           alpha=1 - self.confint)\n        df_res['interval_l'] = interval_l\n        df_res['interval_u'] = interval_u\n\n        if 'Intercept' in df_res:\n            df_res.drop(labels=['Intercept'], axis=1, inplace=True)\n\n        return df_res", "language": "python", "code": "def _predict(self, fit, df):\n        \"\"\"\n        Return a df with predictions and confidence interval\n\n        Notes\n        -----\n        The df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        The result will depend on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Parameters\n        ----------\n        fit : Statsmodels fit\n        df : pandas DataFrame or None (default)\n            If None, use self.df\n\n\n        Returns\n        -------\n        df_res : pandas DataFrame\n            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'\n        \"\"\"\n\n        # Add model results to data as column 'predictions'\n        df_res = df.copy()\n        if 'Intercept' in fit.model.exog_names:\n            df_res['Intercept'] = 1.0\n        df_res['predicted'] = fit.predict(df_res)\n        if not self.allow_negative_predictions:\n            df_res.loc[df_res['predicted'] < 0, 'predicted'] = 0\n\n        prstd, interval_l, interval_u = wls_prediction_std(fit,\n                                                           df_res[fit.model.exog_names],\n                                                           alpha=1 - self.confint)\n        df_res['interval_l'] = interval_l\n        df_res['interval_u'] = interval_u\n\n        if 'Intercept' in df_res:\n            df_res.drop(labels=['Intercept'], axis=1, inplace=True)\n\n        return df_res", "code_tokens": ["def", "_predict", "(", "self", ",", "fit", ",", "df", ")", ":", "# Add model results to data as column 'predictions'", "df_res", "=", "df", ".", "copy", "(", ")", "if", "'Intercept'", "in", "fit", ".", "model", ".", "exog_names", ":", "df_res", "[", "'Intercept'", "]", "=", "1.0", "df_res", "[", "'predicted'", "]", "=", "fit", ".", "predict", "(", "df_res", ")", "if", "not", "self", ".", "allow_negative_predictions", ":", "df_res", ".", "loc", "[", "df_res", "[", "'predicted'", "]", "<", "0", ",", "'predicted'", "]", "=", "0", "prstd", ",", "interval_l", ",", "interval_u", "=", "wls_prediction_std", "(", "fit", ",", "df_res", "[", "fit", ".", "model", ".", "exog_names", "]", ",", "alpha", "=", "1", "-", "self", ".", "confint", ")", "df_res", "[", "'interval_l'", "]", "=", "interval_l", "df_res", "[", "'interval_u'", "]", "=", "interval_u", "if", "'Intercept'", "in", "df_res", ":", "df_res", ".", "drop", "(", "labels", "=", "[", "'Intercept'", "]", ",", "axis", "=", "1", ",", "inplace", "=", "True", ")", "return", "df_res"], "docstring": "Return a df with predictions and confidence interval\n\n        Notes\n        -----\n        The df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        The result will depend on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Parameters\n        ----------\n        fit : Statsmodels fit\n        df : pandas DataFrame or None (default)\n            If None, use self.df\n\n\n        Returns\n        -------\n        df_res : pandas DataFrame\n            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'", "docstring_tokens": ["Return", "a", "df", "with", "predictions", "and", "confidence", "interval"], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/regression.py#L292-L338", "partition": "train", "up_fun_num": 11, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\nimport datetime as dt\nimport pandas as pd\n\nfrom .plotting import plot_style\n\nplt = plot_style()\nimport numpy as np\nimport statsmodels.formula.api as fm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom patsy import ModelDesc, Term, LookupFactor\nfrom copy import deepcopy\nimport re\n\nfrom .analysis import Analysis\n\n\nclass MultiVarLinReg(Analysis):\n    \"\"\"\n    Multi-variable linear regression based on statsmodels and Ordinary Least Squares (ols)\n\n    Pass a dataframe with the variable to be modelled y (dependent variable) and the possible independent variables x.\n    Specify as string the name of the dependent variable, and optionally pass a list with names of\n    independent variables to try (by default all other columns will be tried as independent variables).\n\n    The analysis is based on a forward-selection approach: starting from a simple model, the model is iteratively\n    refined and verified until no statistical relevant improvements can be obtained.  Each model in the iteration loop\n    is stored in the attribute self.list_of_fits.  The selected model is self.fit (=pointer to the last element of\n    self.list_of_fits).\n\n    The dataframe can contain daily, weekly, monthly, yearly ... values.  Each row is an instance.\n\n\n    Examples\n    --------\n\n    >> mvlr = MultiVarLinReg(df, 'gas', p_max=0.04)\n    >> mvlr = MultiVarLinReg(df, 'gas', list_of_x=['heatingDegreeDays14', 'GlobalHorizontalIrradiance', 'WindSpeed'])\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        df,\n        y,\n        p_max=0.05,\n        list_of_x=None,\n        confint=0.95,\n        cross_validation=False,\n        allow_negative_predictions=False,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Datetimeindex and both independent variables (x) and dependent variable (y) as columns\n        y : str\n            Name of the dependent (endogeneous) variable to model\n        p_max : float (default=0.05)\n            Acceptable p-value of the t-statistic for estimated parameters\n        list_of_x : list of str (default=None)\n            If None (default), try to build a model with all columns in the dataframe\n            If a list with column names is given, only try these columns as independent variables\n        confint : float, default=0.95\n            Two-sided confidence interval for predictions.\n        cross_validation : bool, default=False\n            If True, compute the model based on cross-validation (leave one out)\n            Only possible if the df has less than 15 entries.\n            Note : this will take much longer computation times!\n        allow_negative_predictions : bool, default=False\n            If True, allow predictions to be negative.\n            For gas consumption or PV production, this is not physical so allow_negative_predictions should be False\n        \"\"\"\n        self.df = df.copy()  # type: pd.DataFrame\n        assert (\n            y in self.df.columns\n        ), \"The dependent variable {} is not a column in the dataframe\".format(y)\n        self.y = y\n\n        self.p_max = p_max\n        self.list_of_x = list_of_x or self.df.columns.tolist()\n        self.confint = confint\n        self.cross_validation = cross_validation\n        self.allow_negative_predictions = allow_negative_predictions\n        try:\n            self.list_of_x.remove(self.y)\n        except ValueError:\n            pass\n        self._fit = None\n        self._list_of_fits = []\n\n    @property\n    def fit(self):\n        if self._fit is None:\n            raise UnboundLocalError(\n                'Run \"do_analysis()\" first to fit a model to the data.'\n            )\n        else:\n            return self._fit\n\n    @property\n    def list_of_fits(self):\n        if not self._list_of_fits:\n            raise UnboundLocalError(\n                'Run \"do_analysis()\" first to fit a model to the data.'\n            )\n        else:\n            return self._list_of_fits\n\n    def do_analysis(self):\n        \"\"\"\n        Find the best model (fit) and create self.list_of_fits and self.fit\n\n        \"\"\"\n        if self.cross_validation:\n            return self._do_analysis_cross_validation()\n        else:\n            return self._do_analysis_no_cross_validation()\n\n    def _do_analysis_no_cross_validation(self):\n        \"\"\"\n        Find the best model (fit) and create self.list_of_fits and self.fit\n        \"\"\"\n\n        # first model is just the mean\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        # ...then add another term for each candidate\n        # model_terms += [Term([LookupFactor(c)]) for c in candidates]\n        model_desc = ModelDesc(response_term, model_terms)\n        self._list_of_fits.append(fm.ols(model_desc, data=self.df).fit())\n        # try to improve the model until no improvements can be found\n\n        while all_model_terms_dict:\n            # try each x and overwrite the best_fit if we find a better one\n            # the first best_fit is the one from the previous round\n            ref_fit = self._list_of_fits[-1]\n            best_fit = self._list_of_fits[-1]\n            best_bic = best_fit.bic\n            for x, term in all_model_terms_dict.items():\n                # make new_fit, compare with best found so far\n                model_desc = ModelDesc(\n                    response_term, ref_fit.model.formula.rhs_termlist + [term]\n                )\n                fit = fm.ols(model_desc, data=self.df).fit()\n                if fit.bic < best_bic:\n                    best_bic = fit.bic\n                    best_fit = fit\n                    best_x = x\n            # Sometimes, the obtained fit may be better, but contains unsignificant parameters.\n            # Correct the fit by removing the unsignificant parameters and estimate again\n            best_fit = self._prune(best_fit, p_max=self.p_max)\n\n            # if best_fit does not contain more variables than ref fit, exit\n            if len(best_fit.model.formula.rhs_termlist) == len(\n                ref_fit.model.formula.rhs_termlist\n            ):\n                break\n            else:\n                self._list_of_fits.append(best_fit)\n                all_model_terms_dict.pop(best_x)\n        self._fit = self._list_of_fits[-1]\n\n    def _do_analysis_cross_validation(self):\n        \"\"\"\n        Find the best model (fit) based on cross-valiation (leave one out)\n\n        \"\"\"\n        assert (\n            len(self.df) < 15\n        ), \"Cross-validation is not implemented if your sample contains more than 15 datapoints\"\n\n        # initialization: first model is the mean, but compute cv correctly.\n        errors = []\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        model_desc = ModelDesc(response_term, model_terms)\n        for i in self.df.index:\n            # make new_fit, compute cross-validation and store error\n            df_ = self.df.drop(i, axis=0)\n            fit = fm.ols(model_desc, data=df_).fit()\n            cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n            errors.append(cross_prediction[\"predicted\"] - cross_prediction[self.y])\n\n        self._list_of_fits = [fm.ols(model_desc, data=self.df).fit()]\n        self.list_of_cverrors = [np.mean(np.abs(np.array(errors)))]\n\n        # try to improve the model until no improvements can be found\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        while all_model_terms_dict:\n            # import pdb;pdb.set_trace()\n            # try each x in all_exog and overwrite if we find a better one\n            # at the end of iteration (and not earlier), save the best of the iteration\n            better_model_found = False\n            best = dict(fit=self._list_of_fits[-1], cverror=self.list_of_cverrors[-1])\n            for x, term in all_model_terms_dict.items():\n                model_desc = ModelDesc(\n                    response_term,\n                    self._list_of_fits[-1].model.formula.rhs_termlist + [term],\n                )\n                # cross_validation, currently only implemented for monthly data\n                # compute the mean error for a given formula based on leave-one-out.\n                errors = []\n                for i in self.df.index:\n                    # make new_fit, compute cross-validation and store error\n                    df_ = self.df.drop(i, axis=0)\n                    fit = fm.ols(model_desc, data=df_).fit()\n                    cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n                    errors.append(\n                        cross_prediction[\"predicted\"] - cross_prediction[self.y]\n                    )\n                cverror = np.mean(np.abs(np.array(errors)))\n                # compare the model with the current fit\n                if cverror < best[\"cverror\"]:\n                    # better model, keep it\n                    # first, reidentify using all the datapoints\n                    best[\"fit\"] = fm.ols(model_desc, data=self.df).fit()\n                    best[\"cverror\"] = cverror\n                    better_model_found = True\n                    best_x = x\n\n            if better_model_found:\n                self._list_of_fits.append(best[\"fit\"])\n                self.list_of_cverrors.append(best[\"cverror\"])\n\n            else:\n                # if we did not find a better model, exit\n                break\n\n            # next iteration with the found exog removed\n            all_model_terms_dict.pop(best_x)\n\n        self._fit = self._list_of_fits[-1]\n\n    def _prune(self, fit, p_max):\n        \"\"\"\n        If the fit contains statistically insignificant parameters, remove them.\n        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max\n\n        Parameters\n        ----------\n        fit: fm.ols fit object\n            Can contain insignificant parameters\n        p_max : float\n            Maximum allowed probability of the t-statistic\n\n        Returns\n        -------\n        fit: fm.ols fit object\n            Won't contain any insignificant parameters\n\n        \"\"\"\n\n        def remove_from_model_desc(x, model_desc):\n            \"\"\"\n            Return a model_desc without x\n            \"\"\"\n\n            rhs_termlist = []\n            for t in model_desc.rhs_termlist:\n                if not t.factors:\n                    # intercept, add anyway\n                    rhs_termlist.append(t)\n                elif not x == t.factors[0]._varname:\n                    # this is not the term with x\n                    rhs_termlist.append(t)\n\n            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)\n            return md\n\n        corrected_model_desc = ModelDesc(\n            fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:]\n        )\n        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n        try:\n            pars_to_prune.remove(\"Intercept\")\n        except:\n            pass\n        while pars_to_prune:\n            corrected_model_desc = remove_from_model_desc(\n                pars_to_prune[0], corrected_model_desc\n            )\n            fit = fm.ols(corrected_model_desc, data=self.df).fit()\n            pars_to_prune = (\n                fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n            )\n            try:\n                pars_to_prune.remove(\"Intercept\")\n            except:\n                pass\n        return fit\n\n    @staticmethod\n    def find_best_rsquared(list_of_fits):\n        \"\"\"Return the best fit, based on rsquared\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.rsquared)\n        return res[-1]\n\n    @staticmethod\n    def find_best_akaike(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.aic)\n        return res[0]\n\n    @staticmethod\n    def find_best_bic(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.bic)\n        return res[0]\n\n    def add_prediction(self):\n        \"\"\"\n        Add predictions and confidence interval to self.df\n        self.df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        Parameters\n        ----------\n        None, but the result depends on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Returns\n        -------\n        Nothing, adds columns to self.df\n        \"\"\"\n        self.df = self._predict(fit=self.fit, df=self.df)\n\n    def plot(self, model=True, bar_chart=True, **kwargs):\n        \"\"\"\n        Plot measurements and predictions.\n\n        By default, use self._fit and self.df, but both can be overruled by the arguments df and fit\n        This function will detect if the data has been used for the modelling or not and will\n        visualize them differently.\n\n        Parameters\n        ----------\n        model : boolean, default=True\n            If True, show the modified energy signature\n        bar_chart : boolean, default=True\n            If True, make a bar chart with predicted and measured data\n\n        Other Parameters\n        ----------------\n        df : pandas Dataframe, default=None\n            The data to be plotted.  If None, use self.df\n            If the dataframe does not have a column 'predicted', a prediction will be made\n        fit : statsmodels fit, default=None\n            The model to be used.  if None, use self._fit\n\n        Returns\n        -------\n        figures : List of plt.figure objects.\n\n        \"\"\"\n        plot_style()\n        figures = []\n        fit = kwargs.get(\"fit\", self.fit)\n        df = kwargs.get(\"df\", self.df)\n\n        if not \"predicted\" in df.columns:\n            df = self._predict(fit=fit, df=df)\n        # split the df in the auto-validation and prognosis part\n        df_auto = df.loc[self.df.index[0] : self.df.index[-1]]\n        if df_auto.empty:\n            df_prog = df\n        else:\n            df_prog = df.loc[df_auto.index[-1] :].iloc[1:]\n\n        if model:\n            # The first variable in the formula is the most significant.  Use it as abcis for the plot\n            try:\n                exog1 = fit.model.exog_names[1]\n            except IndexError:\n                exog1 = self.list_of_x[0]\n\n            # plot model as an adjusted trendline\n            # get sorted model values\n            dfmodel = df[[exog1, \"predicted\", \"interval_u\", \"interval_l\"]]\n            dfmodel.index = dfmodel[exog1]\n            dfmodel = dfmodel.sort_index()\n            plt.plot(dfmodel.index, dfmodel[\"predicted\"], \"--\", color=\"royalblue\")\n            plt.plot(dfmodel.index, dfmodel[\"interval_l\"], \":\", color=\"royalblue\")\n            plt.plot(dfmodel.index, dfmodel[\"interval_u\"], \":\", color=\"royalblue\")\n            # plot dots for the measurements\n            if len(df_auto) > 0:\n                plt.plot(\n                    df_auto[exog1],\n                    df_auto[self.y],\n                    \"o\",\n                    mfc=\"orangered\",\n                    mec=\"orangered\",\n                    ms=8,\n                    label=\"Data used for model fitting\",\n                )\n            if len(df_prog) > 0:\n                plt.plot(\n                    df_prog[exog1],\n                    df_prog[self.y],\n                    \"o\",\n                    mfc=\"seagreen\",\n                    mec=\"seagreen\",\n                    ms=8,\n                    label=\"Data not used for model fitting\",\n                )\n            plt.title(\"rsquared={:.2f} - BIC={:.1f}\".format(fit.rsquared, fit.bic))\n            plt.xlabel(exog1)\n            figures.append(plt.gcf())\n\n        if bar_chart:\n            ind = np.arange(len(df.index))  # the x locations for the groups\n            width = 0.35  # the width of the bars\n\n            fig, ax = plt.subplots()\n            title = \"Measured\"  # will be appended based on the available data\n            if len(df_auto) > 0:\n                model = ax.bar(\n                    ind[: len(df_auto)],\n                    df_auto[\"predicted\"],\n                    width * 2,\n                    color=\"#FDD787\",\n                    ecolor=\"#FDD787\",\n                    yerr=df_auto[\"interval_u\"] - df_auto[\"predicted\"],\n                    label=self.y + \" modelled\",\n                )\n                title = title + \", modelled\"\n            if len(df_prog) > 0:\n                prog = ax.bar(\n                    ind[len(df_auto) :],\n                    df_prog[\"predicted\"],\n                    width * 2,\n                    color=\"#6CD5A1\",\n                    ecolor=\"#6CD5A1\",\n                    yerr=df_prog[\"interval_u\"] - df_prog[\"predicted\"],\n                    label=self.y + \" expected\",\n                )\n                title = title + \" and predicted\"\n\n            meas = ax.bar(\n                ind, df[self.y], width, label=self.y + \" measured\", color=\"#D5756C\"\n            )\n            # add some text for labels, title and axes ticks\n            ax.set_title(\"{} {}\".format(title, self.y))\n            ax.set_xticks(ind)\n            ax.set_xticklabels(\n                [x.strftime(\"%d-%m-%Y\") for x in df.index], rotation=\"vertical\"\n            )\n            ax.yaxis.grid(True)\n            ax.xaxis.grid(False)\n\n            plt.legend(ncol=3, loc=\"upper center\")\n            figures.append(plt.gcf())\n\n        plt.show()\n\n        return figures\n\n    def _modeldesc_to_dict(self, md):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        d = {\"lhs_termlist\": [md.lhs_termlist[0].factors[0].name()]}\n        rhs_termlist = []\n\n        # add other terms, if any\n        for term in md.rhs_termlist[:]:\n            if len(term.factors) == 0:\n                # intercept, represent by empty string\n                rhs_termlist.append(\"\")\n            else:\n                rhs_termlist.append(term.factors[0].name())\n\n        d[\"rhs_termlist\"] = rhs_termlist\n        return d\n\n    def _modeldesc_from_dict(self, d):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        lhs_termlist = [Term([LookupFactor(d[\"lhs_termlist\"][0])])]\n        rhs_termlist = []\n        for name in d[\"rhs_termlist\"]:\n            if name == \"\":\n                rhs_termlist.append(Term([]))\n            else:\n                rhs_termlist.append(Term([LookupFactor(name)]))\n\n        md = ModelDesc(lhs_termlist, rhs_termlist)\n        return md\n\n    def __getstate__(self):\n        \"\"\"\n        Remove attributes that cannot be pickled and store as dict.\n\n        Each fit has a model.formula which is a patsy ModelDesc and this cannot be pickled.\n        We use our knowledge of this ModelDesc (as we build it up manually in the do_analysis() method)\n        and decompose it into a dictionary.  This dictionary is stored in the list 'formulas',\n        one dict per fit.\n\n        Finally we have to remove each fit entirely (not just the formula), it is built-up again\n        from self.formulas in the __setstate__ method.\n        \"\"\"\n        d = self.__dict__\n        d[\"formulas\"] = []\n        for fit in self._list_of_fits:\n            d[\"formulas\"].append(self._modeldesc_to_dict(fit.model.formula))\n            # delattr(fit.model, 'formula')\n        d.pop(\"_list_of_fits\")\n        d.pop(\"_fit\")\n\n        print(\n            \"Pickling...  Removing the 'formula' from each fit.model.\\n\\\n             You have to unpickle your object or run __setstate__(self.__dict__) to restore them.\".format(\n                d\n            )\n        )\n        return d\n\n    def __setstate__(self, state):\n        \"\"\"Restore the attributes that cannot be pickled\"\"\"\n        for k, v in state.items():\n            if k is not \"formulas\":\n                setattr(self, k, v)\n        self._list_of_fits = []\n        for formula in state[\"formulas\"]:\n            self._list_of_fits.append(\n                fm.ols(self._modeldesc_from_dict(formula), data=self.df).fit()\n            )\n        self._fit = self._list_of_fits[-1]\n", "levels": [0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import datetime as dt", "import pandas as pd", "from .plotting import plot_style", "import numpy as np", "import statsmodels.formula.api as fm", "from statsmodels.sandbox.regression.predstd import wls_prediction_std", "from patsy import ModelDesc, Term, LookupFactor", "from copy import deepcopy", "import re", "from .analysis import Analysis"], "function": ["class MultiVarLinReg(Analysis):\n", "    def fit(self):\n", "    def list_of_fits(self):\n", "    def do_analysis(self):\n", "    def _do_analysis_no_cross_validation(self):\n", "    def _do_analysis_cross_validation(self):\n", "    def _prune(self, fit, p_max):\n", "        def remove_from_model_desc(x, model_desc):\n", "    def find_best_rsquared(list_of_fits):\n", "    def find_best_akaike(list_of_fits):\n", "    def find_best_bic(list_of_fits):\n", "    def add_prediction(self):\n", "    def plot(self, model=True, bar_chart=True, **kwargs):\n", "    def _modeldesc_to_dict(self, md):\n", "    def _modeldesc_from_dict(self, d):\n", "    def __getstate__(self):\n", "    def __setstate__(self, state):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "relative_abundance", "original_string": "def relative_abundance(biomf, sampleIDs=None):\n    \"\"\"\n    Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.\n    \"\"\"\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample)\n                     for otuID in otuIDs} for sample in sampleIDs}", "language": "python", "code": "def relative_abundance(biomf, sampleIDs=None):\n    \"\"\"\n    Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.\n    \"\"\"\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample)\n                     for otuID in otuIDs} for sample in sampleIDs}", "code_tokens": ["def", "relative_abundance", "(", "biomf", ",", "sampleIDs", "=", "None", ")", ":", "if", "sampleIDs", "is", "None", ":", "sampleIDs", "=", "biomf", ".", "ids", "(", ")", "else", ":", "try", ":", "for", "sid", "in", "sampleIDs", ":", "assert", "sid", "in", "biomf", ".", "ids", "(", ")", "except", "AssertionError", ":", "raise", "ValueError", "(", "\"\\nError while calculating relative abundances: The sampleIDs provided do\"", "\" not match the sampleIDs in biom file. Please double check the sampleIDs\"", "\" provided.\\n\"", ")", "otuIDs", "=", "biomf", ".", "ids", "(", "axis", "=", "\"observation\"", ")", "norm_biomf", "=", "biomf", ".", "norm", "(", "inplace", "=", "False", ")", "return", "{", "sample", ":", "{", "otuID", ":", "norm_biomf", ".", "get_value_by_ids", "(", "otuID", ",", "sample", ")", "for", "otuID", "in", "otuIDs", "}", "for", "sample", "in", "sampleIDs", "}"], "docstring": "Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.", "docstring_tokens": ["Calculate", "the", "relative", "abundance", "of", "each", "OTUID", "in", "a", "Sample", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L11-L41", "partition": "train", "up_fun_num": 0, "context": "\"\"\"\n:Date: Created on Feb 19, 2013\n:Author: Shareef Dabdoub\n:Abstract: This module provides methods for calculating various metrics with regards to\n           each OTU in an input OTU abundance table.\n\"\"\"\nimport math\nfrom collections import defaultdict\n\n\ndef mean_otu_pct_abundance(ra, otuIDs):\n    \"\"\"\n    Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.\n    \"\"\"\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = (\n            sum([ra[sid][oid] for sid in sids if oid in ra[sid]]) / len(sids) * 100\n        )\n    return otumeans\n\n\ndef MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {\n            sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n            for sample in ra.keys()\n        }\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)\n\n\ndef raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results\n\n\ndef transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}\n\n\ndef arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {\n        col_id: {row_id: arcsint(rel_abd[col_id][row_id]) for row_id in rel_abd[col_id]}\n        for col_id in rel_abd\n    }\n", "levels": [0, 0, 0, 0, 0], "package": ["import math", "from collections import defaultdict"], "function": ["def mean_otu_pct_abundance(ra, otuIDs):\n", "def MRA(biomf, sampleIDs=None, transform=None):\n", "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n", "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n", "def arcsine_sqrt_transform(rel_abd):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "mean_otu_pct_abundance", "original_string": "def mean_otu_pct_abundance(ra, otuIDs):\n    \"\"\"\n    Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.\n    \"\"\"\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = sum([ra[sid][oid] for sid in sids\n                             if oid in ra[sid]]) / len(sids) * 100\n    return otumeans", "language": "python", "code": "def mean_otu_pct_abundance(ra, otuIDs):\n    \"\"\"\n    Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.\n    \"\"\"\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = sum([ra[sid][oid] for sid in sids\n                             if oid in ra[sid]]) / len(sids) * 100\n    return otumeans", "code_tokens": ["def", "mean_otu_pct_abundance", "(", "ra", ",", "otuIDs", ")", ":", "sids", "=", "ra", ".", "keys", "(", ")", "otumeans", "=", "defaultdict", "(", "int", ")", "for", "oid", "in", "otuIDs", ":", "otumeans", "[", "oid", "]", "=", "sum", "(", "[", "ra", "[", "sid", "]", "[", "oid", "]", "for", "sid", "in", "sids", "if", "oid", "in", "ra", "[", "sid", "]", "]", ")", "/", "len", "(", "sids", ")", "*", "100", "return", "otumeans"], "docstring": "Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.", "docstring_tokens": ["Calculate", "the", "mean", "OTU", "abundance", "percentage", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L44-L67", "partition": "train", "up_fun_num": 1, "context": "\"\"\"\n:Date: Created on Feb 19, 2013\n:Author: Shareef Dabdoub\n:Abstract: This module provides methods for calculating various metrics with regards to\n           each OTU in an input OTU abundance table.\n\"\"\"\nimport math\nfrom collections import defaultdict\n\n\ndef relative_abundance(biomf, sampleIDs=None):\n    \"\"\"\n    Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.\n    \"\"\"\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {\n        sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample) for otuID in otuIDs}\n        for sample in sampleIDs\n    }\n\n\ndef MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {\n            sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n            for sample in ra.keys()\n        }\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)\n\n\ndef raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results\n\n\ndef transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}\n\n\ndef arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {\n        col_id: {row_id: arcsint(rel_abd[col_id][row_id]) for row_id in rel_abd[col_id]}\n        for col_id in rel_abd\n    }\n", "levels": [0, 0, 0, 0, 0], "package": ["import math", "from collections import defaultdict"], "function": ["def relative_abundance(biomf, sampleIDs=None):\n", "def MRA(biomf, sampleIDs=None, transform=None):\n", "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n", "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n", "def arcsine_sqrt_transform(rel_abd):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "MRA", "original_string": "def MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n              for sample in ra.keys()}\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)", "language": "python", "code": "def MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n              for sample in ra.keys()}\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)", "code_tokens": ["def", "MRA", "(", "biomf", ",", "sampleIDs", "=", "None", ",", "transform", "=", "None", ")", ":", "ra", "=", "relative_abundance", "(", "biomf", ",", "sampleIDs", ")", "if", "transform", "is", "not", "None", ":", "ra", "=", "{", "sample", ":", "{", "otuID", ":", "transform", "(", "abd", ")", "for", "otuID", ",", "abd", "in", "ra", "[", "sample", "]", ".", "items", "(", ")", "}", "for", "sample", "in", "ra", ".", "keys", "(", ")", "}", "otuIDs", "=", "biomf", ".", "ids", "(", "axis", "=", "\"observation\"", ")", "return", "mean_otu_pct_abundance", "(", "ra", ",", "otuIDs", ")"], "docstring": "Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.", "docstring_tokens": ["Calculate", "the", "mean", "relative", "abundance", "percentage", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L70-L92", "partition": "train", "up_fun_num": 2, "context": "\"\"\"\n:Date: Created on Feb 19, 2013\n:Author: Shareef Dabdoub\n:Abstract: This module provides methods for calculating various metrics with regards to\n           each OTU in an input OTU abundance table.\n\"\"\"\nimport math\nfrom collections import defaultdict\n\n\ndef relative_abundance(biomf, sampleIDs=None):\n    \"\"\"\n    Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.\n    \"\"\"\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {\n        sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample) for otuID in otuIDs}\n        for sample in sampleIDs\n    }\n\n\ndef mean_otu_pct_abundance(ra, otuIDs):\n    \"\"\"\n    Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.\n    \"\"\"\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = (\n            sum([ra[sid][oid] for sid in sids if oid in ra[sid]]) / len(sids) * 100\n        )\n    return otumeans\n\n\ndef raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results\n\n\ndef transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}\n\n\ndef arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {\n        col_id: {row_id: arcsint(rel_abd[col_id][row_id]) for row_id in rel_abd[col_id]}\n        for col_id in rel_abd\n    }\n", "levels": [0, 0, 0, 0, 0], "package": ["import math", "from collections import defaultdict"], "function": ["def relative_abundance(biomf, sampleIDs=None):\n", "def mean_otu_pct_abundance(ra, otuIDs):\n", "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n", "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n", "def arcsine_sqrt_transform(rel_abd):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "raw_abundance", "original_string": "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results", "language": "python", "code": "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results", "code_tokens": ["def", "raw_abundance", "(", "biomf", ",", "sampleIDs", "=", "None", ",", "sample_abd", "=", "True", ")", ":", "results", "=", "defaultdict", "(", "int", ")", "if", "sampleIDs", "is", "None", ":", "sampleIDs", "=", "biomf", ".", "ids", "(", ")", "else", ":", "try", ":", "for", "sid", "in", "sampleIDs", ":", "assert", "sid", "in", "biomf", ".", "ids", "(", ")", "except", "AssertionError", ":", "raise", "ValueError", "(", "\"\\nError while calculating raw total abundances: The sampleIDs provided \"", "\"do not match the sampleIDs in biom file. Please double check the \"", "\"sampleIDs provided.\\n\"", ")", "otuIDs", "=", "biomf", ".", "ids", "(", "axis", "=", "\"observation\"", ")", "for", "sampleID", "in", "sampleIDs", ":", "for", "otuID", "in", "otuIDs", ":", "abd", "=", "biomf", ".", "get_value_by_ids", "(", "otuID", ",", "sampleID", ")", "if", "sample_abd", ":", "results", "[", "sampleID", "]", "+=", "abd", "else", ":", "results", "[", "otuID", "]", "+=", "abd", "return", "results"], "docstring": "Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.", "docstring_tokens": ["Calculate", "the", "total", "number", "of", "sequences", "in", "each", "OTU", "or", "SampleID", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L95-L135", "partition": "train", "up_fun_num": 3, "context": "\"\"\"\n:Date: Created on Feb 19, 2013\n:Author: Shareef Dabdoub\n:Abstract: This module provides methods for calculating various metrics with regards to\n           each OTU in an input OTU abundance table.\n\"\"\"\nimport math\nfrom collections import defaultdict\n\n\ndef relative_abundance(biomf, sampleIDs=None):\n    \"\"\"\n    Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.\n    \"\"\"\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {\n        sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample) for otuID in otuIDs}\n        for sample in sampleIDs\n    }\n\n\ndef mean_otu_pct_abundance(ra, otuIDs):\n    \"\"\"\n    Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.\n    \"\"\"\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = (\n            sum([ra[sid][oid] for sid in sids if oid in ra[sid]]) / len(sids) * 100\n        )\n    return otumeans\n\n\ndef MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {\n            sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n            for sample in ra.keys()\n        }\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)\n\n\ndef transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}\n\n\ndef arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {\n        col_id: {row_id: arcsint(rel_abd[col_id][row_id]) for row_id in rel_abd[col_id]}\n        for col_id in rel_abd\n    }\n", "levels": [0, 0, 0, 0, 0], "package": ["import math", "from collections import defaultdict"], "function": ["def relative_abundance(biomf, sampleIDs=None):\n", "def mean_otu_pct_abundance(ra, otuIDs):\n", "def MRA(biomf, sampleIDs=None, transform=None):\n", "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n", "def arcsine_sqrt_transform(rel_abd):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "transform_raw_abundance", "original_string": "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}", "language": "python", "code": "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}", "code_tokens": ["def", "transform_raw_abundance", "(", "biomf", ",", "fn", "=", "math", ".", "log10", ",", "sampleIDs", "=", "None", ",", "sample_abd", "=", "True", ")", ":", "totals", "=", "raw_abundance", "(", "biomf", ",", "sampleIDs", ",", "sample_abd", ")", "return", "{", "sid", ":", "fn", "(", "abd", ")", "for", "sid", ",", "abd", "in", "totals", ".", "items", "(", ")", "}"], "docstring": "Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.", "docstring_tokens": ["Function", "to", "transform", "the", "total", "abundance", "calculation", "for", "each", "sample", "ID", "to", "another", "format", "based", "on", "user", "given", "transformation", "function", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L138-L155", "partition": "train", "up_fun_num": 4, "context": "\"\"\"\n:Date: Created on Feb 19, 2013\n:Author: Shareef Dabdoub\n:Abstract: This module provides methods for calculating various metrics with regards to\n           each OTU in an input OTU abundance table.\n\"\"\"\nimport math\nfrom collections import defaultdict\n\n\ndef relative_abundance(biomf, sampleIDs=None):\n    \"\"\"\n    Calculate the relative abundance of each OTUID in a Sample.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :rtype: dict\n    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on\n             OTUID's and their values represent the relative abundance of that OTUID in\n             that SampleID.\n    \"\"\"\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {\n        sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample) for otuID in otuIDs}\n        for sample in sampleIDs\n    }\n\n\ndef mean_otu_pct_abundance(ra, otuIDs):\n    \"\"\"\n    Calculate the mean OTU abundance percentage.\n\n    :type ra: Dict\n    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are\n               dictionaries keyed on OTUID's and their values represent the relative\n               abundance of that OTUID in that SampleID. 'ra' is the output of\n               relative_abundance() function.\n\n    :type otuIDs: List\n    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be\n                   measured.\n\n    :rtype: dict\n    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.\n    \"\"\"\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = (\n            sum([ra[sid][oid] for sid in sids if oid in ra[sid]]) / len(sids) * 100\n        )\n    return otumeans\n\n\ndef MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {\n            sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n            for sample in ra.keys()\n        }\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)\n\n\ndef raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\"\n            )\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results\n\n\ndef arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {\n        col_id: {row_id: arcsint(rel_abd[col_id][row_id]) for row_id in rel_abd[col_id]}\n        for col_id in rel_abd\n    }\n", "levels": [0, 0, 0, 0, 0], "package": ["import math", "from collections import defaultdict"], "function": ["def relative_abundance(biomf, sampleIDs=None):\n", "def mean_otu_pct_abundance(ra, otuIDs):\n", "def MRA(biomf, sampleIDs=None, transform=None):\n", "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n", "def arcsine_sqrt_transform(rel_abd):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "print_MannWhitneyU", "original_string": "def print_MannWhitneyU(div_calc):\n    \"\"\"\n    Compute the Mann-Whitney U test for unequal group sample sizes.\n    \"\"\"\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\\\n               \"significance testing.\"\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)", "language": "python", "code": "def print_MannWhitneyU(div_calc):\n    \"\"\"\n    Compute the Mann-Whitney U test for unequal group sample sizes.\n    \"\"\"\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\\\n               \"significance testing.\"\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)", "code_tokens": ["def", "print_MannWhitneyU", "(", "div_calc", ")", ":", "try", ":", "x", "=", "div_calc", ".", "values", "(", ")", "[", "0", "]", ".", "values", "(", ")", "y", "=", "div_calc", ".", "values", "(", ")", "[", "1", "]", ".", "values", "(", ")", "except", ":", "return", "\"Error setting up input arrays for Mann-Whitney U Test. Skipping \"", "\"significance testing.\"", "T", ",", "p", "=", "stats", ".", "mannwhitneyu", "(", "x", ",", "y", ")", "print", "\"\\nMann-Whitney U test statistic:\"", ",", "T", "print", "\"Two-tailed p-value: {}\"", ".", "format", "(", "2", "*", "p", ")"], "docstring": "Compute the Mann-Whitney U test for unequal group sample sizes.", "docstring_tokens": ["Compute", "the", "Mann", "-", "Whitney", "U", "test", "for", "unequal", "group", "sample", "sizes", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L54-L66", "partition": "train", "up_fun_num": 2, "context": "#!/usr/bin/env python\n\"\"\"\nCalculate and plot alpha diversity for two or more sample categories.\n\"\"\"\nimport sys\nimport csv\nimport argparse\nimport os.path as osp\nfrom itertools import izip_longest\nfrom collections import defaultdict\nfrom phylotoast import graph_util as gu, util as putil\n\nimporterrors = []\ntry:\n    import biom\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    import scipy.stats as stats\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from skbio.diversity import alpha\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from matplotlib import pyplot as plt, gridspec\nexcept ImportError as ie:\n    importerrors.append(ie)\nif len(importerrors) != 0:\n    for item in importerrors:\n        print \"Import Error. Please install missing module:\", item\n    sys.exit()\n\n\ndef gather_samples(biomT):\n    return {sid: biomT.data(sid).astype(int) for sid in biomT.ids()}\n\n\ndef calc_diversity(method, parsed_mapf, biom, cats, cats_index):\n    counts = {cat: [] for cat in cats}\n    sample_ids = []\n\n    for sid, sample_counts in gather_samples(biom).items():\n        sample_ids.append(sid)\n        if sid in parsed_mapf:\n            counts[parsed_mapf[sid][cats_index]].append((sid, sample_counts))\n\n    div_calc = {\n        cat: {count[0]: method(count[1]) for count in counts}\n        for cat, counts in counts.items()\n    }\n\n    return div_calc, sample_ids\n\n\ndef print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return (\n            \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\n            \"significance testing.\"\n        )\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(\n        str(len(div_calc)), h\n    )\n    print \"p-value: {}\".format(p)\n\n\ndef plot_group_diversity(\n    diversities, grp_colors, title, diversity_type, out_dir, plot_ext\n):\n    fig_div = plt.figure(figsize=(21, 7))\n    grid = gridspec.GridSpec(1, 2)\n\n    # Disease States Shannon Diversity plots\n    ax_div = fig_div.add_subplot(grid[0, 0])\n\n    for i, grp in enumerate(diversities):\n        gu.plot_kde(diversities[grp].values(), ax_div, title, grp_colors[grp])\n\n    ax_div.set_xlabel(diversity_type.title())\n    ax_div.set_ylabel(\"Density\")\n    ax_div.legend(\n        [plt.Rectangle((0, 0), 1, 1, fc=color) for color in grp_colors.values()],\n        grp_colors.keys(),\n        loc=\"best\",\n    )\n\n    fig_div.savefig(\n        osp.join(out_dir, diversity_type + \".\" + plot_ext),\n        facecolor=\"white\",\n        edgecolor=\"none\",\n        bbox_inches=\"tight\",\n        pad_inches=0.2,\n    )\n\n\ndef write_diversity_metrics(data, sample_ids, fp=None):\n    \"\"\"\n    Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.\n    \"\"\"\n    if fp is None:\n        fp = \"./diversity_data.txt\"\n\n    with open(fp, \"w\") as outf:\n        out = csv.writer(outf, delimiter=\"\\t\")\n        out.writerow([\"SampleID\", \"Group\", \"Calculation\"])\n        for group, d in data.iteritems():\n            for sid, value in d.iteritems():\n                out.writerow([sid, group, value])\n\n\ndef handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\"\n    )\n    parser.add_argument(\"-m\", \"--map_file\", help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\", help=\"Path to the BIOM table\")\n    parser.add_argument(\n        \"-c\", \"--category\", help=\"Specific category from the mapping file.\"\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--diversity\",\n        default=[\"shannon\"],\n        nargs=\"+\",\n        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\",\n    )\n    parser.add_argument(\n        \"--x_label\",\n        default=[None],\n        nargs=\"+\",\n        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\",\n    )\n    parser.add_argument(\n        \"--color_by\",\n        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\",\n    )\n    parser.add_argument(\n        \"--plot_title\",\n        default=\"\",\n        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\",\n    )\n    parser.add_argument(\n        \"-o\", \"--output_dir\", default=\".\", help=\"The directory plots will be saved to.\"\n    )\n    parser.add_argument(\n        \"--image_type\",\n        default=\"png\",\n        help=\"The type of image to save: png, svg, pdf, eps, etc...\",\n    )\n    parser.add_argument(\n        \"--save_calculations\",\n        help=\"Path and name of text file to store the calculated \" \"diversity metrics.\",\n    )\n    parser.add_argument(\n        \"--suppress_stats\",\n        action=\"store_true\",\n        help=\"Do not display \"\n        \"significance testing results which are shown by default.\",\n    )\n    parser.add_argument(\n        \"--show_available_metrics\",\n        action=\"store_true\",\n        help=\"Supply this parameter to see which alpha diversity metrics \"\n        \" are available for usage. No calculations will be performed\"\n        \" if this parameter is provided.\",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    metrics = [m for m in alpha.__all__ if \"_ci\" not in m]\n    try:\n        metrics.remove(\"faith_pd\")\n    except ValueError:\n        pass\n    if args.show_available_metrics:\n        print \"\\nAvailable alpha diversity metrics:\"\n        return \"\\n\".join(metrics)\n\n    # check that the output dir exists, create it if not\n    msg = putil.ensure_dir(args.output_dir)\n    # if an error occurs, print and exit\n    if msg:\n        sys.exit(msg)\n\n    # parse mapping file\n    try:\n        header, sample_map = putil.parse_map_file(args.map_file)\n    except Exception as ioe:\n        err_msg = \"\\nError while processing the mapping file: {}\\n\"\n        sys.exit(err_msg.format(ioe))\n\n    # parse BIOM table\n    try:\n        biom_tbl = biom.load_table(args.biom_fp)\n    except Exception as ioe:\n        err_msg = \"\\nError loading BIOM table file: {}\\n\"\n        sys.exit(err_msg.format(ioe))\n\n    # group samples by category\n    if args.category not in header:\n        sys.exit(\"Category '{}' not found\".format(args.category))\n    cat_idx = header.index(args.category)\n    cat_vals = {entry[cat_idx] for entry in sample_map.values()}\n\n    plot_title = args.plot_title\n\n    colors = putil.color_mapping(sample_map, header, args.category, args.color_by)\n\n    # Perform diversity calculations and density plotting\n    for method, x_label in izip_longest(args.diversity, args.x_label):\n        if x_label is None:\n            x_label = method.title()\n        if method not in alpha.__all__:\n            sys.exit(\"ERROR: Diversity metric not found: {}.\".format(method))\n        elif method in alpha.__all__ and method not in metrics:\n            sys.exit(\"Currently, PhyloToAST does not support {} metric.\".format(method))\n        metric = eval(\"alpha.\" + method)\n        div_calc, sample_ids = calc_diversity(\n            metric, sample_map, biom_tbl, cat_vals, cat_idx\n        )\n\n        if args.save_calculations:\n            write_diversity_metrics(div_calc, sample_ids, args.save_calculations)\n\n        plot_group_diversity(\n            div_calc, colors, plot_title, x_label, args.output_dir, args.image_type\n        )\n\n        # calculate and print significance testing results\n        if not args.suppress_stats:\n            print \"Diversity significance testing: {}\".format(x_label)\n            if len(cat_vals) == 2:\n                print_MannWhitneyU(div_calc)\n            elif len(cat_vals) > 2:\n                print_KruskalWallisH(div_calc)\n            print\n        else:\n            continue\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import sys", "import csv", "import argparse", "import os.path as osp", "from itertools import izip_longest", "from collections import defaultdict", "from phylotoast import graph_util as gu, util as putil", "import biom", "import scipy.stats as stats", "from skbio.diversity import alpha", "from matplotlib import pyplot as plt, gridspec"], "function": ["def gather_samples(biomT):\n", "def calc_diversity(method, parsed_mapf, biom, cats, cats_index):\n", "def print_KruskalWallisH(div_calc):\n", "def write_diversity_metrics(data, sample_ids, fp=None):\n", "def handle_program_options():\n", "def main():\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "print_KruskalWallisH", "original_string": "def print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\\\n               \"significance testing.\"\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(str(len(div_calc)), h)\n    print \"p-value: {}\".format(p)", "language": "python", "code": "def print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\\\n               \"significance testing.\"\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(str(len(div_calc)), h)\n    print \"p-value: {}\".format(p)", "code_tokens": ["def", "print_KruskalWallisH", "(", "div_calc", ")", ":", "calc", "=", "defaultdict", "(", "list", ")", "try", ":", "for", "k1", ",", "v1", "in", "div_calc", ".", "iteritems", "(", ")", ":", "for", "k2", ",", "v2", "in", "v1", ".", "iteritems", "(", ")", ":", "calc", "[", "k1", "]", ".", "append", "(", "v2", ")", "except", ":", "return", "\"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"", "\"significance testing.\"", "h", ",", "p", "=", "stats", ".", "kruskal", "(", "*", "calc", ".", "values", "(", ")", ")", "print", "\"\\nKruskal-Wallis H-test statistic for {} groups: {}\"", ".", "format", "(", "str", "(", "len", "(", "div_calc", ")", ")", ",", "h", ")", "print", "\"p-value: {}\"", ".", "format", "(", "p", ")"], "docstring": "Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.", "docstring_tokens": ["Compute", "the", "Kruskal", "-", "Wallis", "H", "-", "test", "for", "independent", "samples", ".", "A", "typical", "rule", "is", "that", "each", "group", "must", "have", "at", "least", "5", "measurements", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L69-L84", "partition": "train", "up_fun_num": 3, "context": "#!/usr/bin/env python\n\"\"\"\nCalculate and plot alpha diversity for two or more sample categories.\n\"\"\"\nimport sys\nimport csv\nimport argparse\nimport os.path as osp\nfrom itertools import izip_longest\nfrom collections import defaultdict\nfrom phylotoast import graph_util as gu, util as putil\n\nimporterrors = []\ntry:\n    import biom\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    import scipy.stats as stats\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from skbio.diversity import alpha\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from matplotlib import pyplot as plt, gridspec\nexcept ImportError as ie:\n    importerrors.append(ie)\nif len(importerrors) != 0:\n    for item in importerrors:\n        print \"Import Error. Please install missing module:\", item\n    sys.exit()\n\n\ndef gather_samples(biomT):\n    return {sid: biomT.data(sid).astype(int) for sid in biomT.ids()}\n\n\ndef calc_diversity(method, parsed_mapf, biom, cats, cats_index):\n    counts = {cat: [] for cat in cats}\n    sample_ids = []\n\n    for sid, sample_counts in gather_samples(biom).items():\n        sample_ids.append(sid)\n        if sid in parsed_mapf:\n            counts[parsed_mapf[sid][cats_index]].append((sid, sample_counts))\n\n    div_calc = {\n        cat: {count[0]: method(count[1]) for count in counts}\n        for cat, counts in counts.items()\n    }\n\n    return div_calc, sample_ids\n\n\ndef print_MannWhitneyU(div_calc):\n    \"\"\"\n    Compute the Mann-Whitney U test for unequal group sample sizes.\n    \"\"\"\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return (\n            \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\n            \"significance testing.\"\n        )\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)\n\n\ndef plot_group_diversity(\n    diversities, grp_colors, title, diversity_type, out_dir, plot_ext\n):\n    fig_div = plt.figure(figsize=(21, 7))\n    grid = gridspec.GridSpec(1, 2)\n\n    # Disease States Shannon Diversity plots\n    ax_div = fig_div.add_subplot(grid[0, 0])\n\n    for i, grp in enumerate(diversities):\n        gu.plot_kde(diversities[grp].values(), ax_div, title, grp_colors[grp])\n\n    ax_div.set_xlabel(diversity_type.title())\n    ax_div.set_ylabel(\"Density\")\n    ax_div.legend(\n        [plt.Rectangle((0, 0), 1, 1, fc=color) for color in grp_colors.values()],\n        grp_colors.keys(),\n        loc=\"best\",\n    )\n\n    fig_div.savefig(\n        osp.join(out_dir, diversity_type + \".\" + plot_ext),\n        facecolor=\"white\",\n        edgecolor=\"none\",\n        bbox_inches=\"tight\",\n        pad_inches=0.2,\n    )\n\n\ndef write_diversity_metrics(data, sample_ids, fp=None):\n    \"\"\"\n    Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.\n    \"\"\"\n    if fp is None:\n        fp = \"./diversity_data.txt\"\n\n    with open(fp, \"w\") as outf:\n        out = csv.writer(outf, delimiter=\"\\t\")\n        out.writerow([\"SampleID\", \"Group\", \"Calculation\"])\n        for group, d in data.iteritems():\n            for sid, value in d.iteritems():\n                out.writerow([sid, group, value])\n\n\ndef handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\"\n    )\n    parser.add_argument(\"-m\", \"--map_file\", help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\", help=\"Path to the BIOM table\")\n    parser.add_argument(\n        \"-c\", \"--category\", help=\"Specific category from the mapping file.\"\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--diversity\",\n        default=[\"shannon\"],\n        nargs=\"+\",\n        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\",\n    )\n    parser.add_argument(\n        \"--x_label\",\n        default=[None],\n        nargs=\"+\",\n        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\",\n    )\n    parser.add_argument(\n        \"--color_by\",\n        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\",\n    )\n    parser.add_argument(\n        \"--plot_title\",\n        default=\"\",\n        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\",\n    )\n    parser.add_argument(\n        \"-o\", \"--output_dir\", default=\".\", help=\"The directory plots will be saved to.\"\n    )\n    parser.add_argument(\n        \"--image_type\",\n        default=\"png\",\n        help=\"The type of image to save: png, svg, pdf, eps, etc...\",\n    )\n    parser.add_argument(\n        \"--save_calculations\",\n        help=\"Path and name of text file to store the calculated \" \"diversity metrics.\",\n    )\n    parser.add_argument(\n        \"--suppress_stats\",\n        action=\"store_true\",\n        help=\"Do not display \"\n        \"significance testing results which are shown by default.\",\n    )\n    parser.add_argument(\n        \"--show_available_metrics\",\n        action=\"store_true\",\n        help=\"Supply this parameter to see which alpha diversity metrics \"\n        \" are available for usage. No calculations will be performed\"\n        \" if this parameter is provided.\",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    metrics = [m for m in alpha.__all__ if \"_ci\" not in m]\n    try:\n        metrics.remove(\"faith_pd\")\n    except ValueError:\n        pass\n    if args.show_available_metrics:\n        print \"\\nAvailable alpha diversity metrics:\"\n        return \"\\n\".join(metrics)\n\n    # check that the output dir exists, create it if not\n    msg = putil.ensure_dir(args.output_dir)\n    # if an error occurs, print and exit\n    if msg:\n        sys.exit(msg)\n\n    # parse mapping file\n    try:\n        header, sample_map = putil.parse_map_file(args.map_file)\n    except Exception as ioe:\n        err_msg = \"\\nError while processing the mapping file: {}\\n\"\n        sys.exit(err_msg.format(ioe))\n\n    # parse BIOM table\n    try:\n        biom_tbl = biom.load_table(args.biom_fp)\n    except Exception as ioe:\n        err_msg = \"\\nError loading BIOM table file: {}\\n\"\n        sys.exit(err_msg.format(ioe))\n\n    # group samples by category\n    if args.category not in header:\n        sys.exit(\"Category '{}' not found\".format(args.category))\n    cat_idx = header.index(args.category)\n    cat_vals = {entry[cat_idx] for entry in sample_map.values()}\n\n    plot_title = args.plot_title\n\n    colors = putil.color_mapping(sample_map, header, args.category, args.color_by)\n\n    # Perform diversity calculations and density plotting\n    for method, x_label in izip_longest(args.diversity, args.x_label):\n        if x_label is None:\n            x_label = method.title()\n        if method not in alpha.__all__:\n            sys.exit(\"ERROR: Diversity metric not found: {}.\".format(method))\n        elif method in alpha.__all__ and method not in metrics:\n            sys.exit(\"Currently, PhyloToAST does not support {} metric.\".format(method))\n        metric = eval(\"alpha.\" + method)\n        div_calc, sample_ids = calc_diversity(\n            metric, sample_map, biom_tbl, cat_vals, cat_idx\n        )\n\n        if args.save_calculations:\n            write_diversity_metrics(div_calc, sample_ids, args.save_calculations)\n\n        plot_group_diversity(\n            div_calc, colors, plot_title, x_label, args.output_dir, args.image_type\n        )\n\n        # calculate and print significance testing results\n        if not args.suppress_stats:\n            print \"Diversity significance testing: {}\".format(x_label)\n            if len(cat_vals) == 2:\n                print_MannWhitneyU(div_calc)\n            elif len(cat_vals) > 2:\n                print_KruskalWallisH(div_calc)\n            print\n        else:\n            continue\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import sys", "import csv", "import argparse", "import os.path as osp", "from itertools import izip_longest", "from collections import defaultdict", "from phylotoast import graph_util as gu, util as putil", "import biom", "import scipy.stats as stats", "from skbio.diversity import alpha", "from matplotlib import pyplot as plt, gridspec"], "function": ["def gather_samples(biomT):\n", "def calc_diversity(method, parsed_mapf, biom, cats, cats_index):\n", "def print_MannWhitneyU(div_calc):\n", "def write_diversity_metrics(data, sample_ids, fp=None):\n", "def handle_program_options():\n", "def main():\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "handle_program_options", "original_string": "def handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\")\n    parser.add_argument(\"-m\", \"--map_file\",\n                        help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\",\n                        help=\"Path to the BIOM table\")\n    parser.add_argument(\"-c\", \"--category\",\n                        help=\"Specific category from the mapping file.\")\n    parser.add_argument(\"-d\", \"--diversity\", default=[\"shannon\"], nargs=\"+\",\n                        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\")\n    parser.add_argument(\"--x_label\", default=[None], nargs=\"+\",\n                        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\")\n    parser.add_argument(\"--color_by\",\n                        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\")\n    parser.add_argument(\"--plot_title\", default=\"\",\n                        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\")\n    parser.add_argument(\"-o\", \"--output_dir\", default=\".\",\n                        help=\"The directory plots will be saved to.\")\n    parser.add_argument(\"--image_type\", default=\"png\",\n                        help=\"The type of image to save: png, svg, pdf, eps, etc...\")\n    parser.add_argument(\"--save_calculations\",\n                        help=\"Path and name of text file to store the calculated \"\n                        \"diversity metrics.\")\n    parser.add_argument(\"--suppress_stats\", action=\"store_true\", help=\"Do not display \"\n                        \"significance testing results which are shown by default.\")\n    parser.add_argument(\"--show_available_metrics\", action=\"store_true\",\n                        help=\"Supply this parameter to see which alpha diversity metrics \"\n                             \" are available for usage. No calculations will be performed\"\n                             \" if this parameter is provided.\")\n    return parser.parse_args()", "language": "python", "code": "def handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\")\n    parser.add_argument(\"-m\", \"--map_file\",\n                        help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\",\n                        help=\"Path to the BIOM table\")\n    parser.add_argument(\"-c\", \"--category\",\n                        help=\"Specific category from the mapping file.\")\n    parser.add_argument(\"-d\", \"--diversity\", default=[\"shannon\"], nargs=\"+\",\n                        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\")\n    parser.add_argument(\"--x_label\", default=[None], nargs=\"+\",\n                        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\")\n    parser.add_argument(\"--color_by\",\n                        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\")\n    parser.add_argument(\"--plot_title\", default=\"\",\n                        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\")\n    parser.add_argument(\"-o\", \"--output_dir\", default=\".\",\n                        help=\"The directory plots will be saved to.\")\n    parser.add_argument(\"--image_type\", default=\"png\",\n                        help=\"The type of image to save: png, svg, pdf, eps, etc...\")\n    parser.add_argument(\"--save_calculations\",\n                        help=\"Path and name of text file to store the calculated \"\n                        \"diversity metrics.\")\n    parser.add_argument(\"--suppress_stats\", action=\"store_true\", help=\"Do not display \"\n                        \"significance testing results which are shown by default.\")\n    parser.add_argument(\"--show_available_metrics\", action=\"store_true\",\n                        help=\"Supply this parameter to see which alpha diversity metrics \"\n                             \" are available for usage. No calculations will be performed\"\n                             \" if this parameter is provided.\")\n    return parser.parse_args()", "code_tokens": ["def", "handle_program_options", "(", ")", ":", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\"", ")", "parser", ".", "add_argument", "(", "\"-m\"", ",", "\"--map_file\"", ",", "help", "=", "\"QIIME mapping file.\"", ")", "parser", ".", "add_argument", "(", "\"-i\"", ",", "\"--biom_fp\"", ",", "help", "=", "\"Path to the BIOM table\"", ")", "parser", ".", "add_argument", "(", "\"-c\"", ",", "\"--category\"", ",", "help", "=", "\"Specific category from the mapping file.\"", ")", "parser", ".", "add_argument", "(", "\"-d\"", ",", "\"--diversity\"", ",", "default", "=", "[", "\"shannon\"", "]", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\"", ")", "parser", ".", "add_argument", "(", "\"--x_label\"", ",", "default", "=", "[", "None", "]", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\"", ")", "parser", ".", "add_argument", "(", "\"--color_by\"", ",", "help", "=", "\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\"", ")", "parser", ".", "add_argument", "(", "\"--plot_title\"", ",", "default", "=", "\"\"", ",", "help", "=", "\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\"", ")", "parser", ".", "add_argument", "(", "\"-o\"", ",", "\"--output_dir\"", ",", "default", "=", "\".\"", ",", "help", "=", "\"The directory plots will be saved to.\"", ")", "parser", ".", "add_argument", "(", "\"--image_type\"", ",", "default", "=", "\"png\"", ",", "help", "=", "\"The type of image to save: png, svg, pdf, eps, etc...\"", ")", "parser", ".", "add_argument", "(", "\"--save_calculations\"", ",", "help", "=", "\"Path and name of text file to store the calculated \"", "\"diversity metrics.\"", ")", "parser", ".", "add_argument", "(", "\"--suppress_stats\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Do not display \"", "\"significance testing results which are shown by default.\"", ")", "parser", ".", "add_argument", "(", "\"--show_available_metrics\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Supply this parameter to see which alpha diversity metrics \"", "\" are available for usage. No calculations will be performed\"", "\" if this parameter is provided.\"", ")", "return", "parser", ".", "parse_args", "(", ")"], "docstring": "Parses the given options passed in at the command line.", "docstring_tokens": ["Parses", "the", "given", "options", "passed", "in", "at", "the", "command", "line", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L122-L168", "partition": "train", "up_fun_num": 6, "context": "#!/usr/bin/env python\n\"\"\"\nCalculate and plot alpha diversity for two or more sample categories.\n\"\"\"\nimport sys\nimport csv\nimport argparse\nimport os.path as osp\nfrom itertools import izip_longest\nfrom collections import defaultdict\nfrom phylotoast import graph_util as gu, util as putil\n\nimporterrors = []\ntry:\n    import biom\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    import scipy.stats as stats\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from skbio.diversity import alpha\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from matplotlib import pyplot as plt, gridspec\nexcept ImportError as ie:\n    importerrors.append(ie)\nif len(importerrors) != 0:\n    for item in importerrors:\n        print \"Import Error. Please install missing module:\", item\n    sys.exit()\n\n\ndef gather_samples(biomT):\n    return {sid: biomT.data(sid).astype(int) for sid in biomT.ids()}\n\n\ndef calc_diversity(method, parsed_mapf, biom, cats, cats_index):\n    counts = {cat: [] for cat in cats}\n    sample_ids = []\n\n    for sid, sample_counts in gather_samples(biom).items():\n        sample_ids.append(sid)\n        if sid in parsed_mapf:\n            counts[parsed_mapf[sid][cats_index]].append((sid, sample_counts))\n\n    div_calc = {\n        cat: {count[0]: method(count[1]) for count in counts}\n        for cat, counts in counts.items()\n    }\n\n    return div_calc, sample_ids\n\n\ndef print_MannWhitneyU(div_calc):\n    \"\"\"\n    Compute the Mann-Whitney U test for unequal group sample sizes.\n    \"\"\"\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return (\n            \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\n            \"significance testing.\"\n        )\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)\n\n\ndef print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return (\n            \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\n            \"significance testing.\"\n        )\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(\n        str(len(div_calc)), h\n    )\n    print \"p-value: {}\".format(p)\n\n\ndef plot_group_diversity(\n    diversities, grp_colors, title, diversity_type, out_dir, plot_ext\n):\n    fig_div = plt.figure(figsize=(21, 7))\n    grid = gridspec.GridSpec(1, 2)\n\n    # Disease States Shannon Diversity plots\n    ax_div = fig_div.add_subplot(grid[0, 0])\n\n    for i, grp in enumerate(diversities):\n        gu.plot_kde(diversities[grp].values(), ax_div, title, grp_colors[grp])\n\n    ax_div.set_xlabel(diversity_type.title())\n    ax_div.set_ylabel(\"Density\")\n    ax_div.legend(\n        [plt.Rectangle((0, 0), 1, 1, fc=color) for color in grp_colors.values()],\n        grp_colors.keys(),\n        loc=\"best\",\n    )\n\n    fig_div.savefig(\n        osp.join(out_dir, diversity_type + \".\" + plot_ext),\n        facecolor=\"white\",\n        edgecolor=\"none\",\n        bbox_inches=\"tight\",\n        pad_inches=0.2,\n    )\n\n\ndef write_diversity_metrics(data, sample_ids, fp=None):\n    \"\"\"\n    Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.\n    \"\"\"\n    if fp is None:\n        fp = \"./diversity_data.txt\"\n\n    with open(fp, \"w\") as outf:\n        out = csv.writer(outf, delimiter=\"\\t\")\n        out.writerow([\"SampleID\", \"Group\", \"Calculation\"])\n        for group, d in data.iteritems():\n            for sid, value in d.iteritems():\n                out.writerow([sid, group, value])\n\n\ndef main():\n    args = handle_program_options()\n\n    metrics = [m for m in alpha.__all__ if \"_ci\" not in m]\n    try:\n        metrics.remove(\"faith_pd\")\n    except ValueError:\n        pass\n    if args.show_available_metrics:\n        print \"\\nAvailable alpha diversity metrics:\"\n        return \"\\n\".join(metrics)\n\n    # check that the output dir exists, create it if not\n    msg = putil.ensure_dir(args.output_dir)\n    # if an error occurs, print and exit\n    if msg:\n        sys.exit(msg)\n\n    # parse mapping file\n    try:\n        header, sample_map = putil.parse_map_file(args.map_file)\n    except Exception as ioe:\n        err_msg = \"\\nError while processing the mapping file: {}\\n\"\n        sys.exit(err_msg.format(ioe))\n\n    # parse BIOM table\n    try:\n        biom_tbl = biom.load_table(args.biom_fp)\n    except Exception as ioe:\n        err_msg = \"\\nError loading BIOM table file: {}\\n\"\n        sys.exit(err_msg.format(ioe))\n\n    # group samples by category\n    if args.category not in header:\n        sys.exit(\"Category '{}' not found\".format(args.category))\n    cat_idx = header.index(args.category)\n    cat_vals = {entry[cat_idx] for entry in sample_map.values()}\n\n    plot_title = args.plot_title\n\n    colors = putil.color_mapping(sample_map, header, args.category, args.color_by)\n\n    # Perform diversity calculations and density plotting\n    for method, x_label in izip_longest(args.diversity, args.x_label):\n        if x_label is None:\n            x_label = method.title()\n        if method not in alpha.__all__:\n            sys.exit(\"ERROR: Diversity metric not found: {}.\".format(method))\n        elif method in alpha.__all__ and method not in metrics:\n            sys.exit(\"Currently, PhyloToAST does not support {} metric.\".format(method))\n        metric = eval(\"alpha.\" + method)\n        div_calc, sample_ids = calc_diversity(\n            metric, sample_map, biom_tbl, cat_vals, cat_idx\n        )\n\n        if args.save_calculations:\n            write_diversity_metrics(div_calc, sample_ids, args.save_calculations)\n\n        plot_group_diversity(\n            div_calc, colors, plot_title, x_label, args.output_dir, args.image_type\n        )\n\n        # calculate and print significance testing results\n        if not args.suppress_stats:\n            print \"Diversity significance testing: {}\".format(x_label)\n            if len(cat_vals) == 2:\n                print_MannWhitneyU(div_calc)\n            elif len(cat_vals) > 2:\n                print_KruskalWallisH(div_calc)\n            print\n        else:\n            continue\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import sys", "import csv", "import argparse", "import os.path as osp", "from itertools import izip_longest", "from collections import defaultdict", "from phylotoast import graph_util as gu, util as putil", "import biom", "import scipy.stats as stats", "from skbio.diversity import alpha", "from matplotlib import pyplot as plt, gridspec"], "function": ["def gather_samples(biomT):\n", "def calc_diversity(method, parsed_mapf, biom, cats, cats_index):\n", "def print_MannWhitneyU(div_calc):\n", "def print_KruskalWallisH(div_calc):\n", "def write_diversity_metrics(data, sample_ids, fp=None):\n", "def main():\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/search.py", "func_name": "blastdb", "original_string": "def blastdb(fasta, maxfile = 10000000):\n    \"\"\"\n    make blast db\n    \"\"\"\n    db = fasta.rsplit('.', 1)[0]\n    type = check_type(fasta)\n    if type == 'nucl':\n        type = ['nhr', type]\n    else:\n        type = ['phr', type]\n    if os.path.exists('%s.%s' % (db, type[0])) is False \\\n            and os.path.exists('%s.00.%s' % (db, type[0])) is False:\n        print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)\n        os.system('makeblastdb \\\n                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \\\n                % (fasta, db, type[1], maxfile))\n    else:\n        print('# ... database found for: %s' % (fasta), file=sys.stderr)\n    return db", "language": "python", "code": "def blastdb(fasta, maxfile = 10000000):\n    \"\"\"\n    make blast db\n    \"\"\"\n    db = fasta.rsplit('.', 1)[0]\n    type = check_type(fasta)\n    if type == 'nucl':\n        type = ['nhr', type]\n    else:\n        type = ['phr', type]\n    if os.path.exists('%s.%s' % (db, type[0])) is False \\\n            and os.path.exists('%s.00.%s' % (db, type[0])) is False:\n        print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)\n        os.system('makeblastdb \\\n                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \\\n                % (fasta, db, type[1], maxfile))\n    else:\n        print('# ... database found for: %s' % (fasta), file=sys.stderr)\n    return db", "code_tokens": ["def", "blastdb", "(", "fasta", ",", "maxfile", "=", "10000000", ")", ":", "db", "=", "fasta", ".", "rsplit", "(", "'.'", ",", "1", ")", "[", "0", "]", "type", "=", "check_type", "(", "fasta", ")", "if", "type", "==", "'nucl'", ":", "type", "=", "[", "'nhr'", ",", "type", "]", "else", ":", "type", "=", "[", "'phr'", ",", "type", "]", "if", "os", ".", "path", ".", "exists", "(", "'%s.%s'", "%", "(", "db", ",", "type", "[", "0", "]", ")", ")", "is", "False", "and", "os", ".", "path", ".", "exists", "(", "'%s.00.%s'", "%", "(", "db", ",", "type", "[", "0", "]", ")", ")", "is", "False", ":", "print", "(", "'# ... making blastdb for: %s'", "%", "(", "fasta", ")", ",", "file", "=", "sys", ".", "stderr", ")", "os", ".", "system", "(", "'makeblastdb \\\n                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt'", "%", "(", "fasta", ",", "db", ",", "type", "[", "1", "]", ",", "maxfile", ")", ")", "else", ":", "print", "(", "'# ... database found for: %s'", "%", "(", "fasta", ")", ",", "file", "=", "sys", ".", "stderr", ")", "return", "db"], "docstring": "make blast db", "docstring_tokens": ["make", "blast", "db"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/search.py#L28-L46", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for searching a query fasta against a database using either\nusearch or blast\n\"\"\"\n\nimport sys\nimport os\nimport argparse\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef check_type(fasta):\n    nucl = [\"A\", \"T\", \"G\", \"C\"]\n    junk = [\"N\", \"U\", \".\", \"-\", \" \"]\n    type = \"nucl\"\n    for seq in parse_fasta(fasta):\n        seq = seq[1].upper()\n        for residue in seq:\n            if residue in junk:\n                continue\n            if residue not in nucl:\n                type = \"prot\"\n            break\n        break\n    return type\n\n\ndef usearchdb5(fasta):\n    \"\"\"\n    make usearch db\n    \"\"\"\n    if \".udb\" in fasta:\n        print(\"# ... database found: %s\" % (fasta), file=sys.stderr)\n        return db\n    type = check_type(fasta)\n    if type == \"nucl\":\n        type = [\"wdb\", type]\n    else:\n        type = [\"udb\", type]\n    db = \"%s.%s\" % (fasta.rsplit(\".\", 1)[0], type[0])\n    if os.path.exists(db) is False:\n        print(\"# ... making usearch db for: %s\" % (fasta), file=sys.stderr)\n        os.system(\"usearch -make%s %s -output %s >> log.txt\" % (type[0], fasta, db))\n    else:\n        print(\"# ... database found for: %s\" % (fasta), file=sys.stderr)\n    return db\n\n\ndef usearchdb(fasta, alignment=\"local\", usearch_loc=\"usearch\"):\n    \"\"\"\n    make usearch db\n    \"\"\"\n    if \".udb\" in fasta:\n        print(\"# ... database found: %s\" % (fasta), file=sys.stderr)\n        return fasta\n    type = check_type(fasta)\n    db = \"%s.%s.udb\" % (fasta.rsplit(\".\", 1)[0], type)\n    if os.path.exists(db) is False:\n        print(\"# ... making usearch db for: %s\" % (fasta), file=sys.stderr)\n        if alignment == \"local\":\n            os.system(\n                \"%s -makeudb_ublast %s -output %s >> log.txt\" % (usearch_loc, fasta, db)\n            )\n        elif alignment == \"global\":\n            os.system(\n                \"%s -makeudb_usearch %s -output %s >> log.txt\"\n                % (usearch_loc, fasta, db)\n            )\n    else:\n        print(\"# ... database found for: %s\" % (fasta), file=sys.stderr)\n    return db\n\n\ndef phmmer2blast(phmmer, out):\n    out = open(out, \"w\")\n    na = \"n/a\"\n    for line in open(phmmer):\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        na = \"n/a\"\n        if len(line) >= 6:\n            blast = [line[2], line[0], na, na, na, na, na, na, na, na, line[4], line[5]]\n            print(\"\\t\".join(blast), file=out)\n    out.close()\n\n\ndef phmmer(query, db, type, out, threads=\"4\", evalue=\"0.01\"):\n    \"\"\"\n    run phmmer\n    \"\"\"\n    if os.path.exists(out) is False:\n        print(\"# ... running phmmer with %s as query and %s as database\" % (query, db))\n        os.system(\n            \"phmmer -o %s.ph1 --tblout %s.ph2 --acc --noali --notextw -E %s --cpu %s %s %s\"\n            % (out, out, evalue, threads, query, db)\n        )\n    else:\n        print(\n            \"# ... phmmer output found for %s as query and %s as database\" % (query, db)\n        )\n    phmmer2blast(\"%s.ph2\" % out, out)\n\n\ndef blast(query, db, type, out, threads=\"4\", maxtargets=\"100\", megablast=False):\n    \"\"\"\n    run blast\n    \"\"\"\n    if os.path.exists(out) is False:\n        db = blastdb(db)  # make the database file, if necessary\n        print(\"# ... running blast with %s as query and %s as database\" % (query, db))\n        if type == \"nucl\":\n            blast = \"blastn\"\n            if megablast == True:\n                blast = \"blastn -task megablast\"\n        else:\n            blast = \"blastp\"\n        os.system(\n            \"%s \\\n                -query %s -db %s -out %s -outfmt 6 \\\n                -max_target_seqs %s -num_threads %s >> log.txt\"\n            % (blast, query, db, out, maxtargets, threads)\n        )\n    else:\n        print(\n            \"# ... blast output found for %s as query and %s as database\" % (query, db)\n        )\n\n\ndef usearch5(query, db, type, out, threads=\"4\", evalue=\"100\", alignment=\"local\"):\n    \"\"\"\n    run usearch\n    \"\"\"\n    if os.path.exists(out) is False:\n        print(\"# ... running usearch with %s as query and %s as database\" % (query, db))\n        if type[1] == \"nucl\":\n            threads = \"\"\n        else:\n            threads = \"-threads %s\" % (threads)\n        os.system(\n            \"usearch \\\n                -query %s -%s %s -blast6out %s \\\n                -evalue %s %s -%s >> log.txt\"\n            % (query, type[0], db, out, evalue, threads, alignment)\n        )\n    else:\n        print(\n            \"# ... usearch output found for %s as query and %s as database\"\n            % (query, db)\n        )\n\n\ndef usearch(\n    query,\n    db,\n    type,\n    out,\n    threads=\"6\",\n    evalue=\"100\",\n    alignment=\"local\",\n    max_hits=100,\n    cluster=False,\n):\n    \"\"\"\n    run usearch\n    \"\"\"\n    if \"usearch64\" in os.environ:\n        usearch_loc = os.environ[\"usearch64\"]\n    else:\n        usearch_loc = \"usearch\"\n    if os.path.exists(out) is False:\n        db = usearchdb(\n            db, alignment, usearch_loc\n        )  # make the database file, if neceesary\n        print(\n            \"# ... running usearch with %s as query and %s as database\" % (query, db),\n            file=sys.stderr,\n        )\n        if type == \"nucl\":\n            strand = \"-strand both\"\n        else:\n            strand = \"\"\n        if alignment == \"local\" and cluster is False:\n            os.system(\n                \"%s \\\n                    -ublast %s -db %s -blast6out %s \\\n                    -evalue %s -threads %s %s -maxhits %s >> log.txt\"\n                % (usearch_loc, query, db, out, evalue, threads, strand, max_hits)\n            )\n        elif alignment == \"global\" and cluster is False:\n            os.system(\n                \"%s \\\n                    -usearch_global %s -db %s -blast6out %s \\\n                    -id 0.10 -threads %s %s >> log.txt\"\n                % (usearch_loc, query, db, out, threads, strand)\n            )\n        elif alignment == \"local\" and cluster is True:\n            qsub = \"qsub -V -N usearch\"\n            os.system(\n                'echo \"%s -ublast `pwd`/%s -db %s -blast6out `pwd`/%s -evalue %s -threads %s %s -maxhits %s >> `pwd`/log.txt\" | %s'\n                % (usearch_loc, query, db, out, evalue, threads, strand, max_hits, qsub)\n            )\n        else:\n            print(\"specify local or global alignment\", file=sys.stderr)\n            exit()\n    else:\n        print(\n            \"# ... usearch output found for %s as query and %s as database\"\n            % (query, db),\n            file=sys.stderr,\n        )\n\n\ndef outfile(query, method, database, prefix):\n    type = check_type(query)\n    query = query.rsplit(\".\", 1)[0]\n    database = database.rsplit(\".\", 1)[0]\n    if \"/\" in query:\n        query = query.rsplit(\"/\", 1)[1]\n    if \"/\" in database:\n        database = database.rsplit(\"/\", 1)[1]\n    out = \"%s-%s_%s-%s.b6\" % (query, method.split(\"-\")[0], type, database)\n    if prefix is not False:\n        out = \"%s%s\" % (prefix, out)\n    return out, type\n\n\ndef search(\n    query,\n    database,\n    method=\"usearch\",\n    alignment=\"local\",\n    max_hits=100,\n    threads=\"6\",\n    prefix=False,\n):\n    out, type = outfile(query, method, database, prefix)\n    if method == \"usearch\":\n        usearch(\n            query,\n            database,\n            type,\n            out,\n            alignment=alignment,\n            max_hits=max_hits,\n            threads=threads,\n        )\n    elif method == \"usearch-cluster\":\n        usearch(\n            query,\n            database,\n            type,\n            out,\n            alignment=alignment,\n            max_hits=max_hits,\n            threads=threads,\n            cluster=True,\n        )\n    elif method == \"blast\":\n        blast(query, database, type, out, threads=threads)\n    elif method == \"phmmer\":\n        phmmer(query, database, type, out, threads=threads)\n    return out\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# search sequences against database\")\n    parser.add_argument(\"-q\", required=True, help=\"query\")\n    parser.add_argument(\"-d\", required=True, help=\"database\")\n    parser.add_argument(\n        \"-a\",\n        default=\"usearch\",\n        help=\"algorithm: usearch (default), usearch-cluster, blast, phmmer\",\n    )\n    parser.add_argument(\n        \"-m\",\n        required=False,\n        default=100,\n        type=int,\n        help=\"max. number of hits (default = 100)\",\n    )\n    parser.add_argument(\"-t\", default=\"6\", help=\"threads (default = 6)\")\n    args = vars(parser.parse_args())\n    threads, query, database, method = args[\"t\"], args[\"q\"], args[\"d\"], args[\"a\"]\n    if method != \"usearch-cluster\":\n        os.system(\"cat %s\" % (search(query, database, method, threads=threads)))\n    else:\n        search(query, database, method, threads=\"48\")\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import sys", "import os", "import argparse", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def check_type(fasta):\n", "def usearchdb5(fasta):\n", "def usearchdb(fasta, alignment=\"local\", usearch_loc=\"usearch\"):\n", "def phmmer2blast(phmmer, out):\n", "def phmmer(query, db, type, out, threads=\"4\", evalue=\"0.01\"):\n", "def blast(query, db, type, out, threads=\"4\", maxtargets=\"100\", megablast=False):\n", "def usearch5(query, db, type, out, threads=\"4\", evalue=\"100\", alignment=\"local\"):\n", "def outfile(query, method, database, prefix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/search.py", "func_name": "usearchdb", "original_string": "def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):\n    \"\"\"\n    make usearch db\n    \"\"\"\n    if '.udb' in fasta:\n        print('# ... database found: %s' % (fasta), file=sys.stderr)\n        return fasta\n    type = check_type(fasta)\n    db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)\n    if os.path.exists(db) is False:\n        print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)\n        if alignment == 'local':\n            os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))\n        elif alignment == 'global':\n            os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))\n    else:\n        print('# ... database found for: %s' % (fasta), file=sys.stderr)\n    return db", "language": "python", "code": "def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):\n    \"\"\"\n    make usearch db\n    \"\"\"\n    if '.udb' in fasta:\n        print('# ... database found: %s' % (fasta), file=sys.stderr)\n        return fasta\n    type = check_type(fasta)\n    db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)\n    if os.path.exists(db) is False:\n        print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)\n        if alignment == 'local':\n            os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))\n        elif alignment == 'global':\n            os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))\n    else:\n        print('# ... database found for: %s' % (fasta), file=sys.stderr)\n    return db", "code_tokens": ["def", "usearchdb", "(", "fasta", ",", "alignment", "=", "'local'", ",", "usearch_loc", "=", "'usearch'", ")", ":", "if", "'.udb'", "in", "fasta", ":", "print", "(", "'# ... database found: %s'", "%", "(", "fasta", ")", ",", "file", "=", "sys", ".", "stderr", ")", "return", "fasta", "type", "=", "check_type", "(", "fasta", ")", "db", "=", "'%s.%s.udb'", "%", "(", "fasta", ".", "rsplit", "(", "'.'", ",", "1", ")", "[", "0", "]", ",", "type", ")", "if", "os", ".", "path", ".", "exists", "(", "db", ")", "is", "False", ":", "print", "(", "'# ... making usearch db for: %s'", "%", "(", "fasta", ")", ",", "file", "=", "sys", ".", "stderr", ")", "if", "alignment", "==", "'local'", ":", "os", ".", "system", "(", "'%s -makeudb_ublast %s -output %s >> log.txt'", "%", "(", "usearch_loc", ",", "fasta", ",", "db", ")", ")", "elif", "alignment", "==", "'global'", ":", "os", ".", "system", "(", "'%s -makeudb_usearch %s -output %s >> log.txt'", "%", "(", "usearch_loc", ",", "fasta", ",", "db", ")", ")", "else", ":", "print", "(", "'# ... database found for: %s'", "%", "(", "fasta", ")", ",", "file", "=", "sys", ".", "stderr", ")", "return", "db"], "docstring": "make usearch db", "docstring_tokens": ["make", "usearch", "db"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/search.py#L68-L85", "partition": "train", "up_fun_num": 3, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for searching a query fasta against a database using either\nusearch or blast\n\"\"\"\n\nimport sys\nimport os\nimport argparse\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef check_type(fasta):\n    nucl = [\"A\", \"T\", \"G\", \"C\"]\n    junk = [\"N\", \"U\", \".\", \"-\", \" \"]\n    type = \"nucl\"\n    for seq in parse_fasta(fasta):\n        seq = seq[1].upper()\n        for residue in seq:\n            if residue in junk:\n                continue\n            if residue not in nucl:\n                type = \"prot\"\n            break\n        break\n    return type\n\n\ndef blastdb(fasta, maxfile=10000000):\n    \"\"\"\n    make blast db\n    \"\"\"\n    db = fasta.rsplit(\".\", 1)[0]\n    type = check_type(fasta)\n    if type == \"nucl\":\n        type = [\"nhr\", type]\n    else:\n        type = [\"phr\", type]\n    if (\n        os.path.exists(\"%s.%s\" % (db, type[0])) is False\n        and os.path.exists(\"%s.00.%s\" % (db, type[0])) is False\n    ):\n        print(\"# ... making blastdb for: %s\" % (fasta), file=sys.stderr)\n        os.system(\n            \"makeblastdb \\\n                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt\"\n            % (fasta, db, type[1], maxfile)\n        )\n    else:\n        print(\"# ... database found for: %s\" % (fasta), file=sys.stderr)\n    return db\n\n\ndef usearchdb5(fasta):\n    \"\"\"\n    make usearch db\n    \"\"\"\n    if \".udb\" in fasta:\n        print(\"# ... database found: %s\" % (fasta), file=sys.stderr)\n        return db\n    type = check_type(fasta)\n    if type == \"nucl\":\n        type = [\"wdb\", type]\n    else:\n        type = [\"udb\", type]\n    db = \"%s.%s\" % (fasta.rsplit(\".\", 1)[0], type[0])\n    if os.path.exists(db) is False:\n        print(\"# ... making usearch db for: %s\" % (fasta), file=sys.stderr)\n        os.system(\"usearch -make%s %s -output %s >> log.txt\" % (type[0], fasta, db))\n    else:\n        print(\"# ... database found for: %s\" % (fasta), file=sys.stderr)\n    return db\n\n\ndef phmmer2blast(phmmer, out):\n    out = open(out, \"w\")\n    na = \"n/a\"\n    for line in open(phmmer):\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        na = \"n/a\"\n        if len(line) >= 6:\n            blast = [line[2], line[0], na, na, na, na, na, na, na, na, line[4], line[5]]\n            print(\"\\t\".join(blast), file=out)\n    out.close()\n\n\ndef phmmer(query, db, type, out, threads=\"4\", evalue=\"0.01\"):\n    \"\"\"\n    run phmmer\n    \"\"\"\n    if os.path.exists(out) is False:\n        print(\"# ... running phmmer with %s as query and %s as database\" % (query, db))\n        os.system(\n            \"phmmer -o %s.ph1 --tblout %s.ph2 --acc --noali --notextw -E %s --cpu %s %s %s\"\n            % (out, out, evalue, threads, query, db)\n        )\n    else:\n        print(\n            \"# ... phmmer output found for %s as query and %s as database\" % (query, db)\n        )\n    phmmer2blast(\"%s.ph2\" % out, out)\n\n\ndef blast(query, db, type, out, threads=\"4\", maxtargets=\"100\", megablast=False):\n    \"\"\"\n    run blast\n    \"\"\"\n    if os.path.exists(out) is False:\n        db = blastdb(db)  # make the database file, if necessary\n        print(\"# ... running blast with %s as query and %s as database\" % (query, db))\n        if type == \"nucl\":\n            blast = \"blastn\"\n            if megablast == True:\n                blast = \"blastn -task megablast\"\n        else:\n            blast = \"blastp\"\n        os.system(\n            \"%s \\\n                -query %s -db %s -out %s -outfmt 6 \\\n                -max_target_seqs %s -num_threads %s >> log.txt\"\n            % (blast, query, db, out, maxtargets, threads)\n        )\n    else:\n        print(\n            \"# ... blast output found for %s as query and %s as database\" % (query, db)\n        )\n\n\ndef usearch5(query, db, type, out, threads=\"4\", evalue=\"100\", alignment=\"local\"):\n    \"\"\"\n    run usearch\n    \"\"\"\n    if os.path.exists(out) is False:\n        print(\"# ... running usearch with %s as query and %s as database\" % (query, db))\n        if type[1] == \"nucl\":\n            threads = \"\"\n        else:\n            threads = \"-threads %s\" % (threads)\n        os.system(\n            \"usearch \\\n                -query %s -%s %s -blast6out %s \\\n                -evalue %s %s -%s >> log.txt\"\n            % (query, type[0], db, out, evalue, threads, alignment)\n        )\n    else:\n        print(\n            \"# ... usearch output found for %s as query and %s as database\"\n            % (query, db)\n        )\n\n\ndef usearch(\n    query,\n    db,\n    type,\n    out,\n    threads=\"6\",\n    evalue=\"100\",\n    alignment=\"local\",\n    max_hits=100,\n    cluster=False,\n):\n    \"\"\"\n    run usearch\n    \"\"\"\n    if \"usearch64\" in os.environ:\n        usearch_loc = os.environ[\"usearch64\"]\n    else:\n        usearch_loc = \"usearch\"\n    if os.path.exists(out) is False:\n        db = usearchdb(\n            db, alignment, usearch_loc\n        )  # make the database file, if neceesary\n        print(\n            \"# ... running usearch with %s as query and %s as database\" % (query, db),\n            file=sys.stderr,\n        )\n        if type == \"nucl\":\n            strand = \"-strand both\"\n        else:\n            strand = \"\"\n        if alignment == \"local\" and cluster is False:\n            os.system(\n                \"%s \\\n                    -ublast %s -db %s -blast6out %s \\\n                    -evalue %s -threads %s %s -maxhits %s >> log.txt\"\n                % (usearch_loc, query, db, out, evalue, threads, strand, max_hits)\n            )\n        elif alignment == \"global\" and cluster is False:\n            os.system(\n                \"%s \\\n                    -usearch_global %s -db %s -blast6out %s \\\n                    -id 0.10 -threads %s %s >> log.txt\"\n                % (usearch_loc, query, db, out, threads, strand)\n            )\n        elif alignment == \"local\" and cluster is True:\n            qsub = \"qsub -V -N usearch\"\n            os.system(\n                'echo \"%s -ublast `pwd`/%s -db %s -blast6out `pwd`/%s -evalue %s -threads %s %s -maxhits %s >> `pwd`/log.txt\" | %s'\n                % (usearch_loc, query, db, out, evalue, threads, strand, max_hits, qsub)\n            )\n        else:\n            print(\"specify local or global alignment\", file=sys.stderr)\n            exit()\n    else:\n        print(\n            \"# ... usearch output found for %s as query and %s as database\"\n            % (query, db),\n            file=sys.stderr,\n        )\n\n\ndef outfile(query, method, database, prefix):\n    type = check_type(query)\n    query = query.rsplit(\".\", 1)[0]\n    database = database.rsplit(\".\", 1)[0]\n    if \"/\" in query:\n        query = query.rsplit(\"/\", 1)[1]\n    if \"/\" in database:\n        database = database.rsplit(\"/\", 1)[1]\n    out = \"%s-%s_%s-%s.b6\" % (query, method.split(\"-\")[0], type, database)\n    if prefix is not False:\n        out = \"%s%s\" % (prefix, out)\n    return out, type\n\n\ndef search(\n    query,\n    database,\n    method=\"usearch\",\n    alignment=\"local\",\n    max_hits=100,\n    threads=\"6\",\n    prefix=False,\n):\n    out, type = outfile(query, method, database, prefix)\n    if method == \"usearch\":\n        usearch(\n            query,\n            database,\n            type,\n            out,\n            alignment=alignment,\n            max_hits=max_hits,\n            threads=threads,\n        )\n    elif method == \"usearch-cluster\":\n        usearch(\n            query,\n            database,\n            type,\n            out,\n            alignment=alignment,\n            max_hits=max_hits,\n            threads=threads,\n            cluster=True,\n        )\n    elif method == \"blast\":\n        blast(query, database, type, out, threads=threads)\n    elif method == \"phmmer\":\n        phmmer(query, database, type, out, threads=threads)\n    return out\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# search sequences against database\")\n    parser.add_argument(\"-q\", required=True, help=\"query\")\n    parser.add_argument(\"-d\", required=True, help=\"database\")\n    parser.add_argument(\n        \"-a\",\n        default=\"usearch\",\n        help=\"algorithm: usearch (default), usearch-cluster, blast, phmmer\",\n    )\n    parser.add_argument(\n        \"-m\",\n        required=False,\n        default=100,\n        type=int,\n        help=\"max. number of hits (default = 100)\",\n    )\n    parser.add_argument(\"-t\", default=\"6\", help=\"threads (default = 6)\")\n    args = vars(parser.parse_args())\n    threads, query, database, method = args[\"t\"], args[\"q\"], args[\"d\"], args[\"a\"]\n    if method != \"usearch-cluster\":\n        os.system(\"cat %s\" % (search(query, database, method, threads=threads)))\n    else:\n        search(query, database, method, threads=\"48\")\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import sys", "import os", "import argparse", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def check_type(fasta):\n", "def blastdb(fasta, maxfile=10000000):\n", "def usearchdb5(fasta):\n", "def phmmer2blast(phmmer, out):\n", "def phmmer(query, db, type, out, threads=\"4\", evalue=\"0.01\"):\n", "def blast(query, db, type, out, threads=\"4\", maxtargets=\"100\", megablast=False):\n", "def usearch5(query, db, type, out, threads=\"4\", evalue=\"100\", alignment=\"local\"):\n", "def outfile(query, method, database, prefix):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/control.py", "func_name": "_pp", "original_string": "def _pp(dict_data):\n    \"\"\"Pretty print.\"\"\"\n    for key, val in dict_data.items():\n        # pylint: disable=superfluous-parens\n        print('{0:<11}: {1}'.format(key, val))", "language": "python", "code": "def _pp(dict_data):\n    \"\"\"Pretty print.\"\"\"\n    for key, val in dict_data.items():\n        # pylint: disable=superfluous-parens\n        print('{0:<11}: {1}'.format(key, val))", "code_tokens": ["def", "_pp", "(", "dict_data", ")", ":", "for", "key", ",", "val", "in", "dict_data", ".", "items", "(", ")", ":", "# pylint: disable=superfluous-parens", "print", "(", "'{0:<11}: {1}'", ".", "format", "(", "key", ",", "val", ")", ")"], "docstring": "Pretty print.", "docstring_tokens": ["Pretty", "print", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L11-L15", "partition": "train", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.control.\"\"\"\nimport os\nimport sys\nfrom bootstrap_py import package, pypi\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.exceptions import Conflict\n\n\ndef retreive_metadata():\n    \"\"\"Retrieve metadata.\n\n    :rtype: bootstrap_py.classifiers.Classifiers\n    :return: Classifiers()\n    \"\"\"\n    return Classifiers()\n\n\ndef print_licences(params, metadata):\n    \"\"\"Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata\n    \"\"\"\n    if hasattr(params, \"licenses\"):\n        if params.licenses:\n            _pp(metadata.licenses_desc())\n        sys.exit(0)\n\n\ndef check_repository_existence(params):\n    \"\"\"Check repository existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict('Package repository \"{0}\" has already exists.'.format(repodir))\n\n\ndef check_package_existence(params):\n    \"\"\"Check package existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    if not params.no_check:\n        pypi.package_existent(params.name)\n\n\ndef generate_package(params):\n    \"\"\"Generate package repository.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    pkg_data = package.PackageData(params)\n    pkg_tree = package.PackageTree(pkg_data)\n    pkg_tree.generate()\n    pkg_tree.move()\n    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)\n", "levels": [0, 0, 0, 0, 0], "package": ["import os", "import sys", "from bootstrap_py import package, pypi", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.exceptions import Conflict"], "function": ["def retreive_metadata():\n", "def print_licences(params, metadata):\n", "def check_repository_existence(params):\n", "def check_package_existence(params):\n", "def generate_package(params):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/control.py", "func_name": "print_licences", "original_string": "def print_licences(params, metadata):\n    \"\"\"Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata\n    \"\"\"\n    if hasattr(params, 'licenses'):\n        if params.licenses:\n            _pp(metadata.licenses_desc())\n        sys.exit(0)", "language": "python", "code": "def print_licences(params, metadata):\n    \"\"\"Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata\n    \"\"\"\n    if hasattr(params, 'licenses'):\n        if params.licenses:\n            _pp(metadata.licenses_desc())\n        sys.exit(0)", "code_tokens": ["def", "print_licences", "(", "params", ",", "metadata", ")", ":", "if", "hasattr", "(", "params", ",", "'licenses'", ")", ":", "if", "params", ".", "licenses", ":", "_pp", "(", "metadata", ".", "licenses_desc", "(", ")", ")", "sys", ".", "exit", "(", "0", ")"], "docstring": "Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata", "docstring_tokens": ["Print", "licenses", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L27-L36", "partition": "train", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.control.\"\"\"\nimport os\nimport sys\nfrom bootstrap_py import package, pypi\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.exceptions import Conflict\n\n\ndef _pp(dict_data):\n    \"\"\"Pretty print.\"\"\"\n    for key, val in dict_data.items():\n        # pylint: disable=superfluous-parens\n        print(\"{0:<11}: {1}\".format(key, val))\n\n\ndef retreive_metadata():\n    \"\"\"Retrieve metadata.\n\n    :rtype: bootstrap_py.classifiers.Classifiers\n    :return: Classifiers()\n    \"\"\"\n    return Classifiers()\n\n\ndef check_repository_existence(params):\n    \"\"\"Check repository existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict('Package repository \"{0}\" has already exists.'.format(repodir))\n\n\ndef check_package_existence(params):\n    \"\"\"Check package existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    if not params.no_check:\n        pypi.package_existent(params.name)\n\n\ndef generate_package(params):\n    \"\"\"Generate package repository.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    pkg_data = package.PackageData(params)\n    pkg_tree = package.PackageTree(pkg_data)\n    pkg_tree.generate()\n    pkg_tree.move()\n    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)\n", "levels": [0, 0, 0, 0, 0], "package": ["import os", "import sys", "from bootstrap_py import package, pypi", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.exceptions import Conflict"], "function": ["def _pp(dict_data):\n", "def retreive_metadata():\n", "def check_repository_existence(params):\n", "def check_package_existence(params):\n", "def generate_package(params):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/control.py", "func_name": "check_repository_existence", "original_string": "def check_repository_existence(params):\n    \"\"\"Check repository existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict(\n            'Package repository \"{0}\" has already exists.'.format(repodir))", "language": "python", "code": "def check_repository_existence(params):\n    \"\"\"Check repository existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict(\n            'Package repository \"{0}\" has already exists.'.format(repodir))", "code_tokens": ["def", "check_repository_existence", "(", "params", ")", ":", "repodir", "=", "os", ".", "path", ".", "join", "(", "params", ".", "outdir", ",", "params", ".", "name", ")", "if", "os", ".", "path", ".", "isdir", "(", "repodir", ")", ":", "raise", "Conflict", "(", "'Package repository \"{0}\" has already exists.'", ".", "format", "(", "repodir", ")", ")"], "docstring": "Check repository existence.\n\n    :param argparse.Namespace params: parameters", "docstring_tokens": ["Check", "repository", "existence", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L39-L47", "partition": "train", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.control.\"\"\"\nimport os\nimport sys\nfrom bootstrap_py import package, pypi\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.exceptions import Conflict\n\n\ndef _pp(dict_data):\n    \"\"\"Pretty print.\"\"\"\n    for key, val in dict_data.items():\n        # pylint: disable=superfluous-parens\n        print(\"{0:<11}: {1}\".format(key, val))\n\n\ndef retreive_metadata():\n    \"\"\"Retrieve metadata.\n\n    :rtype: bootstrap_py.classifiers.Classifiers\n    :return: Classifiers()\n    \"\"\"\n    return Classifiers()\n\n\ndef print_licences(params, metadata):\n    \"\"\"Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata\n    \"\"\"\n    if hasattr(params, \"licenses\"):\n        if params.licenses:\n            _pp(metadata.licenses_desc())\n        sys.exit(0)\n\n\ndef check_package_existence(params):\n    \"\"\"Check package existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    if not params.no_check:\n        pypi.package_existent(params.name)\n\n\ndef generate_package(params):\n    \"\"\"Generate package repository.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    pkg_data = package.PackageData(params)\n    pkg_tree = package.PackageTree(pkg_data)\n    pkg_tree.generate()\n    pkg_tree.move()\n    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)\n", "levels": [0, 0, 0, 0, 0], "package": ["import os", "import sys", "from bootstrap_py import package, pypi", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.exceptions import Conflict"], "function": ["def _pp(dict_data):\n", "def retreive_metadata():\n", "def print_licences(params, metadata):\n", "def check_package_existence(params):\n", "def generate_package(params):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/control.py", "func_name": "generate_package", "original_string": "def generate_package(params):\n    \"\"\"Generate package repository.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    pkg_data = package.PackageData(params)\n    pkg_tree = package.PackageTree(pkg_data)\n    pkg_tree.generate()\n    pkg_tree.move()\n    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)", "language": "python", "code": "def generate_package(params):\n    \"\"\"Generate package repository.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    pkg_data = package.PackageData(params)\n    pkg_tree = package.PackageTree(pkg_data)\n    pkg_tree.generate()\n    pkg_tree.move()\n    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)", "code_tokens": ["def", "generate_package", "(", "params", ")", ":", "pkg_data", "=", "package", ".", "PackageData", "(", "params", ")", "pkg_tree", "=", "package", ".", "PackageTree", "(", "pkg_data", ")", "pkg_tree", ".", "generate", "(", ")", "pkg_tree", ".", "move", "(", ")", "VCS", "(", "os", ".", "path", ".", "join", "(", "pkg_tree", ".", "outdir", ",", "pkg_tree", ".", "name", ")", ",", "pkg_tree", ".", "pkg_data", ")"], "docstring": "Generate package repository.\n\n    :param argparse.Namespace params: parameters", "docstring_tokens": ["Generate", "package", "repository", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/control.py#L59-L68", "partition": "train", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.control.\"\"\"\nimport os\nimport sys\nfrom bootstrap_py import package, pypi\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.exceptions import Conflict\n\n\ndef _pp(dict_data):\n    \"\"\"Pretty print.\"\"\"\n    for key, val in dict_data.items():\n        # pylint: disable=superfluous-parens\n        print(\"{0:<11}: {1}\".format(key, val))\n\n\ndef retreive_metadata():\n    \"\"\"Retrieve metadata.\n\n    :rtype: bootstrap_py.classifiers.Classifiers\n    :return: Classifiers()\n    \"\"\"\n    return Classifiers()\n\n\ndef print_licences(params, metadata):\n    \"\"\"Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata\n    \"\"\"\n    if hasattr(params, \"licenses\"):\n        if params.licenses:\n            _pp(metadata.licenses_desc())\n        sys.exit(0)\n\n\ndef check_repository_existence(params):\n    \"\"\"Check repository existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict('Package repository \"{0}\" has already exists.'.format(repodir))\n\n\ndef check_package_existence(params):\n    \"\"\"Check package existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    if not params.no_check:\n        pypi.package_existent(params.name)\n", "levels": [0, 0, 0, 0, 0], "package": ["import os", "import sys", "from bootstrap_py import package, pypi", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.exceptions import Conflict"], "function": ["def _pp(dict_data):\n", "def retreive_metadata():\n", "def print_licences(params, metadata):\n", "def check_repository_existence(params):\n", "def check_package_existence(params):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/sam2fastq.py", "func_name": "print_single", "original_string": "def print_single(line, rev):\n    \"\"\"\n    print single reads to stderr\n    \"\"\"\n    if rev is True:\n        seq = rc(['', line[9]])[1]\n        qual = line[10][::-1]\n    else:\n        seq = line[9]\n        qual = line[10]\n    fq = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n    print('\\n'.join(fq), file = sys.stderr)", "language": "python", "code": "def print_single(line, rev):\n    \"\"\"\n    print single reads to stderr\n    \"\"\"\n    if rev is True:\n        seq = rc(['', line[9]])[1]\n        qual = line[10][::-1]\n    else:\n        seq = line[9]\n        qual = line[10]\n    fq = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n    print('\\n'.join(fq), file = sys.stderr)", "code_tokens": ["def", "print_single", "(", "line", ",", "rev", ")", ":", "if", "rev", "is", "True", ":", "seq", "=", "rc", "(", "[", "''", ",", "line", "[", "9", "]", "]", ")", "[", "1", "]", "qual", "=", "line", "[", "10", "]", "[", ":", ":", "-", "1", "]", "else", ":", "seq", "=", "line", "[", "9", "]", "qual", "=", "line", "[", "10", "]", "fq", "=", "[", "'@%s'", "%", "line", "[", "0", "]", ",", "seq", ",", "'+%s'", "%", "line", "[", "0", "]", ",", "qual", "]", "print", "(", "'\\n'", ".", "join", "(", "fq", ")", ",", "file", "=", "sys", ".", "stderr", ")"], "docstring": "print single reads to stderr", "docstring_tokens": ["print", "single", "reads", "to", "stderr"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/sam2fastq.py#L13-L24", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nconvert sam file to fastq file\n\"\"\"\n\nimport sys\nimport os\n\n# ctb\nfrom ctbBio.rc import reverse_complement as rc\n\n\ndef sam2fastq(sam, singles=False, force=False):\n    \"\"\"\n    convert sam to fastq\n    \"\"\"\n    L, R = None, None\n    for line in sam:\n        if line.startswith(\"@\") is True:\n            continue\n        line = line.strip().split()\n        bit = [\n            True if i == \"1\" else False for i in bin(int(line[1])).split(\"b\")[1][::-1]\n        ]\n        while len(bit) < 8:\n            bit.append(False)\n        pair, proper, na, nap, rev, mrev, left, right = bit\n        # make sure read is paired\n        if pair is False:\n            if singles is True:\n                print_single(line, rev)\n            continue\n        # check if sequence is reverse-complemented\n        if rev is True:\n            seq = rc([\"\", line[9]])[1]\n            qual = line[10][::-1]\n        else:\n            seq = line[9]\n            qual = line[10]\n        # check if read is forward or reverse, return when both have been found\n        if left is True:\n            if L is not None and force is False:\n                print(\"sam file is not sorted\", file=sys.stderr)\n                print(\"\\te.g.: %s\" % (line[0]), file=sys.stderr)\n                exit()\n            if L is not None:\n                L = None\n                continue\n            L = [\"@%s\" % line[0], seq, \"+%s\" % line[0], qual]\n            if R is not None:\n                yield L\n                yield R\n                L, R = None, None\n        if right is True:\n            if R is not None and force is False:\n                print(\"sam file is not sorted\", file=sys.stderr)\n                print(\"\\te.g.: %s\" % (line[0]), file=sys.stderr)\n                exit()\n            if R is not None:\n                R = None\n                continue\n            R = [\"@%s\" % line[0], seq, \"+%s\" % line[0], qual]\n            if L is not None:\n                yield L\n                yield R\n                L, R = None, None\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        print(\n            \"usage: sam2fastq.py <mapping.sam> <singles to stderr?: True or False> <force exclude unpaired paired reads: True or False)>\",\n            file=sys.stderr,\n        )\n        exit()\n    sam, singles, force = sys.argv[1], sys.argv[2], sys.argv[3]\n    if sam == \"-\":\n        sam = sys.stdin\n    else:\n        sam = open(sam)\n    if singles == \"True\":\n        singles = True\n    else:\n        singles = False\n    if force == \"True\":\n        force = True\n    else:\n        force = False\n    for seq in sam2fastq(sam, singles, force):\n        print(\"\\n\".join(seq))\n", "levels": [0], "package": ["import sys", "import os", "from ctbBio.rc import reverse_complement as rc"], "function": ["def sam2fastq(sam, singles=False, force=False):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/sam2fastq.py", "func_name": "sam2fastq", "original_string": "def sam2fastq(sam, singles = False, force = False):\n    \"\"\"\n    convert sam to fastq\n    \"\"\"\n    L, R = None, None\n    for line in sam:\n        if line.startswith('@') is True:\n            continue\n        line = line.strip().split()\n        bit = [True if i == '1' else False \\\n                for i in bin(int(line[1])).split('b')[1][::-1]]\n        while len(bit) < 8:\n            bit.append(False)\n        pair, proper, na, nap, rev, mrev, left, right = bit\n        # make sure read is paired\n        if pair is False:\n            if singles is True:\n                print_single(line, rev)\n            continue\n        # check if sequence is reverse-complemented\n        if rev is True:\n            seq = rc(['', line[9]])[1]\n            qual = line[10][::-1]\n        else:\n            seq = line[9]\n            qual = line[10]\n        # check if read is forward or reverse, return when both have been found\n        if left is True:\n            if L is not None and force is False:\n                print('sam file is not sorted', file = sys.stderr)\n                print('\\te.g.: %s' % (line[0]), file = sys.stderr)\n                exit()\n            if L is not None:\n                L = None\n                continue\n            L = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n            if R is not None:\n                yield L\n                yield R\n                L, R = None, None\n        if right is True:\n            if R is not None and force is False:\n                print('sam file is not sorted', file = sys.stderr)\n                print('\\te.g.: %s' % (line[0]), file = sys.stderr)\n                exit()\n            if R is not None:\n                R = None\n                continue\n            R = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n            if L is not None:\n                yield L\n                yield R\n                L, R = None, None", "language": "python", "code": "def sam2fastq(sam, singles = False, force = False):\n    \"\"\"\n    convert sam to fastq\n    \"\"\"\n    L, R = None, None\n    for line in sam:\n        if line.startswith('@') is True:\n            continue\n        line = line.strip().split()\n        bit = [True if i == '1' else False \\\n                for i in bin(int(line[1])).split('b')[1][::-1]]\n        while len(bit) < 8:\n            bit.append(False)\n        pair, proper, na, nap, rev, mrev, left, right = bit\n        # make sure read is paired\n        if pair is False:\n            if singles is True:\n                print_single(line, rev)\n            continue\n        # check if sequence is reverse-complemented\n        if rev is True:\n            seq = rc(['', line[9]])[1]\n            qual = line[10][::-1]\n        else:\n            seq = line[9]\n            qual = line[10]\n        # check if read is forward or reverse, return when both have been found\n        if left is True:\n            if L is not None and force is False:\n                print('sam file is not sorted', file = sys.stderr)\n                print('\\te.g.: %s' % (line[0]), file = sys.stderr)\n                exit()\n            if L is not None:\n                L = None\n                continue\n            L = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n            if R is not None:\n                yield L\n                yield R\n                L, R = None, None\n        if right is True:\n            if R is not None and force is False:\n                print('sam file is not sorted', file = sys.stderr)\n                print('\\te.g.: %s' % (line[0]), file = sys.stderr)\n                exit()\n            if R is not None:\n                R = None\n                continue\n            R = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n            if L is not None:\n                yield L\n                yield R\n                L, R = None, None", "code_tokens": ["def", "sam2fastq", "(", "sam", ",", "singles", "=", "False", ",", "force", "=", "False", ")", ":", "L", ",", "R", "=", "None", ",", "None", "for", "line", "in", "sam", ":", "if", "line", ".", "startswith", "(", "'@'", ")", "is", "True", ":", "continue", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "bit", "=", "[", "True", "if", "i", "==", "'1'", "else", "False", "for", "i", "in", "bin", "(", "int", "(", "line", "[", "1", "]", ")", ")", ".", "split", "(", "'b'", ")", "[", "1", "]", "[", ":", ":", "-", "1", "]", "]", "while", "len", "(", "bit", ")", "<", "8", ":", "bit", ".", "append", "(", "False", ")", "pair", ",", "proper", ",", "na", ",", "nap", ",", "rev", ",", "mrev", ",", "left", ",", "right", "=", "bit", "# make sure read is paired", "if", "pair", "is", "False", ":", "if", "singles", "is", "True", ":", "print_single", "(", "line", ",", "rev", ")", "continue", "# check if sequence is reverse-complemented", "if", "rev", "is", "True", ":", "seq", "=", "rc", "(", "[", "''", ",", "line", "[", "9", "]", "]", ")", "[", "1", "]", "qual", "=", "line", "[", "10", "]", "[", ":", ":", "-", "1", "]", "else", ":", "seq", "=", "line", "[", "9", "]", "qual", "=", "line", "[", "10", "]", "# check if read is forward or reverse, return when both have been found", "if", "left", "is", "True", ":", "if", "L", "is", "not", "None", "and", "force", "is", "False", ":", "print", "(", "'sam file is not sorted'", ",", "file", "=", "sys", ".", "stderr", ")", "print", "(", "'\\te.g.: %s'", "%", "(", "line", "[", "0", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "exit", "(", ")", "if", "L", "is", "not", "None", ":", "L", "=", "None", "continue", "L", "=", "[", "'@%s'", "%", "line", "[", "0", "]", ",", "seq", ",", "'+%s'", "%", "line", "[", "0", "]", ",", "qual", "]", "if", "R", "is", "not", "None", ":", "yield", "L", "yield", "R", "L", ",", "R", "=", "None", ",", "None", "if", "right", "is", "True", ":", "if", "R", "is", "not", "None", "and", "force", "is", "False", ":", "print", "(", "'sam file is not sorted'", ",", "file", "=", "sys", ".", "stderr", ")", "print", "(", "'\\te.g.: %s'", "%", "(", "line", "[", "0", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "exit", "(", ")", "if", "R", "is", "not", "None", ":", "R", "=", "None", "continue", "R", "=", "[", "'@%s'", "%", "line", "[", "0", "]", ",", "seq", ",", "'+%s'", "%", "line", "[", "0", "]", ",", "qual", "]", "if", "L", "is", "not", "None", ":", "yield", "L", "yield", "R", "L", ",", "R", "=", "None", ",", "None"], "docstring": "convert sam to fastq", "docstring_tokens": ["convert", "sam", "to", "fastq"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/sam2fastq.py#L26-L78", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nconvert sam file to fastq file\n\"\"\"\n\nimport sys\nimport os\n\n# ctb\nfrom ctbBio.rc import reverse_complement as rc\n\n\ndef print_single(line, rev):\n    \"\"\"\n    print single reads to stderr\n    \"\"\"\n    if rev is True:\n        seq = rc([\"\", line[9]])[1]\n        qual = line[10][::-1]\n    else:\n        seq = line[9]\n        qual = line[10]\n    fq = [\"@%s\" % line[0], seq, \"+%s\" % line[0], qual]\n    print(\"\\n\".join(fq), file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        print(\n            \"usage: sam2fastq.py <mapping.sam> <singles to stderr?: True or False> <force exclude unpaired paired reads: True or False)>\",\n            file=sys.stderr,\n        )\n        exit()\n    sam, singles, force = sys.argv[1], sys.argv[2], sys.argv[3]\n    if sam == \"-\":\n        sam = sys.stdin\n    else:\n        sam = open(sam)\n    if singles == \"True\":\n        singles = True\n    else:\n        singles = False\n    if force == \"True\":\n        force = True\n    else:\n        force = False\n    for seq in sam2fastq(sam, singles, force):\n        print(\"\\n\".join(seq))\n", "levels": [0], "package": ["import sys", "import os", "from ctbBio.rc import reverse_complement as rc"], "function": ["def print_single(line, rev):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/subset_sam.py", "func_name": "sort_sam", "original_string": "def sort_sam(sam, sort):\n    \"\"\"\n    sort sam file\n    \"\"\"\n    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])\n    if sort is True:\n        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])\n        if sam != '-':\n            if os.path.exists(mapping) is False:\n                os.system(\"\\\n                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\n                    \" % (sbuffer, tempdir, mapping, sam)) \n        else:\n            mapping = 'stdin-sam.sorted.sam'\n            p = Popen(\"sort -k1 --buffer-size=%sG -T %s -o %s\" \\\n                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True) \n            p.communicate()\n        mapping = open(mapping)\n    else:\n        if sam == '-':\n            mapping = sys.stdin\n        else:\n            mapping = open(sam)\n    return mapping", "language": "python", "code": "def sort_sam(sam, sort):\n    \"\"\"\n    sort sam file\n    \"\"\"\n    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])\n    if sort is True:\n        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])\n        if sam != '-':\n            if os.path.exists(mapping) is False:\n                os.system(\"\\\n                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\n                    \" % (sbuffer, tempdir, mapping, sam)) \n        else:\n            mapping = 'stdin-sam.sorted.sam'\n            p = Popen(\"sort -k1 --buffer-size=%sG -T %s -o %s\" \\\n                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True) \n            p.communicate()\n        mapping = open(mapping)\n    else:\n        if sam == '-':\n            mapping = sys.stdin\n        else:\n            mapping = open(sam)\n    return mapping", "code_tokens": ["def", "sort_sam", "(", "sam", ",", "sort", ")", ":", "tempdir", "=", "'%s/'", "%", "(", "os", ".", "path", ".", "abspath", "(", "sam", ")", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "0", "]", ")", "if", "sort", "is", "True", ":", "mapping", "=", "'%s.sorted.sam'", "%", "(", "sam", ".", "rsplit", "(", "'.'", ",", "1", ")", "[", "0", "]", ")", "if", "sam", "!=", "'-'", ":", "if", "os", ".", "path", ".", "exists", "(", "mapping", ")", "is", "False", ":", "os", ".", "system", "(", "\"\\\n                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\n                    \"", "%", "(", "sbuffer", ",", "tempdir", ",", "mapping", ",", "sam", ")", ")", "else", ":", "mapping", "=", "'stdin-sam.sorted.sam'", "p", "=", "Popen", "(", "\"sort -k1 --buffer-size=%sG -T %s -o %s\"", "%", "(", "sbuffer", ",", "tempdir", ",", "mapping", ")", ",", "stdin", "=", "sys", ".", "stdin", ",", "shell", "=", "True", ")", "p", ".", "communicate", "(", ")", "mapping", "=", "open", "(", "mapping", ")", "else", ":", "if", "sam", "==", "'-'", ":", "mapping", "=", "sys", ".", "stdin", "else", ":", "mapping", "=", "open", "(", "sam", ")", "return", "mapping"], "docstring": "sort sam file", "docstring_tokens": ["sort", "sam", "file"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/subset_sam.py#L14-L37", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for randomly subsetting a sam file\n\"\"\"\n\nimport sys\nimport os\nfrom itertools import cycle\nfrom subprocess import Popen, PIPE\nimport argparse\nimport random\n\n\ndef sub_sam(sam, percent, sort=True, sbuffer=False):\n    \"\"\"\n    randomly subset sam file\n    \"\"\"\n    mapping = sort_sam(sam, sort)\n    pool = [1 for i in range(0, percent)] + [0 for i in range(0, 100 - percent)]\n    c = cycle([1, 2])\n    for line in mapping:\n        line = line.strip().split()\n        if line[0].startswith(\"@\"):  # get the sam header\n            yield line\n            continue\n        if int(line[1]) <= 20:  # is this from a single read?\n            if random.choice(pool) == 1:\n                yield line\n        else:\n            n = next(c)\n            if n == 1:\n                prev = line\n            if n == 2 and random.choice(pool) == 1:\n                yield prev\n                yield line\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# randomly subset sam file\")\n    parser.add_argument(\n        \"-s\", required=True, help=\"path to sorted sam file (- for stdin)\"\n    )\n    parser.add_argument(\n        \"-p\",\n        required=True,\n        type=int,\n        help=\"percent of reads to report, e.g. 50 (approximate)\",\n    )\n    parser.add_argument(\"--sort\", action=\"store_true\", help=\"sort the sam file\")\n    parser.add_argument(\n        \"-b\",\n        default=\"100\",\n        help=\"buffer size (GB) to use when sorting sam file (default = 100)\",\n    )\n    args = vars(parser.parse_args())\n    sam, percent, sort, buff = args[\"s\"], args[\"p\"], args[\"sort\"], args[\"b\"]\n    for line in sub_sam(sam, percent, sort, buff):\n        print(\"\\t\".join(line))\n", "levels": [0], "package": ["import sys", "import os", "from itertools import cycle", "from subprocess import Popen, PIPE", "import argparse", "import random"], "function": ["def sub_sam(sam, percent, sort=True, sbuffer=False):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/subset_sam.py", "func_name": "sub_sam", "original_string": "def sub_sam(sam, percent, sort = True, sbuffer = False):\n    \"\"\"\n    randomly subset sam file\n    \"\"\"\n    mapping = sort_sam(sam, sort)\n    pool = [1 for i in range(0, percent)] + [0 for i in range(0, 100 - percent)]\n    c = cycle([1, 2])\n    for line in mapping:\n        line = line.strip().split()\n        if line[0].startswith('@'): # get the sam header\n            yield line\n            continue\n        if int(line[1]) <= 20: # is this from a single read?\n            if random.choice(pool) == 1:\n                yield line\n        else:\n            n = next(c)\n            if n == 1:\n                prev = line\n            if n == 2 and random.choice(pool) == 1:\n                yield prev\n                yield line", "language": "python", "code": "def sub_sam(sam, percent, sort = True, sbuffer = False):\n    \"\"\"\n    randomly subset sam file\n    \"\"\"\n    mapping = sort_sam(sam, sort)\n    pool = [1 for i in range(0, percent)] + [0 for i in range(0, 100 - percent)]\n    c = cycle([1, 2])\n    for line in mapping:\n        line = line.strip().split()\n        if line[0].startswith('@'): # get the sam header\n            yield line\n            continue\n        if int(line[1]) <= 20: # is this from a single read?\n            if random.choice(pool) == 1:\n                yield line\n        else:\n            n = next(c)\n            if n == 1:\n                prev = line\n            if n == 2 and random.choice(pool) == 1:\n                yield prev\n                yield line", "code_tokens": ["def", "sub_sam", "(", "sam", ",", "percent", ",", "sort", "=", "True", ",", "sbuffer", "=", "False", ")", ":", "mapping", "=", "sort_sam", "(", "sam", ",", "sort", ")", "pool", "=", "[", "1", "for", "i", "in", "range", "(", "0", ",", "percent", ")", "]", "+", "[", "0", "for", "i", "in", "range", "(", "0", ",", "100", "-", "percent", ")", "]", "c", "=", "cycle", "(", "[", "1", ",", "2", "]", ")", "for", "line", "in", "mapping", ":", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "if", "line", "[", "0", "]", ".", "startswith", "(", "'@'", ")", ":", "# get the sam header", "yield", "line", "continue", "if", "int", "(", "line", "[", "1", "]", ")", "<=", "20", ":", "# is this from a single read?", "if", "random", ".", "choice", "(", "pool", ")", "==", "1", ":", "yield", "line", "else", ":", "n", "=", "next", "(", "c", ")", "if", "n", "==", "1", ":", "prev", "=", "line", "if", "n", "==", "2", "and", "random", ".", "choice", "(", "pool", ")", "==", "1", ":", "yield", "prev", "yield", "line"], "docstring": "randomly subset sam file", "docstring_tokens": ["randomly", "subset", "sam", "file"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/subset_sam.py#L39-L60", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for randomly subsetting a sam file\n\"\"\"\n\nimport sys\nimport os\nfrom itertools import cycle\nfrom subprocess import Popen, PIPE\nimport argparse\nimport random\n\n\ndef sort_sam(sam, sort):\n    \"\"\"\n    sort sam file\n    \"\"\"\n    tempdir = \"%s/\" % (os.path.abspath(sam).rsplit(\"/\", 1)[0])\n    if sort is True:\n        mapping = \"%s.sorted.sam\" % (sam.rsplit(\".\", 1)[0])\n        if sam != \"-\":\n            if os.path.exists(mapping) is False:\n                os.system(\n                    \"\\\n                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\n                    \"\n                    % (sbuffer, tempdir, mapping, sam)\n                )\n        else:\n            mapping = \"stdin-sam.sorted.sam\"\n            p = Popen(\n                \"sort -k1 --buffer-size=%sG -T %s -o %s\" % (sbuffer, tempdir, mapping),\n                stdin=sys.stdin,\n                shell=True,\n            )\n            p.communicate()\n        mapping = open(mapping)\n    else:\n        if sam == \"-\":\n            mapping = sys.stdin\n        else:\n            mapping = open(sam)\n    return mapping\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# randomly subset sam file\")\n    parser.add_argument(\n        \"-s\", required=True, help=\"path to sorted sam file (- for stdin)\"\n    )\n    parser.add_argument(\n        \"-p\",\n        required=True,\n        type=int,\n        help=\"percent of reads to report, e.g. 50 (approximate)\",\n    )\n    parser.add_argument(\"--sort\", action=\"store_true\", help=\"sort the sam file\")\n    parser.add_argument(\n        \"-b\",\n        default=\"100\",\n        help=\"buffer size (GB) to use when sorting sam file (default = 100)\",\n    )\n    args = vars(parser.parse_args())\n    sam, percent, sort, buff = args[\"s\"], args[\"p\"], args[\"sort\"], args[\"b\"]\n    for line in sub_sam(sam, percent, sort, buff):\n        print(\"\\t\".join(line))\n", "levels": [0], "package": ["import sys", "import os", "from itertools import cycle", "from subprocess import Popen, PIPE", "import argparse", "import random"], "function": ["def sort_sam(sam, sort):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/fastq2fasta.py", "func_name": "fq2fa", "original_string": "def fq2fa(fq):\n    \"\"\"\n    convert fq to fa\n    \"\"\"\n    c = cycle([1, 2, 3, 4])\n    for line in fq:\n        n = next(c)\n        if n == 1:\n            seq = ['>%s' % (line.strip().split('@', 1)[1])]\n        if n == 2:\n            seq.append(line.strip())\n            yield seq", "language": "python", "code": "def fq2fa(fq):\n    \"\"\"\n    convert fq to fa\n    \"\"\"\n    c = cycle([1, 2, 3, 4])\n    for line in fq:\n        n = next(c)\n        if n == 1:\n            seq = ['>%s' % (line.strip().split('@', 1)[1])]\n        if n == 2:\n            seq.append(line.strip())\n            yield seq", "code_tokens": ["def", "fq2fa", "(", "fq", ")", ":", "c", "=", "cycle", "(", "[", "1", ",", "2", ",", "3", ",", "4", "]", ")", "for", "line", "in", "fq", ":", "n", "=", "next", "(", "c", ")", "if", "n", "==", "1", ":", "seq", "=", "[", "'>%s'", "%", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "'@'", ",", "1", ")", "[", "1", "]", ")", "]", "if", "n", "==", "2", ":", "seq", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "yield", "seq"], "docstring": "convert fq to fa", "docstring_tokens": ["convert", "fq", "to", "fa"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/fastq2fasta.py#L11-L22", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for converting fastq file to fasta file\n\"\"\"\n\nimport sys\nimport os\nfrom itertools import cycle\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"specify fastq file\")\n        exit()\n    fq = sys.argv[1]\n    if fq == \"-\":\n        fq = sys.stdin\n    else:\n        fq = open(fq)\n    for seq in fq2fa(fq):\n        print(\"\\n\".join(seq))\n", "levels": [], "package": ["import sys", "import os", "from itertools import cycle"], "function": []}
{"repo": "elbow-jason/Uno-deprecated", "path": "uno/decorators.py", "func_name": "change_return_type", "original_string": "def change_return_type(f):\n    \"\"\"\n    Converts the returned value of wrapped function to the type of the\n    first arg or to the type specified by a kwarg key return_type's value.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if kwargs.has_key('return_type'):\n            return_type = kwargs['return_type']\n            kwargs.pop('return_type')\n            return return_type(f(*args, **kwargs))\n        elif len(args) > 0:\n            return_type = type(args[0])\n            return return_type(f(*args, **kwargs))\n        else:\n            return f(*args, **kwargs)\n    return wrapper", "language": "python", "code": "def change_return_type(f):\n    \"\"\"\n    Converts the returned value of wrapped function to the type of the\n    first arg or to the type specified by a kwarg key return_type's value.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if kwargs.has_key('return_type'):\n            return_type = kwargs['return_type']\n            kwargs.pop('return_type')\n            return return_type(f(*args, **kwargs))\n        elif len(args) > 0:\n            return_type = type(args[0])\n            return return_type(f(*args, **kwargs))\n        else:\n            return f(*args, **kwargs)\n    return wrapper", "code_tokens": ["def", "change_return_type", "(", "f", ")", ":", "@", "wraps", "(", "f", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "kwargs", ".", "has_key", "(", "'return_type'", ")", ":", "return_type", "=", "kwargs", "[", "'return_type'", "]", "kwargs", ".", "pop", "(", "'return_type'", ")", "return", "return_type", "(", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ")", "elif", "len", "(", "args", ")", ">", "0", ":", "return_type", "=", "type", "(", "args", "[", "0", "]", ")", "return", "return_type", "(", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ")", "else", ":", "return", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapper"], "docstring": "Converts the returned value of wrapped function to the type of the\n    first arg or to the type specified by a kwarg key return_type's value.", "docstring_tokens": ["Converts", "the", "returned", "value", "of", "wrapped", "function", "to", "the", "type", "of", "the", "first", "arg", "or", "to", "the", "type", "specified", "by", "a", "kwarg", "key", "return_type", "s", "value", "."], "sha": "4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4", "url": "https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/decorators.py#L11-L27", "partition": "train", "up_fun_num": 1, "context": "# -*- coding: utf-8 -*-\n\nfrom functools import wraps\n\n\ndef setify(i):\n    \"\"\"\n    Iterable to set.\n    \"\"\"\n    return set(i)\n\n\ndef convert_args_to_sets(f):\n    \"\"\"\n    Converts all args to 'set' type via self.setify function.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        args = (setify(x) for x in args)\n        return f(*args, **kwargs)\n\n    return wrapper\n\n\ndef accessible(f):\n    \"\"\"\n    Makes class properties accessible to self.__class__ (i hope) to via creation of an '_accessible_<property>' attr.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        print \"KWARGS= \", kwargs\n        setattr(args[0], kwargs[\"name\"], args[1])\n        print \"property: \", str(getattr(args[0], kwargs[\"name\"]))\n        return f(*args, **kwargs)\n\n    return wrapper\n", "levels": [0, 0, 1, 0, 1], "package": ["from functools import wraps"], "function": ["def setify(i):\n", "def convert_args_to_sets(f):\n", "    def wrapper(*args, **kwargs):\n", "def accessible(f):\n", "    def wrapper(*args, **kwargs):\n"]}
{"repo": "elbow-jason/Uno-deprecated", "path": "uno/decorators.py", "func_name": "convert_args_to_sets", "original_string": "def convert_args_to_sets(f):\n    \"\"\"\n    Converts all args to 'set' type via self.setify function.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        args = (setify(x) for x in args)\n        return f(*args, **kwargs)\n    return wrapper", "language": "python", "code": "def convert_args_to_sets(f):\n    \"\"\"\n    Converts all args to 'set' type via self.setify function.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        args = (setify(x) for x in args)\n        return f(*args, **kwargs)\n    return wrapper", "code_tokens": ["def", "convert_args_to_sets", "(", "f", ")", ":", "@", "wraps", "(", "f", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "args", "=", "(", "setify", "(", "x", ")", "for", "x", "in", "args", ")", "return", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapper"], "docstring": "Converts all args to 'set' type via self.setify function.", "docstring_tokens": ["Converts", "all", "args", "to", "set", "type", "via", "self", ".", "setify", "function", "."], "sha": "4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4", "url": "https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/decorators.py#L30-L38", "partition": "train", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n\nfrom functools import wraps\n\n\ndef setify(i):\n    \"\"\"\n    Iterable to set.\n    \"\"\"\n    return set(i)\n\n\ndef change_return_type(f):\n    \"\"\"\n    Converts the returned value of wrapped function to the type of the\n    first arg or to the type specified by a kwarg key return_type's value.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if kwargs.has_key(\"return_type\"):\n            return_type = kwargs[\"return_type\"]\n            kwargs.pop(\"return_type\")\n            return return_type(f(*args, **kwargs))\n        elif len(args) > 0:\n            return_type = type(args[0])\n            return return_type(f(*args, **kwargs))\n        else:\n            return f(*args, **kwargs)\n\n    return wrapper\n\n\ndef accessible(f):\n    \"\"\"\n    Makes class properties accessible to self.__class__ (i hope) to via creation of an '_accessible_<property>' attr.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        print \"KWARGS= \", kwargs\n        setattr(args[0], kwargs[\"name\"], args[1])\n        print \"property: \", str(getattr(args[0], kwargs[\"name\"]))\n        return f(*args, **kwargs)\n\n    return wrapper\n", "levels": [0, 0, 1, 0, 1], "package": ["from functools import wraps"], "function": ["def setify(i):\n", "def change_return_type(f):\n", "    def wrapper(*args, **kwargs):\n", "def accessible(f):\n", "    def wrapper(*args, **kwargs):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "KBBI._init_entri", "original_string": "def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, 'html.parser')\n        estr = ''\n        for label in sup.find('hr').next_siblings:\n            if label.name == 'hr':\n                self.entri.append(Entri(estr))\n                break\n            if label.name == 'h2':\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = ''\n            estr += str(label).strip()", "language": "python", "code": "def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, 'html.parser')\n        estr = ''\n        for label in sup.find('hr').next_siblings:\n            if label.name == 'hr':\n                self.entri.append(Entri(estr))\n                break\n            if label.name == 'h2':\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = ''\n            estr += str(label).strip()", "code_tokens": ["def", "_init_entri", "(", "self", ",", "laman", ")", ":", "sup", "=", "BeautifulSoup", "(", "laman", ".", "text", ",", "'html.parser'", ")", "estr", "=", "''", "for", "label", "in", "sup", ".", "find", "(", "'hr'", ")", ".", "next_siblings", ":", "if", "label", ".", "name", "==", "'hr'", ":", "self", ".", "entri", ".", "append", "(", "Entri", "(", "estr", ")", ")", "break", "if", "label", ".", "name", "==", "'h2'", ":", "if", "estr", ":", "self", ".", "entri", ".", "append", "(", "Entri", "(", "estr", ")", ")", "estr", "=", "''", "estr", "+=", "str", "(", "label", ")", ".", "strip", "(", ")"], "docstring": "Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response", "docstring_tokens": ["Membuat", "objek", "-", "objek", "entri", "dari", "laman", "yang", "diambil", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L46-L63", "partition": "train", "up_fun_num": 4, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Entri._init_kata_dasar", "original_string": "def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find('a')\n            dasar_no = kata.find('sup')\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + ' [{}]'.format(dasar_no.text.strip()) if dasar_no else kata\n            )", "language": "python", "code": "def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find('a')\n            dasar_no = kata.find('sup')\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + ' [{}]'.format(dasar_no.text.strip()) if dasar_no else kata\n            )", "code_tokens": ["def", "_init_kata_dasar", "(", "self", ",", "dasar", ")", ":", "for", "tiap", "in", "dasar", ":", "kata", "=", "tiap", ".", "find", "(", "'a'", ")", "dasar_no", "=", "kata", ".", "find", "(", "'sup'", ")", "kata", "=", "ambil_teks_dalam_label", "(", "kata", ")", "self", ".", "kata_dasar", ".", "append", "(", "kata", "+", "' [{}]'", ".", "format", "(", "dasar_no", ".", "text", ".", "strip", "(", ")", ")", "if", "dasar_no", "else", "kata", ")"], "docstring": "Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet", "docstring_tokens": ["Memproses", "kata", "dasar", "yang", "ada", "dalam", "nama", "entri", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L126-L139", "partition": "train", "up_fun_num": 10, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Entri.serialisasi", "original_string": "def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna]\n        }", "language": "python", "code": "def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna]\n        }", "code_tokens": ["def", "serialisasi", "(", "self", ")", ":", "return", "{", "\"nama\"", ":", "self", ".", "nama", ",", "\"nomor\"", ":", "self", ".", "nomor", ",", "\"kata_dasar\"", ":", "self", ".", "kata_dasar", ",", "\"pelafalan\"", ":", "self", ".", "pelafalan", ",", "\"bentuk_tidak_baku\"", ":", "self", ".", "bentuk_tidak_baku", ",", "\"varian\"", ":", "self", ".", "varian", ",", "\"makna\"", ":", "[", "makna", ".", "serialisasi", "(", ")", "for", "makna", "in", "self", ".", "makna", "]", "}"], "docstring": "Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict", "docstring_tokens": ["Mengembalikan", "hasil", "serialisasi", "objek", "Entri", "ini", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L141-L156", "partition": "train", "up_fun_num": 11, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Entri._makna", "original_string": "def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return '\\n'.join(\n                str(i) + \". \" + str(makna)\n                for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])", "language": "python", "code": "def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return '\\n'.join(\n                str(i) + \". \" + str(makna)\n                for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])", "code_tokens": ["def", "_makna", "(", "self", ")", ":", "if", "len", "(", "self", ".", "makna", ")", ">", "1", ":", "return", "'\\n'", ".", "join", "(", "str", "(", "i", ")", "+", "\". \"", "+", "str", "(", "makna", ")", "for", "i", ",", "makna", "in", "enumerate", "(", "self", ".", "makna", ",", "1", ")", ")", "return", "str", "(", "self", ".", "makna", "[", "0", "]", ")"], "docstring": "Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str", "docstring_tokens": ["Mengembalikan", "representasi", "string", "untuk", "semua", "makna", "entri", "ini", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L158-L170", "partition": "train", "up_fun_num": 12, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Entri._nama", "original_string": "def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil", "language": "python", "code": "def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil", "code_tokens": ["def", "_nama", "(", "self", ")", ":", "hasil", "=", "self", ".", "nama", "if", "self", ".", "nomor", ":", "hasil", "+=", "\" [{}]\"", ".", "format", "(", "self", ".", "nomor", ")", "if", "self", ".", "kata_dasar", ":", "hasil", "=", "\" \u00bb \".", "j", "oin(", "s", "elf.", "k", "ata_dasar)", " ", " ", " \u00bb \" +", "h", "sil", "return", "hasil"], "docstring": "Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str", "docstring_tokens": ["Mengembalikan", "representasi", "string", "untuk", "nama", "entri", "ini", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L172-L184", "partition": "train", "up_fun_num": 13, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Entri._varian", "original_string": "def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return ''\n        return nama + ': ' + ', '.join(varian)", "language": "python", "code": "def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return ''\n        return nama + ': ' + ', '.join(varian)", "code_tokens": ["def", "_varian", "(", "self", ",", "varian", ")", ":", "if", "varian", "==", "self", ".", "bentuk_tidak_baku", ":", "nama", "=", "\"Bentuk tidak baku\"", "elif", "varian", "==", "self", ".", "varian", ":", "nama", "=", "\"Varian\"", "else", ":", "return", "''", "return", "nama", "+", "': '", "+", "', '", ".", "join", "(", "varian", ")"], "docstring": "Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str", "docstring_tokens": ["Mengembalikan", "representasi", "string", "untuk", "varian", "entri", "ini", ".", "Dapat", "digunakan", "untuk", "Varian", "maupun", "Bentuk", "tidak", "baku", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L186-L202", "partition": "train", "up_fun_num": 14, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Makna._init_kelas", "original_string": "def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color='red')\n        lain = makna_label.find(color='darkgreen')\n        info = makna_label.find(color='green')\n        if kelas:\n            kelas = kelas.find_all('span')\n        if lain:\n            self.kelas = {lain.text.strip(): lain['title'].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += ' ' + makna_label.find(color='grey').text.strip()\n        else:\n            self.kelas = {\n                k.text.strip(): k['title'].strip() for k in kelas\n            } if kelas else {}\n        self.info = info.text.strip() if info else ''", "language": "python", "code": "def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color='red')\n        lain = makna_label.find(color='darkgreen')\n        info = makna_label.find(color='green')\n        if kelas:\n            kelas = kelas.find_all('span')\n        if lain:\n            self.kelas = {lain.text.strip(): lain['title'].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += ' ' + makna_label.find(color='grey').text.strip()\n        else:\n            self.kelas = {\n                k.text.strip(): k['title'].strip() for k in kelas\n            } if kelas else {}\n        self.info = info.text.strip() if info else ''", "code_tokens": ["def", "_init_kelas", "(", "self", ",", "makna_label", ")", ":", "kelas", "=", "makna_label", ".", "find", "(", "color", "=", "'red'", ")", "lain", "=", "makna_label", ".", "find", "(", "color", "=", "'darkgreen'", ")", "info", "=", "makna_label", ".", "find", "(", "color", "=", "'green'", ")", "if", "kelas", ":", "kelas", "=", "kelas", ".", "find_all", "(", "'span'", ")", "if", "lain", ":", "self", ".", "kelas", "=", "{", "lain", ".", "text", ".", "strip", "(", ")", ":", "lain", "[", "'title'", "]", ".", "strip", "(", ")", "}", "self", ".", "submakna", "=", "lain", ".", "next_sibling", ".", "strip", "(", ")", "self", ".", "submakna", "+=", "' '", "+", "makna_label", ".", "find", "(", "color", "=", "'grey'", ")", ".", "text", ".", "strip", "(", ")", "else", ":", "self", ".", "kelas", "=", "{", "k", ".", "text", ".", "strip", "(", ")", ":", "k", "[", "'title'", "]", ".", "strip", "(", ")", "for", "k", "in", "kelas", "}", "if", "kelas", "else", "{", "}", "self", ".", "info", "=", "info", ".", "text", ".", "strip", "(", ")", "if", "info", "else", "''"], "docstring": "Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup", "docstring_tokens": ["Memproses", "kelas", "kata", "yang", "ada", "dalam", "makna", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L239-L259", "partition": "train", "up_fun_num": 19, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Makna._init_contoh", "original_string": "def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(': ')\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2:].strip()\n            self.contoh = contoh.split('; ')\n        else:\n            self.contoh = []", "language": "python", "code": "def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(': ')\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2:].strip()\n            self.contoh = contoh.split('; ')\n        else:\n            self.contoh = []", "code_tokens": ["def", "_init_contoh", "(", "self", ",", "makna_label", ")", ":", "indeks", "=", "makna_label", ".", "text", ".", "find", "(", "': '", ")", "if", "indeks", "!=", "-", "1", ":", "contoh", "=", "makna_label", ".", "text", "[", "indeks", "+", "2", ":", "]", ".", "strip", "(", ")", "self", ".", "contoh", "=", "contoh", ".", "split", "(", "'; '", ")", "else", ":", "self", ".", "contoh", "=", "[", "]"], "docstring": "Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup", "docstring_tokens": ["Memproses", "contoh", "yang", "ada", "dalam", "makna", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L261-L273", "partition": "train", "up_fun_num": 20, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh,\n        }\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def serialisasi(self):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "laymonage/kbbi-python", "path": "kbbi/kbbi.py", "func_name": "Makna.serialisasi", "original_string": "def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh\n        }", "language": "python", "code": "def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"kelas\": self.kelas,\n            \"submakna\": self.submakna,\n            \"info\": self.info,\n            \"contoh\": self.contoh\n        }", "code_tokens": ["def", "serialisasi", "(", "self", ")", ":", "return", "{", "\"kelas\"", ":", "self", ".", "kelas", ",", "\"submakna\"", ":", "self", ".", "submakna", ",", "\"info\"", ":", "self", ".", "info", ",", "\"contoh\"", ":", "self", ".", "contoh", "}"], "docstring": "Mengembalikan hasil serialisasi objek Makna ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict", "docstring_tokens": ["Mengembalikan", "hasil", "serialisasi", "objek", "Makna", "ini", "."], "sha": "1a52ba8bcc6dc4c5c1215f9e00207aca264287d6", "url": "https://github.com/laymonage/kbbi-python/blob/1a52ba8bcc6dc4c5c1215f9e00207aca264287d6/kbbi/kbbi.py#L275-L287", "partition": "train", "up_fun_num": 21, "context": "\"\"\"\n:mod:`kbbi` -- Modul KBBI Python\n================================\n\n.. module:: kbbi\n   :platform: Unix, Windows, Mac\n   :synopsis: Modul ini mengandung implementasi dari modul kbbi.\n.. moduleauthor:: sage <laymonage@gmail.com>\n\"\"\"\n\nfrom re import sub\nfrom urllib.parse import quote\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass KBBI:\n    \"\"\"Sebuah laman dalam KBBI daring.\"\"\"\n\n    class TidakDitemukan(Exception):\n        \"\"\"\n        Galat yang menunjukkan bahwa laman tidak ditemukan dalam KBBI.\n        \"\"\"\n\n        def __init__(self, kata_kunci):\n            super().__init__(kata_kunci + \" tidak ditemukan dalam KBBI!\")\n\n    def __init__(self, kata_kunci):\n        \"\"\"Membuat objek KBBI baru berdasarkan kata_kunci yang diberikan.\n\n        :param kata_kunci: Kata kunci pencarian\n        :type kata_kunci: str\n        \"\"\"\n\n        url = \"https://kbbi.kemdikbud.go.id/entri/\" + quote(kata_kunci)\n        laman = requests.get(url)\n\n        if \"Entri tidak ditemukan.\" in laman.text:\n            raise self.TidakDitemukan(kata_kunci)\n\n        self.nama = kata_kunci.lower()\n        self.entri = []\n        self._init_entri(laman)\n\n    def _init_entri(self, laman):\n        \"\"\"Membuat objek-objek entri dari laman yang diambil.\n\n        :param laman: Laman respons yang dikembalikan oleh KBBI daring.\n        :type laman: Response\n        \"\"\"\n\n        sup = BeautifulSoup(laman.text, \"html.parser\")\n        estr = \"\"\n        for label in sup.find(\"hr\").next_siblings:\n            if label.name == \"hr\":\n                self.entri.append(Entri(estr))\n                break\n            if label.name == \"h2\":\n                if estr:\n                    self.entri.append(Entri(estr))\n                estr = \"\"\n            estr += str(label).strip()\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek KBBI ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {self.nama: [entri.serialisasi() for entri in self.entri]}\n\n    def __str__(self, contoh=True):\n        result = \"\\n\\n\".join(str(entri) for entri in self.entri)\n        if not contoh:\n            result = sub(\":.*--.*\", \"\", result)\n        return result\n\n    def __repr__(self):\n        return \"<KBBI: {}>\".format(self.nama)\n\n\nclass Entri:\n    \"\"\"Sebuah entri dalam sebuah laman KBBI daring.\"\"\"\n\n    def __init__(self, entri_html):\n        \"\"\"Membuat objek Entri baru berdasarkan entri_html yang diberikan.\n\n        :param entri_html: String HTML untuk entri yang ingin diproses.\n        :type entri_html: str\n        \"\"\"\n\n        entri = BeautifulSoup(entri_html, \"html.parser\")\n        judul = entri.find(\"h2\")\n        dasar = judul.find_all(class_=\"rootword\")\n        nomor = judul.find(\"sup\", recursive=False)\n        lafal = judul.find(class_=\"syllable\")\n        varian = judul.find(\"small\")\n        if entri.find(color=\"darkgreen\"):\n            makna = [entri]\n        else:\n            makna = entri.find_all(\"li\")\n\n        self.nama = ambil_teks_dalam_label(judul)\n        self.nomor = nomor.text.strip() if nomor else \"\"\n        self.kata_dasar = []\n        self._init_kata_dasar(dasar)\n        self.pelafalan = lafal.text.strip() if lafal else \"\"\n\n        self.bentuk_tidak_baku = []\n        self.varian = []\n        if varian:\n            bentuk_tidak_baku = varian.find_all(\"b\")\n            if bentuk_tidak_baku:\n                self.bentuk_tidak_baku = \"\".join(\n                    e.text.strip() for e in bentuk_tidak_baku\n                ).split(\", \")\n            else:\n                self.varian = varian.text[len(\"varian: \") :].strip().split(\", \")\n\n        self.makna = [Makna(m) for m in makna]\n\n    def _init_kata_dasar(self, dasar):\n        \"\"\"Memproses kata dasar yang ada dalam nama entri.\n\n        :param dasar: ResultSet untuk label HTML dengan class=\"rootword\"\n        :type dasar: ResultSet\n        \"\"\"\n\n        for tiap in dasar:\n            kata = tiap.find(\"a\")\n            dasar_no = kata.find(\"sup\")\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + \" [{}]\".format(dasar_no.text.strip()) if dasar_no else kata\n            )\n\n    def serialisasi(self):\n        \"\"\"Mengembalikan hasil serialisasi objek Entri ini.\n\n        :returns: Dictionary hasil serialisasi\n        :rtype: dict\n        \"\"\"\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna],\n        }\n\n    def _makna(self):\n        \"\"\"Mengembalikan representasi string untuk semua makna entri ini.\n\n        :returns: String representasi makna-makna\n        :rtype: str\n        \"\"\"\n\n        if len(self.makna) > 1:\n            return \"\\n\".join(\n                str(i) + \". \" + str(makna) for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])\n\n    def _nama(self):\n        \"\"\"Mengembalikan representasi string untuk nama entri ini.\n\n        :returns: String representasi nama entri\n        :rtype: str\n        \"\"\"\n\n        hasil = self.nama\n        if self.nomor:\n            hasil += \" [{}]\".format(self.nomor)\n        if self.kata_dasar:\n            hasil = \" \u00bb \".join(self.kata_dasar) + \" \u00bb \" + hasil\n        return hasil\n\n    def _varian(self, varian):\n        \"\"\"Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk \"Varian\" maupun \"Bentuk tidak baku\".\n\n        :param varian: List bentuk tidak baku atau varian\n        :type varian: list\n        :returns: String representasi varian atau bentuk tidak baku\n        :rtype: str\n        \"\"\"\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return \"\"\n        return nama + \": \" + \", \".join(varian)\n\n    def __str__(self):\n        hasil = self._nama()\n        if self.pelafalan:\n            hasil += \"  \" + self.pelafalan\n        for var in (self.bentuk_tidak_baku, self.varian):\n            if var:\n                hasil += \"\\n\" + self._varian(var)\n        return hasil + \"\\n\" + self._makna()\n\n    def __repr__(self):\n        return \"<Entri: {}>\".format(self._nama())\n\n\nclass Makna:\n    \"\"\"Sebuah makna dalam sebuah entri KBBI daring.\"\"\"\n\n    def __init__(self, makna_label):\n        \"\"\"Membuat objek Makna baru berdasarkan makna_label yang diberikan.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        self.submakna = ambil_teks_dalam_label(makna_label).rstrip(\":\")\n        baku = makna_label.find(\"a\")\n        if baku:\n            self.submakna += \" \" + ambil_teks_dalam_label(baku)\n            nomor = baku.find(\"sup\")\n            if nomor:\n                nomor = nomor.text.strip()\n                self.submakna += \" [{}]\".format(nomor)\n        self._init_kelas(makna_label)\n        self.submakna = self.submakna.split(\"; \")\n        self._init_contoh(makna_label)\n\n    def _init_kelas(self, makna_label):\n        \"\"\"Memproses kelas kata yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        kelas = makna_label.find(color=\"red\")\n        lain = makna_label.find(color=\"darkgreen\")\n        info = makna_label.find(color=\"green\")\n        if kelas:\n            kelas = kelas.find_all(\"span\")\n        if lain:\n            self.kelas = {lain.text.strip(): lain[\"title\"].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += \" \" + makna_label.find(color=\"grey\").text.strip()\n        else:\n            self.kelas = (\n                {k.text.strip(): k[\"title\"].strip() for k in kelas} if kelas else {}\n            )\n        self.info = info.text.strip() if info else \"\"\n\n    def _init_contoh(self, makna_label):\n        \"\"\"Memproses contoh yang ada dalam makna.\n\n        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.\n        :type makna_label: BeautifulSoup\n        \"\"\"\n\n        indeks = makna_label.text.find(\": \")\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2 :].strip()\n            self.contoh = contoh.split(\"; \")\n        else:\n            self.contoh = []\n\n    def _kelas(self):\n        \"\"\"Mengembalikan representasi string untuk semua kelas kata makna ini.\n\n        :returns: String representasi semua kelas kata\n        :rtype: str\n        \"\"\"\n        return \" \".join(\"({})\".format(k) for k in self.kelas)\n\n    def _submakna(self):\n        \"\"\"Mengembalikan representasi string untuk semua submakna makna ini.\n\n        :returns: String representasi semua submakna\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.submakna)\n\n    def _contoh(self):\n        \"\"\"Mengembalikan representasi string untuk semua contoh makna ini.\n\n        :returns: String representasi semua contoh\n        :rtype: str\n        \"\"\"\n        return \"; \".join(self.contoh)\n\n    def __str__(self):\n        hasil = self._kelas() + \"  \" if self.kelas else \"\"\n        hasil += self._submakna()\n        hasil += \" \" + self.info if self.info else \"\"\n        hasil += \": \" + self._contoh() if self.contoh else \"\"\n        return hasil\n\n    def __repr__(self):\n        return \"<Makna: {}>\".format(\"; \".join(self.submakna))\n\n\ndef ambil_teks_dalam_label(sup):\n    \"\"\"Mengambil semua teks dalam sup label HTML (tanpa anak-anaknya).\n\n    :param sup: BeautifulSoup dari suatu label HTML\n    :type sup: BeautifulSoup\n    :returns: String semua teks dalam sup label HTML\n    :rtype: str\n    \"\"\"\n    return \"\".join(i.strip() for i in sup.find_all(text=True, recursive=False))\n", "levels": [0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], "package": ["from re import sub", "from urllib.parse import quote", "import requests", "from bs4 import BeautifulSoup"], "function": ["class KBBI:\n", "    class TidakDitemukan(Exception):\n", "        def __init__(self, kata_kunci):\n", "    def __init__(self, kata_kunci):\n", "    def _init_entri(self, laman):\n", "    def serialisasi(self):\n", "    def __str__(self, contoh=True):\n", "    def __repr__(self):\n", "class Entri:\n", "    def __init__(self, entri_html):\n", "    def _init_kata_dasar(self, dasar):\n", "    def serialisasi(self):\n", "    def _makna(self):\n", "    def _nama(self):\n", "    def _varian(self, varian):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "class Makna:\n", "    def __init__(self, makna_label):\n", "    def _init_kelas(self, makna_label):\n", "    def _init_contoh(self, makna_label):\n", "    def _kelas(self):\n", "    def _submakna(self):\n", "    def _contoh(self):\n", "    def __str__(self):\n", "    def __repr__(self):\n", "def ambil_teks_dalam_label(sup):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/docs.py", "func_name": "build_sphinx", "original_string": "def build_sphinx(pkg_data, projectdir):\n    \"\"\"Build sphinx documentation.\n\n    :rtype: int\n    :return: subprocess.call return code\n\n    :param `bootstrap_py.control.PackageData` pkg_data: package meta data\n    :param str projectdir: project root directory\n    \"\"\"\n    try:\n        version, _minor_version = pkg_data.version.rsplit('.', 1)\n    except ValueError:\n        version = pkg_data.version\n    args = ' '.join(('sphinx-quickstart',\n                     '--sep',\n                     '-q',\n                     '-p \"{name}\"',\n                     '-a \"{author}\"',\n                     '-v \"{version}\"',\n                     '-r \"{release}\"',\n                     '-l en',\n                     '--suffix=.rst',\n                     '--master=index',\n                     '--ext-autodoc',\n                     '--ext-viewcode',\n                     '--makefile',\n                     '{projectdir}')).format(name=pkg_data.name,\n                                             author=pkg_data.author,\n                                             version=version,\n                                             release=pkg_data.version,\n                                             projectdir=projectdir)\n    if subprocess.call(shlex.split(args)) == 0:\n        _touch_gitkeep(projectdir)", "language": "python", "code": "def build_sphinx(pkg_data, projectdir):\n    \"\"\"Build sphinx documentation.\n\n    :rtype: int\n    :return: subprocess.call return code\n\n    :param `bootstrap_py.control.PackageData` pkg_data: package meta data\n    :param str projectdir: project root directory\n    \"\"\"\n    try:\n        version, _minor_version = pkg_data.version.rsplit('.', 1)\n    except ValueError:\n        version = pkg_data.version\n    args = ' '.join(('sphinx-quickstart',\n                     '--sep',\n                     '-q',\n                     '-p \"{name}\"',\n                     '-a \"{author}\"',\n                     '-v \"{version}\"',\n                     '-r \"{release}\"',\n                     '-l en',\n                     '--suffix=.rst',\n                     '--master=index',\n                     '--ext-autodoc',\n                     '--ext-viewcode',\n                     '--makefile',\n                     '{projectdir}')).format(name=pkg_data.name,\n                                             author=pkg_data.author,\n                                             version=version,\n                                             release=pkg_data.version,\n                                             projectdir=projectdir)\n    if subprocess.call(shlex.split(args)) == 0:\n        _touch_gitkeep(projectdir)", "code_tokens": ["def", "build_sphinx", "(", "pkg_data", ",", "projectdir", ")", ":", "try", ":", "version", ",", "_minor_version", "=", "pkg_data", ".", "version", ".", "rsplit", "(", "'.'", ",", "1", ")", "except", "ValueError", ":", "version", "=", "pkg_data", ".", "version", "args", "=", "' '", ".", "join", "(", "(", "'sphinx-quickstart'", ",", "'--sep'", ",", "'-q'", ",", "'-p \"{name}\"'", ",", "'-a \"{author}\"'", ",", "'-v \"{version}\"'", ",", "'-r \"{release}\"'", ",", "'-l en'", ",", "'--suffix=.rst'", ",", "'--master=index'", ",", "'--ext-autodoc'", ",", "'--ext-viewcode'", ",", "'--makefile'", ",", "'{projectdir}'", ")", ")", ".", "format", "(", "name", "=", "pkg_data", ".", "name", ",", "author", "=", "pkg_data", ".", "author", ",", "version", "=", "version", ",", "release", "=", "pkg_data", ".", "version", ",", "projectdir", "=", "projectdir", ")", "if", "subprocess", ".", "call", "(", "shlex", ".", "split", "(", "args", ")", ")", "==", "0", ":", "_touch_gitkeep", "(", "projectdir", ")"], "docstring": "Build sphinx documentation.\n\n    :rtype: int\n    :return: subprocess.call return code\n\n    :param `bootstrap_py.control.PackageData` pkg_data: package meta data\n    :param str projectdir: project root directory", "docstring_tokens": ["Build", "sphinx", "documentation", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/docs.py#L8-L40", "partition": "train", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.docs.\"\"\"\nimport os.path\nimport shlex\nimport subprocess\n\n\ndef _touch_gitkeep(docs_path):\n    with open(os.path.join(docs_path, \"source\", \"_static\", \".gitkeep\"), \"w\") as fobj:\n        fobj.write(\"\")\n", "levels": [0], "package": ["import os.path", "import shlex", "import subprocess"], "function": ["def _touch_gitkeep(docs_path):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/crossmap.py", "func_name": "bowtiedb", "original_string": "def bowtiedb(fa, keepDB):\n    \"\"\"\n    make bowtie db\n    \"\"\"\n    btdir = '%s/bt2' % (os.getcwd())\n    # make directory for\n    if not os.path.exists(btdir):\n        os.mkdir(btdir)\n    btdb = '%s/%s' % (btdir, fa.rsplit('/', 1)[-1])\n    if keepDB is True:\n        if os.path.exists('%s.1.bt2' % (btdb)):\n            return btdb\n    p = subprocess.Popen('bowtie2-build -q %s %s' \\\n        % (fa, btdb), shell = True)\n    p.communicate()\n    return btdb", "language": "python", "code": "def bowtiedb(fa, keepDB):\n    \"\"\"\n    make bowtie db\n    \"\"\"\n    btdir = '%s/bt2' % (os.getcwd())\n    # make directory for\n    if not os.path.exists(btdir):\n        os.mkdir(btdir)\n    btdb = '%s/%s' % (btdir, fa.rsplit('/', 1)[-1])\n    if keepDB is True:\n        if os.path.exists('%s.1.bt2' % (btdb)):\n            return btdb\n    p = subprocess.Popen('bowtie2-build -q %s %s' \\\n        % (fa, btdb), shell = True)\n    p.communicate()\n    return btdb", "code_tokens": ["def", "bowtiedb", "(", "fa", ",", "keepDB", ")", ":", "btdir", "=", "'%s/bt2'", "%", "(", "os", ".", "getcwd", "(", ")", ")", "# make directory for", "if", "not", "os", ".", "path", ".", "exists", "(", "btdir", ")", ":", "os", ".", "mkdir", "(", "btdir", ")", "btdb", "=", "'%s/%s'", "%", "(", "btdir", ",", "fa", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ")", "if", "keepDB", "is", "True", ":", "if", "os", ".", "path", ".", "exists", "(", "'%s.1.bt2'", "%", "(", "btdb", ")", ")", ":", "return", "btdb", "p", "=", "subprocess", ".", "Popen", "(", "'bowtie2-build -q %s %s'", "%", "(", "fa", ",", "btdb", ")", ",", "shell", "=", "True", ")", "p", ".", "communicate", "(", ")", "return", "btdb"], "docstring": "make bowtie db", "docstring_tokens": ["make", "bowtie", "db"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/crossmap.py#L16-L31", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for mapping reads against scaffolds\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport numpy\nimport shutil\nimport random\nimport argparse\nimport subprocess\n\n\ndef bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n    \"\"\"\n    generate bowtie2 command\n    \"\"\"\n    bt2 = \"bowtie2 -x %s -p %s \" % (btd, threads)\n    if f is not False:\n        bt2 += \"-1 %s -2 %s \" % (f, r)\n    if u is not False:\n        bt2 += \"-U %s \" % (u)\n    bt2 += opt\n    if no_shrink is False:\n        if f is False:\n            bt2 += \" | shrinksam -u -k %s-shrunk.sam \" % (sam)\n        else:\n            bt2 += \" | shrinksam -k %s-shrunk.sam \" % (sam)\n    else:\n        bt2 += \" > %s.sam\" % (sam)\n    return bt2\n\n\ndef chunks(l, n):\n    return numpy.array_split(numpy.array(l), n)\n\n\ndef crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n    \"\"\"\n    map all read sets against all fasta files\n    \"\"\"\n    if cluster is True:\n        threads = \"48\"\n    btc = []\n    for fa in fas:\n        btd = bowtiedb(fa, keepDB)\n        F, R, U = reads\n        if F is not False:\n            if U is False:\n                u = False\n            for i, f in enumerate(F):\n                r = R[i]\n                if U is not False:\n                    u = U[i]\n                sam = \"%s/%s-vs-%s\" % (\n                    os.getcwd(),\n                    fa.rsplit(\"/\", 1)[-1],\n                    f.rsplit(\"/\", 1)[-1].rsplit(\".\", 3)[0],\n                )\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n        else:\n            f = False\n            r = False\n            for u in U:\n                sam = \"%s/%s-vs-%s\" % (\n                    os.getcwd(),\n                    fa.rsplit(\"/\", 1)[-1],\n                    u.rsplit(\"/\", 1)[-1].rsplit(\".\", 3)[0],\n                )\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n    if cluster is False:\n        for i in btc:\n            p = subprocess.Popen(i, shell=True)\n            p.communicate()\n    else:\n        ID = \"\".join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))\n        for node, commands in enumerate(chunks(btc, nodes), 1):\n            bs = open(\"%s/crossmap-qsub.%s.%s.sh\" % (os.getcwd(), ID, node), \"w\")\n            print(\"\\n\".join(commands), file=bs)\n            bs.close()\n            p = subprocess.Popen(\"qsub -V -N crossmap %s\" % (bs.name), shell=True)\n            p.communicate()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# cross map using bowtie\")\n    parser.add_argument(\n        \"-f\", nargs=\"*\", action=\"store\", required=True, help=\"path to fasta(s)\"\n    )\n    parser.add_argument(\n        \"-1\", nargs=\"*\", action=\"store\", default=False, help=\"path to forward reads\"\n    )\n    parser.add_argument(\n        \"-2\", nargs=\"*\", action=\"store\", default=False, help=\"path to reverse reads\"\n    )\n    parser.add_argument(\n        \"-U\", nargs=\"*\", action=\"store\", default=False, help=\"path to single reads\"\n    )\n    parser.add_argument(\n        \"-p\",\n        default=\"--very-fast --reorder --quiet\",\n        help='bowtie options (default = \"--very-fast --reorder --quiet\"',\n    )\n    parser.add_argument(\"--no-shrink\", action=\"store_true\", help=\"do not use shrinksam\")\n    parser.add_argument(\n        \"--keepDB\", action=\"store_true\", help=\"do not overwrite bowtie database\"\n    )\n    parser.add_argument(\"-t\", default=\"6\", help=\"number of cpus (default = 6)\")\n    parser.add_argument(\"--cluster\", action=\"store_true\", help=\"run on cluster\")\n    parser.add_argument(\n        \"-n\", default=1, type=int, help=\"number of cluster nodes (default = 1)\"\n    )\n    args = vars(parser.parse_args())\n    fa = [os.path.abspath(i) for i in args[\"f\"]]\n    if (args[\"1\"] is False or args[\"2\"] is False) and args[\"U\"] is False:\n        print(\"# specify -1 and -2 and/or -U\", file=sys.stderr)\n        exit()\n    if args[\"1\"] is not False:\n        f = [os.path.abspath(i) for i in args[\"1\"]]\n        r = [os.path.abspath(i) for i in args[\"2\"]]\n    else:\n        f = r = False\n    if args[\"U\"] is not False:\n        u = [os.path.abspath(i) for i in args[\"U\"]]\n    else:\n        u = False\n    crossmap(\n        fa,\n        [f, r, u],\n        args[\"p\"],\n        args[\"no_shrink\"],\n        args[\"keepDB\"],\n        args[\"t\"],\n        args[\"cluster\"],\n        args[\"n\"],\n    )\n", "levels": [0, 0, 0], "package": ["import os", "import sys", "import json", "import numpy", "import shutil", "import random", "import argparse", "import subprocess"], "function": ["def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n", "def chunks(l, n):\n", "def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/crossmap.py", "func_name": "bowtie", "original_string": "def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n    \"\"\"\n    generate bowtie2 command\n    \"\"\"\n    bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)\n    if f is not False:\n        bt2 += '-1 %s -2 %s ' % (f, r)\n    if u is not False:\n        bt2 += '-U %s ' % (u)\n    bt2 += opt\n    if no_shrink is False:\n        if f is False:\n            bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)\n        else:\n            bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)\n    else:\n        bt2 += ' > %s.sam' % (sam)\n    return bt2", "language": "python", "code": "def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n    \"\"\"\n    generate bowtie2 command\n    \"\"\"\n    bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)\n    if f is not False:\n        bt2 += '-1 %s -2 %s ' % (f, r)\n    if u is not False:\n        bt2 += '-U %s ' % (u)\n    bt2 += opt\n    if no_shrink is False:\n        if f is False:\n            bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)\n        else:\n            bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)\n    else:\n        bt2 += ' > %s.sam' % (sam)\n    return bt2", "code_tokens": ["def", "bowtie", "(", "sam", ",", "btd", ",", "f", ",", "r", ",", "u", ",", "opt", ",", "no_shrink", ",", "threads", ")", ":", "bt2", "=", "'bowtie2 -x %s -p %s '", "%", "(", "btd", ",", "threads", ")", "if", "f", "is", "not", "False", ":", "bt2", "+=", "'-1 %s -2 %s '", "%", "(", "f", ",", "r", ")", "if", "u", "is", "not", "False", ":", "bt2", "+=", "'-U %s '", "%", "(", "u", ")", "bt2", "+=", "opt", "if", "no_shrink", "is", "False", ":", "if", "f", "is", "False", ":", "bt2", "+=", "' | shrinksam -u -k %s-shrunk.sam '", "%", "(", "sam", ")", "else", ":", "bt2", "+=", "' | shrinksam -k %s-shrunk.sam '", "%", "(", "sam", ")", "else", ":", "bt2", "+=", "' > %s.sam'", "%", "(", "sam", ")", "return", "bt2"], "docstring": "generate bowtie2 command", "docstring_tokens": ["generate", "bowtie2", "command"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/crossmap.py#L33-L50", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for mapping reads against scaffolds\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport numpy\nimport shutil\nimport random\nimport argparse\nimport subprocess\n\n\ndef bowtiedb(fa, keepDB):\n    \"\"\"\n    make bowtie db\n    \"\"\"\n    btdir = \"%s/bt2\" % (os.getcwd())\n    # make directory for\n    if not os.path.exists(btdir):\n        os.mkdir(btdir)\n    btdb = \"%s/%s\" % (btdir, fa.rsplit(\"/\", 1)[-1])\n    if keepDB is True:\n        if os.path.exists(\"%s.1.bt2\" % (btdb)):\n            return btdb\n    p = subprocess.Popen(\"bowtie2-build -q %s %s\" % (fa, btdb), shell=True)\n    p.communicate()\n    return btdb\n\n\ndef chunks(l, n):\n    return numpy.array_split(numpy.array(l), n)\n\n\ndef crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n    \"\"\"\n    map all read sets against all fasta files\n    \"\"\"\n    if cluster is True:\n        threads = \"48\"\n    btc = []\n    for fa in fas:\n        btd = bowtiedb(fa, keepDB)\n        F, R, U = reads\n        if F is not False:\n            if U is False:\n                u = False\n            for i, f in enumerate(F):\n                r = R[i]\n                if U is not False:\n                    u = U[i]\n                sam = \"%s/%s-vs-%s\" % (\n                    os.getcwd(),\n                    fa.rsplit(\"/\", 1)[-1],\n                    f.rsplit(\"/\", 1)[-1].rsplit(\".\", 3)[0],\n                )\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n        else:\n            f = False\n            r = False\n            for u in U:\n                sam = \"%s/%s-vs-%s\" % (\n                    os.getcwd(),\n                    fa.rsplit(\"/\", 1)[-1],\n                    u.rsplit(\"/\", 1)[-1].rsplit(\".\", 3)[0],\n                )\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n    if cluster is False:\n        for i in btc:\n            p = subprocess.Popen(i, shell=True)\n            p.communicate()\n    else:\n        ID = \"\".join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))\n        for node, commands in enumerate(chunks(btc, nodes), 1):\n            bs = open(\"%s/crossmap-qsub.%s.%s.sh\" % (os.getcwd(), ID, node), \"w\")\n            print(\"\\n\".join(commands), file=bs)\n            bs.close()\n            p = subprocess.Popen(\"qsub -V -N crossmap %s\" % (bs.name), shell=True)\n            p.communicate()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# cross map using bowtie\")\n    parser.add_argument(\n        \"-f\", nargs=\"*\", action=\"store\", required=True, help=\"path to fasta(s)\"\n    )\n    parser.add_argument(\n        \"-1\", nargs=\"*\", action=\"store\", default=False, help=\"path to forward reads\"\n    )\n    parser.add_argument(\n        \"-2\", nargs=\"*\", action=\"store\", default=False, help=\"path to reverse reads\"\n    )\n    parser.add_argument(\n        \"-U\", nargs=\"*\", action=\"store\", default=False, help=\"path to single reads\"\n    )\n    parser.add_argument(\n        \"-p\",\n        default=\"--very-fast --reorder --quiet\",\n        help='bowtie options (default = \"--very-fast --reorder --quiet\"',\n    )\n    parser.add_argument(\"--no-shrink\", action=\"store_true\", help=\"do not use shrinksam\")\n    parser.add_argument(\n        \"--keepDB\", action=\"store_true\", help=\"do not overwrite bowtie database\"\n    )\n    parser.add_argument(\"-t\", default=\"6\", help=\"number of cpus (default = 6)\")\n    parser.add_argument(\"--cluster\", action=\"store_true\", help=\"run on cluster\")\n    parser.add_argument(\n        \"-n\", default=1, type=int, help=\"number of cluster nodes (default = 1)\"\n    )\n    args = vars(parser.parse_args())\n    fa = [os.path.abspath(i) for i in args[\"f\"]]\n    if (args[\"1\"] is False or args[\"2\"] is False) and args[\"U\"] is False:\n        print(\"# specify -1 and -2 and/or -U\", file=sys.stderr)\n        exit()\n    if args[\"1\"] is not False:\n        f = [os.path.abspath(i) for i in args[\"1\"]]\n        r = [os.path.abspath(i) for i in args[\"2\"]]\n    else:\n        f = r = False\n    if args[\"U\"] is not False:\n        u = [os.path.abspath(i) for i in args[\"U\"]]\n    else:\n        u = False\n    crossmap(\n        fa,\n        [f, r, u],\n        args[\"p\"],\n        args[\"no_shrink\"],\n        args[\"keepDB\"],\n        args[\"t\"],\n        args[\"cluster\"],\n        args[\"n\"],\n    )\n", "levels": [0, 0, 0], "package": ["import os", "import sys", "import json", "import numpy", "import shutil", "import random", "import argparse", "import subprocess"], "function": ["def bowtiedb(fa, keepDB):\n", "def chunks(l, n):\n", "def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/crossmap.py", "func_name": "crossmap", "original_string": "def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n    \"\"\"\n    map all read sets against all fasta files\n    \"\"\"\n    if cluster is True:\n        threads = '48'\n    btc = []\n    for fa in fas:\n        btd = bowtiedb(fa, keepDB)\n        F, R, U = reads\n        if F is not False:\n            if U is False:\n                u = False\n            for i, f in enumerate(F):\n                r = R[i]\n                if U is not False:\n                    u = U[i]\n                sam = '%s/%s-vs-%s' % (os.getcwd(), \\\n                        fa.rsplit('/', 1)[-1], f.rsplit('/', 1)[-1].rsplit('.', 3)[0])\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n        else:\n            f = False\n            r = False\n            for u in U:\n                sam = '%s/%s-vs-%s' % (os.getcwd(), \\\n                        fa.rsplit('/', 1)[-1], u.rsplit('/', 1)[-1].rsplit('.', 3)[0])\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n    if cluster is False:\n        for i in btc:\n            p = subprocess.Popen(i, shell = True)\n            p.communicate()\n    else:\n        ID = ''.join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))\n        for node, commands in enumerate(chunks(btc, nodes), 1):\n            bs = open('%s/crossmap-qsub.%s.%s.sh' % (os.getcwd(), ID, node), 'w')\n            print('\\n'.join(commands), file=bs)\n            bs.close()\n            p = subprocess.Popen(\\\n                    'qsub -V -N crossmap %s' \\\n                        % (bs.name), \\\n                    shell = True)\n            p.communicate()", "language": "python", "code": "def crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n    \"\"\"\n    map all read sets against all fasta files\n    \"\"\"\n    if cluster is True:\n        threads = '48'\n    btc = []\n    for fa in fas:\n        btd = bowtiedb(fa, keepDB)\n        F, R, U = reads\n        if F is not False:\n            if U is False:\n                u = False\n            for i, f in enumerate(F):\n                r = R[i]\n                if U is not False:\n                    u = U[i]\n                sam = '%s/%s-vs-%s' % (os.getcwd(), \\\n                        fa.rsplit('/', 1)[-1], f.rsplit('/', 1)[-1].rsplit('.', 3)[0])\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n        else:\n            f = False\n            r = False\n            for u in U:\n                sam = '%s/%s-vs-%s' % (os.getcwd(), \\\n                        fa.rsplit('/', 1)[-1], u.rsplit('/', 1)[-1].rsplit('.', 3)[0])\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n    if cluster is False:\n        for i in btc:\n            p = subprocess.Popen(i, shell = True)\n            p.communicate()\n    else:\n        ID = ''.join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))\n        for node, commands in enumerate(chunks(btc, nodes), 1):\n            bs = open('%s/crossmap-qsub.%s.%s.sh' % (os.getcwd(), ID, node), 'w')\n            print('\\n'.join(commands), file=bs)\n            bs.close()\n            p = subprocess.Popen(\\\n                    'qsub -V -N crossmap %s' \\\n                        % (bs.name), \\\n                    shell = True)\n            p.communicate()", "code_tokens": ["def", "crossmap", "(", "fas", ",", "reads", ",", "options", ",", "no_shrink", ",", "keepDB", ",", "threads", ",", "cluster", ",", "nodes", ")", ":", "if", "cluster", "is", "True", ":", "threads", "=", "'48'", "btc", "=", "[", "]", "for", "fa", "in", "fas", ":", "btd", "=", "bowtiedb", "(", "fa", ",", "keepDB", ")", "F", ",", "R", ",", "U", "=", "reads", "if", "F", "is", "not", "False", ":", "if", "U", "is", "False", ":", "u", "=", "False", "for", "i", ",", "f", "in", "enumerate", "(", "F", ")", ":", "r", "=", "R", "[", "i", "]", "if", "U", "is", "not", "False", ":", "u", "=", "U", "[", "i", "]", "sam", "=", "'%s/%s-vs-%s'", "%", "(", "os", ".", "getcwd", "(", ")", ",", "fa", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ",", "f", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ".", "rsplit", "(", "'.'", ",", "3", ")", "[", "0", "]", ")", "btc", ".", "append", "(", "bowtie", "(", "sam", ",", "btd", ",", "f", ",", "r", ",", "u", ",", "options", ",", "no_shrink", ",", "threads", ")", ")", "else", ":", "f", "=", "False", "r", "=", "False", "for", "u", "in", "U", ":", "sam", "=", "'%s/%s-vs-%s'", "%", "(", "os", ".", "getcwd", "(", ")", ",", "fa", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ",", "u", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ".", "rsplit", "(", "'.'", ",", "3", ")", "[", "0", "]", ")", "btc", ".", "append", "(", "bowtie", "(", "sam", ",", "btd", ",", "f", ",", "r", ",", "u", ",", "options", ",", "no_shrink", ",", "threads", ")", ")", "if", "cluster", "is", "False", ":", "for", "i", "in", "btc", ":", "p", "=", "subprocess", ".", "Popen", "(", "i", ",", "shell", "=", "True", ")", "p", ".", "communicate", "(", ")", "else", ":", "ID", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "0", ",", "9", ")", "]", ")", "for", "_", "in", "range", "(", "5", ")", ")", "for", "node", ",", "commands", "in", "enumerate", "(", "chunks", "(", "btc", ",", "nodes", ")", ",", "1", ")", ":", "bs", "=", "open", "(", "'%s/crossmap-qsub.%s.%s.sh'", "%", "(", "os", ".", "getcwd", "(", ")", ",", "ID", ",", "node", ")", ",", "'w'", ")", "print", "(", "'\\n'", ".", "join", "(", "commands", ")", ",", "file", "=", "bs", ")", "bs", ".", "close", "(", ")", "p", "=", "subprocess", ".", "Popen", "(", "'qsub -V -N crossmap %s'", "%", "(", "bs", ".", "name", ")", ",", "shell", "=", "True", ")", "p", ".", "communicate", "(", ")"], "docstring": "map all read sets against all fasta files", "docstring_tokens": ["map", "all", "read", "sets", "against", "all", "fasta", "files"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/crossmap.py#L55-L96", "partition": "train", "up_fun_num": 3, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for mapping reads against scaffolds\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport numpy\nimport shutil\nimport random\nimport argparse\nimport subprocess\n\n\ndef bowtiedb(fa, keepDB):\n    \"\"\"\n    make bowtie db\n    \"\"\"\n    btdir = \"%s/bt2\" % (os.getcwd())\n    # make directory for\n    if not os.path.exists(btdir):\n        os.mkdir(btdir)\n    btdb = \"%s/%s\" % (btdir, fa.rsplit(\"/\", 1)[-1])\n    if keepDB is True:\n        if os.path.exists(\"%s.1.bt2\" % (btdb)):\n            return btdb\n    p = subprocess.Popen(\"bowtie2-build -q %s %s\" % (fa, btdb), shell=True)\n    p.communicate()\n    return btdb\n\n\ndef bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n    \"\"\"\n    generate bowtie2 command\n    \"\"\"\n    bt2 = \"bowtie2 -x %s -p %s \" % (btd, threads)\n    if f is not False:\n        bt2 += \"-1 %s -2 %s \" % (f, r)\n    if u is not False:\n        bt2 += \"-U %s \" % (u)\n    bt2 += opt\n    if no_shrink is False:\n        if f is False:\n            bt2 += \" | shrinksam -u -k %s-shrunk.sam \" % (sam)\n        else:\n            bt2 += \" | shrinksam -k %s-shrunk.sam \" % (sam)\n    else:\n        bt2 += \" > %s.sam\" % (sam)\n    return bt2\n\n\ndef chunks(l, n):\n    return numpy.array_split(numpy.array(l), n)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# cross map using bowtie\")\n    parser.add_argument(\n        \"-f\", nargs=\"*\", action=\"store\", required=True, help=\"path to fasta(s)\"\n    )\n    parser.add_argument(\n        \"-1\", nargs=\"*\", action=\"store\", default=False, help=\"path to forward reads\"\n    )\n    parser.add_argument(\n        \"-2\", nargs=\"*\", action=\"store\", default=False, help=\"path to reverse reads\"\n    )\n    parser.add_argument(\n        \"-U\", nargs=\"*\", action=\"store\", default=False, help=\"path to single reads\"\n    )\n    parser.add_argument(\n        \"-p\",\n        default=\"--very-fast --reorder --quiet\",\n        help='bowtie options (default = \"--very-fast --reorder --quiet\"',\n    )\n    parser.add_argument(\"--no-shrink\", action=\"store_true\", help=\"do not use shrinksam\")\n    parser.add_argument(\n        \"--keepDB\", action=\"store_true\", help=\"do not overwrite bowtie database\"\n    )\n    parser.add_argument(\"-t\", default=\"6\", help=\"number of cpus (default = 6)\")\n    parser.add_argument(\"--cluster\", action=\"store_true\", help=\"run on cluster\")\n    parser.add_argument(\n        \"-n\", default=1, type=int, help=\"number of cluster nodes (default = 1)\"\n    )\n    args = vars(parser.parse_args())\n    fa = [os.path.abspath(i) for i in args[\"f\"]]\n    if (args[\"1\"] is False or args[\"2\"] is False) and args[\"U\"] is False:\n        print(\"# specify -1 and -2 and/or -U\", file=sys.stderr)\n        exit()\n    if args[\"1\"] is not False:\n        f = [os.path.abspath(i) for i in args[\"1\"]]\n        r = [os.path.abspath(i) for i in args[\"2\"]]\n    else:\n        f = r = False\n    if args[\"U\"] is not False:\n        u = [os.path.abspath(i) for i in args[\"U\"]]\n    else:\n        u = False\n    crossmap(\n        fa,\n        [f, r, u],\n        args[\"p\"],\n        args[\"no_shrink\"],\n        args[\"keepDB\"],\n        args[\"t\"],\n        args[\"cluster\"],\n        args[\"n\"],\n    )\n", "levels": [0, 0, 0], "package": ["import os", "import sys", "import json", "import numpy", "import shutil", "import random", "import argparse", "import subprocess"], "function": ["def bowtiedb(fa, keepDB):\n", "def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n", "def chunks(l, n):\n"]}
{"repo": "disqus/nydus", "path": "nydus/db/base.py", "func_name": "BaseCluster.get_conn", "original_string": "def get_conn(self, *args, **kwargs):\n        \"\"\"\n        Returns a connection object from the router given ``args``.\n\n        Useful in cases where a connection cannot be automatically determined\n        during all steps of the process. An example of this would be\n        Redis pipelines.\n        \"\"\"\n        connections = self.__connections_for('get_conn', args=args, kwargs=kwargs)\n\n        if len(connections) is 1:\n            return connections[0]\n        else:\n            return connections", "language": "python", "code": "def get_conn(self, *args, **kwargs):\n        \"\"\"\n        Returns a connection object from the router given ``args``.\n\n        Useful in cases where a connection cannot be automatically determined\n        during all steps of the process. An example of this would be\n        Redis pipelines.\n        \"\"\"\n        connections = self.__connections_for('get_conn', args=args, kwargs=kwargs)\n\n        if len(connections) is 1:\n            return connections[0]\n        else:\n            return connections", "code_tokens": ["def", "get_conn", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "connections", "=", "self", ".", "__connections_for", "(", "'get_conn'", ",", "args", "=", "args", ",", "kwargs", "=", "kwargs", ")", "if", "len", "(", "connections", ")", "is", "1", ":", "return", "connections", "[", "0", "]", "else", ":", "return", "connections"], "docstring": "Returns a connection object from the router given ``args``.\n\n        Useful in cases where a connection cannot be automatically determined\n        during all steps of the process. An example of this would be\n        Redis pipelines.", "docstring_tokens": ["Returns", "a", "connection", "object", "from", "the", "router", "given", "args", "."], "sha": "9b505840da47a34f758a830c3992fa5dcb7bb7ad", "url": "https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/base.py#L100-L113", "partition": "train", "up_fun_num": 12, "context": "\"\"\"\nnydus.db.base\n~~~~~~~~~~~~~\n\n:copyright: (c) 2011-2012 DISQUS.\n:license: Apache License 2.0, see LICENSE for more details.\n\"\"\"\n\n__all__ = (\"LazyConnectionHandler\", \"BaseCluster\")\n\nimport collections\nfrom nydus.db.map import DistributedContextManager\nfrom nydus.db.routers import BaseRouter, routing_params\nfrom nydus.utils import apply_defaults\n\n\ndef iter_hosts(hosts):\n    # this can either be a dictionary (with the key acting as the numeric\n    # index) or it can be a sorted list.\n    if isinstance(hosts, collections.Mapping):\n        return hosts.iteritems()\n    return enumerate(hosts)\n\n\ndef create_connection(Connection, num, host_settings, defaults):\n    # host_settings can be an iterable or a dictionary depending on the style\n    # of connection (some connections share options and simply just need to\n    # pass a single host, or a list of hosts)\n    if isinstance(host_settings, collections.Mapping):\n        return Connection(num, **apply_defaults(host_settings, defaults or {}))\n    elif isinstance(host_settings, collections.Iterable):\n        return Connection(num, *host_settings, **defaults or {})\n    return Connection(num, host_settings, **defaults or {})\n\n\nclass BaseCluster(object):\n    \"\"\"\n    Holds a cluster of connections.\n    \"\"\"\n\n    class MaxRetriesExceededError(Exception):\n        pass\n\n    def __init__(\n        self,\n        hosts,\n        backend,\n        router=BaseRouter,\n        max_connection_retries=20,\n        defaults=None,\n    ):\n        self.hosts = dict(\n            (\n                conn_number,\n                create_connection(backend, conn_number, host_settings, defaults),\n            )\n            for conn_number, host_settings in iter_hosts(hosts)\n        )\n        self.max_connection_retries = max_connection_retries\n        self.install_router(router)\n\n    def __len__(self):\n        return len(self.hosts)\n\n    def __getitem__(self, name):\n        return self.hosts[name]\n\n    def __getattr__(self, name):\n        return CallProxy(self, name)\n\n    def __iter__(self):\n        for name in self.hosts.iterkeys():\n            yield name\n\n    def install_router(self, router):\n        self.router = router(self)\n\n    def execute(self, path, args, kwargs):\n        connections = self.__connections_for(path, args=args, kwargs=kwargs)\n\n        results = []\n        for conn in connections:\n            for retry in xrange(self.max_connection_retries):\n                func = conn\n                for piece in path.split(\".\"):\n                    func = getattr(func, piece)\n                try:\n                    results.append(func(*args, **kwargs))\n                except tuple(conn.retryable_exceptions), e:\n                    if not self.router.retryable:\n                        raise e\n                    elif retry == self.max_connection_retries - 1:\n                        raise self.MaxRetriesExceededError(e)\n                    else:\n                        conn = self.__connections_for(\n                            path, retry_for=conn.num, args=args, kwargs=kwargs\n                        )[0]\n                else:\n                    break\n\n        # If we only had one db to query, we simply return that res\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def disconnect(self):\n        \"\"\"Disconnects all connections in cluster\"\"\"\n        for connection in self.hosts.itervalues():\n            connection.disconnect()\n\n    def map(self, workers=None, **kwargs):\n        return DistributedContextManager(self, workers, **kwargs)\n\n    @routing_params\n    def __connections_for(self, attr, args, kwargs, **fkwargs):\n        return [\n            self[n]\n            for n in self.router.get_dbs(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        ]\n\n\nclass CallProxy(object):\n    \"\"\"\n    Handles routing function calls to the proper connection.\n    \"\"\"\n\n    def __init__(self, cluster, path):\n        self.__cluster = cluster\n        self.__path = path\n\n    def __call__(self, *args, **kwargs):\n        return self.__cluster.execute(self.__path, args, kwargs)\n\n    def __getattr__(self, name):\n        return CallProxy(self.__cluster, self.__path + \".\" + name)\n\n\nclass LazyConnectionHandler(dict):\n    \"\"\"\n    Maps clusters of connections within a dictionary.\n    \"\"\"\n\n    def __init__(self, conf_callback):\n        self.conf_callback = conf_callback\n        self.conf_settings = {}\n        self.__is_ready = False\n\n    def __getitem__(self, key):\n        if not self.is_ready():\n            self.reload()\n        return super(LazyConnectionHandler, self).__getitem__(key)\n\n    def is_ready(self):\n        return self.__is_ready\n\n    def reload(self):\n        from nydus.db import create_cluster\n\n        for conn_alias, conn_settings in self.conf_callback().iteritems():\n            self[conn_alias] = create_cluster(conn_settings)\n        self._is_ready = True\n\n    def disconnect(self):\n        \"\"\"Disconnects all connections in cluster\"\"\"\n        for connection in self.itervalues():\n            connection.disconnect()\n", "levels": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1], "package": ["import collections", "from nydus.db.map import DistributedContextManager", "from nydus.db.routers import BaseRouter, routing_params", "from nydus.utils import apply_defaults"], "function": ["def iter_hosts(hosts):\n", "def create_connection(Connection, num, host_settings, defaults):\n", "class BaseCluster(object):\n", "    class MaxRetriesExceededError(Exception):\n", "    def __len__(self):\n", "    def __getitem__(self, name):\n", "    def __getattr__(self, name):\n", "    def __iter__(self):\n", "    def install_router(self, router):\n", "    def execute(self, path, args, kwargs):\n", "    def disconnect(self):\n", "    def map(self, workers=None, **kwargs):\n", "    def __connections_for(self, attr, args, kwargs, **fkwargs):\n", "class CallProxy(object):\n", "    def __init__(self, cluster, path):\n", "    def __call__(self, *args, **kwargs):\n", "    def __getattr__(self, name):\n", "class LazyConnectionHandler(dict):\n", "    def __init__(self, conf_callback):\n", "    def __getitem__(self, key):\n", "    def is_ready(self):\n", "    def reload(self):\n", "    def disconnect(self):\n"]}
{"repo": "scottrice/pysteam", "path": "pysteam/_crc_algorithms.py", "func_name": "Crc.__get_nondirect_init", "original_string": "def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask", "language": "python", "code": "def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask", "code_tokens": ["def", "__get_nondirect_init", "(", "self", ",", "init", ")", ":", "crc", "=", "init", "for", "i", "in", "range", "(", "self", ".", "Width", ")", ":", "bit", "=", "crc", "&", "0x01", "if", "bit", ":", "crc", "^=", "self", ".", "Poly", "crc", ">>=", "1", "if", "bit", ":", "crc", "|=", "self", ".", "MSB_Mask", "return", "crc", "&", "self", ".", "Mask"], "docstring": "return the non-direct init if the direct algorithm has been selected.", "docstring_tokens": ["return", "the", "non", "-", "direct", "init", "if", "the", "direct", "algorithm", "has", "been", "selected", "."], "sha": "1eb2254b5235a053a953e596fa7602d0b110245d", "url": "https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L98-L110", "partition": "train", "up_fun_num": 2, "context": "# Gotten from the fantastic pycrc project (http://github.com/tpircher/pycrc)\n# Unfortunately pycrc doesn't include a setup.py so I can't just include it as\n# a dependency, so I need to copy it here. Sorry!\n\n#  pycrc -- parameterisable CRC calculation utility and C source code generator\n#\n#  Copyright (c) 2006-2013  Thomas Pircher  <tehpeh@gmx.net>\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n#  IN THE SOFTWARE.\n\n\n\"\"\"\nCRC algorithms implemented in Python.\nIf you want to study the Python implementation of the CRC routines, then this\nis a good place to start from.\n\nThe algorithms Bit by Bit, Bit by Bit Fast and Table-Driven are implemented.\n\nThis module can also be used as a library from within Python.\n\nExamples\n========\n\nThis is an example use of the different algorithms:\n\n>>> from crc_algorithms import Crc\n>>>\n>>> crc = Crc(width = 16, poly = 0x8005,\n...           reflect_in = True, xor_in = 0x0000,\n...           reflect_out = True, xor_out = 0x0000)\n>>> print(\"0x%x\" % crc.bit_by_bit(\"123456789\"))\n>>> print(\"0x%x\" % crc.bit_by_bit_fast(\"123456789\"))\n>>> print(\"0x%x\" % crc.table_driven(\"123456789\"))\n\"\"\"\n\n# Class Crc\n###############################################################################\nclass Crc(object):\n    \"\"\"\n    A base class for CRC routines.\n    \"\"\"\n\n    # Class constructor\n    ###############################################################################\n    def __init__(\n        self,\n        width,\n        poly,\n        reflect_in,\n        xor_in,\n        reflect_out,\n        xor_out,\n        table_idx_width=None,\n    ):\n        \"\"\"The Crc constructor.\n\n        The parameters are as follows:\n            width\n            poly\n            reflect_in\n            xor_in\n            reflect_out\n            xor_out\n        \"\"\"\n        self.Width = width\n        self.Poly = poly\n        self.ReflectIn = reflect_in\n        self.XorIn = xor_in\n        self.ReflectOut = reflect_out\n        self.XorOut = xor_out\n        self.TableIdxWidth = table_idx_width\n\n        self.MSB_Mask = 0x1 << (self.Width - 1)\n        self.Mask = ((self.MSB_Mask - 1) << 1) | 1\n        if self.TableIdxWidth != None:\n            self.TableWidth = 1 << self.TableIdxWidth\n        else:\n            self.TableIdxWidth = 8\n            self.TableWidth = 1 << self.TableIdxWidth\n\n        self.DirectInit = self.XorIn\n        self.NonDirectInit = self.__get_nondirect_init(self.XorIn)\n        if self.Width < 8:\n            self.CrcShift = 8 - self.Width\n        else:\n            self.CrcShift = 0\n\n    # function __get_nondirect_init\n    ###############################################################################\n\n    # function reflect\n    ###############################################################################\n    def reflect(self, data, width):\n        \"\"\"\n        reflect a data word, i.e. reverts the bit order.\n        \"\"\"\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x\n\n    # function bit_by_bit\n    ###############################################################################\n    def bit_by_bit(self, in_data):\n        \"\"\"\n        Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = (register << 1) & self.Mask\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function bit_by_bit_fast\n    ###############################################################################\n    def bit_by_bit_fast(self, in_data):\n        \"\"\"\n        This is a slightly modified version of the bit-by-bit algorithm: it\n        does not need to loop over the augmented bits, i.e. the Width 0-bits\n        wich are appended to the input message in the bit-by-bit algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.DirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                if octet & (0x80 >> i):\n                    topbit ^= self.MSB_Mask\n                register <<= 1\n                if topbit:\n                    register ^= self.Poly\n            register &= self.Mask\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function gen_table\n    ###############################################################################\n    def gen_table(self):\n        \"\"\"\n        This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.\n        \"\"\"\n        table_length = 1 << self.TableIdxWidth\n        tbl = [0] * table_length\n        for i in range(table_length):\n            register = i\n            if self.ReflectIn:\n                register = self.reflect(register, self.TableIdxWidth)\n            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)\n            for j in range(self.TableIdxWidth):\n                if register & (self.MSB_Mask << self.CrcShift) != 0:\n                    register = (register << 1) ^ (self.Poly << self.CrcShift)\n                else:\n                    register = register << 1\n            if self.ReflectIn:\n                register = (\n                    self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift\n                )\n            tbl[i] = register & (self.Mask << self.CrcShift)\n        return tbl\n\n    # function table_driven\n    ###############################################################################\n    def table_driven(self, in_data):\n        \"\"\"\n        The Standard table_driven CRC algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        tbl = self.gen_table()\n\n        register = self.DirectInit << self.CrcShift\n        if not self.ReflectIn:\n            for octet in in_data:\n                tblidx = (\n                    (register >> (self.Width - self.TableIdxWidth + self.CrcShift))\n                    ^ octet\n                ) & 0xFF\n                register = (\n                    (register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]\n                ) & (self.Mask << self.CrcShift)\n            register = register >> self.CrcShift\n        else:\n            register = (\n                self.reflect(register, self.Width + self.CrcShift) << self.CrcShift\n            )\n            for octet in in_data:\n                tblidx = ((register >> self.CrcShift) ^ octet) & 0xFF\n                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (\n                    self.Mask << self.CrcShift\n                )\n            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n", "levels": [0, 1, 1, 1, 1, 1], "package": [], "function": ["class Crc(object):\n", "    def reflect(self, data, width):\n", "    def bit_by_bit(self, in_data):\n", "    def bit_by_bit_fast(self, in_data):\n", "    def gen_table(self):\n", "    def table_driven(self, in_data):\n"]}
{"repo": "scottrice/pysteam", "path": "pysteam/_crc_algorithms.py", "func_name": "Crc.reflect", "original_string": "def reflect(self, data, width):\n        \"\"\"\n        reflect a data word, i.e. reverts the bit order.\n        \"\"\"\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x", "language": "python", "code": "def reflect(self, data, width):\n        \"\"\"\n        reflect a data word, i.e. reverts the bit order.\n        \"\"\"\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x", "code_tokens": ["def", "reflect", "(", "self", ",", "data", ",", "width", ")", ":", "x", "=", "data", "&", "0x01", "for", "i", "in", "range", "(", "width", "-", "1", ")", ":", "data", ">>=", "1", "x", "=", "(", "x", "<<", "1", ")", "|", "(", "data", "&", "0x01", ")", "return", "x"], "docstring": "reflect a data word, i.e. reverts the bit order.", "docstring_tokens": ["reflect", "a", "data", "word", "i", ".", "e", ".", "reverts", "the", "bit", "order", "."], "sha": "1eb2254b5235a053a953e596fa7602d0b110245d", "url": "https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L115-L123", "partition": "train", "up_fun_num": 3, "context": "# Gotten from the fantastic pycrc project (http://github.com/tpircher/pycrc)\n# Unfortunately pycrc doesn't include a setup.py so I can't just include it as\n# a dependency, so I need to copy it here. Sorry!\n\n#  pycrc -- parameterisable CRC calculation utility and C source code generator\n#\n#  Copyright (c) 2006-2013  Thomas Pircher  <tehpeh@gmx.net>\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n#  IN THE SOFTWARE.\n\n\n\"\"\"\nCRC algorithms implemented in Python.\nIf you want to study the Python implementation of the CRC routines, then this\nis a good place to start from.\n\nThe algorithms Bit by Bit, Bit by Bit Fast and Table-Driven are implemented.\n\nThis module can also be used as a library from within Python.\n\nExamples\n========\n\nThis is an example use of the different algorithms:\n\n>>> from crc_algorithms import Crc\n>>>\n>>> crc = Crc(width = 16, poly = 0x8005,\n...           reflect_in = True, xor_in = 0x0000,\n...           reflect_out = True, xor_out = 0x0000)\n>>> print(\"0x%x\" % crc.bit_by_bit(\"123456789\"))\n>>> print(\"0x%x\" % crc.bit_by_bit_fast(\"123456789\"))\n>>> print(\"0x%x\" % crc.table_driven(\"123456789\"))\n\"\"\"\n\n# Class Crc\n###############################################################################\nclass Crc(object):\n    \"\"\"\n    A base class for CRC routines.\n    \"\"\"\n\n    # Class constructor\n    ###############################################################################\n    def __init__(\n        self,\n        width,\n        poly,\n        reflect_in,\n        xor_in,\n        reflect_out,\n        xor_out,\n        table_idx_width=None,\n    ):\n        \"\"\"The Crc constructor.\n\n        The parameters are as follows:\n            width\n            poly\n            reflect_in\n            xor_in\n            reflect_out\n            xor_out\n        \"\"\"\n        self.Width = width\n        self.Poly = poly\n        self.ReflectIn = reflect_in\n        self.XorIn = xor_in\n        self.ReflectOut = reflect_out\n        self.XorOut = xor_out\n        self.TableIdxWidth = table_idx_width\n\n        self.MSB_Mask = 0x1 << (self.Width - 1)\n        self.Mask = ((self.MSB_Mask - 1) << 1) | 1\n        if self.TableIdxWidth != None:\n            self.TableWidth = 1 << self.TableIdxWidth\n        else:\n            self.TableIdxWidth = 8\n            self.TableWidth = 1 << self.TableIdxWidth\n\n        self.DirectInit = self.XorIn\n        self.NonDirectInit = self.__get_nondirect_init(self.XorIn)\n        if self.Width < 8:\n            self.CrcShift = 8 - self.Width\n        else:\n            self.CrcShift = 0\n\n    # function __get_nondirect_init\n    ###############################################################################\n    def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc ^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask\n\n    # function reflect\n    ###############################################################################\n\n    # function bit_by_bit\n    ###############################################################################\n    def bit_by_bit(self, in_data):\n        \"\"\"\n        Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = (register << 1) & self.Mask\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function bit_by_bit_fast\n    ###############################################################################\n    def bit_by_bit_fast(self, in_data):\n        \"\"\"\n        This is a slightly modified version of the bit-by-bit algorithm: it\n        does not need to loop over the augmented bits, i.e. the Width 0-bits\n        wich are appended to the input message in the bit-by-bit algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.DirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                if octet & (0x80 >> i):\n                    topbit ^= self.MSB_Mask\n                register <<= 1\n                if topbit:\n                    register ^= self.Poly\n            register &= self.Mask\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function gen_table\n    ###############################################################################\n    def gen_table(self):\n        \"\"\"\n        This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.\n        \"\"\"\n        table_length = 1 << self.TableIdxWidth\n        tbl = [0] * table_length\n        for i in range(table_length):\n            register = i\n            if self.ReflectIn:\n                register = self.reflect(register, self.TableIdxWidth)\n            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)\n            for j in range(self.TableIdxWidth):\n                if register & (self.MSB_Mask << self.CrcShift) != 0:\n                    register = (register << 1) ^ (self.Poly << self.CrcShift)\n                else:\n                    register = register << 1\n            if self.ReflectIn:\n                register = (\n                    self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift\n                )\n            tbl[i] = register & (self.Mask << self.CrcShift)\n        return tbl\n\n    # function table_driven\n    ###############################################################################\n    def table_driven(self, in_data):\n        \"\"\"\n        The Standard table_driven CRC algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        tbl = self.gen_table()\n\n        register = self.DirectInit << self.CrcShift\n        if not self.ReflectIn:\n            for octet in in_data:\n                tblidx = (\n                    (register >> (self.Width - self.TableIdxWidth + self.CrcShift))\n                    ^ octet\n                ) & 0xFF\n                register = (\n                    (register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]\n                ) & (self.Mask << self.CrcShift)\n            register = register >> self.CrcShift\n        else:\n            register = (\n                self.reflect(register, self.Width + self.CrcShift) << self.CrcShift\n            )\n            for octet in in_data:\n                tblidx = ((register >> self.CrcShift) ^ octet) & 0xFF\n                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (\n                    self.Mask << self.CrcShift\n                )\n            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n", "levels": [0, 1, 1, 1, 1, 1], "package": [], "function": ["class Crc(object):\n", "    def __get_nondirect_init(self, init):\n", "    def bit_by_bit(self, in_data):\n", "    def bit_by_bit_fast(self, in_data):\n", "    def gen_table(self):\n", "    def table_driven(self, in_data):\n"]}
{"repo": "scottrice/pysteam", "path": "pysteam/_crc_algorithms.py", "func_name": "Crc.bit_by_bit", "original_string": "def bit_by_bit(self, in_data):\n        \"\"\"\n        Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = ((register << 1) & self.Mask)\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut", "language": "python", "code": "def bit_by_bit(self, in_data):\n        \"\"\"\n        Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = ((register << 1) & self.Mask)\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut", "code_tokens": ["def", "bit_by_bit", "(", "self", ",", "in_data", ")", ":", "# If the input data is a string, convert to bytes.", "if", "isinstance", "(", "in_data", ",", "str", ")", ":", "in_data", "=", "[", "ord", "(", "c", ")", "for", "c", "in", "in_data", "]", "register", "=", "self", ".", "NonDirectInit", "for", "octet", "in", "in_data", ":", "if", "self", ".", "ReflectIn", ":", "octet", "=", "self", ".", "reflect", "(", "octet", ",", "8", ")", "for", "i", "in", "range", "(", "8", ")", ":", "topbit", "=", "register", "&", "self", ".", "MSB_Mask", "register", "=", "(", "(", "register", "<<", "1", ")", "&", "self", ".", "Mask", ")", "|", "(", "(", "octet", ">>", "(", "7", "-", "i", ")", ")", "&", "0x01", ")", "if", "topbit", ":", "register", "^=", "self", ".", "Poly", "for", "i", "in", "range", "(", "self", ".", "Width", ")", ":", "topbit", "=", "register", "&", "self", ".", "MSB_Mask", "register", "=", "(", "(", "register", "<<", "1", ")", "&", "self", ".", "Mask", ")", "if", "topbit", ":", "register", "^=", "self", ".", "Poly", "if", "self", ".", "ReflectOut", ":", "register", "=", "self", ".", "reflect", "(", "register", ",", "self", ".", "Width", ")", "return", "register", "^", "self", ".", "XorOut"], "docstring": "Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.", "docstring_tokens": ["Classic", "simple", "and", "slow", "CRC", "implementation", ".", "This", "function", "iterates", "bit", "by", "bit", "over", "the", "augmented", "input", "message", "and", "returns", "the", "calculated", "CRC", "value", "at", "the", "end", "."], "sha": "1eb2254b5235a053a953e596fa7602d0b110245d", "url": "https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L128-L156", "partition": "train", "up_fun_num": 4, "context": "# Gotten from the fantastic pycrc project (http://github.com/tpircher/pycrc)\n# Unfortunately pycrc doesn't include a setup.py so I can't just include it as\n# a dependency, so I need to copy it here. Sorry!\n\n#  pycrc -- parameterisable CRC calculation utility and C source code generator\n#\n#  Copyright (c) 2006-2013  Thomas Pircher  <tehpeh@gmx.net>\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n#  IN THE SOFTWARE.\n\n\n\"\"\"\nCRC algorithms implemented in Python.\nIf you want to study the Python implementation of the CRC routines, then this\nis a good place to start from.\n\nThe algorithms Bit by Bit, Bit by Bit Fast and Table-Driven are implemented.\n\nThis module can also be used as a library from within Python.\n\nExamples\n========\n\nThis is an example use of the different algorithms:\n\n>>> from crc_algorithms import Crc\n>>>\n>>> crc = Crc(width = 16, poly = 0x8005,\n...           reflect_in = True, xor_in = 0x0000,\n...           reflect_out = True, xor_out = 0x0000)\n>>> print(\"0x%x\" % crc.bit_by_bit(\"123456789\"))\n>>> print(\"0x%x\" % crc.bit_by_bit_fast(\"123456789\"))\n>>> print(\"0x%x\" % crc.table_driven(\"123456789\"))\n\"\"\"\n\n# Class Crc\n###############################################################################\nclass Crc(object):\n    \"\"\"\n    A base class for CRC routines.\n    \"\"\"\n\n    # Class constructor\n    ###############################################################################\n    def __init__(\n        self,\n        width,\n        poly,\n        reflect_in,\n        xor_in,\n        reflect_out,\n        xor_out,\n        table_idx_width=None,\n    ):\n        \"\"\"The Crc constructor.\n\n        The parameters are as follows:\n            width\n            poly\n            reflect_in\n            xor_in\n            reflect_out\n            xor_out\n        \"\"\"\n        self.Width = width\n        self.Poly = poly\n        self.ReflectIn = reflect_in\n        self.XorIn = xor_in\n        self.ReflectOut = reflect_out\n        self.XorOut = xor_out\n        self.TableIdxWidth = table_idx_width\n\n        self.MSB_Mask = 0x1 << (self.Width - 1)\n        self.Mask = ((self.MSB_Mask - 1) << 1) | 1\n        if self.TableIdxWidth != None:\n            self.TableWidth = 1 << self.TableIdxWidth\n        else:\n            self.TableIdxWidth = 8\n            self.TableWidth = 1 << self.TableIdxWidth\n\n        self.DirectInit = self.XorIn\n        self.NonDirectInit = self.__get_nondirect_init(self.XorIn)\n        if self.Width < 8:\n            self.CrcShift = 8 - self.Width\n        else:\n            self.CrcShift = 0\n\n    # function __get_nondirect_init\n    ###############################################################################\n    def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc ^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask\n\n    # function reflect\n    ###############################################################################\n    def reflect(self, data, width):\n        \"\"\"\n        reflect a data word, i.e. reverts the bit order.\n        \"\"\"\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x\n\n    # function bit_by_bit\n    ###############################################################################\n\n    # function bit_by_bit_fast\n    ###############################################################################\n    def bit_by_bit_fast(self, in_data):\n        \"\"\"\n        This is a slightly modified version of the bit-by-bit algorithm: it\n        does not need to loop over the augmented bits, i.e. the Width 0-bits\n        wich are appended to the input message in the bit-by-bit algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.DirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                if octet & (0x80 >> i):\n                    topbit ^= self.MSB_Mask\n                register <<= 1\n                if topbit:\n                    register ^= self.Poly\n            register &= self.Mask\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function gen_table\n    ###############################################################################\n    def gen_table(self):\n        \"\"\"\n        This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.\n        \"\"\"\n        table_length = 1 << self.TableIdxWidth\n        tbl = [0] * table_length\n        for i in range(table_length):\n            register = i\n            if self.ReflectIn:\n                register = self.reflect(register, self.TableIdxWidth)\n            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)\n            for j in range(self.TableIdxWidth):\n                if register & (self.MSB_Mask << self.CrcShift) != 0:\n                    register = (register << 1) ^ (self.Poly << self.CrcShift)\n                else:\n                    register = register << 1\n            if self.ReflectIn:\n                register = (\n                    self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift\n                )\n            tbl[i] = register & (self.Mask << self.CrcShift)\n        return tbl\n\n    # function table_driven\n    ###############################################################################\n    def table_driven(self, in_data):\n        \"\"\"\n        The Standard table_driven CRC algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        tbl = self.gen_table()\n\n        register = self.DirectInit << self.CrcShift\n        if not self.ReflectIn:\n            for octet in in_data:\n                tblidx = (\n                    (register >> (self.Width - self.TableIdxWidth + self.CrcShift))\n                    ^ octet\n                ) & 0xFF\n                register = (\n                    (register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]\n                ) & (self.Mask << self.CrcShift)\n            register = register >> self.CrcShift\n        else:\n            register = (\n                self.reflect(register, self.Width + self.CrcShift) << self.CrcShift\n            )\n            for octet in in_data:\n                tblidx = ((register >> self.CrcShift) ^ octet) & 0xFF\n                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (\n                    self.Mask << self.CrcShift\n                )\n            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n", "levels": [0, 1, 1, 1, 1, 1], "package": [], "function": ["class Crc(object):\n", "    def __get_nondirect_init(self, init):\n", "    def reflect(self, data, width):\n", "    def bit_by_bit_fast(self, in_data):\n", "    def gen_table(self):\n", "    def table_driven(self, in_data):\n"]}
{"repo": "scottrice/pysteam", "path": "pysteam/_crc_algorithms.py", "func_name": "Crc.gen_table", "original_string": "def gen_table(self):\n        \"\"\"\n        This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.\n        \"\"\"\n        table_length = 1 << self.TableIdxWidth\n        tbl = [0] * table_length\n        for i in range(table_length):\n            register = i\n            if self.ReflectIn:\n                register = self.reflect(register, self.TableIdxWidth)\n            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)\n            for j in range(self.TableIdxWidth):\n                if register & (self.MSB_Mask << self.CrcShift) != 0:\n                    register = (register << 1) ^ (self.Poly << self.CrcShift)\n                else:\n                    register = (register << 1)\n            if self.ReflectIn:\n                register = self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift\n            tbl[i] = register & (self.Mask << self.CrcShift)\n        return tbl", "language": "python", "code": "def gen_table(self):\n        \"\"\"\n        This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.\n        \"\"\"\n        table_length = 1 << self.TableIdxWidth\n        tbl = [0] * table_length\n        for i in range(table_length):\n            register = i\n            if self.ReflectIn:\n                register = self.reflect(register, self.TableIdxWidth)\n            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)\n            for j in range(self.TableIdxWidth):\n                if register & (self.MSB_Mask << self.CrcShift) != 0:\n                    register = (register << 1) ^ (self.Poly << self.CrcShift)\n                else:\n                    register = (register << 1)\n            if self.ReflectIn:\n                register = self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift\n            tbl[i] = register & (self.Mask << self.CrcShift)\n        return tbl", "code_tokens": ["def", "gen_table", "(", "self", ")", ":", "table_length", "=", "1", "<<", "self", ".", "TableIdxWidth", "tbl", "=", "[", "0", "]", "*", "table_length", "for", "i", "in", "range", "(", "table_length", ")", ":", "register", "=", "i", "if", "self", ".", "ReflectIn", ":", "register", "=", "self", ".", "reflect", "(", "register", ",", "self", ".", "TableIdxWidth", ")", "register", "=", "register", "<<", "(", "self", ".", "Width", "-", "self", ".", "TableIdxWidth", "+", "self", ".", "CrcShift", ")", "for", "j", "in", "range", "(", "self", ".", "TableIdxWidth", ")", ":", "if", "register", "&", "(", "self", ".", "MSB_Mask", "<<", "self", ".", "CrcShift", ")", "!=", "0", ":", "register", "=", "(", "register", "<<", "1", ")", "^", "(", "self", ".", "Poly", "<<", "self", ".", "CrcShift", ")", "else", ":", "register", "=", "(", "register", "<<", "1", ")", "if", "self", ".", "ReflectIn", ":", "register", "=", "self", ".", "reflect", "(", "register", ">>", "self", ".", "CrcShift", ",", "self", ".", "Width", ")", "<<", "self", ".", "CrcShift", "tbl", "[", "i", "]", "=", "register", "&", "(", "self", ".", "Mask", "<<", "self", ".", "CrcShift", ")", "return", "tbl"], "docstring": "This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.", "docstring_tokens": ["This", "function", "generates", "the", "CRC", "table", "used", "for", "the", "table_driven", "CRC", "algorithm", ".", "The", "Python", "version", "cannot", "handle", "tables", "of", "an", "index", "width", "other", "than", "8", ".", "See", "the", "generated", "C", "code", "for", "tables", "with", "different", "sizes", "instead", "."], "sha": "1eb2254b5235a053a953e596fa7602d0b110245d", "url": "https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L190-L212", "partition": "train", "up_fun_num": 6, "context": "# Gotten from the fantastic pycrc project (http://github.com/tpircher/pycrc)\n# Unfortunately pycrc doesn't include a setup.py so I can't just include it as\n# a dependency, so I need to copy it here. Sorry!\n\n#  pycrc -- parameterisable CRC calculation utility and C source code generator\n#\n#  Copyright (c) 2006-2013  Thomas Pircher  <tehpeh@gmx.net>\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n#  IN THE SOFTWARE.\n\n\n\"\"\"\nCRC algorithms implemented in Python.\nIf you want to study the Python implementation of the CRC routines, then this\nis a good place to start from.\n\nThe algorithms Bit by Bit, Bit by Bit Fast and Table-Driven are implemented.\n\nThis module can also be used as a library from within Python.\n\nExamples\n========\n\nThis is an example use of the different algorithms:\n\n>>> from crc_algorithms import Crc\n>>>\n>>> crc = Crc(width = 16, poly = 0x8005,\n...           reflect_in = True, xor_in = 0x0000,\n...           reflect_out = True, xor_out = 0x0000)\n>>> print(\"0x%x\" % crc.bit_by_bit(\"123456789\"))\n>>> print(\"0x%x\" % crc.bit_by_bit_fast(\"123456789\"))\n>>> print(\"0x%x\" % crc.table_driven(\"123456789\"))\n\"\"\"\n\n# Class Crc\n###############################################################################\nclass Crc(object):\n    \"\"\"\n    A base class for CRC routines.\n    \"\"\"\n\n    # Class constructor\n    ###############################################################################\n    def __init__(\n        self,\n        width,\n        poly,\n        reflect_in,\n        xor_in,\n        reflect_out,\n        xor_out,\n        table_idx_width=None,\n    ):\n        \"\"\"The Crc constructor.\n\n        The parameters are as follows:\n            width\n            poly\n            reflect_in\n            xor_in\n            reflect_out\n            xor_out\n        \"\"\"\n        self.Width = width\n        self.Poly = poly\n        self.ReflectIn = reflect_in\n        self.XorIn = xor_in\n        self.ReflectOut = reflect_out\n        self.XorOut = xor_out\n        self.TableIdxWidth = table_idx_width\n\n        self.MSB_Mask = 0x1 << (self.Width - 1)\n        self.Mask = ((self.MSB_Mask - 1) << 1) | 1\n        if self.TableIdxWidth != None:\n            self.TableWidth = 1 << self.TableIdxWidth\n        else:\n            self.TableIdxWidth = 8\n            self.TableWidth = 1 << self.TableIdxWidth\n\n        self.DirectInit = self.XorIn\n        self.NonDirectInit = self.__get_nondirect_init(self.XorIn)\n        if self.Width < 8:\n            self.CrcShift = 8 - self.Width\n        else:\n            self.CrcShift = 0\n\n    # function __get_nondirect_init\n    ###############################################################################\n    def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc ^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask\n\n    # function reflect\n    ###############################################################################\n    def reflect(self, data, width):\n        \"\"\"\n        reflect a data word, i.e. reverts the bit order.\n        \"\"\"\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x\n\n    # function bit_by_bit\n    ###############################################################################\n    def bit_by_bit(self, in_data):\n        \"\"\"\n        Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = (register << 1) & self.Mask\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function bit_by_bit_fast\n    ###############################################################################\n    def bit_by_bit_fast(self, in_data):\n        \"\"\"\n        This is a slightly modified version of the bit-by-bit algorithm: it\n        does not need to loop over the augmented bits, i.e. the Width 0-bits\n        wich are appended to the input message in the bit-by-bit algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.DirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                if octet & (0x80 >> i):\n                    topbit ^= self.MSB_Mask\n                register <<= 1\n                if topbit:\n                    register ^= self.Poly\n            register &= self.Mask\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function gen_table\n    ###############################################################################\n\n    # function table_driven\n    ###############################################################################\n    def table_driven(self, in_data):\n        \"\"\"\n        The Standard table_driven CRC algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        tbl = self.gen_table()\n\n        register = self.DirectInit << self.CrcShift\n        if not self.ReflectIn:\n            for octet in in_data:\n                tblidx = (\n                    (register >> (self.Width - self.TableIdxWidth + self.CrcShift))\n                    ^ octet\n                ) & 0xFF\n                register = (\n                    (register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]\n                ) & (self.Mask << self.CrcShift)\n            register = register >> self.CrcShift\n        else:\n            register = (\n                self.reflect(register, self.Width + self.CrcShift) << self.CrcShift\n            )\n            for octet in in_data:\n                tblidx = ((register >> self.CrcShift) ^ octet) & 0xFF\n                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (\n                    self.Mask << self.CrcShift\n                )\n            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n", "levels": [0, 1, 1, 1, 1, 1], "package": [], "function": ["class Crc(object):\n", "    def __get_nondirect_init(self, init):\n", "    def reflect(self, data, width):\n", "    def bit_by_bit(self, in_data):\n", "    def bit_by_bit_fast(self, in_data):\n", "    def table_driven(self, in_data):\n"]}
{"repo": "scottrice/pysteam", "path": "pysteam/_crc_algorithms.py", "func_name": "Crc.table_driven", "original_string": "def table_driven(self, in_data):\n        \"\"\"\n        The Standard table_driven CRC algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        tbl = self.gen_table()\n\n        register = self.DirectInit << self.CrcShift\n        if not self.ReflectIn:\n            for octet in in_data:\n                tblidx = ((register >> (self.Width - self.TableIdxWidth + self.CrcShift)) ^ octet) & 0xff\n                register = ((register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)\n            register = register >> self.CrcShift\n        else:\n            register = self.reflect(register, self.Width + self.CrcShift) << self.CrcShift\n            for octet in in_data:\n                tblidx = ((register >> self.CrcShift) ^ octet) & 0xff\n                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)\n            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut", "language": "python", "code": "def table_driven(self, in_data):\n        \"\"\"\n        The Standard table_driven CRC algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        tbl = self.gen_table()\n\n        register = self.DirectInit << self.CrcShift\n        if not self.ReflectIn:\n            for octet in in_data:\n                tblidx = ((register >> (self.Width - self.TableIdxWidth + self.CrcShift)) ^ octet) & 0xff\n                register = ((register << (self.TableIdxWidth - self.CrcShift)) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)\n            register = register >> self.CrcShift\n        else:\n            register = self.reflect(register, self.Width + self.CrcShift) << self.CrcShift\n            for octet in in_data:\n                tblidx = ((register >> self.CrcShift) ^ octet) & 0xff\n                register = ((register >> self.TableIdxWidth) ^ tbl[tblidx]) & (self.Mask << self.CrcShift)\n            register = self.reflect(register, self.Width + self.CrcShift) & self.Mask\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut", "code_tokens": ["def", "table_driven", "(", "self", ",", "in_data", ")", ":", "# If the input data is a string, convert to bytes.", "if", "isinstance", "(", "in_data", ",", "str", ")", ":", "in_data", "=", "[", "ord", "(", "c", ")", "for", "c", "in", "in_data", "]", "tbl", "=", "self", ".", "gen_table", "(", ")", "register", "=", "self", ".", "DirectInit", "<<", "self", ".", "CrcShift", "if", "not", "self", ".", "ReflectIn", ":", "for", "octet", "in", "in_data", ":", "tblidx", "=", "(", "(", "register", ">>", "(", "self", ".", "Width", "-", "self", ".", "TableIdxWidth", "+", "self", ".", "CrcShift", ")", ")", "^", "octet", ")", "&", "0xff", "register", "=", "(", "(", "register", "<<", "(", "self", ".", "TableIdxWidth", "-", "self", ".", "CrcShift", ")", ")", "^", "tbl", "[", "tblidx", "]", ")", "&", "(", "self", ".", "Mask", "<<", "self", ".", "CrcShift", ")", "register", "=", "register", ">>", "self", ".", "CrcShift", "else", ":", "register", "=", "self", ".", "reflect", "(", "register", ",", "self", ".", "Width", "+", "self", ".", "CrcShift", ")", "<<", "self", ".", "CrcShift", "for", "octet", "in", "in_data", ":", "tblidx", "=", "(", "(", "register", ">>", "self", ".", "CrcShift", ")", "^", "octet", ")", "&", "0xff", "register", "=", "(", "(", "register", ">>", "self", ".", "TableIdxWidth", ")", "^", "tbl", "[", "tblidx", "]", ")", "&", "(", "self", ".", "Mask", "<<", "self", ".", "CrcShift", ")", "register", "=", "self", ".", "reflect", "(", "register", ",", "self", ".", "Width", "+", "self", ".", "CrcShift", ")", "&", "self", ".", "Mask", "if", "self", ".", "ReflectOut", ":", "register", "=", "self", ".", "reflect", "(", "register", ",", "self", ".", "Width", ")", "return", "register", "^", "self", ".", "XorOut"], "docstring": "The Standard table_driven CRC algorithm.", "docstring_tokens": ["The", "Standard", "table_driven", "CRC", "algorithm", "."], "sha": "1eb2254b5235a053a953e596fa7602d0b110245d", "url": "https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/_crc_algorithms.py#L217-L242", "partition": "train", "up_fun_num": 7, "context": "# Gotten from the fantastic pycrc project (http://github.com/tpircher/pycrc)\n# Unfortunately pycrc doesn't include a setup.py so I can't just include it as\n# a dependency, so I need to copy it here. Sorry!\n\n#  pycrc -- parameterisable CRC calculation utility and C source code generator\n#\n#  Copyright (c) 2006-2013  Thomas Pircher  <tehpeh@gmx.net>\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n#  IN THE SOFTWARE.\n\n\n\"\"\"\nCRC algorithms implemented in Python.\nIf you want to study the Python implementation of the CRC routines, then this\nis a good place to start from.\n\nThe algorithms Bit by Bit, Bit by Bit Fast and Table-Driven are implemented.\n\nThis module can also be used as a library from within Python.\n\nExamples\n========\n\nThis is an example use of the different algorithms:\n\n>>> from crc_algorithms import Crc\n>>>\n>>> crc = Crc(width = 16, poly = 0x8005,\n...           reflect_in = True, xor_in = 0x0000,\n...           reflect_out = True, xor_out = 0x0000)\n>>> print(\"0x%x\" % crc.bit_by_bit(\"123456789\"))\n>>> print(\"0x%x\" % crc.bit_by_bit_fast(\"123456789\"))\n>>> print(\"0x%x\" % crc.table_driven(\"123456789\"))\n\"\"\"\n\n# Class Crc\n###############################################################################\nclass Crc(object):\n    \"\"\"\n    A base class for CRC routines.\n    \"\"\"\n\n    # Class constructor\n    ###############################################################################\n    def __init__(\n        self,\n        width,\n        poly,\n        reflect_in,\n        xor_in,\n        reflect_out,\n        xor_out,\n        table_idx_width=None,\n    ):\n        \"\"\"The Crc constructor.\n\n        The parameters are as follows:\n            width\n            poly\n            reflect_in\n            xor_in\n            reflect_out\n            xor_out\n        \"\"\"\n        self.Width = width\n        self.Poly = poly\n        self.ReflectIn = reflect_in\n        self.XorIn = xor_in\n        self.ReflectOut = reflect_out\n        self.XorOut = xor_out\n        self.TableIdxWidth = table_idx_width\n\n        self.MSB_Mask = 0x1 << (self.Width - 1)\n        self.Mask = ((self.MSB_Mask - 1) << 1) | 1\n        if self.TableIdxWidth != None:\n            self.TableWidth = 1 << self.TableIdxWidth\n        else:\n            self.TableIdxWidth = 8\n            self.TableWidth = 1 << self.TableIdxWidth\n\n        self.DirectInit = self.XorIn\n        self.NonDirectInit = self.__get_nondirect_init(self.XorIn)\n        if self.Width < 8:\n            self.CrcShift = 8 - self.Width\n        else:\n            self.CrcShift = 0\n\n    # function __get_nondirect_init\n    ###############################################################################\n    def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc ^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask\n\n    # function reflect\n    ###############################################################################\n    def reflect(self, data, width):\n        \"\"\"\n        reflect a data word, i.e. reverts the bit order.\n        \"\"\"\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x\n\n    # function bit_by_bit\n    ###############################################################################\n    def bit_by_bit(self, in_data):\n        \"\"\"\n        Classic simple and slow CRC implementation.  This function iterates bit\n        by bit over the augmented input message and returns the calculated CRC\n        value at the end.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = (register << 1) & self.Mask\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function bit_by_bit_fast\n    ###############################################################################\n    def bit_by_bit_fast(self, in_data):\n        \"\"\"\n        This is a slightly modified version of the bit-by-bit algorithm: it\n        does not need to loop over the augmented bits, i.e. the Width 0-bits\n        wich are appended to the input message in the bit-by-bit algorithm.\n        \"\"\"\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.DirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                if octet & (0x80 >> i):\n                    topbit ^= self.MSB_Mask\n                register <<= 1\n                if topbit:\n                    register ^= self.Poly\n            register &= self.Mask\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut\n\n    # function gen_table\n    ###############################################################################\n    def gen_table(self):\n        \"\"\"\n        This function generates the CRC table used for the table_driven CRC\n        algorithm.  The Python version cannot handle tables of an index width\n        other than 8.  See the generated C code for tables with different sizes\n        instead.\n        \"\"\"\n        table_length = 1 << self.TableIdxWidth\n        tbl = [0] * table_length\n        for i in range(table_length):\n            register = i\n            if self.ReflectIn:\n                register = self.reflect(register, self.TableIdxWidth)\n            register = register << (self.Width - self.TableIdxWidth + self.CrcShift)\n            for j in range(self.TableIdxWidth):\n                if register & (self.MSB_Mask << self.CrcShift) != 0:\n                    register = (register << 1) ^ (self.Poly << self.CrcShift)\n                else:\n                    register = register << 1\n            if self.ReflectIn:\n                register = (\n                    self.reflect(register >> self.CrcShift, self.Width) << self.CrcShift\n                )\n            tbl[i] = register & (self.Mask << self.CrcShift)\n        return tbl\n\n    # function table_driven\n    ###############################################################################\n", "levels": [0, 1, 1, 1, 1, 1], "package": [], "function": ["class Crc(object):\n", "    def __get_nondirect_init(self, init):\n", "    def reflect(self, data, width):\n", "    def bit_by_bit(self, in_data):\n", "    def bit_by_bit_fast(self, in_data):\n", "    def gen_table(self):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/strip_masked.py", "func_name": "parse_masked", "original_string": "def parse_masked(seq, min_len):\n    \"\"\"\n    parse masked sequence into non-masked and masked regions\n    \"\"\"\n    nm, masked = [], [[]]\n    prev = None\n    for base in seq[1]:\n        if base.isupper():\n            nm.append(base)\n            if masked != [[]] and len(masked[-1]) < min_len:\n                nm.extend(masked[-1])\n                del masked[-1]\n            prev = False\n        elif base.islower():\n            if prev is False:\n                masked.append([])\n            masked[-1].append(base)\n            prev = True\n    return nm, masked", "language": "python", "code": "def parse_masked(seq, min_len):\n    \"\"\"\n    parse masked sequence into non-masked and masked regions\n    \"\"\"\n    nm, masked = [], [[]]\n    prev = None\n    for base in seq[1]:\n        if base.isupper():\n            nm.append(base)\n            if masked != [[]] and len(masked[-1]) < min_len:\n                nm.extend(masked[-1])\n                del masked[-1]\n            prev = False\n        elif base.islower():\n            if prev is False:\n                masked.append([])\n            masked[-1].append(base)\n            prev = True\n    return nm, masked", "code_tokens": ["def", "parse_masked", "(", "seq", ",", "min_len", ")", ":", "nm", ",", "masked", "=", "[", "]", ",", "[", "[", "]", "]", "prev", "=", "None", "for", "base", "in", "seq", "[", "1", "]", ":", "if", "base", ".", "isupper", "(", ")", ":", "nm", ".", "append", "(", "base", ")", "if", "masked", "!=", "[", "[", "]", "]", "and", "len", "(", "masked", "[", "-", "1", "]", ")", "<", "min_len", ":", "nm", ".", "extend", "(", "masked", "[", "-", "1", "]", ")", "del", "masked", "[", "-", "1", "]", "prev", "=", "False", "elif", "base", ".", "islower", "(", ")", ":", "if", "prev", "is", "False", ":", "masked", ".", "append", "(", "[", "]", ")", "masked", "[", "-", "1", "]", ".", "append", "(", "base", ")", "prev", "=", "True", "return", "nm", ",", "masked"], "docstring": "parse masked sequence into non-masked and masked regions", "docstring_tokens": ["parse", "masked", "sequence", "into", "non", "-", "masked", "and", "masked", "regions"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_masked.py#L13-L31", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for stripping out masked sequences if the masked sequence\nis above a specified length\n\"\"\"\n\nimport sys\nimport os\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\nimport argparse\n\n\ndef strip_masked(fasta, min_len, print_masked):\n    \"\"\"\n    remove masked regions from fasta file as long as\n    they are longer than min_len\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        nm, masked = parse_masked(seq, min_len)\n        nm = [\"%s removed_masked >=%s\" % (seq[0], min_len), \"\".join(nm)]\n        yield [0, nm]\n        if print_masked is True:\n            for i, m in enumerate([i for i in masked if i != []], 1):\n                m = [\"%s insertion:%s\" % (seq[0], i), \"\".join(m)]\n                yield [1, m]\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# remove masked portion of sequences in fasta file\"\n    )\n    parser.add_argument(\"-f\", required=True, help=\"fasta file\")\n    parser.add_argument(\n        \"-l\",\n        default=0,\n        type=int,\n        help=\"minimum length of masked region required for removal\",\n    )\n    parser.add_argument(\n        \"--print-masked\", action=\"store_true\", help=\"print masked sequences to stderr\"\n    )\n    args = vars(parser.parse_args())\n    fasta, min_len, print_masked = args[\"f\"], args[\"l\"], args[\"print_masked\"]\n    if fasta == \"-\":\n        fasta = sys.stdin\n    else:\n        fasta = open(fasta)\n    for i in strip_masked(fasta, min_len, print_masked):\n        if i[0] == 0:\n            print(\"\\n\".join(i[1]))\n        else:\n            print(\"\\n\".join(i[1]), file=sys.stderr)\n", "levels": [0], "package": ["import sys", "import os", "from ctbBio.fasta import iterate_fasta as parse_fasta", "import argparse"], "function": ["def strip_masked(fasta, min_len, print_masked):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/strip_masked.py", "func_name": "strip_masked", "original_string": "def strip_masked(fasta, min_len, print_masked):\n    \"\"\"\n    remove masked regions from fasta file as long as\n    they are longer than min_len\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        nm, masked = parse_masked(seq, min_len)\n        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]\n        yield [0, nm]\n        if print_masked is True:\n            for i, m in enumerate([i for i in masked if i != []], 1):\n                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]\n                yield [1, m]", "language": "python", "code": "def strip_masked(fasta, min_len, print_masked):\n    \"\"\"\n    remove masked regions from fasta file as long as\n    they are longer than min_len\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        nm, masked = parse_masked(seq, min_len)\n        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]\n        yield [0, nm]\n        if print_masked is True:\n            for i, m in enumerate([i for i in masked if i != []], 1):\n                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]\n                yield [1, m]", "code_tokens": ["def", "strip_masked", "(", "fasta", ",", "min_len", ",", "print_masked", ")", ":", "for", "seq", "in", "parse_fasta", "(", "fasta", ")", ":", "nm", ",", "masked", "=", "parse_masked", "(", "seq", ",", "min_len", ")", "nm", "=", "[", "'%s removed_masked >=%s'", "%", "(", "seq", "[", "0", "]", ",", "min_len", ")", ",", "''", ".", "join", "(", "nm", ")", "]", "yield", "[", "0", ",", "nm", "]", "if", "print_masked", "is", "True", ":", "for", "i", ",", "m", "in", "enumerate", "(", "[", "i", "for", "i", "in", "masked", "if", "i", "!=", "[", "]", "]", ",", "1", ")", ":", "m", "=", "[", "'%s insertion:%s'", "%", "(", "seq", "[", "0", "]", ",", "i", ")", ",", "''", ".", "join", "(", "m", ")", "]", "yield", "[", "1", ",", "m", "]"], "docstring": "remove masked regions from fasta file as long as\n    they are longer than min_len", "docstring_tokens": ["remove", "masked", "regions", "from", "fasta", "file", "as", "long", "as", "they", "are", "longer", "than", "min_len"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_masked.py#L33-L45", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for stripping out masked sequences if the masked sequence\nis above a specified length\n\"\"\"\n\nimport sys\nimport os\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\nimport argparse\n\n\ndef parse_masked(seq, min_len):\n    \"\"\"\n    parse masked sequence into non-masked and masked regions\n    \"\"\"\n    nm, masked = [], [[]]\n    prev = None\n    for base in seq[1]:\n        if base.isupper():\n            nm.append(base)\n            if masked != [[]] and len(masked[-1]) < min_len:\n                nm.extend(masked[-1])\n                del masked[-1]\n            prev = False\n        elif base.islower():\n            if prev is False:\n                masked.append([])\n            masked[-1].append(base)\n            prev = True\n    return nm, masked\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# remove masked portion of sequences in fasta file\"\n    )\n    parser.add_argument(\"-f\", required=True, help=\"fasta file\")\n    parser.add_argument(\n        \"-l\",\n        default=0,\n        type=int,\n        help=\"minimum length of masked region required for removal\",\n    )\n    parser.add_argument(\n        \"--print-masked\", action=\"store_true\", help=\"print masked sequences to stderr\"\n    )\n    args = vars(parser.parse_args())\n    fasta, min_len, print_masked = args[\"f\"], args[\"l\"], args[\"print_masked\"]\n    if fasta == \"-\":\n        fasta = sys.stdin\n    else:\n        fasta = open(fasta)\n    for i in strip_masked(fasta, min_len, print_masked):\n        if i[0] == 0:\n            print(\"\\n\".join(i[1]))\n        else:\n            print(\"\\n\".join(i[1]), file=sys.stderr)\n", "levels": [0], "package": ["import sys", "import os", "from ctbBio.fasta import iterate_fasta as parse_fasta", "import argparse"], "function": ["def parse_masked(seq, min_len):\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/network_plots_gephi.py", "func_name": "get_relative_abundance", "original_string": "def get_relative_abundance(biomfile):\n    \"\"\"\n    Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.\n    \"\"\"\n    biomf = biom.load_table(biomfile)\n    norm_biomf = biomf.norm(inplace=False)\n    rel_abd = {}\n    for sid in norm_biomf.ids():\n        rel_abd[sid] = {}\n        for otuid in norm_biomf.ids(\"observation\"):\n            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=\"observation\")[\"taxonomy\"])\n            otuname = \" \".join(otuname.split(\"_\"))\n            abd = norm_biomf.get_value_by_ids(otuid, sid)\n            rel_abd[sid][otuname] = abd\n    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)\n    return ast_rel_abd", "language": "python", "code": "def get_relative_abundance(biomfile):\n    \"\"\"\n    Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.\n    \"\"\"\n    biomf = biom.load_table(biomfile)\n    norm_biomf = biomf.norm(inplace=False)\n    rel_abd = {}\n    for sid in norm_biomf.ids():\n        rel_abd[sid] = {}\n        for otuid in norm_biomf.ids(\"observation\"):\n            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=\"observation\")[\"taxonomy\"])\n            otuname = \" \".join(otuname.split(\"_\"))\n            abd = norm_biomf.get_value_by_ids(otuid, sid)\n            rel_abd[sid][otuname] = abd\n    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)\n    return ast_rel_abd", "code_tokens": ["def", "get_relative_abundance", "(", "biomfile", ")", ":", "biomf", "=", "biom", ".", "load_table", "(", "biomfile", ")", "norm_biomf", "=", "biomf", ".", "norm", "(", "inplace", "=", "False", ")", "rel_abd", "=", "{", "}", "for", "sid", "in", "norm_biomf", ".", "ids", "(", ")", ":", "rel_abd", "[", "sid", "]", "=", "{", "}", "for", "otuid", "in", "norm_biomf", ".", "ids", "(", "\"observation\"", ")", ":", "otuname", "=", "oc", ".", "otu_name", "(", "norm_biomf", ".", "metadata", "(", "otuid", ",", "axis", "=", "\"observation\"", ")", "[", "\"taxonomy\"", "]", ")", "otuname", "=", "\" \"", ".", "join", "(", "otuname", ".", "split", "(", "\"_\"", ")", ")", "abd", "=", "norm_biomf", ".", "get_value_by_ids", "(", "otuid", ",", "sid", ")", "rel_abd", "[", "sid", "]", "[", "otuname", "]", "=", "abd", "ast_rel_abd", "=", "bc", ".", "arcsine_sqrt_transform", "(", "rel_abd", ")", "return", "ast_rel_abd"], "docstring": "Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.", "docstring_tokens": ["Return", "arcsine", "transformed", "relative", "abundance", "from", "a", "BIOM", "format", "file", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/network_plots_gephi.py#L33-L57", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\"\"\"\nAbstract: Generate network plots using NetworkX and Gephi.\nAuthor: Akshay Paropkari\nDate: 02/10/2016\n\"\"\"\nimport sys\nimport argparse\nfrom collections import defaultdict\n\nimporterrors = []\ntry:\n    import biom\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    import pandas as pd\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    import networkx as nx\nexcept ImportError as ie:\n    importerrors.append(ie)\ntry:\n    from phylotoast import biom_calc as bc, otu_calc as oc\nexcept ImportError as ie:\n    importerrors.append(ie)\nif len(importerrors) > 0:\n    for err in importerrors:\n        print(\"Import Error: {}\".format(err))\n    sys.exit()\n\n\ndef handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Create network plots based \" \"on correlation matrix.\"\n    )\n    parser.add_argument(\"biom_file\", help=\"Biom file OTU table.\")\n    parser.add_argument(\n        \"mapping_file\", help=\"Mapping file for reading \" \"sampleIDs and their groups.\"\n    )\n    parser.add_argument(\n        \"condition_column\",\n        help=\"Column name in mapping file \" \"denoting the categories.\",\n    )\n    parser.add_argument(\n        \"in_corr_mat\",\n        help=\"Correlation matrix file. The \"\n        \"format for the tab-separated file should be: \"\n        \"Category -> Variable -> by Variable -> Correlation\",\n    )\n    parser.add_argument(\"cat_name\", help=\"Category to be plotted.\")\n    parser.add_argument(\n        \"-go\",\n        \"--gexf_out\",\n        help=\"Graph information written to this Graph Exchange\"\n        \" XML Format file. This file can be input to Gephi.\",\n    )\n    parser.add_argument(\n        \"-fp\",\n        \"--fil_pct\",\n        type=float,\n        default=0.75,\n        help=\"Specify the minimum value of correlation \"\n        \"strength to display. By default, all correlations \"\n        \">=0.75 will be shown.\",\n    )\n    parser.add_argument(\"-w\", \"--stats_out_fnh\", help=\"Write out graph statistics.\")\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    # Error handling input files\n    try:\n        relative_abundance = get_relative_abundance(args.biom_file)\n    except OSError as ose:\n        sys.exit(\"\\nError in BIOM file path: {}\\n\".format(ose))\n    try:\n        # Read in the mapping file\n        mapf = pd.read_csv(args.mapping_file, sep=\"\\t\")\n    except IOError as ioe:\n        sys.exit(\"\\nError in mapping file path: {}\\n\".format(ioe))\n    try:\n        # Read the correlation data\n        corr_data = pd.read_csv(args.in_corr_mat, sep=\"\\t\")\n    except IOError as ioe:\n        sys.exit(\"\\nError in correlation matrix file path: {}\\n\".format(ioe))\n\n    # get category-wise nodes\n    cat_sids = defaultdict(list)\n    for rows in mapf.iterrows():\n        row = rows[1]\n        cat_sids[row[args.condition_column]].append(row[\"#SampleID\"])\n\n    # initialize graph\n    G = nx.Graph()\n\n    # get category-wise graph edge data and edge colors\n    for rows in corr_data.iterrows():\n        row = rows[1]\n        if row[\"Category\"] == args.cat_name and abs(row[\"Correlation\"]) > args.fil_pct:\n            if row[\"Correlation\"] > 0:\n                G.add_weighted_edges_from(\n                    [\n                        (\n                            \" \".join(row[\"Variable\"].split(\"_\")),\n                            \" \".join(row[\"by Variable\"].split(\"_\")),\n                            row[\"Correlation\"],\n                        )\n                    ],\n                    color=\"#00CC00\",\n                )  # Green\n            else:\n                G.add_weighted_edges_from(\n                    [\n                        (\n                            \" \".join(row[\"Variable\"].split(\"_\")),\n                            \" \".join(row[\"by Variable\"].split(\"_\")),\n                            (row[\"Correlation\"]) * -1,\n                        )\n                    ],\n                    color=\"#FF0000\",\n                )  # Red\n\n    # add node attributes to graph node object\n    for otu, attr in G.node.items():\n        total_abd = 0\n        for sid in cat_sids[args.cat_name]:\n            total_abd += relative_abundance[sid][otu]\n        try:\n            mean_otu_cat_abd = total_abd / len(cat_sids[args.cat_name])\n        except ZeroDivisionError as zde:\n            sys.exit(\n                \"\\nPlease check to see if category name in mapping file and that is\"\n                \" supplied in `--cat_name` parameter match up.\\nError: {}\\n\".format(zde)\n            )\n        G.node[otu][\"node_size\"] = mean_otu_cat_abd\n\n    # convert node labels to integers\n    H = nx.convert_node_labels_to_integers(\n        G, first_label=1, ordering=\"decreasing degree\", label_attribute=\"id\"\n    )\n\n    # Write out GEXF file for using with Gephi\n    if args.gexf_out:\n        nx.write_gexf(H, args.gexf_out, version=\"1.2draft\")\n\n    # Write out betweenness centrality measure to file\n    if args.stats_out_fnh:\n        with open(args.stats_out_fnh, \"w\") as poi:\n            poi.write(\"OTU Node\\tDegree Centrality\\tBetweenness Centrality\\n\")\n            dc = nx.degree_centrality(G)\n            bc = nx.betweenness_centrality(G, weight=\"weight\")\n            for key in sorted(bc.keys()):\n                poi.write(\"{}\\t{}\\t{}\\n\".format(key, dc[key], bc[key]))\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "levels": [0, 0], "package": ["import sys", "import argparse", "from collections import defaultdict", "import biom", "import pandas as pd", "import networkx as nx", "from phylotoast import biom_calc as bc, otu_calc as oc"], "function": ["def handle_program_options():\n", "def main():\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/iTol.py", "func_name": "find_otu", "original_string": "def find_otu(otuid, tree):\n    \"\"\"\n    Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.\n    \"\"\"\n    for m in re.finditer(otuid, tree):\n        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]\n        if before in [\"(\", \",\", \")\"] and after in [\":\", \";\"]:\n            return m.start()\n    return None", "language": "python", "code": "def find_otu(otuid, tree):\n    \"\"\"\n    Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.\n    \"\"\"\n    for m in re.finditer(otuid, tree):\n        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]\n        if before in [\"(\", \",\", \")\"] and after in [\":\", \";\"]:\n            return m.start()\n    return None", "code_tokens": ["def", "find_otu", "(", "otuid", ",", "tree", ")", ":", "for", "m", "in", "re", ".", "finditer", "(", "otuid", ",", "tree", ")", ":", "before", ",", "after", "=", "tree", "[", "m", ".", "start", "(", ")", "-", "1", "]", ",", "tree", "[", "m", ".", "start", "(", ")", "+", "len", "(", "otuid", ")", "]", "if", "before", "in", "[", "\"(\"", ",", "\",\"", ",", "\")\"", "]", "and", "after", "in", "[", "\":\"", ",", "\";\"", "]", ":", "return", "m", ".", "start", "(", ")", "return", "None"], "docstring": "Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.", "docstring_tokens": ["Find", "an", "OTU", "ID", "in", "a", "Newick", "-", "format", "tree", ".", "Return", "the", "starting", "position", "of", "the", "ID", "or", "None", "if", "not", "found", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/iTol.py#L17-L26", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\"\"\"\nCreated on Feb 8, 2012\n\nAuthor: Shareef M Dabdoub\n\"\"\"\nimport sys\nimport re\nimport argparse\nfrom phylotoast import biom_calc as bc, otu_calc as oc, util\n\ntry:\n    import biom\nexcept ImportError:\n    sys.exit(\"Please install missing module: {}.\".format(\"biom-format\"))\n\n\ndef newick_replace_otuids(tree, biomf):\n    \"\"\"\n    Replace the OTU ids in the Newick phylogenetic tree format with truncated\n    OTU names\n    \"\"\"\n    for val, id_, md in biomf.iter(axis=\"observation\"):\n        otu_loc = find_otu(id_, tree)\n        if otu_loc is not None:\n            tree = (\n                tree[:otu_loc]\n                + oc.otu_name(md[\"taxonomy\"])\n                + tree[otu_loc + len(id_) :]\n            )\n    return tree\n\n\ndef handle_program_options():\n    parser = argparse.ArgumentParser(\n        description=\"Create files appropriate for\\\n                                     use in the iTol visualization program by \\\n                                     using the abundance data from a \\\n                                     biom-format file and groups specified in \\\n                                     a QIIME mapping file. The program also \\\n                                     modifies a Newick-format phylogenetic \\\n                                     tree file to use proper taxonomic names \\\n                                     instead of OTU IDs for useful display in \\\n                                     iTol.\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--otu_table\",\n        required=True,\n        help=\"The biom-format file with OTU-Sample abundance \\\n                              data.\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--mapping\",\n        required=True,\n        help=\"The mapping file specifying group information \\\n                              for each sample.\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--input_tree\",\n        default=\"\",\n        help=\"A phylogenetic tree in Newick format to be \\\n                              modified by exchanging the OTU ID node names for\\\n                              taxonomic names.\",\n    )\n    parser.add_argument(\n        \"-e\", \"--output_tre\", default=\"iTol.tre\", help=\"The output .tre file\"\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_itol_table\",\n        default=\"iTol_table.txt\",\n        help=\"Other than a phylogenetic tree, the main input \\\n                              to iTol is a dataset file containing some \\\n                              representation of the abundance of every OTU \\\n                              across the specified data groups. This program \\\n                              provides multiple calculation methods. See the \\\n                              --analysis_metric option for details.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--map_categories\",\n        default=None,\n        help=\"Any mapping categories, such as treatment type, \\\n                              that will be used to group the data in the \\\n                              output iTol table. For example, one category \\\n                              with three types will result in three data \\\n                              columns in the final output. Two categories with\\\n                              three types each will result in six data \\\n                              columns. Default is no categories and all the \\\n                              data will be treated as a single group.\",\n    )\n    parser.add_argument(\n        \"-a\",\n        \"--analysis_metric\",\n        default=\"MRA\",\n        choices=[\"MRA\", \"NMRA\", \"raw\"],\n        help=\"Specifies which metric is calculated on the \\\n                              abundance data in the OTU table. Available \\\n                              options: MRE - mean relative abundance \\\n                              (Abundance data is normalized by total sample \\\n                              abundance, then averaged across OTU), NMRE - \\\n                              normalized mean relative abundance (MRE \\\n                              normalized by the total MRE across the groups \\\n                              as specified in --map_categories), raw (outputs \\\n                              the actual sequence abundance data for \\\n                              each OTU).\",\n    )\n    parser.add_argument(\n        \"--stabilize_variance\",\n        action=\"store_true\",\n        default=False,\n        help=\"Apply the variance-stabilizing arcsine square\\\n                              root transformation to the OTU proportion data.\",\n    )\n    parser.add_argument(\n        \"--keep_otuids\",\n        action=\"store_true\",\n        default=False,\n        help=\"Keep OTU IDs in the output files instead of.\"\n        \"replacing them with shortened taxonomic names.\",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    try:\n        with open(args.otu_table):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with OTU_Sample abundance data file:{}\\n\".format(ioe))\n\n    try:\n        with open(args.mapping):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with mapping file:{}\\n\".format(ioe))\n\n    # input data\n    biomf = biom.load_table(args.otu_table)\n    map_header, imap = util.parse_map_file(args.mapping)\n\n    # rewrite tree file with otu names, skip if keep_otuids specified\n    if args.input_tree and not args.keep_otuids:\n        with open(args.input_tree) as treF, open(args.output_tre, \"w\") as outF:\n            tree = treF.readline()\n            if \"'\" in tree:\n                tree = tree.replace(\"'\", \"\")\n            outF.write(newick_replace_otuids(tree, biomf))\n\n    if not args.keep_otuids:\n        oid_rows = {\n            id_: md[\"taxonomy\"] for val, id_, md in biomf.iter(axis=\"observation\")\n        }\n\n    # calculate analysis results\n    categories = None\n    if args.map_categories is not None and args.analysis_metric != \"raw\":\n        categories = args.map_categories.split(\",\")\n\n    # set transform if --stabilize_variance is specfied\n    tform = bc.arcsine_sqrt_transform if args.stabilize_variance else None\n\n    groups = util.gather_categories(imap, map_header, categories)\n    for group in groups.values():\n        if args.analysis_metric in [\"MRA\", \"NMRA\"]:\n            results = bc.MRA(biomf, group.sids, transform=tform)\n        elif args.analysis_metric == \"raw\":\n            results = bc.transform_raw_abundance(\n                biomf, sampleIDs=group.sids, sample_abd=False\n            )\n        if args.keep_otuids:\n            group.results.update({oid: results[oid] for oid in results})\n        else:\n            group.results.update(\n                {oc.otu_name(oid_rows[oid]): results[oid] for oid in results}\n            )\n\n    # write iTol data set file\n    with open(args.output_itol_table, \"w\") as itolF:\n        if args.analysis_metric == \"raw\":\n            itolF.write(\"DATASET_GRADIENT\\nSEPARATOR TAB\\n\")\n            itolF.write(\"DATASET_LABEL\\tLog Total Abundance\\n\")\n            itolF.write(\"COLOR\\t#000000\\n\")\n            itolF.write(\"LEGEND_TITLE\\tLog Total Abundance\\n\")\n            itolF.write(\"LEGEND_SHAPES\\t1\\n\")\n            itolF.write(\"LEGEND_COLORS\\t#000000\\n\")\n            itolF.write(\"LEGEND_LABELS\\tLog Total Abundance\\n\")\n            itolF.write(\"COLOR_MIN\\t#FFFFFF\\n\")\n            itolF.write(\"COLOR_MAX\\t#000000\\n\")\n        else:\n            itolF.write(\"DATASET_MULTIBAR\\nSEPARATOR TAB\\n\")\n            itolF.write(\"DATASET_LABEL\\t{}\\n\".format(args.analysis_metric))\n            itolF.write(\n                \"FIELD_COLORS\\t{}\\n\".format(\n                    \"\\t\".join([\"#ff0000\" for _ in range(len(groups))])\n                )\n            )\n            itolF.write(\"FIELD_LABELS\\t\" + \"\\t\".join(groups.keys()) + \"\\n\")\n            itolF.write(\"LEGEND_TITLE\\t{}\\n\".format(args.analysis_metric))\n            itolF.write(\n                \"LEGEND_SHAPES\\t{}\\n\".format(\n                    \"\\t\".join([\"1\" for _ in range(len(groups))])\n                )\n            )\n            itolF.write(\n                \"LEGEND_COLORS\\t{}\\n\".format(\n                    \"\\t\".join([\"#ff0000\" for _ in range(len(groups))])\n                )\n            )\n            itolF.write(\"LEGEND_LABELS\\t\" + \"\\t\".join(groups.keys()) + \"\\n\")\n            itolF.write(\"WIDTH\\t300\\n\")\n        itolF.write(\"DATA\\n\")\n\n        if args.keep_otuids:\n            all_otus = frozenset({id_ for id_ in biomf.ids(axis=\"observation\")})\n        else:\n            all_otus = frozenset(\n                {\n                    oc.otu_name(md[\"taxonomy\"])\n                    for val, id_, md in biomf.iter(axis=\"observation\")\n                }\n            )\n\n        for oname in all_otus:\n            row = [\"{name}\"]  # \\t{s:.2f}\\t{ns:.2f}\\n\"\n            row_data = {\"name\": oname}\n            msum = 0\n            for name, group in groups.iteritems():\n                row.append(\"{{{}:.5f}}\".format(name))\n                if oname in group.results:\n                    row_data[name] = group.results[oname]\n                else:\n                    row_data[name] = 0.0\n                msum += row_data[name]\n            # normalize avg relative abundance data\n            if args.analysis_metric == \"NMRA\" and msum > 0:\n                row_data.update(\n                    {\n                        key: data / msum\n                        for key, data in row_data.items()\n                        if key != \"name\"\n                    }\n                )\n            itolF.write(\"\\t\".join(row).format(**row_data) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 0, 0], "package": ["import sys", "import re", "import argparse", "from phylotoast import biom_calc as bc, otu_calc as oc, util", "import biom"], "function": ["def newick_replace_otuids(tree, biomf):\n", "def handle_program_options():\n", "def main():\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/iTol.py", "func_name": "newick_replace_otuids", "original_string": "def newick_replace_otuids(tree, biomf):\n    \"\"\"\n    Replace the OTU ids in the Newick phylogenetic tree format with truncated\n    OTU names\n    \"\"\"\n    for val, id_, md in biomf.iter(axis=\"observation\"):\n        otu_loc = find_otu(id_, tree)\n        if otu_loc is not None:\n            tree = tree[:otu_loc] + \\\n                   oc.otu_name(md[\"taxonomy\"]) + \\\n                   tree[otu_loc + len(id_):]\n    return tree", "language": "python", "code": "def newick_replace_otuids(tree, biomf):\n    \"\"\"\n    Replace the OTU ids in the Newick phylogenetic tree format with truncated\n    OTU names\n    \"\"\"\n    for val, id_, md in biomf.iter(axis=\"observation\"):\n        otu_loc = find_otu(id_, tree)\n        if otu_loc is not None:\n            tree = tree[:otu_loc] + \\\n                   oc.otu_name(md[\"taxonomy\"]) + \\\n                   tree[otu_loc + len(id_):]\n    return tree", "code_tokens": ["def", "newick_replace_otuids", "(", "tree", ",", "biomf", ")", ":", "for", "val", ",", "id_", ",", "md", "in", "biomf", ".", "iter", "(", "axis", "=", "\"observation\"", ")", ":", "otu_loc", "=", "find_otu", "(", "id_", ",", "tree", ")", "if", "otu_loc", "is", "not", "None", ":", "tree", "=", "tree", "[", ":", "otu_loc", "]", "+", "oc", ".", "otu_name", "(", "md", "[", "\"taxonomy\"", "]", ")", "+", "tree", "[", "otu_loc", "+", "len", "(", "id_", ")", ":", "]", "return", "tree"], "docstring": "Replace the OTU ids in the Newick phylogenetic tree format with truncated\n    OTU names", "docstring_tokens": ["Replace", "the", "OTU", "ids", "in", "the", "Newick", "phylogenetic", "tree", "format", "with", "truncated", "OTU", "names"], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/iTol.py#L29-L40", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python\n\"\"\"\nCreated on Feb 8, 2012\n\nAuthor: Shareef M Dabdoub\n\"\"\"\nimport sys\nimport re\nimport argparse\nfrom phylotoast import biom_calc as bc, otu_calc as oc, util\n\ntry:\n    import biom\nexcept ImportError:\n    sys.exit(\"Please install missing module: {}.\".format(\"biom-format\"))\n\n\ndef find_otu(otuid, tree):\n    \"\"\"\n    Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.\n    \"\"\"\n    for m in re.finditer(otuid, tree):\n        before, after = tree[m.start() - 1], tree[m.start() + len(otuid)]\n        if before in [\"(\", \",\", \")\"] and after in [\":\", \";\"]:\n            return m.start()\n    return None\n\n\ndef handle_program_options():\n    parser = argparse.ArgumentParser(\n        description=\"Create files appropriate for\\\n                                     use in the iTol visualization program by \\\n                                     using the abundance data from a \\\n                                     biom-format file and groups specified in \\\n                                     a QIIME mapping file. The program also \\\n                                     modifies a Newick-format phylogenetic \\\n                                     tree file to use proper taxonomic names \\\n                                     instead of OTU IDs for useful display in \\\n                                     iTol.\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--otu_table\",\n        required=True,\n        help=\"The biom-format file with OTU-Sample abundance \\\n                              data.\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--mapping\",\n        required=True,\n        help=\"The mapping file specifying group information \\\n                              for each sample.\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--input_tree\",\n        default=\"\",\n        help=\"A phylogenetic tree in Newick format to be \\\n                              modified by exchanging the OTU ID node names for\\\n                              taxonomic names.\",\n    )\n    parser.add_argument(\n        \"-e\", \"--output_tre\", default=\"iTol.tre\", help=\"The output .tre file\"\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_itol_table\",\n        default=\"iTol_table.txt\",\n        help=\"Other than a phylogenetic tree, the main input \\\n                              to iTol is a dataset file containing some \\\n                              representation of the abundance of every OTU \\\n                              across the specified data groups. This program \\\n                              provides multiple calculation methods. See the \\\n                              --analysis_metric option for details.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--map_categories\",\n        default=None,\n        help=\"Any mapping categories, such as treatment type, \\\n                              that will be used to group the data in the \\\n                              output iTol table. For example, one category \\\n                              with three types will result in three data \\\n                              columns in the final output. Two categories with\\\n                              three types each will result in six data \\\n                              columns. Default is no categories and all the \\\n                              data will be treated as a single group.\",\n    )\n    parser.add_argument(\n        \"-a\",\n        \"--analysis_metric\",\n        default=\"MRA\",\n        choices=[\"MRA\", \"NMRA\", \"raw\"],\n        help=\"Specifies which metric is calculated on the \\\n                              abundance data in the OTU table. Available \\\n                              options: MRE - mean relative abundance \\\n                              (Abundance data is normalized by total sample \\\n                              abundance, then averaged across OTU), NMRE - \\\n                              normalized mean relative abundance (MRE \\\n                              normalized by the total MRE across the groups \\\n                              as specified in --map_categories), raw (outputs \\\n                              the actual sequence abundance data for \\\n                              each OTU).\",\n    )\n    parser.add_argument(\n        \"--stabilize_variance\",\n        action=\"store_true\",\n        default=False,\n        help=\"Apply the variance-stabilizing arcsine square\\\n                              root transformation to the OTU proportion data.\",\n    )\n    parser.add_argument(\n        \"--keep_otuids\",\n        action=\"store_true\",\n        default=False,\n        help=\"Keep OTU IDs in the output files instead of.\"\n        \"replacing them with shortened taxonomic names.\",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    try:\n        with open(args.otu_table):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with OTU_Sample abundance data file:{}\\n\".format(ioe))\n\n    try:\n        with open(args.mapping):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with mapping file:{}\\n\".format(ioe))\n\n    # input data\n    biomf = biom.load_table(args.otu_table)\n    map_header, imap = util.parse_map_file(args.mapping)\n\n    # rewrite tree file with otu names, skip if keep_otuids specified\n    if args.input_tree and not args.keep_otuids:\n        with open(args.input_tree) as treF, open(args.output_tre, \"w\") as outF:\n            tree = treF.readline()\n            if \"'\" in tree:\n                tree = tree.replace(\"'\", \"\")\n            outF.write(newick_replace_otuids(tree, biomf))\n\n    if not args.keep_otuids:\n        oid_rows = {\n            id_: md[\"taxonomy\"] for val, id_, md in biomf.iter(axis=\"observation\")\n        }\n\n    # calculate analysis results\n    categories = None\n    if args.map_categories is not None and args.analysis_metric != \"raw\":\n        categories = args.map_categories.split(\",\")\n\n    # set transform if --stabilize_variance is specfied\n    tform = bc.arcsine_sqrt_transform if args.stabilize_variance else None\n\n    groups = util.gather_categories(imap, map_header, categories)\n    for group in groups.values():\n        if args.analysis_metric in [\"MRA\", \"NMRA\"]:\n            results = bc.MRA(biomf, group.sids, transform=tform)\n        elif args.analysis_metric == \"raw\":\n            results = bc.transform_raw_abundance(\n                biomf, sampleIDs=group.sids, sample_abd=False\n            )\n        if args.keep_otuids:\n            group.results.update({oid: results[oid] for oid in results})\n        else:\n            group.results.update(\n                {oc.otu_name(oid_rows[oid]): results[oid] for oid in results}\n            )\n\n    # write iTol data set file\n    with open(args.output_itol_table, \"w\") as itolF:\n        if args.analysis_metric == \"raw\":\n            itolF.write(\"DATASET_GRADIENT\\nSEPARATOR TAB\\n\")\n            itolF.write(\"DATASET_LABEL\\tLog Total Abundance\\n\")\n            itolF.write(\"COLOR\\t#000000\\n\")\n            itolF.write(\"LEGEND_TITLE\\tLog Total Abundance\\n\")\n            itolF.write(\"LEGEND_SHAPES\\t1\\n\")\n            itolF.write(\"LEGEND_COLORS\\t#000000\\n\")\n            itolF.write(\"LEGEND_LABELS\\tLog Total Abundance\\n\")\n            itolF.write(\"COLOR_MIN\\t#FFFFFF\\n\")\n            itolF.write(\"COLOR_MAX\\t#000000\\n\")\n        else:\n            itolF.write(\"DATASET_MULTIBAR\\nSEPARATOR TAB\\n\")\n            itolF.write(\"DATASET_LABEL\\t{}\\n\".format(args.analysis_metric))\n            itolF.write(\n                \"FIELD_COLORS\\t{}\\n\".format(\n                    \"\\t\".join([\"#ff0000\" for _ in range(len(groups))])\n                )\n            )\n            itolF.write(\"FIELD_LABELS\\t\" + \"\\t\".join(groups.keys()) + \"\\n\")\n            itolF.write(\"LEGEND_TITLE\\t{}\\n\".format(args.analysis_metric))\n            itolF.write(\n                \"LEGEND_SHAPES\\t{}\\n\".format(\n                    \"\\t\".join([\"1\" for _ in range(len(groups))])\n                )\n            )\n            itolF.write(\n                \"LEGEND_COLORS\\t{}\\n\".format(\n                    \"\\t\".join([\"#ff0000\" for _ in range(len(groups))])\n                )\n            )\n            itolF.write(\"LEGEND_LABELS\\t\" + \"\\t\".join(groups.keys()) + \"\\n\")\n            itolF.write(\"WIDTH\\t300\\n\")\n        itolF.write(\"DATA\\n\")\n\n        if args.keep_otuids:\n            all_otus = frozenset({id_ for id_ in biomf.ids(axis=\"observation\")})\n        else:\n            all_otus = frozenset(\n                {\n                    oc.otu_name(md[\"taxonomy\"])\n                    for val, id_, md in biomf.iter(axis=\"observation\")\n                }\n            )\n\n        for oname in all_otus:\n            row = [\"{name}\"]  # \\t{s:.2f}\\t{ns:.2f}\\n\"\n            row_data = {\"name\": oname}\n            msum = 0\n            for name, group in groups.iteritems():\n                row.append(\"{{{}:.5f}}\".format(name))\n                if oname in group.results:\n                    row_data[name] = group.results[oname]\n                else:\n                    row_data[name] = 0.0\n                msum += row_data[name]\n            # normalize avg relative abundance data\n            if args.analysis_metric == \"NMRA\" and msum > 0:\n                row_data.update(\n                    {\n                        key: data / msum\n                        for key, data in row_data.items()\n                        if key != \"name\"\n                    }\n                )\n            itolF.write(\"\\t\".join(row).format(**row_data) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 0, 0], "package": ["import sys", "import re", "import argparse", "from phylotoast import biom_calc as bc, otu_calc as oc, util", "import biom"], "function": ["def find_otu(otuid, tree):\n", "def handle_program_options():\n", "def main():\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/cluster_ani.py", "func_name": "genome_info", "original_string": "def genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg       = info['#SCGs']\n        dups      = info['#SCG duplicates']\n        length    = info['genome size (bp)']\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info['genome size (bp)'], genome]", "language": "python", "code": "def genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg       = info['#SCGs']\n        dups      = info['#SCG duplicates']\n        length    = info['genome size (bp)']\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info['genome size (bp)'], genome]", "code_tokens": ["def", "genome_info", "(", "genome", ",", "info", ")", ":", "try", ":", "scg", "=", "info", "[", "'#SCGs'", "]", "dups", "=", "info", "[", "'#SCG duplicates'", "]", "length", "=", "info", "[", "'genome size (bp)'", "]", "return", "[", "scg", "-", "dups", ",", "length", ",", "genome", "]", "except", ":", "return", "[", "False", ",", "False", ",", "info", "[", "'genome size (bp)'", "]", ",", "genome", "]"], "docstring": "return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome", "docstring_tokens": ["return", "genome", "info", "for", "choosing", "representative"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L97-L112", "partition": "train", "up_fun_num": 3, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for clustering genomes based on average nucleotide\nidentity\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport subprocess\nfrom networkx import Graph as Graph\nfrom networkx import connected_components as connected_components\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n    \"\"\"\n    Create mash files for multiple fasta files\n    Input:\n        fastas <list[str]>  -- paths to fasta files\n        mash_file <str>     -- path to output mash file\n        threads <int>       -- # threads for parallelization\n        kmer <int>          -- kmer size for mash sketching\n        force <boolean>     -- force overwrite of all mash files\n    \"\"\"\n    mash_processes = set()\n    sketches = [fasta + \".msh\" for fasta in fastas]\n    devnull = open(os.devnull, \"w\")\n    # Perform the sketching\n    for fasta, sketch in zip(fastas, sketches):\n        if os.path.isfile(sketch):\n            continue\n        mash_cmd = [\"/opt/bin/bio/mash\", \"sketch\", \"-o\", fasta, \"-k\", str(kmer), fasta]\n        mash_processes.add(subprocess.Popen(mash_cmd, stderr=devnull))\n        if len(mash_processes) >= threads:\n            os.wait()\n            mash_processes.difference_update(\n                [mp for mp in mash_processes if mp.poll() is not None]\n            )\n    # Collect stragglers\n    for mp in mash_processes:\n        if mp.poll() is None:\n            mp.wait()\n    # Paste sketches into single mash\n    paste_mashes(sketches, mash_file, force=force)\n    return\n\n\ndef paste_mashes(sketches, pasted_mash, force=False):\n    \"\"\"\n    Combine mash files into single sketch\n    Input:\n        sketches <list[str]>  -- paths to sketch files\n        pasted_mash <str>     -- path to output mash file\n        force <boolean>     -- force overwrite of all mash file\n    \"\"\"\n    if os.path.isfile(pasted_mash):\n        if force:\n            subprocess.Popen([\"rm\", pasted_mash]).wait()\n        else:\n            return\n    pasted_mash = pasted_mash.rsplit(\".msh\")[0]\n    mash_cmd = [\"/opt/bin/bio/mash\", \"paste\", pasted_mash]\n    mash_cmd.extend(sketches)\n    process = subprocess.Popen(mash_cmd)\n    process.wait()\n    return\n\n\ndef ani(fastas, mash_file, sim_threshold, threads):\n    \"\"\"\n    Use mash to estimate ANI of genomes\n    Input:\n        fastas <list[str]>      -- paths to fasta files\n        sim_threshold <float>   -- fractional cutoff % identity for cluster joining\n        mash_file <str>         -- pasted sketch file of all fastas being compared\n        threads <int>           -- number threads for distance estimation\n    \"\"\"\n    ANI = Graph()\n    # use Mash to estimate ANI\n    for fasta in fastas:\n        indiv_mash = fasta + \".msh\"\n        if os.path.isfile(indiv_mash):\n            cmp_file = indiv_mash\n        else:\n            cmp_file = fasta\n        mash_cmd = [\"/opt/bin/bio/mash\", \"dist\", cmp_file, mash_file]\n        process = subprocess.Popen(mash_cmd, stdout=subprocess.PIPE)\n        for pair in process.communicate()[0].splitlines():\n            a, b, dist, p, shared = pair.decode().strip().split()\n            a = a.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            b = b.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            p = float(p)\n            similarity = (1 - float(dist)) * 100\n            if similarity >= sim_threshold:\n                ANI.add_edge(a, b, si=similarity, pval=p, sharedK=shared)\n        process.wait()\n    return ANI\n\n\ndef print_clusters(fastas, info, ANI):\n    \"\"\"\n    choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome\n    \"\"\"\n    header = [\n        \"#cluster\",\n        \"num. genomes\",\n        \"rep.\",\n        \"genome\",\n        \"#SCGs\",\n        \"#SCG duplicates\",\n        \"genome size (bp)\",\n        \"fragments\",\n        \"list\",\n    ]\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted(\n            [genome_info(genome, info[genome]) for genome in cluster],\n            key=lambda x: x[0:],\n            reverse=True,\n        )\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    info[genome][\"#SCGs\"],\n                    info[genome][\"#SCG duplicates\"],\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            except:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    \"n/a\",\n                    \"n/a\",\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            if rep == genome:\n                stats = [\"*%s\" % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set(\n        [i.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0] for i in fastas]\n    )\n    for cluster_num, genome in enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                info[genome][\"#SCGs\"],\n                info[genome][\"#SCG duplicates\"],\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        except:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                \"n/a\",\n                \"n/a\",\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        yield stats\n\n\ndef to_int(i):\n    \"\"\"\n    convert to integer, if possible\n    \"\"\"\n    try:\n        return int(i)\n    except:\n        return i\n\n\ndef parse_ggKbase_tables(tables, id_type):\n    \"\"\"\n    convert ggKbase genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"name\"):\n                header = line\n                header[4] = \"genome size (bp)\"\n                header[12] = \"#SCGs\"\n                header[13] = \"#SCG duplicates\"\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False:  # try to use name and code ID\n                if \"UNK\" in code or \"unknown\" in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print(\"# duplicate name or code in table(s)\", file=sys.stderr)\n                    print(\"# %s and/or %s\" % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item: stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item: stat for item, stat in zip(header, info)}\n            else:\n                if id_type == \"name\":\n                    ID = name\n                elif id_type == \"code\":\n                    ID = code\n                else:\n                    print(\"# specify name or code column using -id\", file=sys.stderr)\n                    exit()\n                ID = ID.replace(\" \", \"\")\n                g2info[ID] = {item: stat for item, stat in zip(header, info)}\n                if g2info[ID][\"genome size (bp)\"] == \"\":\n                    g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"Bin Id\"):\n                header = line\n                header[8] = \"genome size (bp)\"\n                header[5] = \"#SCGs\"\n                header[6] = \"#SCG duplicates\"\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(\" \", \"\")\n            g2info[ID] = {item: stat for item, stat in zip(header, info)}\n            if g2info[ID][\"genome size (bp)\"] == \"\":\n                g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef genome_lengths(fastas, info):\n    \"\"\"\n    get genome lengths\n    \"\"\"\n    if info is False:\n        info = {}\n    for genome in fastas:\n        name = genome.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n        if name in info:\n            continue\n        length = 0\n        fragments = 0\n        for seq in parse_fasta(genome):\n            length += len(seq[1])\n            fragments += 1\n        info[name] = {\"genome size (bp)\": length, \"# contigs\": fragments}\n    return info\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# cluster genomes based on average nucleotide identity (ani)\"\n    )\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fastas\")\n    parser.add_argument(\n        \"-m\",\n        action=\"store\",\n        required=True,\n        type=str,\n        help=\"mash file (will be created if it does not exist)\",\n    )\n    parser.add_argument(\n        \"-s\",\n        default=98,\n        type=float,\n        required=False,\n        help=\"percent similarity (default = 98)\",\n    )\n    parser.add_argument(\n        \"-g\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"ggKbase genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-c\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"checkM genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-id\",\n        default=False,\n        help=\"use name or code column in ggKbase table (default: try both)\",\n    )\n    parser.add_argument(\n        \"-t\", required=False, default=6, type=int, help=\"threads (default = 6)\"\n    )\n    args = vars(parser.parse_args())\n    fastas, similarity, id_type, threads, mash_file = (\n        args[\"f\"],\n        args[\"s\"],\n        args[\"id\"],\n        args[\"t\"],\n        args[\"m\"],\n    )\n    gg, cm = args[\"g\"], args[\"c\"]\n    if \".msh\" not in mash_file:\n        mash_file = \"%s.msh\" % (mash_file)\n    info = False  # assume no marker gene file is given (either ggKbase or checkM)\n    if gg is not False:\n        info = parse_ggKbase_tables(gg, id_type)\n    elif cm is not False:\n        info = parse_checkM_tables(cm)\n    info = genome_lengths(fastas, info)\n    make_mashes(fastas, mash_file, threads)\n    ANI = ani(fastas, mash_file, similarity, threads)\n    for genome in print_clusters(fastas, info, ANI):\n        print(\"\\t\".join([str(i) for i in genome]))\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import subprocess", "from networkx import Graph as Graph", "from networkx import connected_components as connected_components", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n", "def paste_mashes(sketches, pasted_mash, force=False):\n", "def ani(fastas, mash_file, sim_threshold, threads):\n", "def print_clusters(fastas, info, ANI):\n", "def to_int(i):\n", "def parse_ggKbase_tables(tables, id_type):\n", "def parse_checkM_tables(tables):\n", "def genome_lengths(fastas, info):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/cluster_ani.py", "func_name": "print_clusters", "original_string": "def print_clusters(fastas, info, ANI):\n    \"\"\"\n    choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome\n    \"\"\"\n    header = ['#cluster', 'num. genomes', 'rep.', 'genome', '#SCGs', '#SCG duplicates', \\\n            'genome size (bp)', 'fragments', 'list']\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted([genome_info(genome, info[genome]) \\\n                            for genome in cluster], \\\n                            key = lambda x: x[0:], reverse = True)\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [size, rep, genome, \\\n                            info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \\\n                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]\n            except:\n                stats = [size, rep, genome, \\\n                            'n/a', 'n/a', \\\n                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]\n            if rep == genome:\n                stats = ['*%s' % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set([i.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0] for i in fastas])\n    for cluster_num, genome in \\\n            enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = ['*%s' % (cluster_num), 1, genome, genome, \\\n                        info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \\\n                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]\n        except:\n            stats = ['*%s' % (cluster_num), 1, genome, genome, \\\n                        'n/a', 'n/a', \\\n                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]\n        yield stats", "language": "python", "code": "def print_clusters(fastas, info, ANI):\n    \"\"\"\n    choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome\n    \"\"\"\n    header = ['#cluster', 'num. genomes', 'rep.', 'genome', '#SCGs', '#SCG duplicates', \\\n            'genome size (bp)', 'fragments', 'list']\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted([genome_info(genome, info[genome]) \\\n                            for genome in cluster], \\\n                            key = lambda x: x[0:], reverse = True)\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [size, rep, genome, \\\n                            info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \\\n                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]\n            except:\n                stats = [size, rep, genome, \\\n                            'n/a', 'n/a', \\\n                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]\n            if rep == genome:\n                stats = ['*%s' % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set([i.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0] for i in fastas])\n    for cluster_num, genome in \\\n            enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = ['*%s' % (cluster_num), 1, genome, genome, \\\n                        info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \\\n                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]\n        except:\n            stats = ['*%s' % (cluster_num), 1, genome, genome, \\\n                        'n/a', 'n/a', \\\n                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]\n        yield stats", "code_tokens": ["def", "print_clusters", "(", "fastas", ",", "info", ",", "ANI", ")", ":", "header", "=", "[", "'#cluster'", ",", "'num. genomes'", ",", "'rep.'", ",", "'genome'", ",", "'#SCGs'", ",", "'#SCG duplicates'", ",", "'genome size (bp)'", ",", "'fragments'", ",", "'list'", "]", "yield", "header", "in_cluster", "=", "[", "]", "for", "cluster_num", ",", "cluster", "in", "enumerate", "(", "connected_components", "(", "ANI", ")", ")", ":", "cluster", "=", "sorted", "(", "[", "genome_info", "(", "genome", ",", "info", "[", "genome", "]", ")", "for", "genome", "in", "cluster", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", ":", "]", ",", "reverse", "=", "True", ")", "rep", "=", "cluster", "[", "0", "]", "[", "-", "1", "]", "cluster", "=", "[", "i", "[", "-", "1", "]", "for", "i", "in", "cluster", "]", "size", "=", "len", "(", "cluster", ")", "for", "genome", "in", "cluster", ":", "in_cluster", ".", "append", "(", "genome", ")", "try", ":", "stats", "=", "[", "size", ",", "rep", ",", "genome", ",", "info", "[", "genome", "]", "[", "'#SCGs'", "]", ",", "info", "[", "genome", "]", "[", "'#SCG duplicates'", "]", ",", "info", "[", "genome", "]", "[", "'genome size (bp)'", "]", ",", "info", "[", "genome", "]", "[", "'# contigs'", "]", ",", "cluster", "]", "except", ":", "stats", "=", "[", "size", ",", "rep", ",", "genome", ",", "'n/a'", ",", "'n/a'", ",", "info", "[", "genome", "]", "[", "'genome size (bp)'", "]", ",", "info", "[", "genome", "]", "[", "'# contigs'", "]", ",", "cluster", "]", "if", "rep", "==", "genome", ":", "stats", "=", "[", "'*%s'", "%", "(", "cluster_num", ")", "]", "+", "stats", "else", ":", "stats", "=", "[", "cluster_num", "]", "+", "stats", "yield", "stats", "# print singletons", "try", ":", "start", "=", "cluster_num", "+", "1", "except", ":", "start", "=", "0", "fastas", "=", "set", "(", "[", "i", ".", "rsplit", "(", "'.'", ",", "1", ")", "[", "0", "]", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ".", "rsplit", "(", "'.contigs'", ")", "[", "0", "]", "for", "i", "in", "fastas", "]", ")", "for", "cluster_num", ",", "genome", "in", "enumerate", "(", "fastas", ".", "difference", "(", "set", "(", "in_cluster", ")", ")", ",", "start", ")", ":", "try", ":", "stats", "=", "[", "'*%s'", "%", "(", "cluster_num", ")", ",", "1", ",", "genome", ",", "genome", ",", "info", "[", "genome", "]", "[", "'#SCGs'", "]", ",", "info", "[", "genome", "]", "[", "'#SCG duplicates'", "]", ",", "info", "[", "genome", "]", "[", "'genome size (bp)'", "]", ",", "info", "[", "genome", "]", "[", "'# contigs'", "]", ",", "[", "genome", "]", "]", "except", ":", "stats", "=", "[", "'*%s'", "%", "(", "cluster_num", ")", ",", "1", ",", "genome", ",", "genome", ",", "'n/a'", ",", "'n/a'", ",", "info", "[", "genome", "]", "[", "'genome size (bp)'", "]", ",", "info", "[", "genome", "]", "[", "'# contigs'", "]", ",", "[", "genome", "]", "]", "yield", "stats"], "docstring": "choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome", "docstring_tokens": ["choose", "represenative", "genome", "and", "print", "cluster", "information"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L114-L163", "partition": "train", "up_fun_num": 4, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for clustering genomes based on average nucleotide\nidentity\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport subprocess\nfrom networkx import Graph as Graph\nfrom networkx import connected_components as connected_components\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n    \"\"\"\n    Create mash files for multiple fasta files\n    Input:\n        fastas <list[str]>  -- paths to fasta files\n        mash_file <str>     -- path to output mash file\n        threads <int>       -- # threads for parallelization\n        kmer <int>          -- kmer size for mash sketching\n        force <boolean>     -- force overwrite of all mash files\n    \"\"\"\n    mash_processes = set()\n    sketches = [fasta + \".msh\" for fasta in fastas]\n    devnull = open(os.devnull, \"w\")\n    # Perform the sketching\n    for fasta, sketch in zip(fastas, sketches):\n        if os.path.isfile(sketch):\n            continue\n        mash_cmd = [\"/opt/bin/bio/mash\", \"sketch\", \"-o\", fasta, \"-k\", str(kmer), fasta]\n        mash_processes.add(subprocess.Popen(mash_cmd, stderr=devnull))\n        if len(mash_processes) >= threads:\n            os.wait()\n            mash_processes.difference_update(\n                [mp for mp in mash_processes if mp.poll() is not None]\n            )\n    # Collect stragglers\n    for mp in mash_processes:\n        if mp.poll() is None:\n            mp.wait()\n    # Paste sketches into single mash\n    paste_mashes(sketches, mash_file, force=force)\n    return\n\n\ndef paste_mashes(sketches, pasted_mash, force=False):\n    \"\"\"\n    Combine mash files into single sketch\n    Input:\n        sketches <list[str]>  -- paths to sketch files\n        pasted_mash <str>     -- path to output mash file\n        force <boolean>     -- force overwrite of all mash file\n    \"\"\"\n    if os.path.isfile(pasted_mash):\n        if force:\n            subprocess.Popen([\"rm\", pasted_mash]).wait()\n        else:\n            return\n    pasted_mash = pasted_mash.rsplit(\".msh\")[0]\n    mash_cmd = [\"/opt/bin/bio/mash\", \"paste\", pasted_mash]\n    mash_cmd.extend(sketches)\n    process = subprocess.Popen(mash_cmd)\n    process.wait()\n    return\n\n\ndef ani(fastas, mash_file, sim_threshold, threads):\n    \"\"\"\n    Use mash to estimate ANI of genomes\n    Input:\n        fastas <list[str]>      -- paths to fasta files\n        sim_threshold <float>   -- fractional cutoff % identity for cluster joining\n        mash_file <str>         -- pasted sketch file of all fastas being compared\n        threads <int>           -- number threads for distance estimation\n    \"\"\"\n    ANI = Graph()\n    # use Mash to estimate ANI\n    for fasta in fastas:\n        indiv_mash = fasta + \".msh\"\n        if os.path.isfile(indiv_mash):\n            cmp_file = indiv_mash\n        else:\n            cmp_file = fasta\n        mash_cmd = [\"/opt/bin/bio/mash\", \"dist\", cmp_file, mash_file]\n        process = subprocess.Popen(mash_cmd, stdout=subprocess.PIPE)\n        for pair in process.communicate()[0].splitlines():\n            a, b, dist, p, shared = pair.decode().strip().split()\n            a = a.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            b = b.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            p = float(p)\n            similarity = (1 - float(dist)) * 100\n            if similarity >= sim_threshold:\n                ANI.add_edge(a, b, si=similarity, pval=p, sharedK=shared)\n        process.wait()\n    return ANI\n\n\ndef genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg = info[\"#SCGs\"]\n        dups = info[\"#SCG duplicates\"]\n        length = info[\"genome size (bp)\"]\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info[\"genome size (bp)\"], genome]\n\n\ndef to_int(i):\n    \"\"\"\n    convert to integer, if possible\n    \"\"\"\n    try:\n        return int(i)\n    except:\n        return i\n\n\ndef parse_ggKbase_tables(tables, id_type):\n    \"\"\"\n    convert ggKbase genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"name\"):\n                header = line\n                header[4] = \"genome size (bp)\"\n                header[12] = \"#SCGs\"\n                header[13] = \"#SCG duplicates\"\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False:  # try to use name and code ID\n                if \"UNK\" in code or \"unknown\" in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print(\"# duplicate name or code in table(s)\", file=sys.stderr)\n                    print(\"# %s and/or %s\" % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item: stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item: stat for item, stat in zip(header, info)}\n            else:\n                if id_type == \"name\":\n                    ID = name\n                elif id_type == \"code\":\n                    ID = code\n                else:\n                    print(\"# specify name or code column using -id\", file=sys.stderr)\n                    exit()\n                ID = ID.replace(\" \", \"\")\n                g2info[ID] = {item: stat for item, stat in zip(header, info)}\n                if g2info[ID][\"genome size (bp)\"] == \"\":\n                    g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"Bin Id\"):\n                header = line\n                header[8] = \"genome size (bp)\"\n                header[5] = \"#SCGs\"\n                header[6] = \"#SCG duplicates\"\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(\" \", \"\")\n            g2info[ID] = {item: stat for item, stat in zip(header, info)}\n            if g2info[ID][\"genome size (bp)\"] == \"\":\n                g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef genome_lengths(fastas, info):\n    \"\"\"\n    get genome lengths\n    \"\"\"\n    if info is False:\n        info = {}\n    for genome in fastas:\n        name = genome.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n        if name in info:\n            continue\n        length = 0\n        fragments = 0\n        for seq in parse_fasta(genome):\n            length += len(seq[1])\n            fragments += 1\n        info[name] = {\"genome size (bp)\": length, \"# contigs\": fragments}\n    return info\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# cluster genomes based on average nucleotide identity (ani)\"\n    )\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fastas\")\n    parser.add_argument(\n        \"-m\",\n        action=\"store\",\n        required=True,\n        type=str,\n        help=\"mash file (will be created if it does not exist)\",\n    )\n    parser.add_argument(\n        \"-s\",\n        default=98,\n        type=float,\n        required=False,\n        help=\"percent similarity (default = 98)\",\n    )\n    parser.add_argument(\n        \"-g\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"ggKbase genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-c\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"checkM genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-id\",\n        default=False,\n        help=\"use name or code column in ggKbase table (default: try both)\",\n    )\n    parser.add_argument(\n        \"-t\", required=False, default=6, type=int, help=\"threads (default = 6)\"\n    )\n    args = vars(parser.parse_args())\n    fastas, similarity, id_type, threads, mash_file = (\n        args[\"f\"],\n        args[\"s\"],\n        args[\"id\"],\n        args[\"t\"],\n        args[\"m\"],\n    )\n    gg, cm = args[\"g\"], args[\"c\"]\n    if \".msh\" not in mash_file:\n        mash_file = \"%s.msh\" % (mash_file)\n    info = False  # assume no marker gene file is given (either ggKbase or checkM)\n    if gg is not False:\n        info = parse_ggKbase_tables(gg, id_type)\n    elif cm is not False:\n        info = parse_checkM_tables(cm)\n    info = genome_lengths(fastas, info)\n    make_mashes(fastas, mash_file, threads)\n    ANI = ani(fastas, mash_file, similarity, threads)\n    for genome in print_clusters(fastas, info, ANI):\n        print(\"\\t\".join([str(i) for i in genome]))\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import subprocess", "from networkx import Graph as Graph", "from networkx import connected_components as connected_components", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n", "def paste_mashes(sketches, pasted_mash, force=False):\n", "def ani(fastas, mash_file, sim_threshold, threads):\n", "def genome_info(genome, info):\n", "def to_int(i):\n", "def parse_ggKbase_tables(tables, id_type):\n", "def parse_checkM_tables(tables):\n", "def genome_lengths(fastas, info):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/cluster_ani.py", "func_name": "parse_ggKbase_tables", "original_string": "def parse_ggKbase_tables(tables, id_type):\n    \"\"\"\n    convert ggKbase genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split('\\t')\n            if line[0].startswith('name'):\n                header = line\n                header[4] = 'genome size (bp)'\n                header[12] = '#SCGs'\n                header[13] = '#SCG duplicates'\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False: # try to use name and code ID\n                if 'UNK' in code or 'unknown' in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print('# duplicate name or code in table(s)', file=sys.stderr)\n                    print('# %s and/or %s' % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item:stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item:stat for item, stat in zip(header, info)}\n            else:\n                if id_type == 'name':\n                    ID = name\n                elif id_type == 'code':\n                    ID = code\n                else:\n                    print('# specify name or code column using -id', file=sys.stderr)\n                    exit()\n                ID = ID.replace(' ', '')\n                g2info[ID] = {item:stat for item, stat in zip(header, info)}\n                if g2info[ID]['genome size (bp)'] == '':\n                    g2info[ID]['genome size (bp)'] = 0\n    return g2info", "language": "python", "code": "def parse_ggKbase_tables(tables, id_type):\n    \"\"\"\n    convert ggKbase genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split('\\t')\n            if line[0].startswith('name'):\n                header = line\n                header[4] = 'genome size (bp)'\n                header[12] = '#SCGs'\n                header[13] = '#SCG duplicates'\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False: # try to use name and code ID\n                if 'UNK' in code or 'unknown' in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print('# duplicate name or code in table(s)', file=sys.stderr)\n                    print('# %s and/or %s' % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item:stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item:stat for item, stat in zip(header, info)}\n            else:\n                if id_type == 'name':\n                    ID = name\n                elif id_type == 'code':\n                    ID = code\n                else:\n                    print('# specify name or code column using -id', file=sys.stderr)\n                    exit()\n                ID = ID.replace(' ', '')\n                g2info[ID] = {item:stat for item, stat in zip(header, info)}\n                if g2info[ID]['genome size (bp)'] == '':\n                    g2info[ID]['genome size (bp)'] = 0\n    return g2info", "code_tokens": ["def", "parse_ggKbase_tables", "(", "tables", ",", "id_type", ")", ":", "g2info", "=", "{", "}", "for", "table", "in", "tables", ":", "for", "line", "in", "open", "(", "table", ")", ":", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "if", "line", "[", "0", "]", ".", "startswith", "(", "'name'", ")", ":", "header", "=", "line", "header", "[", "4", "]", "=", "'genome size (bp)'", "header", "[", "12", "]", "=", "'#SCGs'", "header", "[", "13", "]", "=", "'#SCG duplicates'", "continue", "name", ",", "code", ",", "info", "=", "line", "[", "0", "]", ",", "line", "[", "1", "]", ",", "line", "info", "=", "[", "to_int", "(", "i", ")", "for", "i", "in", "info", "]", "if", "id_type", "is", "False", ":", "# try to use name and code ID", "if", "'UNK'", "in", "code", "or", "'unknown'", "in", "code", ":", "code", "=", "name", "if", "(", "name", "!=", "code", ")", "and", "(", "name", "and", "code", "in", "g2info", ")", ":", "print", "(", "'# duplicate name or code in table(s)'", ",", "file", "=", "sys", ".", "stderr", ")", "print", "(", "'# %s and/or %s'", "%", "(", "name", ",", "code", ")", ",", "file", "=", "sys", ".", "stderr", ")", "exit", "(", ")", "if", "name", "not", "in", "g2info", ":", "g2info", "[", "name", "]", "=", "{", "item", ":", "stat", "for", "item", ",", "stat", "in", "zip", "(", "header", ",", "info", ")", "}", "if", "code", "not", "in", "g2info", ":", "g2info", "[", "code", "]", "=", "{", "item", ":", "stat", "for", "item", ",", "stat", "in", "zip", "(", "header", ",", "info", ")", "}", "else", ":", "if", "id_type", "==", "'name'", ":", "ID", "=", "name", "elif", "id_type", "==", "'code'", ":", "ID", "=", "code", "else", ":", "print", "(", "'# specify name or code column using -id'", ",", "file", "=", "sys", ".", "stderr", ")", "exit", "(", ")", "ID", "=", "ID", ".", "replace", "(", "' '", ",", "''", ")", "g2info", "[", "ID", "]", "=", "{", "item", ":", "stat", "for", "item", ",", "stat", "in", "zip", "(", "header", ",", "info", ")", "}", "if", "g2info", "[", "ID", "]", "[", "'genome size (bp)'", "]", "==", "''", ":", "g2info", "[", "ID", "]", "[", "'genome size (bp)'", "]", "=", "0", "return", "g2info"], "docstring": "convert ggKbase genome info tables to dictionary", "docstring_tokens": ["convert", "ggKbase", "genome", "info", "tables", "to", "dictionary"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L174-L213", "partition": "train", "up_fun_num": 6, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for clustering genomes based on average nucleotide\nidentity\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport subprocess\nfrom networkx import Graph as Graph\nfrom networkx import connected_components as connected_components\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n    \"\"\"\n    Create mash files for multiple fasta files\n    Input:\n        fastas <list[str]>  -- paths to fasta files\n        mash_file <str>     -- path to output mash file\n        threads <int>       -- # threads for parallelization\n        kmer <int>          -- kmer size for mash sketching\n        force <boolean>     -- force overwrite of all mash files\n    \"\"\"\n    mash_processes = set()\n    sketches = [fasta + \".msh\" for fasta in fastas]\n    devnull = open(os.devnull, \"w\")\n    # Perform the sketching\n    for fasta, sketch in zip(fastas, sketches):\n        if os.path.isfile(sketch):\n            continue\n        mash_cmd = [\"/opt/bin/bio/mash\", \"sketch\", \"-o\", fasta, \"-k\", str(kmer), fasta]\n        mash_processes.add(subprocess.Popen(mash_cmd, stderr=devnull))\n        if len(mash_processes) >= threads:\n            os.wait()\n            mash_processes.difference_update(\n                [mp for mp in mash_processes if mp.poll() is not None]\n            )\n    # Collect stragglers\n    for mp in mash_processes:\n        if mp.poll() is None:\n            mp.wait()\n    # Paste sketches into single mash\n    paste_mashes(sketches, mash_file, force=force)\n    return\n\n\ndef paste_mashes(sketches, pasted_mash, force=False):\n    \"\"\"\n    Combine mash files into single sketch\n    Input:\n        sketches <list[str]>  -- paths to sketch files\n        pasted_mash <str>     -- path to output mash file\n        force <boolean>     -- force overwrite of all mash file\n    \"\"\"\n    if os.path.isfile(pasted_mash):\n        if force:\n            subprocess.Popen([\"rm\", pasted_mash]).wait()\n        else:\n            return\n    pasted_mash = pasted_mash.rsplit(\".msh\")[0]\n    mash_cmd = [\"/opt/bin/bio/mash\", \"paste\", pasted_mash]\n    mash_cmd.extend(sketches)\n    process = subprocess.Popen(mash_cmd)\n    process.wait()\n    return\n\n\ndef ani(fastas, mash_file, sim_threshold, threads):\n    \"\"\"\n    Use mash to estimate ANI of genomes\n    Input:\n        fastas <list[str]>      -- paths to fasta files\n        sim_threshold <float>   -- fractional cutoff % identity for cluster joining\n        mash_file <str>         -- pasted sketch file of all fastas being compared\n        threads <int>           -- number threads for distance estimation\n    \"\"\"\n    ANI = Graph()\n    # use Mash to estimate ANI\n    for fasta in fastas:\n        indiv_mash = fasta + \".msh\"\n        if os.path.isfile(indiv_mash):\n            cmp_file = indiv_mash\n        else:\n            cmp_file = fasta\n        mash_cmd = [\"/opt/bin/bio/mash\", \"dist\", cmp_file, mash_file]\n        process = subprocess.Popen(mash_cmd, stdout=subprocess.PIPE)\n        for pair in process.communicate()[0].splitlines():\n            a, b, dist, p, shared = pair.decode().strip().split()\n            a = a.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            b = b.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            p = float(p)\n            similarity = (1 - float(dist)) * 100\n            if similarity >= sim_threshold:\n                ANI.add_edge(a, b, si=similarity, pval=p, sharedK=shared)\n        process.wait()\n    return ANI\n\n\ndef genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg = info[\"#SCGs\"]\n        dups = info[\"#SCG duplicates\"]\n        length = info[\"genome size (bp)\"]\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info[\"genome size (bp)\"], genome]\n\n\ndef print_clusters(fastas, info, ANI):\n    \"\"\"\n    choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome\n    \"\"\"\n    header = [\n        \"#cluster\",\n        \"num. genomes\",\n        \"rep.\",\n        \"genome\",\n        \"#SCGs\",\n        \"#SCG duplicates\",\n        \"genome size (bp)\",\n        \"fragments\",\n        \"list\",\n    ]\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted(\n            [genome_info(genome, info[genome]) for genome in cluster],\n            key=lambda x: x[0:],\n            reverse=True,\n        )\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    info[genome][\"#SCGs\"],\n                    info[genome][\"#SCG duplicates\"],\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            except:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    \"n/a\",\n                    \"n/a\",\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            if rep == genome:\n                stats = [\"*%s\" % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set(\n        [i.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0] for i in fastas]\n    )\n    for cluster_num, genome in enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                info[genome][\"#SCGs\"],\n                info[genome][\"#SCG duplicates\"],\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        except:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                \"n/a\",\n                \"n/a\",\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        yield stats\n\n\ndef to_int(i):\n    \"\"\"\n    convert to integer, if possible\n    \"\"\"\n    try:\n        return int(i)\n    except:\n        return i\n\n\ndef parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"Bin Id\"):\n                header = line\n                header[8] = \"genome size (bp)\"\n                header[5] = \"#SCGs\"\n                header[6] = \"#SCG duplicates\"\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(\" \", \"\")\n            g2info[ID] = {item: stat for item, stat in zip(header, info)}\n            if g2info[ID][\"genome size (bp)\"] == \"\":\n                g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef genome_lengths(fastas, info):\n    \"\"\"\n    get genome lengths\n    \"\"\"\n    if info is False:\n        info = {}\n    for genome in fastas:\n        name = genome.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n        if name in info:\n            continue\n        length = 0\n        fragments = 0\n        for seq in parse_fasta(genome):\n            length += len(seq[1])\n            fragments += 1\n        info[name] = {\"genome size (bp)\": length, \"# contigs\": fragments}\n    return info\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# cluster genomes based on average nucleotide identity (ani)\"\n    )\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fastas\")\n    parser.add_argument(\n        \"-m\",\n        action=\"store\",\n        required=True,\n        type=str,\n        help=\"mash file (will be created if it does not exist)\",\n    )\n    parser.add_argument(\n        \"-s\",\n        default=98,\n        type=float,\n        required=False,\n        help=\"percent similarity (default = 98)\",\n    )\n    parser.add_argument(\n        \"-g\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"ggKbase genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-c\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"checkM genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-id\",\n        default=False,\n        help=\"use name or code column in ggKbase table (default: try both)\",\n    )\n    parser.add_argument(\n        \"-t\", required=False, default=6, type=int, help=\"threads (default = 6)\"\n    )\n    args = vars(parser.parse_args())\n    fastas, similarity, id_type, threads, mash_file = (\n        args[\"f\"],\n        args[\"s\"],\n        args[\"id\"],\n        args[\"t\"],\n        args[\"m\"],\n    )\n    gg, cm = args[\"g\"], args[\"c\"]\n    if \".msh\" not in mash_file:\n        mash_file = \"%s.msh\" % (mash_file)\n    info = False  # assume no marker gene file is given (either ggKbase or checkM)\n    if gg is not False:\n        info = parse_ggKbase_tables(gg, id_type)\n    elif cm is not False:\n        info = parse_checkM_tables(cm)\n    info = genome_lengths(fastas, info)\n    make_mashes(fastas, mash_file, threads)\n    ANI = ani(fastas, mash_file, similarity, threads)\n    for genome in print_clusters(fastas, info, ANI):\n        print(\"\\t\".join([str(i) for i in genome]))\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import subprocess", "from networkx import Graph as Graph", "from networkx import connected_components as connected_components", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n", "def paste_mashes(sketches, pasted_mash, force=False):\n", "def ani(fastas, mash_file, sim_threshold, threads):\n", "def genome_info(genome, info):\n", "def print_clusters(fastas, info, ANI):\n", "def to_int(i):\n", "def parse_checkM_tables(tables):\n", "def genome_lengths(fastas, info):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/cluster_ani.py", "func_name": "parse_checkM_tables", "original_string": "def parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split('\\t')\n            if line[0].startswith('Bin Id'):\n                header = line\n                header[8] = 'genome size (bp)'\n                header[5] = '#SCGs'\n                header[6] = '#SCG duplicates'\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(' ', '')\n            g2info[ID] = {item:stat for item, stat in zip(header, info)}\n            if g2info[ID]['genome size (bp)'] == '':\n                g2info[ID]['genome size (bp)'] = 0\n    return g2info", "language": "python", "code": "def parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split('\\t')\n            if line[0].startswith('Bin Id'):\n                header = line\n                header[8] = 'genome size (bp)'\n                header[5] = '#SCGs'\n                header[6] = '#SCG duplicates'\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(' ', '')\n            g2info[ID] = {item:stat for item, stat in zip(header, info)}\n            if g2info[ID]['genome size (bp)'] == '':\n                g2info[ID]['genome size (bp)'] = 0\n    return g2info", "code_tokens": ["def", "parse_checkM_tables", "(", "tables", ")", ":", "g2info", "=", "{", "}", "for", "table", "in", "tables", ":", "for", "line", "in", "open", "(", "table", ")", ":", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "if", "line", "[", "0", "]", ".", "startswith", "(", "'Bin Id'", ")", ":", "header", "=", "line", "header", "[", "8", "]", "=", "'genome size (bp)'", "header", "[", "5", "]", "=", "'#SCGs'", "header", "[", "6", "]", "=", "'#SCG duplicates'", "continue", "ID", ",", "info", "=", "line", "[", "0", "]", ",", "line", "info", "=", "[", "to_int", "(", "i", ")", "for", "i", "in", "info", "]", "ID", "=", "ID", ".", "replace", "(", "' '", ",", "''", ")", "g2info", "[", "ID", "]", "=", "{", "item", ":", "stat", "for", "item", ",", "stat", "in", "zip", "(", "header", ",", "info", ")", "}", "if", "g2info", "[", "ID", "]", "[", "'genome size (bp)'", "]", "==", "''", ":", "g2info", "[", "ID", "]", "[", "'genome size (bp)'", "]", "=", "0", "return", "g2info"], "docstring": "convert checkM genome info tables to dictionary", "docstring_tokens": ["convert", "checkM", "genome", "info", "tables", "to", "dictionary"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L215-L235", "partition": "train", "up_fun_num": 7, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for clustering genomes based on average nucleotide\nidentity\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport subprocess\nfrom networkx import Graph as Graph\nfrom networkx import connected_components as connected_components\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n    \"\"\"\n    Create mash files for multiple fasta files\n    Input:\n        fastas <list[str]>  -- paths to fasta files\n        mash_file <str>     -- path to output mash file\n        threads <int>       -- # threads for parallelization\n        kmer <int>          -- kmer size for mash sketching\n        force <boolean>     -- force overwrite of all mash files\n    \"\"\"\n    mash_processes = set()\n    sketches = [fasta + \".msh\" for fasta in fastas]\n    devnull = open(os.devnull, \"w\")\n    # Perform the sketching\n    for fasta, sketch in zip(fastas, sketches):\n        if os.path.isfile(sketch):\n            continue\n        mash_cmd = [\"/opt/bin/bio/mash\", \"sketch\", \"-o\", fasta, \"-k\", str(kmer), fasta]\n        mash_processes.add(subprocess.Popen(mash_cmd, stderr=devnull))\n        if len(mash_processes) >= threads:\n            os.wait()\n            mash_processes.difference_update(\n                [mp for mp in mash_processes if mp.poll() is not None]\n            )\n    # Collect stragglers\n    for mp in mash_processes:\n        if mp.poll() is None:\n            mp.wait()\n    # Paste sketches into single mash\n    paste_mashes(sketches, mash_file, force=force)\n    return\n\n\ndef paste_mashes(sketches, pasted_mash, force=False):\n    \"\"\"\n    Combine mash files into single sketch\n    Input:\n        sketches <list[str]>  -- paths to sketch files\n        pasted_mash <str>     -- path to output mash file\n        force <boolean>     -- force overwrite of all mash file\n    \"\"\"\n    if os.path.isfile(pasted_mash):\n        if force:\n            subprocess.Popen([\"rm\", pasted_mash]).wait()\n        else:\n            return\n    pasted_mash = pasted_mash.rsplit(\".msh\")[0]\n    mash_cmd = [\"/opt/bin/bio/mash\", \"paste\", pasted_mash]\n    mash_cmd.extend(sketches)\n    process = subprocess.Popen(mash_cmd)\n    process.wait()\n    return\n\n\ndef ani(fastas, mash_file, sim_threshold, threads):\n    \"\"\"\n    Use mash to estimate ANI of genomes\n    Input:\n        fastas <list[str]>      -- paths to fasta files\n        sim_threshold <float>   -- fractional cutoff % identity for cluster joining\n        mash_file <str>         -- pasted sketch file of all fastas being compared\n        threads <int>           -- number threads for distance estimation\n    \"\"\"\n    ANI = Graph()\n    # use Mash to estimate ANI\n    for fasta in fastas:\n        indiv_mash = fasta + \".msh\"\n        if os.path.isfile(indiv_mash):\n            cmp_file = indiv_mash\n        else:\n            cmp_file = fasta\n        mash_cmd = [\"/opt/bin/bio/mash\", \"dist\", cmp_file, mash_file]\n        process = subprocess.Popen(mash_cmd, stdout=subprocess.PIPE)\n        for pair in process.communicate()[0].splitlines():\n            a, b, dist, p, shared = pair.decode().strip().split()\n            a = a.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            b = b.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            p = float(p)\n            similarity = (1 - float(dist)) * 100\n            if similarity >= sim_threshold:\n                ANI.add_edge(a, b, si=similarity, pval=p, sharedK=shared)\n        process.wait()\n    return ANI\n\n\ndef genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg = info[\"#SCGs\"]\n        dups = info[\"#SCG duplicates\"]\n        length = info[\"genome size (bp)\"]\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info[\"genome size (bp)\"], genome]\n\n\ndef print_clusters(fastas, info, ANI):\n    \"\"\"\n    choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome\n    \"\"\"\n    header = [\n        \"#cluster\",\n        \"num. genomes\",\n        \"rep.\",\n        \"genome\",\n        \"#SCGs\",\n        \"#SCG duplicates\",\n        \"genome size (bp)\",\n        \"fragments\",\n        \"list\",\n    ]\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted(\n            [genome_info(genome, info[genome]) for genome in cluster],\n            key=lambda x: x[0:],\n            reverse=True,\n        )\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    info[genome][\"#SCGs\"],\n                    info[genome][\"#SCG duplicates\"],\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            except:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    \"n/a\",\n                    \"n/a\",\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            if rep == genome:\n                stats = [\"*%s\" % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set(\n        [i.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0] for i in fastas]\n    )\n    for cluster_num, genome in enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                info[genome][\"#SCGs\"],\n                info[genome][\"#SCG duplicates\"],\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        except:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                \"n/a\",\n                \"n/a\",\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        yield stats\n\n\ndef to_int(i):\n    \"\"\"\n    convert to integer, if possible\n    \"\"\"\n    try:\n        return int(i)\n    except:\n        return i\n\n\ndef parse_ggKbase_tables(tables, id_type):\n    \"\"\"\n    convert ggKbase genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"name\"):\n                header = line\n                header[4] = \"genome size (bp)\"\n                header[12] = \"#SCGs\"\n                header[13] = \"#SCG duplicates\"\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False:  # try to use name and code ID\n                if \"UNK\" in code or \"unknown\" in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print(\"# duplicate name or code in table(s)\", file=sys.stderr)\n                    print(\"# %s and/or %s\" % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item: stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item: stat for item, stat in zip(header, info)}\n            else:\n                if id_type == \"name\":\n                    ID = name\n                elif id_type == \"code\":\n                    ID = code\n                else:\n                    print(\"# specify name or code column using -id\", file=sys.stderr)\n                    exit()\n                ID = ID.replace(\" \", \"\")\n                g2info[ID] = {item: stat for item, stat in zip(header, info)}\n                if g2info[ID][\"genome size (bp)\"] == \"\":\n                    g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef genome_lengths(fastas, info):\n    \"\"\"\n    get genome lengths\n    \"\"\"\n    if info is False:\n        info = {}\n    for genome in fastas:\n        name = genome.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n        if name in info:\n            continue\n        length = 0\n        fragments = 0\n        for seq in parse_fasta(genome):\n            length += len(seq[1])\n            fragments += 1\n        info[name] = {\"genome size (bp)\": length, \"# contigs\": fragments}\n    return info\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# cluster genomes based on average nucleotide identity (ani)\"\n    )\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fastas\")\n    parser.add_argument(\n        \"-m\",\n        action=\"store\",\n        required=True,\n        type=str,\n        help=\"mash file (will be created if it does not exist)\",\n    )\n    parser.add_argument(\n        \"-s\",\n        default=98,\n        type=float,\n        required=False,\n        help=\"percent similarity (default = 98)\",\n    )\n    parser.add_argument(\n        \"-g\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"ggKbase genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-c\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"checkM genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-id\",\n        default=False,\n        help=\"use name or code column in ggKbase table (default: try both)\",\n    )\n    parser.add_argument(\n        \"-t\", required=False, default=6, type=int, help=\"threads (default = 6)\"\n    )\n    args = vars(parser.parse_args())\n    fastas, similarity, id_type, threads, mash_file = (\n        args[\"f\"],\n        args[\"s\"],\n        args[\"id\"],\n        args[\"t\"],\n        args[\"m\"],\n    )\n    gg, cm = args[\"g\"], args[\"c\"]\n    if \".msh\" not in mash_file:\n        mash_file = \"%s.msh\" % (mash_file)\n    info = False  # assume no marker gene file is given (either ggKbase or checkM)\n    if gg is not False:\n        info = parse_ggKbase_tables(gg, id_type)\n    elif cm is not False:\n        info = parse_checkM_tables(cm)\n    info = genome_lengths(fastas, info)\n    make_mashes(fastas, mash_file, threads)\n    ANI = ani(fastas, mash_file, similarity, threads)\n    for genome in print_clusters(fastas, info, ANI):\n        print(\"\\t\".join([str(i) for i in genome]))\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import subprocess", "from networkx import Graph as Graph", "from networkx import connected_components as connected_components", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n", "def paste_mashes(sketches, pasted_mash, force=False):\n", "def ani(fastas, mash_file, sim_threshold, threads):\n", "def genome_info(genome, info):\n", "def print_clusters(fastas, info, ANI):\n", "def to_int(i):\n", "def parse_ggKbase_tables(tables, id_type):\n", "def genome_lengths(fastas, info):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/cluster_ani.py", "func_name": "genome_lengths", "original_string": "def genome_lengths(fastas, info):\n    \"\"\"\n    get genome lengths\n    \"\"\"\n    if info is False:\n        info = {}\n    for genome in fastas:\n        name = genome.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]\n        if name in info:\n            continue\n        length = 0\n        fragments = 0\n        for seq in parse_fasta(genome):\n            length += len(seq[1])\n            fragments += 1\n        info[name] = {'genome size (bp)':length, '# contigs':fragments}\n    return info", "language": "python", "code": "def genome_lengths(fastas, info):\n    \"\"\"\n    get genome lengths\n    \"\"\"\n    if info is False:\n        info = {}\n    for genome in fastas:\n        name = genome.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]\n        if name in info:\n            continue\n        length = 0\n        fragments = 0\n        for seq in parse_fasta(genome):\n            length += len(seq[1])\n            fragments += 1\n        info[name] = {'genome size (bp)':length, '# contigs':fragments}\n    return info", "code_tokens": ["def", "genome_lengths", "(", "fastas", ",", "info", ")", ":", "if", "info", "is", "False", ":", "info", "=", "{", "}", "for", "genome", "in", "fastas", ":", "name", "=", "genome", ".", "rsplit", "(", "'.'", ",", "1", ")", "[", "0", "]", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "-", "1", "]", ".", "rsplit", "(", "'.contigs'", ")", "[", "0", "]", "if", "name", "in", "info", ":", "continue", "length", "=", "0", "fragments", "=", "0", "for", "seq", "in", "parse_fasta", "(", "genome", ")", ":", "length", "+=", "len", "(", "seq", "[", "1", "]", ")", "fragments", "+=", "1", "info", "[", "name", "]", "=", "{", "'genome size (bp)'", ":", "length", ",", "'# contigs'", ":", "fragments", "}", "return", "info"], "docstring": "get genome lengths", "docstring_tokens": ["get", "genome", "lengths"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/cluster_ani.py#L237-L253", "partition": "train", "up_fun_num": 8, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for clustering genomes based on average nucleotide\nidentity\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport subprocess\nfrom networkx import Graph as Graph\nfrom networkx import connected_components as connected_components\n\n# ctb\nfrom ctbBio.fasta import iterate_fasta as parse_fasta\n\n\ndef make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n    \"\"\"\n    Create mash files for multiple fasta files\n    Input:\n        fastas <list[str]>  -- paths to fasta files\n        mash_file <str>     -- path to output mash file\n        threads <int>       -- # threads for parallelization\n        kmer <int>          -- kmer size for mash sketching\n        force <boolean>     -- force overwrite of all mash files\n    \"\"\"\n    mash_processes = set()\n    sketches = [fasta + \".msh\" for fasta in fastas]\n    devnull = open(os.devnull, \"w\")\n    # Perform the sketching\n    for fasta, sketch in zip(fastas, sketches):\n        if os.path.isfile(sketch):\n            continue\n        mash_cmd = [\"/opt/bin/bio/mash\", \"sketch\", \"-o\", fasta, \"-k\", str(kmer), fasta]\n        mash_processes.add(subprocess.Popen(mash_cmd, stderr=devnull))\n        if len(mash_processes) >= threads:\n            os.wait()\n            mash_processes.difference_update(\n                [mp for mp in mash_processes if mp.poll() is not None]\n            )\n    # Collect stragglers\n    for mp in mash_processes:\n        if mp.poll() is None:\n            mp.wait()\n    # Paste sketches into single mash\n    paste_mashes(sketches, mash_file, force=force)\n    return\n\n\ndef paste_mashes(sketches, pasted_mash, force=False):\n    \"\"\"\n    Combine mash files into single sketch\n    Input:\n        sketches <list[str]>  -- paths to sketch files\n        pasted_mash <str>     -- path to output mash file\n        force <boolean>     -- force overwrite of all mash file\n    \"\"\"\n    if os.path.isfile(pasted_mash):\n        if force:\n            subprocess.Popen([\"rm\", pasted_mash]).wait()\n        else:\n            return\n    pasted_mash = pasted_mash.rsplit(\".msh\")[0]\n    mash_cmd = [\"/opt/bin/bio/mash\", \"paste\", pasted_mash]\n    mash_cmd.extend(sketches)\n    process = subprocess.Popen(mash_cmd)\n    process.wait()\n    return\n\n\ndef ani(fastas, mash_file, sim_threshold, threads):\n    \"\"\"\n    Use mash to estimate ANI of genomes\n    Input:\n        fastas <list[str]>      -- paths to fasta files\n        sim_threshold <float>   -- fractional cutoff % identity for cluster joining\n        mash_file <str>         -- pasted sketch file of all fastas being compared\n        threads <int>           -- number threads for distance estimation\n    \"\"\"\n    ANI = Graph()\n    # use Mash to estimate ANI\n    for fasta in fastas:\n        indiv_mash = fasta + \".msh\"\n        if os.path.isfile(indiv_mash):\n            cmp_file = indiv_mash\n        else:\n            cmp_file = fasta\n        mash_cmd = [\"/opt/bin/bio/mash\", \"dist\", cmp_file, mash_file]\n        process = subprocess.Popen(mash_cmd, stdout=subprocess.PIPE)\n        for pair in process.communicate()[0].splitlines():\n            a, b, dist, p, shared = pair.decode().strip().split()\n            a = a.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            b = b.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0]\n            p = float(p)\n            similarity = (1 - float(dist)) * 100\n            if similarity >= sim_threshold:\n                ANI.add_edge(a, b, si=similarity, pval=p, sharedK=shared)\n        process.wait()\n    return ANI\n\n\ndef genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg = info[\"#SCGs\"]\n        dups = info[\"#SCG duplicates\"]\n        length = info[\"genome size (bp)\"]\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info[\"genome size (bp)\"], genome]\n\n\ndef print_clusters(fastas, info, ANI):\n    \"\"\"\n    choose represenative genome and\n    print cluster information\n\n    *if ggKbase table is provided, use SCG info to choose best genome\n    \"\"\"\n    header = [\n        \"#cluster\",\n        \"num. genomes\",\n        \"rep.\",\n        \"genome\",\n        \"#SCGs\",\n        \"#SCG duplicates\",\n        \"genome size (bp)\",\n        \"fragments\",\n        \"list\",\n    ]\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted(\n            [genome_info(genome, info[genome]) for genome in cluster],\n            key=lambda x: x[0:],\n            reverse=True,\n        )\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    info[genome][\"#SCGs\"],\n                    info[genome][\"#SCG duplicates\"],\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            except:\n                stats = [\n                    size,\n                    rep,\n                    genome,\n                    \"n/a\",\n                    \"n/a\",\n                    info[genome][\"genome size (bp)\"],\n                    info[genome][\"# contigs\"],\n                    cluster,\n                ]\n            if rep == genome:\n                stats = [\"*%s\" % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set(\n        [i.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1].rsplit(\".contigs\")[0] for i in fastas]\n    )\n    for cluster_num, genome in enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                info[genome][\"#SCGs\"],\n                info[genome][\"#SCG duplicates\"],\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        except:\n            stats = [\n                \"*%s\" % (cluster_num),\n                1,\n                genome,\n                genome,\n                \"n/a\",\n                \"n/a\",\n                info[genome][\"genome size (bp)\"],\n                info[genome][\"# contigs\"],\n                [genome],\n            ]\n        yield stats\n\n\ndef to_int(i):\n    \"\"\"\n    convert to integer, if possible\n    \"\"\"\n    try:\n        return int(i)\n    except:\n        return i\n\n\ndef parse_ggKbase_tables(tables, id_type):\n    \"\"\"\n    convert ggKbase genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"name\"):\n                header = line\n                header[4] = \"genome size (bp)\"\n                header[12] = \"#SCGs\"\n                header[13] = \"#SCG duplicates\"\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False:  # try to use name and code ID\n                if \"UNK\" in code or \"unknown\" in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print(\"# duplicate name or code in table(s)\", file=sys.stderr)\n                    print(\"# %s and/or %s\" % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item: stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item: stat for item, stat in zip(header, info)}\n            else:\n                if id_type == \"name\":\n                    ID = name\n                elif id_type == \"code\":\n                    ID = code\n                else:\n                    print(\"# specify name or code column using -id\", file=sys.stderr)\n                    exit()\n                ID = ID.replace(\" \", \"\")\n                g2info[ID] = {item: stat for item, stat in zip(header, info)}\n                if g2info[ID][\"genome size (bp)\"] == \"\":\n                    g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\ndef parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split(\"\\t\")\n            if line[0].startswith(\"Bin Id\"):\n                header = line\n                header[8] = \"genome size (bp)\"\n                header[5] = \"#SCGs\"\n                header[6] = \"#SCG duplicates\"\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(\" \", \"\")\n            g2info[ID] = {item: stat for item, stat in zip(header, info)}\n            if g2info[ID][\"genome size (bp)\"] == \"\":\n                g2info[ID][\"genome size (bp)\"] = 0\n    return g2info\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# cluster genomes based on average nucleotide identity (ani)\"\n    )\n    parser.add_argument(\"-f\", nargs=\"*\", action=\"store\", required=True, help=\"fastas\")\n    parser.add_argument(\n        \"-m\",\n        action=\"store\",\n        required=True,\n        type=str,\n        help=\"mash file (will be created if it does not exist)\",\n    )\n    parser.add_argument(\n        \"-s\",\n        default=98,\n        type=float,\n        required=False,\n        help=\"percent similarity (default = 98)\",\n    )\n    parser.add_argument(\n        \"-g\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"ggKbase genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-c\",\n        nargs=\"*\",\n        action=\"store\",\n        required=False,\n        default=False,\n        help=\"checkM genome table for selecting representative (optional)\",\n    )\n    parser.add_argument(\n        \"-id\",\n        default=False,\n        help=\"use name or code column in ggKbase table (default: try both)\",\n    )\n    parser.add_argument(\n        \"-t\", required=False, default=6, type=int, help=\"threads (default = 6)\"\n    )\n    args = vars(parser.parse_args())\n    fastas, similarity, id_type, threads, mash_file = (\n        args[\"f\"],\n        args[\"s\"],\n        args[\"id\"],\n        args[\"t\"],\n        args[\"m\"],\n    )\n    gg, cm = args[\"g\"], args[\"c\"]\n    if \".msh\" not in mash_file:\n        mash_file = \"%s.msh\" % (mash_file)\n    info = False  # assume no marker gene file is given (either ggKbase or checkM)\n    if gg is not False:\n        info = parse_ggKbase_tables(gg, id_type)\n    elif cm is not False:\n        info = parse_checkM_tables(cm)\n    info = genome_lengths(fastas, info)\n    make_mashes(fastas, mash_file, threads)\n    ANI = ani(fastas, mash_file, similarity, threads)\n    for genome in print_clusters(fastas, info, ANI):\n        print(\"\\t\".join([str(i) for i in genome]))\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import subprocess", "from networkx import Graph as Graph", "from networkx import connected_components as connected_components", "from ctbBio.fasta import iterate_fasta as parse_fasta"], "function": ["def make_mashes(fastas, mash_file, threads, kmer=21, force=False):\n", "def paste_mashes(sketches, pasted_mash, force=False):\n", "def ani(fastas, mash_file, sim_threshold, threads):\n", "def genome_info(genome, info):\n", "def print_clusters(fastas, info, ANI):\n", "def to_int(i):\n", "def parse_ggKbase_tables(tables, id_type):\n", "def parse_checkM_tables(tables):\n"]}
{"repo": "disqus/nydus", "path": "nydus/db/routers/base.py", "func_name": "BaseRouter.get_dbs", "original_string": "def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs)", "language": "python", "code": "def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs)", "code_tokens": ["def", "get_dbs", "(", "self", ",", "attr", ",", "args", ",", "kwargs", ",", "*", "*", "fkwargs", ")", ":", "if", "not", "self", ".", "_ready", ":", "if", "not", "self", ".", "setup_router", "(", "args", "=", "args", ",", "kwargs", "=", "kwargs", ",", "*", "*", "fkwargs", ")", ":", "raise", "self", ".", "UnableToSetupRouter", "(", ")", "retval", "=", "self", ".", "_pre_routing", "(", "attr", "=", "attr", ",", "args", "=", "args", ",", "kwargs", "=", "kwargs", ",", "*", "*", "fkwargs", ")", "if", "retval", "is", "not", "None", ":", "args", ",", "kwargs", "=", "retval", "if", "not", "(", "args", "or", "kwargs", ")", ":", "return", "self", ".", "cluster", ".", "hosts", ".", "keys", "(", ")", "try", ":", "db_nums", "=", "self", ".", "_route", "(", "attr", "=", "attr", ",", "args", "=", "args", ",", "kwargs", "=", "kwargs", ",", "*", "*", "fkwargs", ")", "except", "Exception", "as", "e", ":", "self", ".", "_handle_exception", "(", "e", ")", "db_nums", "=", "[", "]", "return", "self", ".", "_post_routing", "(", "attr", "=", "attr", ",", "db_nums", "=", "db_nums", ",", "args", "=", "args", ",", "kwargs", "=", "kwargs", ",", "*", "*", "fkwargs", ")"], "docstring": "Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]", "docstring_tokens": ["Returns", "a", "list", "of", "db", "keys", "to", "route", "the", "given", "call", "to", "."], "sha": "9b505840da47a34f758a830c3992fa5dcb7bb7ad", "url": "https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L50-L81", "partition": "train", "up_fun_num": 5, "context": "\"\"\"\nnydus.db.base\n~~~~~~~~~~~~~\n\n:copyright: (c) 2011-2012 DISQUS.\n:license: Apache License 2.0, see LICENSE for more details.\n\"\"\"\n\n__all__ = ('BaseRouter', 'RoundRobinRouter', 'routing_params')\n\nimport time\n\nfrom functools import wraps\nfrom itertools import cycle\n\n\ndef routing_params(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        if kwargs.get('kwargs') is None:\n            kwargs['kwargs'] = {}\n\n        if kwargs.get('args') is None:\n            kwargs['args'] = ()\n\n        return func(*args, **kwargs)\n    wrapped.__wraps__ = getattr(func, '__wraps__', func)\n    return wrapped\n\n\nclass BaseRouter(object):\n    \"\"\"\n    Handles routing requests to a specific connection in a single cluster.\n\n    For the most part, all public functions will receive arguments as ``key=value``\n    pairs and should expect as much. Functions which receive ``args`` and ``kwargs``\n    from the calling function will receive default values for those, and need not\n    worry about handling missing arguments.\n    \"\"\"\n    retryable = False\n\n    class UnableToSetupRouter(Exception):\n        pass\n\n    def __init__(self, cluster=None, *args, **kwargs):\n        self._ready = False\n        self.cluster = cluster\n\n    @routing_params\n\n    # Backwards compatibilty\n    get_db = get_dbs\n\n    @routing_params\n    def setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Call method to perform any setup\n        \"\"\"\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any initialization for the router\n        Returns False if setup could not be completed\n        \"\"\"\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any prerouting with this method and return the key\n        \"\"\"\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform routing and return db_nums\n        \"\"\"\n        return self.cluster.hosts.keys()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any postrouting actions and return db_nums\n        \"\"\"\n        return db_nums\n\n    def _handle_exception(self, e):\n        \"\"\"\n        Handle/transform exceptions and return it\n        \"\"\"\n        raise e\n\n\nclass RoundRobinRouter(BaseRouter):\n    \"\"\"\n    Basic retry router that performs round robin\n    \"\"\"\n\n    # Raised if all hosts in the hash have been marked as down\n    class HostListExhausted(Exception):\n        pass\n\n    class InvalidDBNum(Exception):\n        pass\n\n    # If this router can be retried on if a particular db index it gave out did\n    # not work\n    retryable = True\n\n    # How many requests to serve in a situation when a host is down before\n    # the down hosts are assesed for readmittance back into the pool of serving\n    # requests.\n    #\n    # If the attempt_reconnect_threshold is hit, it does not guarantee that the\n    # down hosts will be put back - only that the router will CHECK to see if\n    # the hosts CAN be put back.  The elegibility of a host being put back is\n    # handlede in the check_down_connections method, which by default will\n    # readmit a host if it was marked down more than retry_timeout seconds ago.\n    attempt_reconnect_threshold = 100000\n\n    # Number of seconds a host must be marked down before it is elligable to be\n    # put back in the pool and retried.\n    retry_timeout = 30\n\n    def __init__(self, *args, **kwargs):\n        self._get_db_attempts = 0\n        self._down_connections = {}\n\n        super(RoundRobinRouter, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def ensure_db_num(cls, db_num):\n        try:\n            return int(db_num)\n        except ValueError:\n            raise cls.InvalidDBNum()\n\n    def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)\n\n    def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)\n\n    def mark_connection_down(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections[db_num] = time.time()\n\n    def mark_connection_up(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections.pop(db_num, None)\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        self._hosts_cycler = cycle(self.cluster.hosts.keys())\n\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n        self._get_db_attempts += 1\n\n        if self._get_db_attempts > self.attempt_reconnect_threshold:\n            self.check_down_connections()\n\n        if retry_for is not None:\n            self.mark_connection_down(retry_for)\n\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        now = time.time()\n\n        for i in xrange(len(self.cluster)):\n            db_num = self._hosts_cycler.next()\n\n            marked_down_at = self._down_connections.get(db_num, False)\n\n            if not marked_down_at or (marked_down_at + self.retry_timeout <= now):\n                return [db_num]\n        else:\n            raise self.HostListExhausted()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        if db_nums and db_nums[0] in self._down_connections:\n            self.mark_connection_up(db_nums[0])\n\n        return db_nums", "levels": [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "from functools import wraps", "from itertools import cycle"], "function": ["def routing_params(func):\n", "    def wrapped(*args, **kwargs):\n", "class BaseRouter(object):\n", "    class UnableToSetupRouter(Exception):\n", "    def __init__(self, cluster=None, *args, **kwargs):\n", "    def setup_router(self, args, kwargs, **fkwargs):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n", "    def _handle_exception(self, e):\n", "class RoundRobinRouter(BaseRouter):\n", "    class HostListExhausted(Exception):\n", "    class InvalidDBNum(Exception):\n", "    def __init__(self, *args, **kwargs):\n", "    def ensure_db_num(cls, db_num):\n", "    def check_down_connections(self):\n", "    def flush_down_connections(self):\n", "    def mark_connection_down(self, db_num):\n", "    def mark_connection_up(self, db_num):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n"]}
{"repo": "disqus/nydus", "path": "nydus/db/routers/base.py", "func_name": "BaseRouter.setup_router", "original_string": "def setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Call method to perform any setup\n        \"\"\"\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready", "language": "python", "code": "def setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Call method to perform any setup\n        \"\"\"\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready", "code_tokens": ["def", "setup_router", "(", "self", ",", "args", ",", "kwargs", ",", "*", "*", "fkwargs", ")", ":", "self", ".", "_ready", "=", "self", ".", "_setup_router", "(", "args", "=", "args", ",", "kwargs", "=", "kwargs", ",", "*", "*", "fkwargs", ")", "return", "self", ".", "_ready"], "docstring": "Call method to perform any setup", "docstring_tokens": ["Call", "method", "to", "perform", "any", "setup"], "sha": "9b505840da47a34f758a830c3992fa5dcb7bb7ad", "url": "https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L87-L93", "partition": "train", "up_fun_num": 6, "context": "\"\"\"\nnydus.db.base\n~~~~~~~~~~~~~\n\n:copyright: (c) 2011-2012 DISQUS.\n:license: Apache License 2.0, see LICENSE for more details.\n\"\"\"\n\n__all__ = (\"BaseRouter\", \"RoundRobinRouter\", \"routing_params\")\n\nimport time\n\nfrom functools import wraps\nfrom itertools import cycle\n\n\ndef routing_params(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        if kwargs.get(\"kwargs\") is None:\n            kwargs[\"kwargs\"] = {}\n\n        if kwargs.get(\"args\") is None:\n            kwargs[\"args\"] = ()\n\n        return func(*args, **kwargs)\n\n    wrapped.__wraps__ = getattr(func, \"__wraps__\", func)\n    return wrapped\n\n\nclass BaseRouter(object):\n    \"\"\"\n    Handles routing requests to a specific connection in a single cluster.\n\n    For the most part, all public functions will receive arguments as ``key=value``\n    pairs and should expect as much. Functions which receive ``args`` and ``kwargs``\n    from the calling function will receive default values for those, and need not\n    worry about handling missing arguments.\n    \"\"\"\n\n    retryable = False\n\n    class UnableToSetupRouter(Exception):\n        pass\n\n    def __init__(self, cluster=None, *args, **kwargs):\n        self._ready = False\n        self.cluster = cluster\n\n    @routing_params\n    def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(\n            attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs\n        )\n\n    # Backwards compatibilty\n    get_db = get_dbs\n\n    @routing_params\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any initialization for the router\n        Returns False if setup could not be completed\n        \"\"\"\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any prerouting with this method and return the key\n        \"\"\"\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform routing and return db_nums\n        \"\"\"\n        return self.cluster.hosts.keys()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any postrouting actions and return db_nums\n        \"\"\"\n        return db_nums\n\n    def _handle_exception(self, e):\n        \"\"\"\n        Handle/transform exceptions and return it\n        \"\"\"\n        raise e\n\n\nclass RoundRobinRouter(BaseRouter):\n    \"\"\"\n    Basic retry router that performs round robin\n    \"\"\"\n\n    # Raised if all hosts in the hash have been marked as down\n    class HostListExhausted(Exception):\n        pass\n\n    class InvalidDBNum(Exception):\n        pass\n\n    # If this router can be retried on if a particular db index it gave out did\n    # not work\n    retryable = True\n\n    # How many requests to serve in a situation when a host is down before\n    # the down hosts are assesed for readmittance back into the pool of serving\n    # requests.\n    #\n    # If the attempt_reconnect_threshold is hit, it does not guarantee that the\n    # down hosts will be put back - only that the router will CHECK to see if\n    # the hosts CAN be put back.  The elegibility of a host being put back is\n    # handlede in the check_down_connections method, which by default will\n    # readmit a host if it was marked down more than retry_timeout seconds ago.\n    attempt_reconnect_threshold = 100000\n\n    # Number of seconds a host must be marked down before it is elligable to be\n    # put back in the pool and retried.\n    retry_timeout = 30\n\n    def __init__(self, *args, **kwargs):\n        self._get_db_attempts = 0\n        self._down_connections = {}\n\n        super(RoundRobinRouter, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def ensure_db_num(cls, db_num):\n        try:\n            return int(db_num)\n        except ValueError:\n            raise cls.InvalidDBNum()\n\n    def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)\n\n    def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)\n\n    def mark_connection_down(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections[db_num] = time.time()\n\n    def mark_connection_up(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections.pop(db_num, None)\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        self._hosts_cycler = cycle(self.cluster.hosts.keys())\n\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n        self._get_db_attempts += 1\n\n        if self._get_db_attempts > self.attempt_reconnect_threshold:\n            self.check_down_connections()\n\n        if retry_for is not None:\n            self.mark_connection_down(retry_for)\n\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        now = time.time()\n\n        for i in xrange(len(self.cluster)):\n            db_num = self._hosts_cycler.next()\n\n            marked_down_at = self._down_connections.get(db_num, False)\n\n            if not marked_down_at or (marked_down_at + self.retry_timeout <= now):\n                return [db_num]\n        else:\n            raise self.HostListExhausted()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        if db_nums and db_nums[0] in self._down_connections:\n            self.mark_connection_up(db_nums[0])\n\n        return db_nums\n", "levels": [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "from functools import wraps", "from itertools import cycle"], "function": ["def routing_params(func):\n", "    def wrapped(*args, **kwargs):\n", "class BaseRouter(object):\n", "    class UnableToSetupRouter(Exception):\n", "    def __init__(self, cluster=None, *args, **kwargs):\n", "    def get_dbs(self, attr, args, kwargs, **fkwargs):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n", "    def _handle_exception(self, e):\n", "class RoundRobinRouter(BaseRouter):\n", "    class HostListExhausted(Exception):\n", "    class InvalidDBNum(Exception):\n", "    def __init__(self, *args, **kwargs):\n", "    def ensure_db_num(cls, db_num):\n", "    def check_down_connections(self):\n", "    def flush_down_connections(self):\n", "    def mark_connection_down(self, db_num):\n", "    def mark_connection_up(self, db_num):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n"]}
{"repo": "disqus/nydus", "path": "nydus/db/routers/base.py", "func_name": "BaseRouter._route", "original_string": "def _route(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform routing and return db_nums\n        \"\"\"\n        return self.cluster.hosts.keys()", "language": "python", "code": "def _route(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform routing and return db_nums\n        \"\"\"\n        return self.cluster.hosts.keys()", "code_tokens": ["def", "_route", "(", "self", ",", "attr", ",", "args", ",", "kwargs", ",", "*", "*", "fkwargs", ")", ":", "return", "self", ".", "cluster", ".", "hosts", ".", "keys", "(", ")"], "docstring": "Perform routing and return db_nums", "docstring_tokens": ["Perform", "routing", "and", "return", "db_nums"], "sha": "9b505840da47a34f758a830c3992fa5dcb7bb7ad", "url": "https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L111-L115", "partition": "train", "up_fun_num": 9, "context": "\"\"\"\nnydus.db.base\n~~~~~~~~~~~~~\n\n:copyright: (c) 2011-2012 DISQUS.\n:license: Apache License 2.0, see LICENSE for more details.\n\"\"\"\n\n__all__ = (\"BaseRouter\", \"RoundRobinRouter\", \"routing_params\")\n\nimport time\n\nfrom functools import wraps\nfrom itertools import cycle\n\n\ndef routing_params(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        if kwargs.get(\"kwargs\") is None:\n            kwargs[\"kwargs\"] = {}\n\n        if kwargs.get(\"args\") is None:\n            kwargs[\"args\"] = ()\n\n        return func(*args, **kwargs)\n\n    wrapped.__wraps__ = getattr(func, \"__wraps__\", func)\n    return wrapped\n\n\nclass BaseRouter(object):\n    \"\"\"\n    Handles routing requests to a specific connection in a single cluster.\n\n    For the most part, all public functions will receive arguments as ``key=value``\n    pairs and should expect as much. Functions which receive ``args`` and ``kwargs``\n    from the calling function will receive default values for those, and need not\n    worry about handling missing arguments.\n    \"\"\"\n\n    retryable = False\n\n    class UnableToSetupRouter(Exception):\n        pass\n\n    def __init__(self, cluster=None, *args, **kwargs):\n        self._ready = False\n        self.cluster = cluster\n\n    @routing_params\n    def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(\n            attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs\n        )\n\n    # Backwards compatibilty\n    get_db = get_dbs\n\n    @routing_params\n    def setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Call method to perform any setup\n        \"\"\"\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any initialization for the router\n        Returns False if setup could not be completed\n        \"\"\"\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any prerouting with this method and return the key\n        \"\"\"\n        return args, kwargs\n\n    @routing_params\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any postrouting actions and return db_nums\n        \"\"\"\n        return db_nums\n\n    def _handle_exception(self, e):\n        \"\"\"\n        Handle/transform exceptions and return it\n        \"\"\"\n        raise e\n\n\nclass RoundRobinRouter(BaseRouter):\n    \"\"\"\n    Basic retry router that performs round robin\n    \"\"\"\n\n    # Raised if all hosts in the hash have been marked as down\n    class HostListExhausted(Exception):\n        pass\n\n    class InvalidDBNum(Exception):\n        pass\n\n    # If this router can be retried on if a particular db index it gave out did\n    # not work\n    retryable = True\n\n    # How many requests to serve in a situation when a host is down before\n    # the down hosts are assesed for readmittance back into the pool of serving\n    # requests.\n    #\n    # If the attempt_reconnect_threshold is hit, it does not guarantee that the\n    # down hosts will be put back - only that the router will CHECK to see if\n    # the hosts CAN be put back.  The elegibility of a host being put back is\n    # handlede in the check_down_connections method, which by default will\n    # readmit a host if it was marked down more than retry_timeout seconds ago.\n    attempt_reconnect_threshold = 100000\n\n    # Number of seconds a host must be marked down before it is elligable to be\n    # put back in the pool and retried.\n    retry_timeout = 30\n\n    def __init__(self, *args, **kwargs):\n        self._get_db_attempts = 0\n        self._down_connections = {}\n\n        super(RoundRobinRouter, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def ensure_db_num(cls, db_num):\n        try:\n            return int(db_num)\n        except ValueError:\n            raise cls.InvalidDBNum()\n\n    def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)\n\n    def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)\n\n    def mark_connection_down(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections[db_num] = time.time()\n\n    def mark_connection_up(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections.pop(db_num, None)\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        self._hosts_cycler = cycle(self.cluster.hosts.keys())\n\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n        self._get_db_attempts += 1\n\n        if self._get_db_attempts > self.attempt_reconnect_threshold:\n            self.check_down_connections()\n\n        if retry_for is not None:\n            self.mark_connection_down(retry_for)\n\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        now = time.time()\n\n        for i in xrange(len(self.cluster)):\n            db_num = self._hosts_cycler.next()\n\n            marked_down_at = self._down_connections.get(db_num, False)\n\n            if not marked_down_at or (marked_down_at + self.retry_timeout <= now):\n                return [db_num]\n        else:\n            raise self.HostListExhausted()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        if db_nums and db_nums[0] in self._down_connections:\n            self.mark_connection_up(db_nums[0])\n\n        return db_nums\n", "levels": [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "from functools import wraps", "from itertools import cycle"], "function": ["def routing_params(func):\n", "    def wrapped(*args, **kwargs):\n", "class BaseRouter(object):\n", "    class UnableToSetupRouter(Exception):\n", "    def __init__(self, cluster=None, *args, **kwargs):\n", "    def get_dbs(self, attr, args, kwargs, **fkwargs):\n", "    def setup_router(self, args, kwargs, **fkwargs):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n", "    def _handle_exception(self, e):\n", "class RoundRobinRouter(BaseRouter):\n", "    class HostListExhausted(Exception):\n", "    class InvalidDBNum(Exception):\n", "    def __init__(self, *args, **kwargs):\n", "    def ensure_db_num(cls, db_num):\n", "    def check_down_connections(self):\n", "    def flush_down_connections(self):\n", "    def mark_connection_down(self, db_num):\n", "    def mark_connection_up(self, db_num):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n"]}
{"repo": "disqus/nydus", "path": "nydus/db/routers/base.py", "func_name": "RoundRobinRouter.check_down_connections", "original_string": "def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)", "language": "python", "code": "def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)", "code_tokens": ["def", "check_down_connections", "(", "self", ")", ":", "now", "=", "time", ".", "time", "(", ")", "for", "db_num", ",", "marked_down_at", "in", "self", ".", "_down_connections", ".", "items", "(", ")", ":", "if", "marked_down_at", "+", "self", ".", "retry_timeout", "<=", "now", ":", "self", ".", "mark_connection_up", "(", "db_num", ")"], "docstring": "Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.", "docstring_tokens": ["Iterates", "through", "all", "connections", "which", "were", "previously", "listed", "as", "unavailable", "and", "marks", "any", "that", "have", "expired", "their", "retry_timeout", "as", "being", "up", "."], "sha": "9b505840da47a34f758a830c3992fa5dcb7bb7ad", "url": "https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L175-L184", "partition": "train", "up_fun_num": 17, "context": "\"\"\"\nnydus.db.base\n~~~~~~~~~~~~~\n\n:copyright: (c) 2011-2012 DISQUS.\n:license: Apache License 2.0, see LICENSE for more details.\n\"\"\"\n\n__all__ = (\"BaseRouter\", \"RoundRobinRouter\", \"routing_params\")\n\nimport time\n\nfrom functools import wraps\nfrom itertools import cycle\n\n\ndef routing_params(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        if kwargs.get(\"kwargs\") is None:\n            kwargs[\"kwargs\"] = {}\n\n        if kwargs.get(\"args\") is None:\n            kwargs[\"args\"] = ()\n\n        return func(*args, **kwargs)\n\n    wrapped.__wraps__ = getattr(func, \"__wraps__\", func)\n    return wrapped\n\n\nclass BaseRouter(object):\n    \"\"\"\n    Handles routing requests to a specific connection in a single cluster.\n\n    For the most part, all public functions will receive arguments as ``key=value``\n    pairs and should expect as much. Functions which receive ``args`` and ``kwargs``\n    from the calling function will receive default values for those, and need not\n    worry about handling missing arguments.\n    \"\"\"\n\n    retryable = False\n\n    class UnableToSetupRouter(Exception):\n        pass\n\n    def __init__(self, cluster=None, *args, **kwargs):\n        self._ready = False\n        self.cluster = cluster\n\n    @routing_params\n    def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(\n            attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs\n        )\n\n    # Backwards compatibilty\n    get_db = get_dbs\n\n    @routing_params\n    def setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Call method to perform any setup\n        \"\"\"\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any initialization for the router\n        Returns False if setup could not be completed\n        \"\"\"\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any prerouting with this method and return the key\n        \"\"\"\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform routing and return db_nums\n        \"\"\"\n        return self.cluster.hosts.keys()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any postrouting actions and return db_nums\n        \"\"\"\n        return db_nums\n\n    def _handle_exception(self, e):\n        \"\"\"\n        Handle/transform exceptions and return it\n        \"\"\"\n        raise e\n\n\nclass RoundRobinRouter(BaseRouter):\n    \"\"\"\n    Basic retry router that performs round robin\n    \"\"\"\n\n    # Raised if all hosts in the hash have been marked as down\n    class HostListExhausted(Exception):\n        pass\n\n    class InvalidDBNum(Exception):\n        pass\n\n    # If this router can be retried on if a particular db index it gave out did\n    # not work\n    retryable = True\n\n    # How many requests to serve in a situation when a host is down before\n    # the down hosts are assesed for readmittance back into the pool of serving\n    # requests.\n    #\n    # If the attempt_reconnect_threshold is hit, it does not guarantee that the\n    # down hosts will be put back - only that the router will CHECK to see if\n    # the hosts CAN be put back.  The elegibility of a host being put back is\n    # handlede in the check_down_connections method, which by default will\n    # readmit a host if it was marked down more than retry_timeout seconds ago.\n    attempt_reconnect_threshold = 100000\n\n    # Number of seconds a host must be marked down before it is elligable to be\n    # put back in the pool and retried.\n    retry_timeout = 30\n\n    def __init__(self, *args, **kwargs):\n        self._get_db_attempts = 0\n        self._down_connections = {}\n\n        super(RoundRobinRouter, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def ensure_db_num(cls, db_num):\n        try:\n            return int(db_num)\n        except ValueError:\n            raise cls.InvalidDBNum()\n\n    def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)\n\n    def mark_connection_down(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections[db_num] = time.time()\n\n    def mark_connection_up(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections.pop(db_num, None)\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        self._hosts_cycler = cycle(self.cluster.hosts.keys())\n\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n        self._get_db_attempts += 1\n\n        if self._get_db_attempts > self.attempt_reconnect_threshold:\n            self.check_down_connections()\n\n        if retry_for is not None:\n            self.mark_connection_down(retry_for)\n\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        now = time.time()\n\n        for i in xrange(len(self.cluster)):\n            db_num = self._hosts_cycler.next()\n\n            marked_down_at = self._down_connections.get(db_num, False)\n\n            if not marked_down_at or (marked_down_at + self.retry_timeout <= now):\n                return [db_num]\n        else:\n            raise self.HostListExhausted()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        if db_nums and db_nums[0] in self._down_connections:\n            self.mark_connection_up(db_nums[0])\n\n        return db_nums\n", "levels": [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "from functools import wraps", "from itertools import cycle"], "function": ["def routing_params(func):\n", "    def wrapped(*args, **kwargs):\n", "class BaseRouter(object):\n", "    class UnableToSetupRouter(Exception):\n", "    def __init__(self, cluster=None, *args, **kwargs):\n", "    def get_dbs(self, attr, args, kwargs, **fkwargs):\n", "    def setup_router(self, args, kwargs, **fkwargs):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n", "    def _handle_exception(self, e):\n", "class RoundRobinRouter(BaseRouter):\n", "    class HostListExhausted(Exception):\n", "    class InvalidDBNum(Exception):\n", "    def __init__(self, *args, **kwargs):\n", "    def ensure_db_num(cls, db_num):\n", "    def flush_down_connections(self):\n", "    def mark_connection_down(self, db_num):\n", "    def mark_connection_up(self, db_num):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n"]}
{"repo": "disqus/nydus", "path": "nydus/db/routers/base.py", "func_name": "RoundRobinRouter.flush_down_connections", "original_string": "def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)", "language": "python", "code": "def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)", "code_tokens": ["def", "flush_down_connections", "(", "self", ")", ":", "self", ".", "_get_db_attempts", "=", "0", "for", "db_num", "in", "self", ".", "_down_connections", ".", "keys", "(", ")", ":", "self", ".", "mark_connection_up", "(", "db_num", ")"], "docstring": "Marks all connections which were previously listed as unavailable as being up.", "docstring_tokens": ["Marks", "all", "connections", "which", "were", "previously", "listed", "as", "unavailable", "as", "being", "up", "."], "sha": "9b505840da47a34f758a830c3992fa5dcb7bb7ad", "url": "https://github.com/disqus/nydus/blob/9b505840da47a34f758a830c3992fa5dcb7bb7ad/nydus/db/routers/base.py#L186-L192", "partition": "train", "up_fun_num": 18, "context": "\"\"\"\nnydus.db.base\n~~~~~~~~~~~~~\n\n:copyright: (c) 2011-2012 DISQUS.\n:license: Apache License 2.0, see LICENSE for more details.\n\"\"\"\n\n__all__ = (\"BaseRouter\", \"RoundRobinRouter\", \"routing_params\")\n\nimport time\n\nfrom functools import wraps\nfrom itertools import cycle\n\n\ndef routing_params(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        if kwargs.get(\"kwargs\") is None:\n            kwargs[\"kwargs\"] = {}\n\n        if kwargs.get(\"args\") is None:\n            kwargs[\"args\"] = ()\n\n        return func(*args, **kwargs)\n\n    wrapped.__wraps__ = getattr(func, \"__wraps__\", func)\n    return wrapped\n\n\nclass BaseRouter(object):\n    \"\"\"\n    Handles routing requests to a specific connection in a single cluster.\n\n    For the most part, all public functions will receive arguments as ``key=value``\n    pairs and should expect as much. Functions which receive ``args`` and ``kwargs``\n    from the calling function will receive default values for those, and need not\n    worry about handling missing arguments.\n    \"\"\"\n\n    retryable = False\n\n    class UnableToSetupRouter(Exception):\n        pass\n\n    def __init__(self, cluster=None, *args, **kwargs):\n        self._ready = False\n        self.cluster = cluster\n\n    @routing_params\n    def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(\n            attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs\n        )\n\n    # Backwards compatibilty\n    get_db = get_dbs\n\n    @routing_params\n    def setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Call method to perform any setup\n        \"\"\"\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any initialization for the router\n        Returns False if setup could not be completed\n        \"\"\"\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any prerouting with this method and return the key\n        \"\"\"\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform routing and return db_nums\n        \"\"\"\n        return self.cluster.hosts.keys()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        \"\"\"\n        Perform any postrouting actions and return db_nums\n        \"\"\"\n        return db_nums\n\n    def _handle_exception(self, e):\n        \"\"\"\n        Handle/transform exceptions and return it\n        \"\"\"\n        raise e\n\n\nclass RoundRobinRouter(BaseRouter):\n    \"\"\"\n    Basic retry router that performs round robin\n    \"\"\"\n\n    # Raised if all hosts in the hash have been marked as down\n    class HostListExhausted(Exception):\n        pass\n\n    class InvalidDBNum(Exception):\n        pass\n\n    # If this router can be retried on if a particular db index it gave out did\n    # not work\n    retryable = True\n\n    # How many requests to serve in a situation when a host is down before\n    # the down hosts are assesed for readmittance back into the pool of serving\n    # requests.\n    #\n    # If the attempt_reconnect_threshold is hit, it does not guarantee that the\n    # down hosts will be put back - only that the router will CHECK to see if\n    # the hosts CAN be put back.  The elegibility of a host being put back is\n    # handlede in the check_down_connections method, which by default will\n    # readmit a host if it was marked down more than retry_timeout seconds ago.\n    attempt_reconnect_threshold = 100000\n\n    # Number of seconds a host must be marked down before it is elligable to be\n    # put back in the pool and retried.\n    retry_timeout = 30\n\n    def __init__(self, *args, **kwargs):\n        self._get_db_attempts = 0\n        self._down_connections = {}\n\n        super(RoundRobinRouter, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def ensure_db_num(cls, db_num):\n        try:\n            return int(db_num)\n        except ValueError:\n            raise cls.InvalidDBNum()\n\n    def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)\n\n    def mark_connection_down(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections[db_num] = time.time()\n\n    def mark_connection_up(self, db_num):\n        db_num = self.ensure_db_num(db_num)\n        self._down_connections.pop(db_num, None)\n\n    @routing_params\n    def _setup_router(self, args, kwargs, **fkwargs):\n        self._hosts_cycler = cycle(self.cluster.hosts.keys())\n\n        return True\n\n    @routing_params\n    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n        self._get_db_attempts += 1\n\n        if self._get_db_attempts > self.attempt_reconnect_threshold:\n            self.check_down_connections()\n\n        if retry_for is not None:\n            self.mark_connection_down(retry_for)\n\n        return args, kwargs\n\n    @routing_params\n    def _route(self, attr, args, kwargs, **fkwargs):\n        now = time.time()\n\n        for i in xrange(len(self.cluster)):\n            db_num = self._hosts_cycler.next()\n\n            marked_down_at = self._down_connections.get(db_num, False)\n\n            if not marked_down_at or (marked_down_at + self.retry_timeout <= now):\n                return [db_num]\n        else:\n            raise self.HostListExhausted()\n\n    @routing_params\n    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n        if db_nums and db_nums[0] in self._down_connections:\n            self.mark_connection_up(db_nums[0])\n\n        return db_nums\n", "levels": [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "from functools import wraps", "from itertools import cycle"], "function": ["def routing_params(func):\n", "    def wrapped(*args, **kwargs):\n", "class BaseRouter(object):\n", "    class UnableToSetupRouter(Exception):\n", "    def __init__(self, cluster=None, *args, **kwargs):\n", "    def get_dbs(self, attr, args, kwargs, **fkwargs):\n", "    def setup_router(self, args, kwargs, **fkwargs):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n", "    def _handle_exception(self, e):\n", "class RoundRobinRouter(BaseRouter):\n", "    class HostListExhausted(Exception):\n", "    class InvalidDBNum(Exception):\n", "    def __init__(self, *args, **kwargs):\n", "    def ensure_db_num(cls, db_num):\n", "    def check_down_connections(self):\n", "    def mark_connection_down(self, db_num):\n", "    def mark_connection_up(self, db_num):\n", "    def _setup_router(self, args, kwargs, **fkwargs):\n", "    def _pre_routing(self, attr, args, kwargs, retry_for=None, **fkwargs):\n", "    def _route(self, attr, args, kwargs, **fkwargs):\n", "    def _post_routing(self, attr, db_nums, args, kwargs, **fkwargs):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/analysis.py", "func_name": "standby", "original_string": "def standby(df, resolution='24h', time_window=None):\n    \"\"\"\n    Compute standby power\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Electricity Power\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    df : pandas.Series with DateTimeIndex in the given resolution\n    \"\"\"\n\n    if df.empty:\n        raise EmptyDataFrame()\n\n    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame\n    def parse_time(t):\n        if isinstance(t, numbers.Number):\n            return pd.Timestamp.utcfromtimestamp(t).time()\n        else:\n            return pd.Timestamp(t).time()\n\n\n    # first filter based on the time-window\n    if time_window is not None:\n        t_start = parse_time(time_window[0])\n        t_end = parse_time(time_window[1])\n        if t_start > t_end:\n            # start before midnight\n            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]\n        else:\n            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]\n\n    return df.resample(resolution).min()", "language": "python", "code": "def standby(df, resolution='24h', time_window=None):\n    \"\"\"\n    Compute standby power\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Electricity Power\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    df : pandas.Series with DateTimeIndex in the given resolution\n    \"\"\"\n\n    if df.empty:\n        raise EmptyDataFrame()\n\n    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame\n    def parse_time(t):\n        if isinstance(t, numbers.Number):\n            return pd.Timestamp.utcfromtimestamp(t).time()\n        else:\n            return pd.Timestamp(t).time()\n\n\n    # first filter based on the time-window\n    if time_window is not None:\n        t_start = parse_time(time_window[0])\n        t_end = parse_time(time_window[1])\n        if t_start > t_end:\n            # start before midnight\n            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]\n        else:\n            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]\n\n    return df.resample(resolution).min()", "code_tokens": ["def", "standby", "(", "df", ",", "resolution", "=", "'24h'", ",", "time_window", "=", "None", ")", ":", "if", "df", ".", "empty", ":", "raise", "EmptyDataFrame", "(", ")", "df", "=", "pd", ".", "DataFrame", "(", "df", ")", "# if df was a pd.Series, convert to DataFrame", "def", "parse_time", "(", "t", ")", ":", "if", "isinstance", "(", "t", ",", "numbers", ".", "Number", ")", ":", "return", "pd", ".", "Timestamp", ".", "utcfromtimestamp", "(", "t", ")", ".", "time", "(", ")", "else", ":", "return", "pd", ".", "Timestamp", "(", "t", ")", ".", "time", "(", ")", "# first filter based on the time-window", "if", "time_window", "is", "not", "None", ":", "t_start", "=", "parse_time", "(", "time_window", "[", "0", "]", ")", "t_end", "=", "parse_time", "(", "time_window", "[", "1", "]", ")", "if", "t_start", ">", "t_end", ":", "# start before midnight", "df", "=", "df", "[", "(", "df", ".", "index", ".", "time", ">=", "t_start", ")", "|", "(", "df", ".", "index", ".", "time", "<", "t_end", ")", "]", "else", ":", "df", "=", "df", "[", "(", "df", ".", "index", ".", "time", ">=", "t_start", ")", "&", "(", "df", ".", "index", ".", "time", "<", "t_end", ")", "]", "return", "df", ".", "resample", "(", "resolution", ")", ".", "min", "(", ")"], "docstring": "Compute standby power\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Electricity Power\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    df : pandas.Series with DateTimeIndex in the given resolution", "docstring_tokens": ["Compute", "standby", "power"], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L72-L115", "partition": "train", "up_fun_num": 8, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\n\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport numbers\nfrom opengrid.library.exceptions import EmptyDataFrame\n\n\nclass Analysis(object):\n    \"\"\"\n    Generic Analysis\n\n    An analysis should have a dataframe as input\n    self.result should be used as 'output dataframe'\n    It also has output methods: plot, to json...\n    \"\"\"\n\n    def __init__(self, df, *args, **kwargs):\n        self.df = df\n        self.do_analysis(*args, **kwargs)\n\n    def do_analysis(self, *args, **kwargs):\n        # To be overwritten by inheriting class\n        self.result = self.df.copy()\n\n    def plot(self):\n        self.result.plot()\n\n    def to_json(self):\n        return self.result.to_json()\n\n\nclass DailyAgg(Analysis):\n    \"\"\"\n    Obtain a dataframe with daily aggregated data according to an aggregation operator\n    like min, max or mean\n    - for the entire day if starttime and endtime are not specified\n    - within a time-range specified by starttime and endtime.\n      This can be used eg. to get the minimum consumption during the night.\n    \"\"\"\n\n    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n        \"\"\"\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            With pandas.DatetimeIndex and one or more columns\n        agg : str\n            'min', 'max', or another aggregation function\n        starttime, endtime : datetime.time objects\n            For each day, only consider the time between starttime and endtime\n            If None, use begin of day/end of day respectively\n        \"\"\"\n        super(DailyAgg, self).__init__(df, agg, starttime=starttime, endtime=endtime)\n\n    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n        if not self.df.empty:\n            df = self.df[\n                (self.df.index.time >= starttime) & (self.df.index.time < endtime)\n            ]\n            df = df.resample(\"D\", how=agg)\n            self.result = df\n        else:\n            self.result = pd.DataFrame()\n\n\ndef share_of_standby(df, resolution=\"24h\", time_window=None):\n    \"\"\"\n    Compute the share of the standby power in the total consumption.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Power (typically electricity, can be anything)\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    fraction : float between 0-1 with the share of the standby consumption\n    \"\"\"\n\n    p_sb = standby(df, resolution, time_window)\n    df = df.resample(resolution).mean()\n    p_tot = df.sum()\n    p_standby = p_sb.sum()\n    share_standby = p_standby / p_tot\n    res = share_standby.iloc[0]\n    return res\n\n\ndef count_peaks(ts):\n    \"\"\"\n    Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    on_toggles = ts.diff() > 3000\n    shifted = np.logical_not(on_toggles.shift(1))\n    result = on_toggles & shifted\n    count = result.sum()\n    return count\n\n\ndef load_factor(ts, resolution=None, norm=None):\n    \"\"\"\n    Calculate the ratio of input vs. norm over a given interval.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        timeseries\n    resolution : str, optional\n        interval over which to calculate the ratio\n        default: resolution of the input timeseries\n    norm : int | float, optional\n        denominator of the ratio\n        default: the maximum of the input timeseries\n\n    Returns\n    -------\n    pandas.Series\n    \"\"\"\n    if norm is None:\n        norm = ts.max()\n\n    if resolution is not None:\n        ts = ts.resample(rule=resolution).mean()\n\n    lf = ts / norm\n\n    return lf\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0], "package": ["import datetime as dt", "import pandas as pd", "import numpy as np", "import numbers", "from opengrid.library.exceptions import EmptyDataFrame"], "function": ["class Analysis(object):\n", "    def __init__(self, df, *args, **kwargs):\n", "    def do_analysis(self, *args, **kwargs):\n", "    def plot(self):\n", "    def to_json(self):\n", "class DailyAgg(Analysis):\n", "    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "def share_of_standby(df, resolution=\"24h\", time_window=None):\n", "def count_peaks(ts):\n", "def load_factor(ts, resolution=None, norm=None):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/analysis.py", "func_name": "share_of_standby", "original_string": "def share_of_standby(df, resolution='24h', time_window=None):\n    \"\"\"\n    Compute the share of the standby power in the total consumption.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Power (typically electricity, can be anything)\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    fraction : float between 0-1 with the share of the standby consumption\n    \"\"\"\n\n    p_sb = standby(df, resolution, time_window)\n    df = df.resample(resolution).mean()\n    p_tot = df.sum()\n    p_standby = p_sb.sum()\n    share_standby = p_standby / p_tot\n    res = share_standby.iloc[0]\n    return res", "language": "python", "code": "def share_of_standby(df, resolution='24h', time_window=None):\n    \"\"\"\n    Compute the share of the standby power in the total consumption.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Power (typically electricity, can be anything)\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    fraction : float between 0-1 with the share of the standby consumption\n    \"\"\"\n\n    p_sb = standby(df, resolution, time_window)\n    df = df.resample(resolution).mean()\n    p_tot = df.sum()\n    p_standby = p_sb.sum()\n    share_standby = p_standby / p_tot\n    res = share_standby.iloc[0]\n    return res", "code_tokens": ["def", "share_of_standby", "(", "df", ",", "resolution", "=", "'24h'", ",", "time_window", "=", "None", ")", ":", "p_sb", "=", "standby", "(", "df", ",", "resolution", ",", "time_window", ")", "df", "=", "df", ".", "resample", "(", "resolution", ")", ".", "mean", "(", ")", "p_tot", "=", "df", ".", "sum", "(", ")", "p_standby", "=", "p_sb", ".", "sum", "(", ")", "share_standby", "=", "p_standby", "/", "p_tot", "res", "=", "share_standby", ".", "iloc", "[", "0", "]", "return", "res"], "docstring": "Compute the share of the standby power in the total consumption.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Power (typically electricity, can be anything)\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    fraction : float between 0-1 with the share of the standby consumption", "docstring_tokens": ["Compute", "the", "share", "of", "the", "standby", "power", "in", "the", "total", "consumption", "."], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L118-L146", "partition": "train", "up_fun_num": 10, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\n\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport numbers\nfrom opengrid.library.exceptions import EmptyDataFrame\n\n\nclass Analysis(object):\n    \"\"\"\n    Generic Analysis\n\n    An analysis should have a dataframe as input\n    self.result should be used as 'output dataframe'\n    It also has output methods: plot, to json...\n    \"\"\"\n\n    def __init__(self, df, *args, **kwargs):\n        self.df = df\n        self.do_analysis(*args, **kwargs)\n\n    def do_analysis(self, *args, **kwargs):\n        # To be overwritten by inheriting class\n        self.result = self.df.copy()\n\n    def plot(self):\n        self.result.plot()\n\n    def to_json(self):\n        return self.result.to_json()\n\n\nclass DailyAgg(Analysis):\n    \"\"\"\n    Obtain a dataframe with daily aggregated data according to an aggregation operator\n    like min, max or mean\n    - for the entire day if starttime and endtime are not specified\n    - within a time-range specified by starttime and endtime.\n      This can be used eg. to get the minimum consumption during the night.\n    \"\"\"\n\n    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n        \"\"\"\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            With pandas.DatetimeIndex and one or more columns\n        agg : str\n            'min', 'max', or another aggregation function\n        starttime, endtime : datetime.time objects\n            For each day, only consider the time between starttime and endtime\n            If None, use begin of day/end of day respectively\n        \"\"\"\n        super(DailyAgg, self).__init__(df, agg, starttime=starttime, endtime=endtime)\n\n    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n        if not self.df.empty:\n            df = self.df[\n                (self.df.index.time >= starttime) & (self.df.index.time < endtime)\n            ]\n            df = df.resample(\"D\", how=agg)\n            self.result = df\n        else:\n            self.result = pd.DataFrame()\n\n\ndef standby(df, resolution=\"24h\", time_window=None):\n    \"\"\"\n    Compute standby power\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Electricity Power\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    df : pandas.Series with DateTimeIndex in the given resolution\n    \"\"\"\n\n    if df.empty:\n        raise EmptyDataFrame()\n\n    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame\n\n    def parse_time(t):\n        if isinstance(t, numbers.Number):\n            return pd.Timestamp.utcfromtimestamp(t).time()\n        else:\n            return pd.Timestamp(t).time()\n\n    # first filter based on the time-window\n    if time_window is not None:\n        t_start = parse_time(time_window[0])\n        t_end = parse_time(time_window[1])\n        if t_start > t_end:\n            # start before midnight\n            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]\n        else:\n            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]\n\n    return df.resample(resolution).min()\n\n\ndef count_peaks(ts):\n    \"\"\"\n    Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    on_toggles = ts.diff() > 3000\n    shifted = np.logical_not(on_toggles.shift(1))\n    result = on_toggles & shifted\n    count = result.sum()\n    return count\n\n\ndef load_factor(ts, resolution=None, norm=None):\n    \"\"\"\n    Calculate the ratio of input vs. norm over a given interval.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        timeseries\n    resolution : str, optional\n        interval over which to calculate the ratio\n        default: resolution of the input timeseries\n    norm : int | float, optional\n        denominator of the ratio\n        default: the maximum of the input timeseries\n\n    Returns\n    -------\n    pandas.Series\n    \"\"\"\n    if norm is None:\n        norm = ts.max()\n\n    if resolution is not None:\n        ts = ts.resample(rule=resolution).mean()\n\n    lf = ts / norm\n\n    return lf\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0], "package": ["import datetime as dt", "import pandas as pd", "import numpy as np", "import numbers", "from opengrid.library.exceptions import EmptyDataFrame"], "function": ["class Analysis(object):\n", "    def __init__(self, df, *args, **kwargs):\n", "    def do_analysis(self, *args, **kwargs):\n", "    def plot(self):\n", "    def to_json(self):\n", "class DailyAgg(Analysis):\n", "    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "def standby(df, resolution=\"24h\", time_window=None):\n", "    def parse_time(t):\n", "def count_peaks(ts):\n", "def load_factor(ts, resolution=None, norm=None):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/analysis.py", "func_name": "count_peaks", "original_string": "def count_peaks(ts):\n    \"\"\"\n    Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    on_toggles = ts.diff() > 3000\n    shifted = np.logical_not(on_toggles.shift(1))\n    result = on_toggles & shifted\n    count = result.sum()\n    return count", "language": "python", "code": "def count_peaks(ts):\n    \"\"\"\n    Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    on_toggles = ts.diff() > 3000\n    shifted = np.logical_not(on_toggles.shift(1))\n    result = on_toggles & shifted\n    count = result.sum()\n    return count", "code_tokens": ["def", "count_peaks", "(", "ts", ")", ":", "on_toggles", "=", "ts", ".", "diff", "(", ")", ">", "3000", "shifted", "=", "np", ".", "logical_not", "(", "on_toggles", ".", "shift", "(", "1", ")", ")", "result", "=", "on_toggles", "&", "shifted", "count", "=", "result", ".", "sum", "(", ")", "return", "count"], "docstring": "Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int", "docstring_tokens": ["Toggle", "counter", "for", "gas", "boilers"], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L149-L169", "partition": "train", "up_fun_num": 11, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\n\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport numbers\nfrom opengrid.library.exceptions import EmptyDataFrame\n\n\nclass Analysis(object):\n    \"\"\"\n    Generic Analysis\n\n    An analysis should have a dataframe as input\n    self.result should be used as 'output dataframe'\n    It also has output methods: plot, to json...\n    \"\"\"\n\n    def __init__(self, df, *args, **kwargs):\n        self.df = df\n        self.do_analysis(*args, **kwargs)\n\n    def do_analysis(self, *args, **kwargs):\n        # To be overwritten by inheriting class\n        self.result = self.df.copy()\n\n    def plot(self):\n        self.result.plot()\n\n    def to_json(self):\n        return self.result.to_json()\n\n\nclass DailyAgg(Analysis):\n    \"\"\"\n    Obtain a dataframe with daily aggregated data according to an aggregation operator\n    like min, max or mean\n    - for the entire day if starttime and endtime are not specified\n    - within a time-range specified by starttime and endtime.\n      This can be used eg. to get the minimum consumption during the night.\n    \"\"\"\n\n    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n        \"\"\"\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            With pandas.DatetimeIndex and one or more columns\n        agg : str\n            'min', 'max', or another aggregation function\n        starttime, endtime : datetime.time objects\n            For each day, only consider the time between starttime and endtime\n            If None, use begin of day/end of day respectively\n        \"\"\"\n        super(DailyAgg, self).__init__(df, agg, starttime=starttime, endtime=endtime)\n\n    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n        if not self.df.empty:\n            df = self.df[\n                (self.df.index.time >= starttime) & (self.df.index.time < endtime)\n            ]\n            df = df.resample(\"D\", how=agg)\n            self.result = df\n        else:\n            self.result = pd.DataFrame()\n\n\ndef standby(df, resolution=\"24h\", time_window=None):\n    \"\"\"\n    Compute standby power\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Electricity Power\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    df : pandas.Series with DateTimeIndex in the given resolution\n    \"\"\"\n\n    if df.empty:\n        raise EmptyDataFrame()\n\n    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame\n\n    def parse_time(t):\n        if isinstance(t, numbers.Number):\n            return pd.Timestamp.utcfromtimestamp(t).time()\n        else:\n            return pd.Timestamp(t).time()\n\n    # first filter based on the time-window\n    if time_window is not None:\n        t_start = parse_time(time_window[0])\n        t_end = parse_time(time_window[1])\n        if t_start > t_end:\n            # start before midnight\n            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]\n        else:\n            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]\n\n    return df.resample(resolution).min()\n\n\ndef share_of_standby(df, resolution=\"24h\", time_window=None):\n    \"\"\"\n    Compute the share of the standby power in the total consumption.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Power (typically electricity, can be anything)\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    fraction : float between 0-1 with the share of the standby consumption\n    \"\"\"\n\n    p_sb = standby(df, resolution, time_window)\n    df = df.resample(resolution).mean()\n    p_tot = df.sum()\n    p_standby = p_sb.sum()\n    share_standby = p_standby / p_tot\n    res = share_standby.iloc[0]\n    return res\n\n\ndef load_factor(ts, resolution=None, norm=None):\n    \"\"\"\n    Calculate the ratio of input vs. norm over a given interval.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        timeseries\n    resolution : str, optional\n        interval over which to calculate the ratio\n        default: resolution of the input timeseries\n    norm : int | float, optional\n        denominator of the ratio\n        default: the maximum of the input timeseries\n\n    Returns\n    -------\n    pandas.Series\n    \"\"\"\n    if norm is None:\n        norm = ts.max()\n\n    if resolution is not None:\n        ts = ts.resample(rule=resolution).mean()\n\n    lf = ts / norm\n\n    return lf\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0], "package": ["import datetime as dt", "import pandas as pd", "import numpy as np", "import numbers", "from opengrid.library.exceptions import EmptyDataFrame"], "function": ["class Analysis(object):\n", "    def __init__(self, df, *args, **kwargs):\n", "    def do_analysis(self, *args, **kwargs):\n", "    def plot(self):\n", "    def to_json(self):\n", "class DailyAgg(Analysis):\n", "    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "def standby(df, resolution=\"24h\", time_window=None):\n", "    def parse_time(t):\n", "def share_of_standby(df, resolution=\"24h\", time_window=None):\n", "def load_factor(ts, resolution=None, norm=None):\n"]}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/analysis.py", "func_name": "load_factor", "original_string": "def load_factor(ts, resolution=None, norm=None):\n    \"\"\"\n    Calculate the ratio of input vs. norm over a given interval.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        timeseries\n    resolution : str, optional\n        interval over which to calculate the ratio\n        default: resolution of the input timeseries\n    norm : int | float, optional\n        denominator of the ratio\n        default: the maximum of the input timeseries\n\n    Returns\n    -------\n    pandas.Series\n    \"\"\"\n    if norm is None:\n        norm = ts.max()\n\n    if resolution is not None:\n        ts = ts.resample(rule=resolution).mean()\n\n    lf = ts / norm\n\n    return lf", "language": "python", "code": "def load_factor(ts, resolution=None, norm=None):\n    \"\"\"\n    Calculate the ratio of input vs. norm over a given interval.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        timeseries\n    resolution : str, optional\n        interval over which to calculate the ratio\n        default: resolution of the input timeseries\n    norm : int | float, optional\n        denominator of the ratio\n        default: the maximum of the input timeseries\n\n    Returns\n    -------\n    pandas.Series\n    \"\"\"\n    if norm is None:\n        norm = ts.max()\n\n    if resolution is not None:\n        ts = ts.resample(rule=resolution).mean()\n\n    lf = ts / norm\n\n    return lf", "code_tokens": ["def", "load_factor", "(", "ts", ",", "resolution", "=", "None", ",", "norm", "=", "None", ")", ":", "if", "norm", "is", "None", ":", "norm", "=", "ts", ".", "max", "(", ")", "if", "resolution", "is", "not", "None", ":", "ts", "=", "ts", ".", "resample", "(", "rule", "=", "resolution", ")", ".", "mean", "(", ")", "lf", "=", "ts", "/", "norm", "return", "lf"], "docstring": "Calculate the ratio of input vs. norm over a given interval.\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        timeseries\n    resolution : str, optional\n        interval over which to calculate the ratio\n        default: resolution of the input timeseries\n    norm : int | float, optional\n        denominator of the ratio\n        default: the maximum of the input timeseries\n\n    Returns\n    -------\n    pandas.Series", "docstring_tokens": ["Calculate", "the", "ratio", "of", "input", "vs", ".", "norm", "over", "a", "given", "interval", "."], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/analysis.py#L172-L199", "partition": "train", "up_fun_num": 12, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral analysis functions.\n\nTry to write all methods such that they take a dataframe as input\nand return a dataframe or list of dataframes.\n\"\"\"\n\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport numbers\nfrom opengrid.library.exceptions import EmptyDataFrame\n\n\nclass Analysis(object):\n    \"\"\"\n    Generic Analysis\n\n    An analysis should have a dataframe as input\n    self.result should be used as 'output dataframe'\n    It also has output methods: plot, to json...\n    \"\"\"\n\n    def __init__(self, df, *args, **kwargs):\n        self.df = df\n        self.do_analysis(*args, **kwargs)\n\n    def do_analysis(self, *args, **kwargs):\n        # To be overwritten by inheriting class\n        self.result = self.df.copy()\n\n    def plot(self):\n        self.result.plot()\n\n    def to_json(self):\n        return self.result.to_json()\n\n\nclass DailyAgg(Analysis):\n    \"\"\"\n    Obtain a dataframe with daily aggregated data according to an aggregation operator\n    like min, max or mean\n    - for the entire day if starttime and endtime are not specified\n    - within a time-range specified by starttime and endtime.\n      This can be used eg. to get the minimum consumption during the night.\n    \"\"\"\n\n    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n        \"\"\"\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            With pandas.DatetimeIndex and one or more columns\n        agg : str\n            'min', 'max', or another aggregation function\n        starttime, endtime : datetime.time objects\n            For each day, only consider the time between starttime and endtime\n            If None, use begin of day/end of day respectively\n        \"\"\"\n        super(DailyAgg, self).__init__(df, agg, starttime=starttime, endtime=endtime)\n\n    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n        if not self.df.empty:\n            df = self.df[\n                (self.df.index.time >= starttime) & (self.df.index.time < endtime)\n            ]\n            df = df.resample(\"D\", how=agg)\n            self.result = df\n        else:\n            self.result = pd.DataFrame()\n\n\ndef standby(df, resolution=\"24h\", time_window=None):\n    \"\"\"\n    Compute standby power\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Electricity Power\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    df : pandas.Series with DateTimeIndex in the given resolution\n    \"\"\"\n\n    if df.empty:\n        raise EmptyDataFrame()\n\n    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame\n\n    def parse_time(t):\n        if isinstance(t, numbers.Number):\n            return pd.Timestamp.utcfromtimestamp(t).time()\n        else:\n            return pd.Timestamp(t).time()\n\n    # first filter based on the time-window\n    if time_window is not None:\n        t_start = parse_time(time_window[0])\n        t_end = parse_time(time_window[1])\n        if t_start > t_end:\n            # start before midnight\n            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]\n        else:\n            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]\n\n    return df.resample(resolution).min()\n\n\ndef share_of_standby(df, resolution=\"24h\", time_window=None):\n    \"\"\"\n    Compute the share of the standby power in the total consumption.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame or pandas.Series\n        Power (typically electricity, can be anything)\n    resolution : str, default='d'\n        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation\n        of the minimum.\n        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'\n    time_window : tuple with start-hour and end-hour, default=None\n        Specify the start-time and end-time for the analysis.\n        Only data within this time window will be considered.\n        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects\n\n    Returns\n    -------\n    fraction : float between 0-1 with the share of the standby consumption\n    \"\"\"\n\n    p_sb = standby(df, resolution, time_window)\n    df = df.resample(resolution).mean()\n    p_tot = df.sum()\n    p_standby = p_sb.sum()\n    share_standby = p_standby / p_tot\n    res = share_standby.iloc[0]\n    return res\n\n\ndef count_peaks(ts):\n    \"\"\"\n    Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    on_toggles = ts.diff() > 3000\n    shifted = np.logical_not(on_toggles.shift(1))\n    result = on_toggles & shifted\n    count = result.sum()\n    return count\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0], "package": ["import datetime as dt", "import pandas as pd", "import numpy as np", "import numbers", "from opengrid.library.exceptions import EmptyDataFrame"], "function": ["class Analysis(object):\n", "    def __init__(self, df, *args, **kwargs):\n", "    def do_analysis(self, *args, **kwargs):\n", "    def plot(self):\n", "    def to_json(self):\n", "class DailyAgg(Analysis):\n", "    def __init__(self, df, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "    def do_analysis(self, agg, starttime=dt.time.min, endtime=dt.time.max):\n", "def standby(df, resolution=\"24h\", time_window=None):\n", "    def parse_time(t):\n", "def share_of_standby(df, resolution=\"24h\", time_window=None):\n", "def count_peaks(ts):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/besthits.py", "func_name": "top_hits", "original_string": "def top_hits(hits, num, column, reverse):\n    \"\"\"\n    get top hits after sorting by column number\n    \"\"\"\n    hits.sort(key = itemgetter(column), reverse = reverse)\n    for hit in hits[0:num]:\n        yield hit", "language": "python", "code": "def top_hits(hits, num, column, reverse):\n    \"\"\"\n    get top hits after sorting by column number\n    \"\"\"\n    hits.sort(key = itemgetter(column), reverse = reverse)\n    for hit in hits[0:num]:\n        yield hit", "code_tokens": ["def", "top_hits", "(", "hits", ",", "num", ",", "column", ",", "reverse", ")", ":", "hits", ".", "sort", "(", "key", "=", "itemgetter", "(", "column", ")", ",", "reverse", "=", "reverse", ")", "for", "hit", "in", "hits", "[", "0", ":", "num", "]", ":", "yield", "hit"], "docstring": "get top hits after sorting by column number", "docstring_tokens": ["get", "top", "hits", "after", "sorting", "by", "column", "number"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L17-L23", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nfilter specified number of hits from a blast or HMM search\nthat pass evalue and bit score thresholds\n\nnote: file must be sorted by query ID (and domain # for domtblout),\nbut does not have to be sorted by significance\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport pandas as pd\nfrom operator import itemgetter\n\n\ndef numBlast_sort(blast, numHits, evalueT, bitT):\n    \"\"\"\n    parse b6 output with sorting\n    \"\"\"\n    header = [\n        \"#query\",\n        \"target\",\n        \"pident\",\n        \"alen\",\n        \"mismatch\",\n        \"gapopen\",\n        \"qstart\",\n        \"qend\",\n        \"tstart\",\n        \"tend\",\n        \"evalue\",\n        \"bitscore\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in blast:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split(\"\\t\")\n        # Evalue and Bitscore thresholds\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#query\"]):\n        df = df.sort_values(by=[\"bitscore\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numBlast(blast, numHits, evalueT=False, bitT=False, sort=False):\n    \"\"\"\n    parse b6 output\n    \"\"\"\n    if sort is True:\n        for hit in numBlast_sort(blast, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#query\",\n        \"target\",\n        \"pident\",\n        \"alen\",\n        \"mismatch\",\n        \"gapopen\",\n        \"qstart\",\n        \"qend\",\n        \"tstart\",\n        \"tend\",\n        \"evalue\",\n        \"bitscore\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in blast:\n        line = line.strip().split(\"\\t\")\n        ID = line[0]\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if ID != prev:\n            if len(hits) > 0:\n                # column is 1 + line index\n                for hit in top_hits(hits, numHits, 11, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 11, True):\n        yield hit\n\n\ndef numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm domain table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bit = line[11], line[13]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:22], \" \".join(line[22:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\", \"domain #\"]):\n        df = df.sort_values(by=[\"domain score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm domain table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # create ID based on query name and domain number\n        ID = line[0] + line[9]\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bitscore = line[11], line[13]\n        line[11], line[13] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 13, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 13, True):\n        yield hit\n\n\ndef numTblout_sort(tblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm hmm table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain full Evalue and full score thresholds\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bit = line[4], line[5]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:18], \" \".join(line[18:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\"]):\n        df = df.sort_values(by=[\"full score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numTblout(tblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numTblout_sort(tblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # ID and scores\n        ID = line[0]\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bitscore = line[4], line[5]\n        line[4], line[5] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 5, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 5, True):\n        yield hit\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# filter blast or HMM tab output\")\n    parser.add_argument(\n        \"-i\",\n        default=\"-\",\n        help=\"path to search results (sorted by query; default = stdin)\",\n    )\n    parser.add_argument(\"-n\", default=1, type=int, help=\"number of hits (default = 1)\")\n    parser.add_argument(\n        \"-e\", default=False, type=float, help=\"e-value threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-b\", default=False, type=float, help=\"bit score threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-f\",\n        default=\"b6\",\n        type=str,\n        help=\"format (default = b6, options: b6, domtblout, tblout)\",\n    )\n    parser.add_argument(\n        \"--sort\",\n        action=\"store_true\",\n        help=\"sort hits by query name (evalues are sorted by default)\",\n    )\n    args = vars(parser.parse_args())\n    # check if file is from stdin\n    if args[\"i\"] == \"-\":\n        args[\"i\"] = sys.stdin\n    else:\n        args[\"i\"] = open(args[\"i\"])\n    if args[\"f\"] == \"b6\":\n        for hit in numBlast(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"domtblout\":\n        for hit in numDomtblout(\n            args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]\n        ):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"tblout\":\n        for hit in numTblout(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    else:\n        print(\"unsupported format:\", args[\"f\"])\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import pandas as pd", "from operator import itemgetter"], "function": ["def numBlast_sort(blast, numHits, evalueT, bitT):\n", "def numBlast(blast, numHits, evalueT=False, bitT=False, sort=False):\n", "def numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n", "def numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n", "def numTblout_sort(tblout, numHits, evalueT, bitT):\n", "def numTblout(tblout, numHits, evalueT, bitT, sort):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/besthits.py", "func_name": "numBlast_sort", "original_string": "def numBlast_sort(blast, numHits, evalueT, bitT):\n    \"\"\"\n    parse b6 output with sorting\n    \"\"\"\n    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',\n              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']\n    yield header\n    hmm = {h:[] for h in header}\n    for line in blast:\n        if line.startswith('#'):\n            continue\n        line = line.strip().split('\\t')\n        # Evalue and Bitscore thresholds\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by = ['#query']):\n        df = df.sort_values(by = ['bitscore'], ascending = False)\n        for hit in df[header].values[0:numHits]:\n            yield hit", "language": "python", "code": "def numBlast_sort(blast, numHits, evalueT, bitT):\n    \"\"\"\n    parse b6 output with sorting\n    \"\"\"\n    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',\n              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']\n    yield header\n    hmm = {h:[] for h in header}\n    for line in blast:\n        if line.startswith('#'):\n            continue\n        line = line.strip().split('\\t')\n        # Evalue and Bitscore thresholds\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by = ['#query']):\n        df = df.sort_values(by = ['bitscore'], ascending = False)\n        for hit in df[header].values[0:numHits]:\n            yield hit", "code_tokens": ["def", "numBlast_sort", "(", "blast", ",", "numHits", ",", "evalueT", ",", "bitT", ")", ":", "header", "=", "[", "'#query'", ",", "'target'", ",", "'pident'", ",", "'alen'", ",", "'mismatch'", ",", "'gapopen'", ",", "'qstart'", ",", "'qend'", ",", "'tstart'", ",", "'tend'", ",", "'evalue'", ",", "'bitscore'", "]", "yield", "header", "hmm", "=", "{", "h", ":", "[", "]", "for", "h", "in", "header", "}", "for", "line", "in", "blast", ":", "if", "line", ".", "startswith", "(", "'#'", ")", ":", "continue", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "# Evalue and Bitscore thresholds", "line", "[", "10", "]", ",", "line", "[", "11", "]", "=", "float", "(", "line", "[", "10", "]", ")", ",", "float", "(", "line", "[", "11", "]", ")", "evalue", ",", "bit", "=", "line", "[", "10", "]", ",", "line", "[", "11", "]", "if", "evalueT", "is", "not", "False", "and", "evalue", ">", "evalueT", ":", "continue", "if", "bitT", "is", "not", "False", "and", "bit", "<", "bitT", ":", "continue", "for", "i", ",", "h", "in", "zip", "(", "line", ",", "header", ")", ":", "hmm", "[", "h", "]", ".", "append", "(", "i", ")", "hmm", "=", "pd", ".", "DataFrame", "(", "hmm", ")", "for", "query", ",", "df", "in", "hmm", ".", "groupby", "(", "by", "=", "[", "'#query'", "]", ")", ":", "df", "=", "df", ".", "sort_values", "(", "by", "=", "[", "'bitscore'", "]", ",", "ascending", "=", "False", ")", "for", "hit", "in", "df", "[", "header", "]", ".", "values", "[", "0", ":", "numHits", "]", ":", "yield", "hit"], "docstring": "parse b6 output with sorting", "docstring_tokens": ["parse", "b6", "output", "with", "sorting"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L25-L50", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nfilter specified number of hits from a blast or HMM search\nthat pass evalue and bit score thresholds\n\nnote: file must be sorted by query ID (and domain # for domtblout),\nbut does not have to be sorted by significance\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport pandas as pd\nfrom operator import itemgetter\n\n\ndef top_hits(hits, num, column, reverse):\n    \"\"\"\n    get top hits after sorting by column number\n    \"\"\"\n    hits.sort(key=itemgetter(column), reverse=reverse)\n    for hit in hits[0:num]:\n        yield hit\n\n\ndef numBlast(blast, numHits, evalueT=False, bitT=False, sort=False):\n    \"\"\"\n    parse b6 output\n    \"\"\"\n    if sort is True:\n        for hit in numBlast_sort(blast, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#query\",\n        \"target\",\n        \"pident\",\n        \"alen\",\n        \"mismatch\",\n        \"gapopen\",\n        \"qstart\",\n        \"qend\",\n        \"tstart\",\n        \"tend\",\n        \"evalue\",\n        \"bitscore\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in blast:\n        line = line.strip().split(\"\\t\")\n        ID = line[0]\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if ID != prev:\n            if len(hits) > 0:\n                # column is 1 + line index\n                for hit in top_hits(hits, numHits, 11, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 11, True):\n        yield hit\n\n\ndef numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm domain table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bit = line[11], line[13]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:22], \" \".join(line[22:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\", \"domain #\"]):\n        df = df.sort_values(by=[\"domain score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm domain table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # create ID based on query name and domain number\n        ID = line[0] + line[9]\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bitscore = line[11], line[13]\n        line[11], line[13] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 13, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 13, True):\n        yield hit\n\n\ndef numTblout_sort(tblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm hmm table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain full Evalue and full score thresholds\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bit = line[4], line[5]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:18], \" \".join(line[18:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\"]):\n        df = df.sort_values(by=[\"full score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numTblout(tblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numTblout_sort(tblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # ID and scores\n        ID = line[0]\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bitscore = line[4], line[5]\n        line[4], line[5] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 5, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 5, True):\n        yield hit\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# filter blast or HMM tab output\")\n    parser.add_argument(\n        \"-i\",\n        default=\"-\",\n        help=\"path to search results (sorted by query; default = stdin)\",\n    )\n    parser.add_argument(\"-n\", default=1, type=int, help=\"number of hits (default = 1)\")\n    parser.add_argument(\n        \"-e\", default=False, type=float, help=\"e-value threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-b\", default=False, type=float, help=\"bit score threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-f\",\n        default=\"b6\",\n        type=str,\n        help=\"format (default = b6, options: b6, domtblout, tblout)\",\n    )\n    parser.add_argument(\n        \"--sort\",\n        action=\"store_true\",\n        help=\"sort hits by query name (evalues are sorted by default)\",\n    )\n    args = vars(parser.parse_args())\n    # check if file is from stdin\n    if args[\"i\"] == \"-\":\n        args[\"i\"] = sys.stdin\n    else:\n        args[\"i\"] = open(args[\"i\"])\n    if args[\"f\"] == \"b6\":\n        for hit in numBlast(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"domtblout\":\n        for hit in numDomtblout(\n            args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]\n        ):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"tblout\":\n        for hit in numTblout(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    else:\n        print(\"unsupported format:\", args[\"f\"])\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import pandas as pd", "from operator import itemgetter"], "function": ["def top_hits(hits, num, column, reverse):\n", "def numBlast(blast, numHits, evalueT=False, bitT=False, sort=False):\n", "def numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n", "def numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n", "def numTblout_sort(tblout, numHits, evalueT, bitT):\n", "def numTblout(tblout, numHits, evalueT, bitT, sort):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/besthits.py", "func_name": "numBlast", "original_string": "def numBlast(blast, numHits, evalueT = False, bitT = False, sort = False):\n    \"\"\"\n    parse b6 output\n    \"\"\"\n    if sort is True:\n        for hit in numBlast_sort(blast, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',\n              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']\n    yield header\n    prev, hits = None, []\n    for line in blast:\n        line = line.strip().split('\\t')\n        ID = line[0]\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if ID != prev:\n            if len(hits) > 0:\n                # column is 1 + line index\n                for hit in top_hits(hits, numHits, 11, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 11, True):\n        yield hit", "language": "python", "code": "def numBlast(blast, numHits, evalueT = False, bitT = False, sort = False):\n    \"\"\"\n    parse b6 output\n    \"\"\"\n    if sort is True:\n        for hit in numBlast_sort(blast, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',\n              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']\n    yield header\n    prev, hits = None, []\n    for line in blast:\n        line = line.strip().split('\\t')\n        ID = line[0]\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if ID != prev:\n            if len(hits) > 0:\n                # column is 1 + line index\n                for hit in top_hits(hits, numHits, 11, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 11, True):\n        yield hit", "code_tokens": ["def", "numBlast", "(", "blast", ",", "numHits", ",", "evalueT", "=", "False", ",", "bitT", "=", "False", ",", "sort", "=", "False", ")", ":", "if", "sort", "is", "True", ":", "for", "hit", "in", "numBlast_sort", "(", "blast", ",", "numHits", ",", "evalueT", ",", "bitT", ")", ":", "yield", "hit", "return", "header", "=", "[", "'#query'", ",", "'target'", ",", "'pident'", ",", "'alen'", ",", "'mismatch'", ",", "'gapopen'", ",", "'qstart'", ",", "'qend'", ",", "'tstart'", ",", "'tend'", ",", "'evalue'", ",", "'bitscore'", "]", "yield", "header", "prev", ",", "hits", "=", "None", ",", "[", "]", "for", "line", "in", "blast", ":", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "ID", "=", "line", "[", "0", "]", "line", "[", "10", "]", ",", "line", "[", "11", "]", "=", "float", "(", "line", "[", "10", "]", ")", ",", "float", "(", "line", "[", "11", "]", ")", "evalue", ",", "bit", "=", "line", "[", "10", "]", ",", "line", "[", "11", "]", "if", "ID", "!=", "prev", ":", "if", "len", "(", "hits", ")", ">", "0", ":", "# column is 1 + line index", "for", "hit", "in", "top_hits", "(", "hits", ",", "numHits", ",", "11", ",", "True", ")", ":", "yield", "hit", "hits", "=", "[", "]", "if", "evalueT", "==", "False", "and", "bitT", "==", "False", ":", "hits", ".", "append", "(", "line", ")", "elif", "evalue", "<=", "evalueT", "and", "bitT", "==", "False", ":", "hits", ".", "append", "(", "line", ")", "elif", "evalue", "<=", "evalueT", "and", "bit", ">=", "bitT", ":", "hits", ".", "append", "(", "line", ")", "elif", "evalueT", "==", "False", "and", "bit", ">=", "bitT", ":", "hits", ".", "append", "(", "line", ")", "prev", "=", "ID", "for", "hit", "in", "top_hits", "(", "hits", ",", "numHits", ",", "11", ",", "True", ")", ":", "yield", "hit"], "docstring": "parse b6 output", "docstring_tokens": ["parse", "b6", "output"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L52-L85", "partition": "train", "up_fun_num": 2, "context": "#!/usr/bin/env python3\n\n\"\"\"\nfilter specified number of hits from a blast or HMM search\nthat pass evalue and bit score thresholds\n\nnote: file must be sorted by query ID (and domain # for domtblout),\nbut does not have to be sorted by significance\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport pandas as pd\nfrom operator import itemgetter\n\n\ndef top_hits(hits, num, column, reverse):\n    \"\"\"\n    get top hits after sorting by column number\n    \"\"\"\n    hits.sort(key=itemgetter(column), reverse=reverse)\n    for hit in hits[0:num]:\n        yield hit\n\n\ndef numBlast_sort(blast, numHits, evalueT, bitT):\n    \"\"\"\n    parse b6 output with sorting\n    \"\"\"\n    header = [\n        \"#query\",\n        \"target\",\n        \"pident\",\n        \"alen\",\n        \"mismatch\",\n        \"gapopen\",\n        \"qstart\",\n        \"qend\",\n        \"tstart\",\n        \"tend\",\n        \"evalue\",\n        \"bitscore\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in blast:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split(\"\\t\")\n        # Evalue and Bitscore thresholds\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#query\"]):\n        df = df.sort_values(by=[\"bitscore\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm domain table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bit = line[11], line[13]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:22], \" \".join(line[22:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\", \"domain #\"]):\n        df = df.sort_values(by=[\"domain score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm domain table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # create ID based on query name and domain number\n        ID = line[0] + line[9]\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bitscore = line[11], line[13]\n        line[11], line[13] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 13, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 13, True):\n        yield hit\n\n\ndef numTblout_sort(tblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm hmm table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain full Evalue and full score thresholds\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bit = line[4], line[5]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:18], \" \".join(line[18:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\"]):\n        df = df.sort_values(by=[\"full score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numTblout(tblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numTblout_sort(tblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # ID and scores\n        ID = line[0]\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bitscore = line[4], line[5]\n        line[4], line[5] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 5, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 5, True):\n        yield hit\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# filter blast or HMM tab output\")\n    parser.add_argument(\n        \"-i\",\n        default=\"-\",\n        help=\"path to search results (sorted by query; default = stdin)\",\n    )\n    parser.add_argument(\"-n\", default=1, type=int, help=\"number of hits (default = 1)\")\n    parser.add_argument(\n        \"-e\", default=False, type=float, help=\"e-value threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-b\", default=False, type=float, help=\"bit score threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-f\",\n        default=\"b6\",\n        type=str,\n        help=\"format (default = b6, options: b6, domtblout, tblout)\",\n    )\n    parser.add_argument(\n        \"--sort\",\n        action=\"store_true\",\n        help=\"sort hits by query name (evalues are sorted by default)\",\n    )\n    args = vars(parser.parse_args())\n    # check if file is from stdin\n    if args[\"i\"] == \"-\":\n        args[\"i\"] = sys.stdin\n    else:\n        args[\"i\"] = open(args[\"i\"])\n    if args[\"f\"] == \"b6\":\n        for hit in numBlast(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"domtblout\":\n        for hit in numDomtblout(\n            args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]\n        ):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"tblout\":\n        for hit in numTblout(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    else:\n        print(\"unsupported format:\", args[\"f\"])\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import pandas as pd", "from operator import itemgetter"], "function": ["def top_hits(hits, num, column, reverse):\n", "def numBlast_sort(blast, numHits, evalueT, bitT):\n", "def numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n", "def numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n", "def numTblout_sort(tblout, numHits, evalueT, bitT):\n", "def numTblout(tblout, numHits, evalueT, bitT, sort):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/besthits.py", "func_name": "numDomtblout", "original_string": "def numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm domain table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = ['#target name', 'target accession', 'tlen',\n              'query name', 'query accession', 'qlen',\n              'full E-value', 'full score', 'full bias',\n              'domain #', '# domains',\n              'domain c-Evalue', 'domain i-Evalue', 'domain score', 'domain bias',\n              'hmm from', 'hmm to', 'seq from', 'seq to', 'env from', 'env to',\n              'acc', 'target description']\n    yield header\n    prev, hits = None, []\n    for line in domtblout:\n        if line.startswith('#'):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = ' '.join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # create ID based on query name and domain number\n        ID = line[0] + line[9]\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bitscore = line[11], line[13]\n        line[11], line[13] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 13, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 13, True):\n        yield hit", "language": "python", "code": "def numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm domain table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = ['#target name', 'target accession', 'tlen',\n              'query name', 'query accession', 'qlen',\n              'full E-value', 'full score', 'full bias',\n              'domain #', '# domains',\n              'domain c-Evalue', 'domain i-Evalue', 'domain score', 'domain bias',\n              'hmm from', 'hmm to', 'seq from', 'seq to', 'env from', 'env to',\n              'acc', 'target description']\n    yield header\n    prev, hits = None, []\n    for line in domtblout:\n        if line.startswith('#'):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = ' '.join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # create ID based on query name and domain number\n        ID = line[0] + line[9]\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bitscore = line[11], line[13]\n        line[11], line[13] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 13, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 13, True):\n        yield hit", "code_tokens": ["def", "numDomtblout", "(", "domtblout", ",", "numHits", ",", "evalueT", ",", "bitT", ",", "sort", ")", ":", "if", "sort", "is", "True", ":", "for", "hit", "in", "numDomtblout_sort", "(", "domtblout", ",", "numHits", ",", "evalueT", ",", "bitT", ")", ":", "yield", "hit", "return", "header", "=", "[", "'#target name'", ",", "'target accession'", ",", "'tlen'", ",", "'query name'", ",", "'query accession'", ",", "'qlen'", ",", "'full E-value'", ",", "'full score'", ",", "'full bias'", ",", "'domain #'", ",", "'# domains'", ",", "'domain c-Evalue'", ",", "'domain i-Evalue'", ",", "'domain score'", ",", "'domain bias'", ",", "'hmm from'", ",", "'hmm to'", ",", "'seq from'", ",", "'seq to'", ",", "'env from'", ",", "'env to'", ",", "'acc'", ",", "'target description'", "]", "yield", "header", "prev", ",", "hits", "=", "None", ",", "[", "]", "for", "line", "in", "domtblout", ":", "if", "line", ".", "startswith", "(", "'#'", ")", ":", "continue", "# parse line and get description", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "desc", "=", "' '", ".", "join", "(", "line", "[", "18", ":", "]", ")", "line", "=", "line", "[", "0", ":", "18", "]", "line", ".", "append", "(", "desc", ")", "# create ID based on query name and domain number", "ID", "=", "line", "[", "0", "]", "+", "line", "[", "9", "]", "# domain c-Evalue and domain score thresholds", "line", "[", "11", "]", ",", "line", "[", "13", "]", "=", "float", "(", "line", "[", "11", "]", ")", ",", "float", "(", "line", "[", "13", "]", ")", "evalue", ",", "bitscore", "=", "line", "[", "11", "]", ",", "line", "[", "13", "]", "line", "[", "11", "]", ",", "line", "[", "13", "]", "=", "evalue", ",", "bitscore", "if", "ID", "!=", "prev", ":", "if", "len", "(", "hits", ")", ">", "0", ":", "for", "hit", "in", "top_hits", "(", "hits", ",", "numHits", ",", "13", ",", "True", ")", ":", "yield", "hit", "hits", "=", "[", "]", "if", "evalueT", "==", "False", "and", "bitT", "==", "False", ":", "hits", ".", "append", "(", "line", ")", "elif", "evalue", "<=", "evalueT", "and", "bitT", "==", "False", ":", "hits", ".", "append", "(", "line", ")", "elif", "evalue", "<=", "evalueT", "and", "bit", ">=", "bitT", ":", "hits", ".", "append", "(", "line", ")", "elif", "evalueT", "==", "False", "and", "bit", ">=", "bitT", ":", "hits", ".", "append", "(", "line", ")", "prev", "=", "ID", "for", "hit", "in", "top_hits", "(", "hits", ",", "numHits", ",", "13", ",", "True", ")", ":", "yield", "hit"], "docstring": "parse hmm domain table output\n    this version is faster but does not work unless the table is sorted", "docstring_tokens": ["parse", "hmm", "domain", "table", "output", "this", "version", "is", "faster", "but", "does", "not", "work", "unless", "the", "table", "is", "sorted"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/besthits.py#L121-L168", "partition": "train", "up_fun_num": 4, "context": "#!/usr/bin/env python3\n\n\"\"\"\nfilter specified number of hits from a blast or HMM search\nthat pass evalue and bit score thresholds\n\nnote: file must be sorted by query ID (and domain # for domtblout),\nbut does not have to be sorted by significance\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport pandas as pd\nfrom operator import itemgetter\n\n\ndef top_hits(hits, num, column, reverse):\n    \"\"\"\n    get top hits after sorting by column number\n    \"\"\"\n    hits.sort(key=itemgetter(column), reverse=reverse)\n    for hit in hits[0:num]:\n        yield hit\n\n\ndef numBlast_sort(blast, numHits, evalueT, bitT):\n    \"\"\"\n    parse b6 output with sorting\n    \"\"\"\n    header = [\n        \"#query\",\n        \"target\",\n        \"pident\",\n        \"alen\",\n        \"mismatch\",\n        \"gapopen\",\n        \"qstart\",\n        \"qend\",\n        \"tstart\",\n        \"tend\",\n        \"evalue\",\n        \"bitscore\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in blast:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split(\"\\t\")\n        # Evalue and Bitscore thresholds\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#query\"]):\n        df = df.sort_values(by=[\"bitscore\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numBlast(blast, numHits, evalueT=False, bitT=False, sort=False):\n    \"\"\"\n    parse b6 output\n    \"\"\"\n    if sort is True:\n        for hit in numBlast_sort(blast, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#query\",\n        \"target\",\n        \"pident\",\n        \"alen\",\n        \"mismatch\",\n        \"gapopen\",\n        \"qstart\",\n        \"qend\",\n        \"tstart\",\n        \"tend\",\n        \"evalue\",\n        \"bitscore\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in blast:\n        line = line.strip().split(\"\\t\")\n        ID = line[0]\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if ID != prev:\n            if len(hits) > 0:\n                # column is 1 + line index\n                for hit in top_hits(hits, numHits, 11, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 11, True):\n        yield hit\n\n\ndef numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm domain table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"tlen\",\n        \"query name\",\n        \"query accession\",\n        \"qlen\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"domain #\",\n        \"# domains\",\n        \"domain c-Evalue\",\n        \"domain i-Evalue\",\n        \"domain score\",\n        \"domain bias\",\n        \"hmm from\",\n        \"hmm to\",\n        \"seq from\",\n        \"seq to\",\n        \"env from\",\n        \"env to\",\n        \"acc\",\n        \"target description\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in domtblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bit = line[11], line[13]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:22], \" \".join(line[22:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\", \"domain #\"]):\n        df = df.sort_values(by=[\"domain score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numTblout_sort(tblout, numHits, evalueT, bitT):\n    \"\"\"\n    parse hmm hmm table output with sorting\n    \"\"\"\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    hmm = {h: [] for h in header}\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        line = line.strip().split()\n        # domain full Evalue and full score thresholds\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bit = line[4], line[5]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        line, desc = line[0:18], \" \".join(line[18:])\n        line.append(desc)\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by=[\"#target name\"]):\n        df = df.sort_values(by=[\"full score\"], ascending=False)\n        for hit in df[header].values[0:numHits]:\n            yield hit\n\n\ndef numTblout(tblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numTblout_sort(tblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = [\n        \"#target name\",\n        \"target accession\",\n        \"query name\",\n        \"query accession\",\n        \"full E-value\",\n        \"full score\",\n        \"full bias\",\n        \"best E-value\",\n        \"best score\",\n        \"best bias\",\n        \"exp\",\n        \"reg\",\n        \"clu\",\n        \"ov\",\n        \"env\",\n        \"dom\",\n        \"rep\",\n        \"inc\",\n        \"description of target\",\n    ]\n    yield header\n    prev, hits = None, []\n    for line in tblout:\n        if line.startswith(\"#\"):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = \" \".join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # ID and scores\n        ID = line[0]\n        line[4], line[5] = float(line[4]), float(line[5])\n        evalue, bitscore = line[4], line[5]\n        line[4], line[5] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 5, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 5, True):\n        yield hit\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"# filter blast or HMM tab output\")\n    parser.add_argument(\n        \"-i\",\n        default=\"-\",\n        help=\"path to search results (sorted by query; default = stdin)\",\n    )\n    parser.add_argument(\"-n\", default=1, type=int, help=\"number of hits (default = 1)\")\n    parser.add_argument(\n        \"-e\", default=False, type=float, help=\"e-value threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-b\", default=False, type=float, help=\"bit score threshold (default = None)\"\n    )\n    parser.add_argument(\n        \"-f\",\n        default=\"b6\",\n        type=str,\n        help=\"format (default = b6, options: b6, domtblout, tblout)\",\n    )\n    parser.add_argument(\n        \"--sort\",\n        action=\"store_true\",\n        help=\"sort hits by query name (evalues are sorted by default)\",\n    )\n    args = vars(parser.parse_args())\n    # check if file is from stdin\n    if args[\"i\"] == \"-\":\n        args[\"i\"] = sys.stdin\n    else:\n        args[\"i\"] = open(args[\"i\"])\n    if args[\"f\"] == \"b6\":\n        for hit in numBlast(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"domtblout\":\n        for hit in numDomtblout(\n            args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]\n        ):\n            print(\"\\t\".join([str(i) for i in hit]))\n    elif args[\"f\"] == \"tblout\":\n        for hit in numTblout(args[\"i\"], args[\"n\"], args[\"e\"], args[\"b\"], args[\"sort\"]):\n            print(\"\\t\".join([str(i) for i in hit]))\n    else:\n        print(\"unsupported format:\", args[\"f\"])\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import pandas as pd", "from operator import itemgetter"], "function": ["def top_hits(hits, num, column, reverse):\n", "def numBlast_sort(blast, numHits, evalueT, bitT):\n", "def numBlast(blast, numHits, evalueT=False, bitT=False, sort=False):\n", "def numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n", "def numTblout_sort(tblout, numHits, evalueT, bitT):\n", "def numTblout(tblout, numHits, evalueT, bitT, sort):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/stockholm2fa.py", "func_name": "stock2fa", "original_string": "def stock2fa(stock):\n    \"\"\"\n    convert stockholm to fasta\n    \"\"\"\n    seqs = {}\n    for line in stock:\n        if line.startswith('#') is False and line.startswith(' ') is False and len(line) > 3:\n            id, seq = line.strip().split()\n            id = id.rsplit('/', 1)[0]\n            id = re.split('[0-9]\\|', id, 1)[-1]\n            if id not in seqs:\n                seqs[id] = []\n            seqs[id].append(seq)\n        if line.startswith('//'):\n            break\n    return seqs", "language": "python", "code": "def stock2fa(stock):\n    \"\"\"\n    convert stockholm to fasta\n    \"\"\"\n    seqs = {}\n    for line in stock:\n        if line.startswith('#') is False and line.startswith(' ') is False and len(line) > 3:\n            id, seq = line.strip().split()\n            id = id.rsplit('/', 1)[0]\n            id = re.split('[0-9]\\|', id, 1)[-1]\n            if id not in seqs:\n                seqs[id] = []\n            seqs[id].append(seq)\n        if line.startswith('//'):\n            break\n    return seqs", "code_tokens": ["def", "stock2fa", "(", "stock", ")", ":", "seqs", "=", "{", "}", "for", "line", "in", "stock", ":", "if", "line", ".", "startswith", "(", "'#'", ")", "is", "False", "and", "line", ".", "startswith", "(", "' '", ")", "is", "False", "and", "len", "(", "line", ")", ">", "3", ":", "id", ",", "seq", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "id", "=", "id", ".", "rsplit", "(", "'/'", ",", "1", ")", "[", "0", "]", "id", "=", "re", ".", "split", "(", "'[0-9]\\|'", ",", "id", ",", "1", ")", "[", "-", "1", "]", "if", "id", "not", "in", "seqs", ":", "seqs", "[", "id", "]", "=", "[", "]", "seqs", "[", "id", "]", ".", "append", "(", "seq", ")", "if", "line", ".", "startswith", "(", "'//'", ")", ":", "break", "return", "seqs"], "docstring": "convert stockholm to fasta", "docstring_tokens": ["convert", "stockholm", "to", "fasta"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/stockholm2fa.py#L11-L26", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for converting a stockholm formatted alignment to fasta\n\"\"\"\n\nimport os\nimport re\nimport sys\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"specify stockholm formatted alignment\")\n        exit()\n    stock = sys.argv[1]\n    if stock == \"-\":\n        stock = sys.stdin\n    else:\n        stock = open(stock)\n    for id, seq in list(stock2fa(stock).items()):\n        print(\"\\n\".join([\">%s\" % (id), \"\".join(seq)]))\n", "levels": [], "package": ["import os", "import re", "import sys"], "function": []}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/utils.py", "func_name": "week_schedule", "original_string": "def week_schedule(index, on_time=None, off_time=None, off_days=None):\n    \"\"\" Return boolean time series following given week schedule.\n\n    Parameters\n    ----------\n    index : pandas.DatetimeIndex\n        Datetime index\n    on_time : str or datetime.time\n        Daily opening time. Default: '09:00'\n    off_time : str or datetime.time\n        Daily closing time. Default: '17:00'\n    off_days : list of str\n        List of weekdays. Default: ['Sunday', 'Monday']\n\n    Returns\n    -------\n    pandas.Series of bool\n        True when on, False otherwise for given datetime index\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from opengrid.library.utils import week_schedule\n    >>> index = pd.date_range('20170701', '20170710', freq='H')\n    >>> week_schedule(index)\n    \"\"\"\n    if on_time is None:\n        on_time = '9:00'\n    if off_time is None:\n        off_time = '17:00'\n    if off_days is None:\n        off_days = ['Sunday', 'Monday']\n    if not isinstance(on_time, datetime.time):\n        on_time = pd.to_datetime(on_time, format='%H:%M').time()\n    if not isinstance(off_time, datetime.time):\n        off_time = pd.to_datetime(off_time, format='%H:%M').time()\n    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))\n    return pd.Series(times, index=index)", "language": "python", "code": "def week_schedule(index, on_time=None, off_time=None, off_days=None):\n    \"\"\" Return boolean time series following given week schedule.\n\n    Parameters\n    ----------\n    index : pandas.DatetimeIndex\n        Datetime index\n    on_time : str or datetime.time\n        Daily opening time. Default: '09:00'\n    off_time : str or datetime.time\n        Daily closing time. Default: '17:00'\n    off_days : list of str\n        List of weekdays. Default: ['Sunday', 'Monday']\n\n    Returns\n    -------\n    pandas.Series of bool\n        True when on, False otherwise for given datetime index\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from opengrid.library.utils import week_schedule\n    >>> index = pd.date_range('20170701', '20170710', freq='H')\n    >>> week_schedule(index)\n    \"\"\"\n    if on_time is None:\n        on_time = '9:00'\n    if off_time is None:\n        off_time = '17:00'\n    if off_days is None:\n        off_days = ['Sunday', 'Monday']\n    if not isinstance(on_time, datetime.time):\n        on_time = pd.to_datetime(on_time, format='%H:%M').time()\n    if not isinstance(off_time, datetime.time):\n        off_time = pd.to_datetime(off_time, format='%H:%M').time()\n    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))\n    return pd.Series(times, index=index)", "code_tokens": ["def", "week_schedule", "(", "index", ",", "on_time", "=", "None", ",", "off_time", "=", "None", ",", "off_days", "=", "None", ")", ":", "if", "on_time", "is", "None", ":", "on_time", "=", "'9:00'", "if", "off_time", "is", "None", ":", "off_time", "=", "'17:00'", "if", "off_days", "is", "None", ":", "off_days", "=", "[", "'Sunday'", ",", "'Monday'", "]", "if", "not", "isinstance", "(", "on_time", ",", "datetime", ".", "time", ")", ":", "on_time", "=", "pd", ".", "to_datetime", "(", "on_time", ",", "format", "=", "'%H:%M'", ")", ".", "time", "(", ")", "if", "not", "isinstance", "(", "off_time", ",", "datetime", ".", "time", ")", ":", "off_time", "=", "pd", ".", "to_datetime", "(", "off_time", ",", "format", "=", "'%H:%M'", ")", ".", "time", "(", ")", "times", "=", "(", "index", ".", "time", ">=", "on_time", ")", "&", "(", "index", ".", "time", "<", "off_time", ")", "&", "(", "~", "index", ".", "weekday_name", ".", "isin", "(", "off_days", ")", ")", "return", "pd", ".", "Series", "(", "times", ",", "index", "=", "index", ")"], "docstring": "Return boolean time series following given week schedule.\n\n    Parameters\n    ----------\n    index : pandas.DatetimeIndex\n        Datetime index\n    on_time : str or datetime.time\n        Daily opening time. Default: '09:00'\n    off_time : str or datetime.time\n        Daily closing time. Default: '17:00'\n    off_days : list of str\n        List of weekdays. Default: ['Sunday', 'Monday']\n\n    Returns\n    -------\n    pandas.Series of bool\n        True when on, False otherwise for given datetime index\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from opengrid.library.utils import week_schedule\n    >>> index = pd.date_range('20170701', '20170710', freq='H')\n    >>> week_schedule(index)", "docstring_tokens": ["Return", "boolean", "time", "series", "following", "given", "week", "schedule", "."], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/utils.py#L10-L47", "partition": "train", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n\"\"\"\nGeneral util functions\n\n\"\"\"\nimport datetime\nimport pandas as pd\n", "levels": [], "package": ["import datetime", "import pandas as pd"], "function": []}
{"repo": "opengridcc/opengrid", "path": "opengrid/library/plotting.py", "func_name": "carpet", "original_string": "def carpet(timeseries, **kwargs):\n    \"\"\"\n    Draw a carpet plot of a pandas timeseries.\n\n    The carpet plot reads like a letter. Every day one line is added to the\n    bottom of the figure, minute for minute moving from left (morning) to right\n    (evening).\n    The color denotes the level of consumption and is scaled logarithmically.\n    If vmin and vmax are not provided as inputs, the minimum and maximum of the\n    colorbar represent the minimum and maximum of the (resampled) timeseries.\n\n    Parameters\n    ----------\n    timeseries : pandas.Series\n    vmin, vmax : If not None, either or both of these values determine the range\n    of the z axis. If None, the range is given by the minimum and/or maximum\n    of the (resampled) timeseries.\n    zlabel, title : If not None, these determine the labels of z axis and/or\n    title. If None, the name of the timeseries is used if defined.\n    cmap : matplotlib.cm instance, default coolwarm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pandas as pd\n    >>> from opengrid.library import plotting\n    >>> plt = plotting.plot_style()\n    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')\n    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')\n    >>> im = plotting.carpet(ser)\n    \"\"\"\n\n    # define optional input parameters\n    cmap = kwargs.pop('cmap', cm.coolwarm)\n    norm = kwargs.pop('norm', LogNorm())\n    interpolation = kwargs.pop('interpolation', 'nearest')\n    cblabel = kwargs.pop('zlabel', timeseries.name if timeseries.name else '')\n    title = kwargs.pop('title', 'carpet plot: ' + timeseries.name if timeseries.name else '')\n\n    # data preparation\n    if timeseries.dropna().empty:\n        print('skipped {} - no data'.format(title))\n        return\n    ts = timeseries.resample('15min').interpolate()\n    vmin = max(0.1, kwargs.pop('vmin', ts[ts > 0].min()))\n    vmax = max(vmin, kwargs.pop('vmax', ts.quantile(.999)))\n\n    # convert to dataframe with date as index and time as columns by\n    # first replacing the index by a MultiIndex\n    mpldatetimes = date2num(ts.index.to_pydatetime())\n    ts.index = pd.MultiIndex.from_arrays(\n        [np.floor(mpldatetimes), 2 + mpldatetimes % 1])  # '2 +': matplotlib bug workaround.\n    # and then unstacking the second index level to columns\n    df = ts.unstack()\n\n    # data plotting\n\n    fig, ax = plt.subplots()\n    # define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks)\n    extent = [df.columns[0], df.columns[-1], df.index[-1] + 0.5, df.index[0] - 0.5]\n    im = plt.imshow(df, vmin=vmin, vmax=vmax, extent=extent, cmap=cmap, aspect='auto', norm=norm,\n                    interpolation=interpolation, **kwargs)\n\n    # figure formatting\n\n    # x axis\n    ax.xaxis_date()\n    ax.xaxis.set_major_locator(HourLocator(interval=2))\n    ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))\n    ax.xaxis.grid(True)\n    plt.xlabel('UTC Time')\n\n    # y axis\n    ax.yaxis_date()\n    dmin, dmax = ax.yaxis.get_data_interval()\n    number_of_days = (num2date(dmax) - num2date(dmin)).days\n    # AutoDateLocator is not suited in case few data is available\n    if abs(number_of_days) <= 35:\n        ax.yaxis.set_major_locator(DayLocator())\n    else:\n        ax.yaxis.set_major_locator(AutoDateLocator())\n    ax.yaxis.set_major_formatter(DateFormatter(\"%a, %d %b %Y\"))\n\n    # plot colorbar\n    cbticks = np.logspace(np.log10(vmin), np.log10(vmax), 11, endpoint=True)\n    cb = plt.colorbar(format='%.0f', ticks=cbticks)\n    cb.set_label(cblabel)\n\n    # plot title\n    plt.title(title)\n\n    return im", "language": "python", "code": "def carpet(timeseries, **kwargs):\n    \"\"\"\n    Draw a carpet plot of a pandas timeseries.\n\n    The carpet plot reads like a letter. Every day one line is added to the\n    bottom of the figure, minute for minute moving from left (morning) to right\n    (evening).\n    The color denotes the level of consumption and is scaled logarithmically.\n    If vmin and vmax are not provided as inputs, the minimum and maximum of the\n    colorbar represent the minimum and maximum of the (resampled) timeseries.\n\n    Parameters\n    ----------\n    timeseries : pandas.Series\n    vmin, vmax : If not None, either or both of these values determine the range\n    of the z axis. If None, the range is given by the minimum and/or maximum\n    of the (resampled) timeseries.\n    zlabel, title : If not None, these determine the labels of z axis and/or\n    title. If None, the name of the timeseries is used if defined.\n    cmap : matplotlib.cm instance, default coolwarm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pandas as pd\n    >>> from opengrid.library import plotting\n    >>> plt = plotting.plot_style()\n    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')\n    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')\n    >>> im = plotting.carpet(ser)\n    \"\"\"\n\n    # define optional input parameters\n    cmap = kwargs.pop('cmap', cm.coolwarm)\n    norm = kwargs.pop('norm', LogNorm())\n    interpolation = kwargs.pop('interpolation', 'nearest')\n    cblabel = kwargs.pop('zlabel', timeseries.name if timeseries.name else '')\n    title = kwargs.pop('title', 'carpet plot: ' + timeseries.name if timeseries.name else '')\n\n    # data preparation\n    if timeseries.dropna().empty:\n        print('skipped {} - no data'.format(title))\n        return\n    ts = timeseries.resample('15min').interpolate()\n    vmin = max(0.1, kwargs.pop('vmin', ts[ts > 0].min()))\n    vmax = max(vmin, kwargs.pop('vmax', ts.quantile(.999)))\n\n    # convert to dataframe with date as index and time as columns by\n    # first replacing the index by a MultiIndex\n    mpldatetimes = date2num(ts.index.to_pydatetime())\n    ts.index = pd.MultiIndex.from_arrays(\n        [np.floor(mpldatetimes), 2 + mpldatetimes % 1])  # '2 +': matplotlib bug workaround.\n    # and then unstacking the second index level to columns\n    df = ts.unstack()\n\n    # data plotting\n\n    fig, ax = plt.subplots()\n    # define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks)\n    extent = [df.columns[0], df.columns[-1], df.index[-1] + 0.5, df.index[0] - 0.5]\n    im = plt.imshow(df, vmin=vmin, vmax=vmax, extent=extent, cmap=cmap, aspect='auto', norm=norm,\n                    interpolation=interpolation, **kwargs)\n\n    # figure formatting\n\n    # x axis\n    ax.xaxis_date()\n    ax.xaxis.set_major_locator(HourLocator(interval=2))\n    ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))\n    ax.xaxis.grid(True)\n    plt.xlabel('UTC Time')\n\n    # y axis\n    ax.yaxis_date()\n    dmin, dmax = ax.yaxis.get_data_interval()\n    number_of_days = (num2date(dmax) - num2date(dmin)).days\n    # AutoDateLocator is not suited in case few data is available\n    if abs(number_of_days) <= 35:\n        ax.yaxis.set_major_locator(DayLocator())\n    else:\n        ax.yaxis.set_major_locator(AutoDateLocator())\n    ax.yaxis.set_major_formatter(DateFormatter(\"%a, %d %b %Y\"))\n\n    # plot colorbar\n    cbticks = np.logspace(np.log10(vmin), np.log10(vmax), 11, endpoint=True)\n    cb = plt.colorbar(format='%.0f', ticks=cbticks)\n    cb.set_label(cblabel)\n\n    # plot title\n    plt.title(title)\n\n    return im", "code_tokens": ["def", "carpet", "(", "timeseries", ",", "*", "*", "kwargs", ")", ":", "# define optional input parameters", "cmap", "=", "kwargs", ".", "pop", "(", "'cmap'", ",", "cm", ".", "coolwarm", ")", "norm", "=", "kwargs", ".", "pop", "(", "'norm'", ",", "LogNorm", "(", ")", ")", "interpolation", "=", "kwargs", ".", "pop", "(", "'interpolation'", ",", "'nearest'", ")", "cblabel", "=", "kwargs", ".", "pop", "(", "'zlabel'", ",", "timeseries", ".", "name", "if", "timeseries", ".", "name", "else", "''", ")", "title", "=", "kwargs", ".", "pop", "(", "'title'", ",", "'carpet plot: '", "+", "timeseries", ".", "name", "if", "timeseries", ".", "name", "else", "''", ")", "# data preparation", "if", "timeseries", ".", "dropna", "(", ")", ".", "empty", ":", "print", "(", "'skipped {} - no data'", ".", "format", "(", "title", ")", ")", "return", "ts", "=", "timeseries", ".", "resample", "(", "'15min'", ")", ".", "interpolate", "(", ")", "vmin", "=", "max", "(", "0.1", ",", "kwargs", ".", "pop", "(", "'vmin'", ",", "ts", "[", "ts", ">", "0", "]", ".", "min", "(", ")", ")", ")", "vmax", "=", "max", "(", "vmin", ",", "kwargs", ".", "pop", "(", "'vmax'", ",", "ts", ".", "quantile", "(", ".999", ")", ")", ")", "# convert to dataframe with date as index and time as columns by", "# first replacing the index by a MultiIndex", "mpldatetimes", "=", "date2num", "(", "ts", ".", "index", ".", "to_pydatetime", "(", ")", ")", "ts", ".", "index", "=", "pd", ".", "MultiIndex", ".", "from_arrays", "(", "[", "np", ".", "floor", "(", "mpldatetimes", ")", ",", "2", "+", "mpldatetimes", "%", "1", "]", ")", "# '2 +': matplotlib bug workaround.", "# and then unstacking the second index level to columns", "df", "=", "ts", ".", "unstack", "(", ")", "# data plotting", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "# define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks)", "extent", "=", "[", "df", ".", "columns", "[", "0", "]", ",", "df", ".", "columns", "[", "-", "1", "]", ",", "df", ".", "index", "[", "-", "1", "]", "+", "0.5", ",", "df", ".", "index", "[", "0", "]", "-", "0.5", "]", "im", "=", "plt", ".", "imshow", "(", "df", ",", "vmin", "=", "vmin", ",", "vmax", "=", "vmax", ",", "extent", "=", "extent", ",", "cmap", "=", "cmap", ",", "aspect", "=", "'auto'", ",", "norm", "=", "norm", ",", "interpolation", "=", "interpolation", ",", "*", "*", "kwargs", ")", "# figure formatting", "# x axis", "ax", ".", "xaxis_date", "(", ")", "ax", ".", "xaxis", ".", "set_major_locator", "(", "HourLocator", "(", "interval", "=", "2", ")", ")", "ax", ".", "xaxis", ".", "set_major_formatter", "(", "DateFormatter", "(", "'%H:%M'", ")", ")", "ax", ".", "xaxis", ".", "grid", "(", "True", ")", "plt", ".", "xlabel", "(", "'UTC Time'", ")", "# y axis", "ax", ".", "yaxis_date", "(", ")", "dmin", ",", "dmax", "=", "ax", ".", "yaxis", ".", "get_data_interval", "(", ")", "number_of_days", "=", "(", "num2date", "(", "dmax", ")", "-", "num2date", "(", "dmin", ")", ")", ".", "days", "# AutoDateLocator is not suited in case few data is available", "if", "abs", "(", "number_of_days", ")", "<=", "35", ":", "ax", ".", "yaxis", ".", "set_major_locator", "(", "DayLocator", "(", ")", ")", "else", ":", "ax", ".", "yaxis", ".", "set_major_locator", "(", "AutoDateLocator", "(", ")", ")", "ax", ".", "yaxis", ".", "set_major_formatter", "(", "DateFormatter", "(", "\"%a, %d %b %Y\"", ")", ")", "# plot colorbar", "cbticks", "=", "np", ".", "logspace", "(", "np", ".", "log10", "(", "vmin", ")", ",", "np", ".", "log10", "(", "vmax", ")", ",", "11", ",", "endpoint", "=", "True", ")", "cb", "=", "plt", ".", "colorbar", "(", "format", "=", "'%.0f'", ",", "ticks", "=", "cbticks", ")", "cb", ".", "set_label", "(", "cblabel", ")", "# plot title", "plt", ".", "title", "(", "title", ")", "return", "im"], "docstring": "Draw a carpet plot of a pandas timeseries.\n\n    The carpet plot reads like a letter. Every day one line is added to the\n    bottom of the figure, minute for minute moving from left (morning) to right\n    (evening).\n    The color denotes the level of consumption and is scaled logarithmically.\n    If vmin and vmax are not provided as inputs, the minimum and maximum of the\n    colorbar represent the minimum and maximum of the (resampled) timeseries.\n\n    Parameters\n    ----------\n    timeseries : pandas.Series\n    vmin, vmax : If not None, either or both of these values determine the range\n    of the z axis. If None, the range is given by the minimum and/or maximum\n    of the (resampled) timeseries.\n    zlabel, title : If not None, these determine the labels of z axis and/or\n    title. If None, the name of the timeseries is used if defined.\n    cmap : matplotlib.cm instance, default coolwarm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pandas as pd\n    >>> from opengrid.library import plotting\n    >>> plt = plotting.plot_style()\n    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')\n    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')\n    >>> im = plotting.carpet(ser)", "docstring_tokens": ["Draw", "a", "carpet", "plot", "of", "a", "pandas", "timeseries", "."], "sha": "69b8da3c8fcea9300226c45ef0628cd6d4307651", "url": "https://github.com/opengridcc/opengrid/blob/69b8da3c8fcea9300226c45ef0628cd6d4307651/opengrid/library/plotting.py#L34-L125", "partition": "train", "up_fun_num": 1, "context": "import os\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import (\n    date2num,\n    num2date,\n    HourLocator,\n    DayLocator,\n    AutoDateLocator,\n    DateFormatter,\n)\nfrom matplotlib.colors import LogNorm\n\n\ndef plot_style():\n    # matplotlib inline, only when jupyter notebook\n    # try-except causes problems in Pycharm Console\n    if \"JPY_PARENT_PID\" in os.environ:\n        get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n\n    matplotlib.style.use(\"seaborn-talk\")\n    matplotlib.style.use(\"seaborn-whitegrid\")\n    matplotlib.style.use(\"seaborn-deep\")\n\n    plt.rcParams[\"figure.figsize\"] = 16, 6\n\n    # To overrule the legend style\n    plt.rcParams[\"legend.facecolor\"] = \"#ffffff\"\n    plt.rcParams[\"legend.frameon\"] = True\n    plt.rcParams[\"legend.framealpha\"] = 1\n\n    return plt\n\n\ndef boxplot(df, plot_mean=False, plot_ids=None, title=None, xlabel=None, ylabel=None):\n    \"\"\"\n    Plot boxplots\n\n    Plot the boxplots of a dataframe in time\n\n    Parameters\n    ----------\n    df: Pandas Dataframe\n        Every collumn is a timeseries\n    plot_mean: bool\n        Wether or not to plot the means\n    plot_ids: [str]\n        List of id's to plot\n\n    Returns\n    -------\n    matplotlib figure\n    \"\"\"\n\n    df = df.applymap(float)\n    description = df.apply(pd.DataFrame.describe, axis=1)\n\n    # plot\n    plt = plot_style()\n\n    plt.boxplot(df)\n    # plt.setp(bp['boxes'], color='black')\n    # plt.setp(bp['whiskers'], color='black')\n    if plot_ids is not None:\n        for id in plot_ids:\n            if id in df.columns:\n                plt.scatter(x=range(1, len(df) + 1), y=df[id], label=str(id))\n\n    if plot_mean:\n        plt.scatter(\n            x=range(1, len(df) + 1),\n            y=description[\"mean\"],\n            label=\"Mean\",\n            color=\"k\",\n            s=30,\n            marker=\"+\",\n        )\n\n    ax = plt.gca()\n    ax.set_xticklabels(df.index)\n    # plt.xticks(rotation=45)\n\n    plt.legend()\n    if title is not None:\n        plt.title(title)\n    if xlabel is not None:\n        plt.xlabel(xlabel)\n    if ylabel is not None:\n        plt.ylabel(ylabel)\n\n    return plt.gcf()\n", "levels": [0, 0], "package": ["import os", "import os", "import numpy as np", "import pandas as pd", "import matplotlib", "import pandas as pd", "import numpy as np", "import matplotlib.cm as cm", "import matplotlib.pyplot as plt", "from matplotlib.dates import (", "from matplotlib.colors import LogNorm"], "function": ["def plot_style():\n", "def boxplot(df, plot_mean=False, plot_ids=None, title=None, xlabel=None, ylabel=None):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "calc_pident_ignore_gaps", "original_string": "def calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0 # matches\n    mm = 0 # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == '-' or A == '.' or B == '-' or B == '.':\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m)/float((m + mm))) * 100\n    except:\n        return 0", "language": "python", "code": "def calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0 # matches\n    mm = 0 # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == '-' or A == '.' or B == '-' or B == '.':\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m)/float((m + mm))) * 100\n    except:\n        return 0", "code_tokens": ["def", "calc_pident_ignore_gaps", "(", "a", ",", "b", ")", ":", "m", "=", "0", "# matches", "mm", "=", "0", "# mismatches", "for", "A", ",", "B", "in", "zip", "(", "list", "(", "a", ")", ",", "list", "(", "b", ")", ")", ":", "if", "A", "==", "'-'", "or", "A", "==", "'.'", "or", "B", "==", "'-'", "or", "B", "==", "'.'", ":", "continue", "if", "A", "==", "B", ":", "m", "+=", "1", "else", ":", "mm", "+=", "1", "try", ":", "return", "float", "(", "float", "(", "m", ")", "/", "float", "(", "(", "m", "+", "mm", ")", ")", ")", "*", "100", "except", ":", "return", "0"], "docstring": "calculate percent identity", "docstring_tokens": ["calculate", "percent", "identity"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L34-L50", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "remove_gaps", "original_string": "def remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == '-' or a == '.' or b == '-' or b == '.':\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return ''.join(a_seq), ''.join(b_seq)", "language": "python", "code": "def remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == '-' or a == '.' or b == '-' or b == '.':\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return ''.join(a_seq), ''.join(b_seq)", "code_tokens": ["def", "remove_gaps", "(", "A", ",", "B", ")", ":", "a_seq", ",", "b_seq", "=", "[", "]", ",", "[", "]", "for", "a", ",", "b", "in", "zip", "(", "list", "(", "A", ")", ",", "list", "(", "B", ")", ")", ":", "if", "a", "==", "'-'", "or", "a", "==", "'.'", "or", "b", "==", "'-'", "or", "b", "==", "'.'", ":", "continue", "a_seq", ".", "append", "(", "a", ")", "b_seq", ".", "append", "(", "b", ")", "return", "''", ".", "join", "(", "a_seq", ")", ",", "''", ".", "join", "(", "b_seq", ")"], "docstring": "skip column if either is a gap", "docstring_tokens": ["skip", "column", "if", "either", "is", "a", "gap"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L52-L62", "partition": "train", "up_fun_num": 2, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "compare_seqs", "original_string": "def compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1] # actual sequences\n    if len(a) != len(b):\n        print('# reads are not the same length', file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident", "language": "python", "code": "def compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1] # actual sequences\n    if len(a) != len(b):\n        print('# reads are not the same length', file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident", "code_tokens": ["def", "compare_seqs", "(", "seqs", ")", ":", "A", ",", "B", ",", "ignore_gaps", "=", "seqs", "a", ",", "b", "=", "A", "[", "1", "]", ",", "B", "[", "1", "]", "# actual sequences", "if", "len", "(", "a", ")", "!=", "len", "(", "b", ")", ":", "print", "(", "'# reads are not the same length'", ",", "file", "=", "sys", ".", "stderr", ")", "exit", "(", ")", "if", "ignore_gaps", "is", "True", ":", "pident", "=", "calc_pident_ignore_gaps", "(", "a", ",", "b", ")", "else", ":", "pident", "=", "calc_pident", "(", "a", ",", "b", ")", "return", "A", "[", "0", "]", ",", "B", "[", "0", "]", ",", "pident"], "docstring": "compare pairs of sequences", "docstring_tokens": ["compare", "pairs", "of", "sequences"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L64-L77", "partition": "train", "up_fun_num": 3, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "compare_seqs_leven", "original_string": "def compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1]) # actual sequences\n    if len(a) != len(b):\n        print('# reads are not the same length', file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident", "language": "python", "code": "def compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1]) # actual sequences\n    if len(a) != len(b):\n        print('# reads are not the same length', file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident", "code_tokens": ["def", "compare_seqs_leven", "(", "seqs", ")", ":", "A", ",", "B", ",", "ignore_gaps", "=", "seqs", "a", ",", "b", "=", "remove_gaps", "(", "A", "[", "1", "]", ",", "B", "[", "1", "]", ")", "# actual sequences", "if", "len", "(", "a", ")", "!=", "len", "(", "b", ")", ":", "print", "(", "'# reads are not the same length'", ",", "file", "=", "sys", ".", "stderr", ")", "exit", "(", ")", "pident", "=", "lr", "(", "a", ",", "b", ")", "*", "100", "return", "A", "[", "0", "]", ",", "B", "[", "0", "]", ",", "pident"], "docstring": "calculate Levenshtein ratio of sequences", "docstring_tokens": ["calculate", "Levenshtein", "ratio", "of", "sequences"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L79-L89", "partition": "train", "up_fun_num": 4, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "pairwise_compare", "original_string": "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index = True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = ((i[0], i[1], ignore_gaps) for i in itertools.combinations(list(seqs.values()), 2))\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total = (num_seqs*num_seqs)/2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)", "language": "python", "code": "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index = True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = ((i[0], i[1], ignore_gaps) for i in itertools.combinations(list(seqs.values()), 2))\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total = (num_seqs*num_seqs)/2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)", "code_tokens": ["def", "pairwise_compare", "(", "afa", ",", "leven", ",", "threads", ",", "print_list", ",", "ignore_gaps", ")", ":", "# load sequences into dictionary", "seqs", "=", "{", "seq", "[", "0", "]", ":", "seq", "for", "seq", "in", "nr_fasta", "(", "[", "afa", "]", ",", "append_index", "=", "True", ")", "}", "num_seqs", "=", "len", "(", "seqs", ")", "# define all pairs", "pairs", "=", "(", "(", "i", "[", "0", "]", ",", "i", "[", "1", "]", ",", "ignore_gaps", ")", "for", "i", "in", "itertools", ".", "combinations", "(", "list", "(", "seqs", ".", "values", "(", ")", ")", ",", "2", ")", ")", "pool", "=", "multithread", "(", "threads", ")", "# calc percent identity between all pairs - parallelize", "if", "leven", "is", "True", ":", "pident", "=", "pool", ".", "map", "(", "compare_seqs_leven", ",", "pairs", ")", "else", ":", "compare", "=", "pool", ".", "imap_unordered", "(", "compare_seqs", ",", "pairs", ")", "pident", "=", "[", "i", "for", "i", "in", "tqdm", "(", "compare", ",", "total", "=", "(", "num_seqs", "*", "num_seqs", ")", "/", "2", ")", "]", "pool", ".", "close", "(", ")", "pool", ".", "terminate", "(", ")", "pool", ".", "join", "(", ")", "return", "to_dictionary", "(", "pident", ",", "print_list", ")"], "docstring": "make pairwise sequence comparisons between aligned sequences", "docstring_tokens": ["make", "pairwise", "sequence", "comparisons", "between", "aligned", "sequences"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L91-L110", "partition": "train", "up_fun_num": 5, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "print_pairwise", "original_string": "def print_pairwise(pw, median = False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if '>' in names[0]:\n            yield ['#'] + [i.split('>')[1] for i in names if '>' in i]\n        else:\n            yield ['#'] + names\n        for a in names:\n            if '>' in a:\n                yield [a.split('>')[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append('-')\n                yield [a] + out", "language": "python", "code": "def print_pairwise(pw, median = False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if '>' in names[0]:\n            yield ['#'] + [i.split('>')[1] for i in names if '>' in i]\n        else:\n            yield ['#'] + names\n        for a in names:\n            if '>' in a:\n                yield [a.split('>')[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append('-')\n                yield [a] + out", "code_tokens": ["def", "print_pairwise", "(", "pw", ",", "median", "=", "False", ")", ":", "names", "=", "sorted", "(", "set", "(", "[", "i", "for", "i", "in", "pw", "]", ")", ")", "if", "len", "(", "names", ")", "!=", "0", ":", "if", "'>'", "in", "names", "[", "0", "]", ":", "yield", "[", "'#'", "]", "+", "[", "i", ".", "split", "(", "'>'", ")", "[", "1", "]", "for", "i", "in", "names", "if", "'>'", "in", "i", "]", "else", ":", "yield", "[", "'#'", "]", "+", "names", "for", "a", "in", "names", ":", "if", "'>'", "in", "a", ":", "yield", "[", "a", ".", "split", "(", "'>'", ")", "[", "1", "]", "]", "+", "[", "pw", "[", "a", "]", "[", "b", "]", "for", "b", "in", "names", "]", "else", ":", "out", "=", "[", "]", "for", "b", "in", "names", ":", "if", "b", "in", "pw", "[", "a", "]", ":", "if", "median", "is", "False", ":", "out", ".", "append", "(", "max", "(", "pw", "[", "a", "]", "[", "b", "]", ")", ")", "else", ":", "out", ".", "append", "(", "np", ".", "median", "(", "pw", "[", "a", "]", "[", "b", "]", ")", ")", "else", ":", "out", ".", "append", "(", "'-'", ")", "yield", "[", "a", "]", "+", "out"], "docstring": "print matrix of pidents to stdout", "docstring_tokens": ["print", "matrix", "of", "pidents", "to", "stdout"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L132-L155", "partition": "train", "up_fun_num": 7, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "print_comps", "original_string": "def print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print('n/a')\n    else:\n        print('# min: %s, max: %s, mean: %s' % \\\n            (min(comps), max(comps), np.mean(comps)))", "language": "python", "code": "def print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print('n/a')\n    else:\n        print('# min: %s, max: %s, mean: %s' % \\\n            (min(comps), max(comps), np.mean(comps)))", "code_tokens": ["def", "print_comps", "(", "comps", ")", ":", "if", "comps", "==", "[", "]", ":", "print", "(", "'n/a'", ")", "else", ":", "print", "(", "'# min: %s, max: %s, mean: %s'", "%", "(", "min", "(", "comps", ")", ",", "max", "(", "comps", ")", ",", "np", ".", "mean", "(", "comps", ")", ")", ")"], "docstring": "print stats for comparisons", "docstring_tokens": ["print", "stats", "for", "comparisons"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L157-L165", "partition": "train", "up_fun_num": 8, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def compare_clades(pw):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "compare_clades", "original_string": "def compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if ';' not in a or ';' not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(';')[i], b.split(';')[i]\n                if i == 0 and '_' in cA and '_' in cB:\n                    cA = cA.rsplit('_', 1)[1]\n                    cB = cB.rsplit('_', 1)[1]\n                elif '>' in cA or '>' in cB:\n                    cA = cA.split('>')[1]\n                    cB = cB.split('>')[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print('\\n# min. within')\n        for clade, pidents in list(wi.items()):\n            print('\\t'.join(['wi:%s' % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print('\\n# max. between')\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print('\\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))\n                if comp[0] != '#':\n                    comps.extend([j for j in comp[1:] if j != '-'])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print('\\n# median between')\n        for comp in print_pairwise(bt, median = True):\n            if comp is not None:\n                print('\\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))\n                if comp[0] != '#':\n                    comps.extend([j for j in comp[1:] if j != '-'])\n        print_comps(comps)", "language": "python", "code": "def compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if ';' not in a or ';' not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(';')[i], b.split(';')[i]\n                if i == 0 and '_' in cA and '_' in cB:\n                    cA = cA.rsplit('_', 1)[1]\n                    cB = cB.rsplit('_', 1)[1]\n                elif '>' in cA or '>' in cB:\n                    cA = cA.split('>')[1]\n                    cB = cB.split('>')[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print('\\n# min. within')\n        for clade, pidents in list(wi.items()):\n            print('\\t'.join(['wi:%s' % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print('\\n# max. between')\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print('\\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))\n                if comp[0] != '#':\n                    comps.extend([j for j in comp[1:] if j != '-'])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print('\\n# median between')\n        for comp in print_pairwise(bt, median = True):\n            if comp is not None:\n                print('\\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))\n                if comp[0] != '#':\n                    comps.extend([j for j in comp[1:] if j != '-'])\n        print_comps(comps)", "code_tokens": ["def", "compare_clades", "(", "pw", ")", ":", "names", "=", "sorted", "(", "set", "(", "[", "i", "for", "i", "in", "pw", "]", ")", ")", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "wi", ",", "bt", "=", "{", "}", ",", "{", "}", "for", "a", "in", "names", ":", "for", "b", "in", "pw", "[", "a", "]", ":", "if", "';'", "not", "in", "a", "or", "';'", "not", "in", "b", ":", "continue", "pident", "=", "pw", "[", "a", "]", "[", "b", "]", "cA", ",", "cB", "=", "a", ".", "split", "(", "';'", ")", "[", "i", "]", ",", "b", ".", "split", "(", "';'", ")", "[", "i", "]", "if", "i", "==", "0", "and", "'_'", "in", "cA", "and", "'_'", "in", "cB", ":", "cA", "=", "cA", ".", "rsplit", "(", "'_'", ",", "1", ")", "[", "1", "]", "cB", "=", "cB", ".", "rsplit", "(", "'_'", ",", "1", ")", "[", "1", "]", "elif", "'>'", "in", "cA", "or", "'>'", "in", "cB", ":", "cA", "=", "cA", ".", "split", "(", "'>'", ")", "[", "1", "]", "cB", "=", "cB", ".", "split", "(", "'>'", ")", "[", "1", "]", "if", "cA", "==", "cB", ":", "if", "cA", "not", "in", "wi", ":", "wi", "[", "cA", "]", "=", "[", "]", "wi", "[", "cA", "]", ".", "append", "(", "pident", ")", "else", ":", "if", "cA", "not", "in", "bt", ":", "bt", "[", "cA", "]", "=", "{", "}", "if", "cB", "not", "in", "bt", "[", "cA", "]", ":", "bt", "[", "cA", "]", "[", "cB", "]", "=", "[", "]", "bt", "[", "cA", "]", "[", "cB", "]", ".", "append", "(", "pident", ")", "print", "(", "'\\n# min. within'", ")", "for", "clade", ",", "pidents", "in", "list", "(", "wi", ".", "items", "(", ")", ")", ":", "print", "(", "'\\t'", ".", "join", "(", "[", "'wi:%s'", "%", "str", "(", "i", ")", ",", "clade", ",", "str", "(", "min", "(", "pidents", ")", ")", "]", ")", ")", "# print matrix of maximum between groups", "comps", "=", "[", "]", "print", "(", "'\\n# max. between'", ")", "for", "comp", "in", "print_pairwise", "(", "bt", ")", ":", "if", "comp", "is", "not", "None", ":", "print", "(", "'\\t'", ".", "join", "(", "[", "'bt:%s'", "%", "str", "(", "i", ")", "]", "+", "[", "str", "(", "j", ")", "for", "j", "in", "comp", "]", ")", ")", "if", "comp", "[", "0", "]", "!=", "'#'", ":", "comps", ".", "extend", "(", "[", "j", "for", "j", "in", "comp", "[", "1", ":", "]", "if", "j", "!=", "'-'", "]", ")", "print_comps", "(", "comps", ")", "# print matrix of median between groups", "comps", "=", "[", "]", "print", "(", "'\\n# median between'", ")", "for", "comp", "in", "print_pairwise", "(", "bt", ",", "median", "=", "True", ")", ":", "if", "comp", "is", "not", "None", ":", "print", "(", "'\\t'", ".", "join", "(", "[", "'bt:%s'", "%", "str", "(", "i", ")", "]", "+", "[", "str", "(", "j", ")", "for", "j", "in", "comp", "]", ")", ")", "if", "comp", "[", "0", "]", "!=", "'#'", ":", "comps", ".", "extend", "(", "[", "j", "for", "j", "in", "comp", "[", "1", ":", "]", "if", "j", "!=", "'-'", "]", ")", "print_comps", "(", "comps", ")"], "docstring": "print min. pident within each clade and then matrix of between-clade max.", "docstring_tokens": ["print", "min", ".", "pident", "within", "each", "clade", "and", "then", "matrix", "of", "between", "-", "clade", "max", "."], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L167-L216", "partition": "train", "up_fun_num": 9, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split(\"\\t\")\n        if line[0].startswith(\"#\"):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != \"-\":\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def matrix2dictionary(matrix):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/compare_aligned.py", "func_name": "matrix2dictionary", "original_string": "def matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split('\\t')\n        if line[0].startswith('#'):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != '-':\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw", "language": "python", "code": "def matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split('\\t')\n        if line[0].startswith('#'):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != '-':\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw", "code_tokens": ["def", "matrix2dictionary", "(", "matrix", ")", ":", "pw", "=", "{", "}", "for", "line", "in", "matrix", ":", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "if", "line", "[", "0", "]", ".", "startswith", "(", "'#'", ")", ":", "names", "=", "line", "[", "1", ":", "]", "continue", "a", "=", "line", "[", "0", "]", "for", "i", ",", "pident", "in", "enumerate", "(", "line", "[", "1", ":", "]", ")", ":", "b", "=", "names", "[", "i", "]", "if", "a", "not", "in", "pw", ":", "pw", "[", "a", "]", "=", "{", "}", "if", "b", "not", "in", "pw", ":", "pw", "[", "b", "]", "=", "{", "}", "if", "pident", "!=", "'-'", ":", "pident", "=", "float", "(", "pident", ")", "pw", "[", "a", "]", "[", "b", "]", "=", "pident", "pw", "[", "b", "]", "[", "a", "]", "=", "pident", "return", "pw"], "docstring": "convert matrix to dictionary of comparisons", "docstring_tokens": ["convert", "matrix", "to", "dictionary", "of", "comparisons"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/compare_aligned.py#L218-L239", "partition": "train", "up_fun_num": 10, "context": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nimport itertools\nimport numpy as np\nfrom tqdm import tqdm as tqdm\nfrom multiprocessing import Pool as multithread\n\n# from Levenshtein import ratio as lr\n\nfrom ctbBio.nr_fasta import de_rep as nr_fasta\n\n\ndef calc_pident(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \".\" or B == \".\":\n            continue\n        if A == \"-\" and B == \"-\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0  # matches\n    mm = 0  # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == \"-\" or A == \".\" or B == \"-\" or B == \".\":\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m) / float((m + mm))) * 100\n    except:\n        return 0\n\n\ndef remove_gaps(A, B):\n    \"\"\"\n    skip column if either is a gap\n    \"\"\"\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == \"-\" or a == \".\" or b == \"-\" or b == \".\":\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return \"\".join(a_seq), \"\".join(b_seq)\n\n\ndef compare_seqs(seqs):\n    \"\"\"\n    compare pairs of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1]  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident\n\n\ndef compare_seqs_leven(seqs):\n    \"\"\"\n    calculate Levenshtein ratio of sequences\n    \"\"\"\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1])  # actual sequences\n    if len(a) != len(b):\n        print(\"# reads are not the same length\", file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident\n\n\ndef pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index=True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = (\n        (i[0], i[1], ignore_gaps)\n        for i in itertools.combinations(list(seqs.values()), 2)\n    )\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total=(num_seqs * num_seqs) / 2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)\n\n\ndef to_dictionary(pw, print_list):\n    \"\"\"\n    - convert list of comparisons to dictionary\n    - print list of pidents (if requested) to stderr\n    \"\"\"\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: \"-\"}\n        if b not in pairs:\n            pairs[b] = {b: \"-\"}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split(\">\")[1], b.split(\">\")[1]\n            print(\"\\t\".join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print(\"\\t\".join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs\n\n\ndef print_pairwise(pw, median=False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if \">\" in names[0]:\n            yield [\"#\"] + [i.split(\">\")[1] for i in names if \">\" in i]\n        else:\n            yield [\"#\"] + names\n        for a in names:\n            if \">\" in a:\n                yield [a.split(\">\")[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append(\"-\")\n                yield [a] + out\n\n\ndef print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print(\"n/a\")\n    else:\n        print(\"# min: %s, max: %s, mean: %s\" % (min(comps), max(comps), np.mean(comps)))\n\n\ndef compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if \";\" not in a or \";\" not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(\";\")[i], b.split(\";\")[i]\n                if i == 0 and \"_\" in cA and \"_\" in cB:\n                    cA = cA.rsplit(\"_\", 1)[1]\n                    cB = cB.rsplit(\"_\", 1)[1]\n                elif \">\" in cA or \">\" in cB:\n                    cA = cA.split(\">\")[1]\n                    cB = cB.split(\">\")[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print(\"\\n# min. within\")\n        for clade, pidents in list(wi.items()):\n            print(\"\\t\".join([\"wi:%s\" % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print(\"\\n# max. between\")\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print(\"\\n# median between\")\n        for comp in print_pairwise(bt, median=True):\n            if comp is not None:\n                print(\"\\t\".join([\"bt:%s\" % str(i)] + [str(j) for j in comp]))\n                if comp[0] != \"#\":\n                    comps.extend([j for j in comp[1:] if j != \"-\"])\n        print_comps(comps)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"# calculate percent identity of aligned reads\"\n    )\n    parser.add_argument(\"-a\", default=False, help=\"aligned fasta file\")\n    parser.add_argument(\n        \"-m\", default=False, help=\"matrix of comparisons (for clade calcs.)\"\n    )\n    parser.add_argument(\n        \"--list\",\n        action=\"store_true\",\n        help=\"print list of pair-wise identities to stderr\",\n    )\n    parser.add_argument(\"--no-matrix\", action=\"store_false\", help=\"do not print matrix\")\n    parser.add_argument(\n        \"--clades\",\n        action=\"store_true\",\n        help=\"compare clades based on header, e.g. >[0]Bacteria;[1]OD1;[2]unknown or >Bacteria;OD1;unknown\",\n    )\n    parser.add_argument(\n        \"--ignore-gaps\", action=\"store_true\", help=\"ignore gaps in alignment\"\n    )\n    parser.add_argument(\n        \"--leven\", action=\"store_true\", help=\"calculate Levenshtein ratio\"\n    )\n    parser.add_argument(\n        \"-t\", default=6, type=int, help=\"number of threads (default: 6)\"\n    )\n    args = vars(parser.parse_args())\n    afa, matrix, print_list, print_matrix, clades, ignore_gaps, leven, threads = (\n        args[\"a\"],\n        args[\"m\"],\n        args[\"list\"],\n        args[\"no_matrix\"],\n        args[\"clades\"],\n        args[\"ignore_gaps\"],\n        args[\"leven\"],\n        args[\"t\"],\n    )\n    if (afa is False and matrix is False) or (afa is not False and matrix is not False):\n        print(\"# use -a or -m; -h for help\", file=sys.stderr)\n        exit()\n    if afa is not False:\n        if afa == \"-\":\n            afa = sys.stdin\n        else:\n            afa = open(afa)\n        pairwise = pairwise_compare(afa, leven, threads, print_list, ignore_gaps)\n        if print_matrix is True:\n            for i in print_pairwise(pairwise):\n                print(\"\\t\".join([str(j) for j in i]))\n        if clades is True:\n            compare_clades(pairwise)\n    if matrix is not False:\n        pairwise = matrix2dictionary(open(matrix))\n        compare_clades(pairwise)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "import sys", "import argparse", "import itertools", "import numpy as np", "from tqdm import tqdm as tqdm", "from multiprocessing import Pool as multithread", "from ctbBio.nr_fasta import de_rep as nr_fasta"], "function": ["def calc_pident(a, b):\n", "def calc_pident_ignore_gaps(a, b):\n", "def remove_gaps(A, B):\n", "def compare_seqs(seqs):\n", "def compare_seqs_leven(seqs):\n", "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n", "def to_dictionary(pw, print_list):\n", "def print_pairwise(pw, median=False):\n", "def print_comps(comps):\n", "def compare_clades(pw):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/commands.py", "func_name": "setoption", "original_string": "def setoption(parser, metadata=None):\n    \"\"\"Set argument parser option.\"\"\"\n    parser.add_argument('-v', action='version',\n                        version=__version__)\n    subparsers = parser.add_subparsers(help='sub commands help')\n    create_cmd = subparsers.add_parser('create')\n    create_cmd.add_argument('name',\n                            help='Specify Python package name.')\n    create_cmd.add_argument('-d', dest='description', action='store',\n                            help='Short description about your package.')\n    create_cmd.add_argument('-a', dest='author', action='store',\n                            required=True,\n                            help='Python package author name.')\n    create_cmd.add_argument('-e', dest='email', action='store',\n                            required=True,\n                            help='Python package author email address.')\n    create_cmd.add_argument('-l', dest='license',\n                            choices=metadata.licenses().keys(),\n                            default='GPLv3+',\n                            help='Specify license. (default: %(default)s)')\n    create_cmd.add_argument('-s', dest='status',\n                            choices=metadata.status().keys(),\n                            default='Alpha',\n                            help=('Specify development status. '\n                                  '(default: %(default)s)'))\n    create_cmd.add_argument('--no-check', action='store_true',\n                            help='No checking package name in PyPI.')\n    create_cmd.add_argument('--with-samples', action='store_true',\n                            help='Generate package with sample code.')\n    group = create_cmd.add_mutually_exclusive_group(required=True)\n    group.add_argument('-U', dest='username', action='store',\n                       help='Specify GitHub username.')\n    group.add_argument('-u', dest='url', action='store', type=valid_url,\n                       help='Python package homepage url.')\n    create_cmd.add_argument('-o', dest='outdir', action='store',\n                            default=os.path.abspath(os.path.curdir),\n                            help='Specify output directory. (default: $PWD)')\n    list_cmd = subparsers.add_parser('list')\n    list_cmd.add_argument('-l', dest='licenses', action='store_true',\n                          help='show license choices.')", "language": "python", "code": "def setoption(parser, metadata=None):\n    \"\"\"Set argument parser option.\"\"\"\n    parser.add_argument('-v', action='version',\n                        version=__version__)\n    subparsers = parser.add_subparsers(help='sub commands help')\n    create_cmd = subparsers.add_parser('create')\n    create_cmd.add_argument('name',\n                            help='Specify Python package name.')\n    create_cmd.add_argument('-d', dest='description', action='store',\n                            help='Short description about your package.')\n    create_cmd.add_argument('-a', dest='author', action='store',\n                            required=True,\n                            help='Python package author name.')\n    create_cmd.add_argument('-e', dest='email', action='store',\n                            required=True,\n                            help='Python package author email address.')\n    create_cmd.add_argument('-l', dest='license',\n                            choices=metadata.licenses().keys(),\n                            default='GPLv3+',\n                            help='Specify license. (default: %(default)s)')\n    create_cmd.add_argument('-s', dest='status',\n                            choices=metadata.status().keys(),\n                            default='Alpha',\n                            help=('Specify development status. '\n                                  '(default: %(default)s)'))\n    create_cmd.add_argument('--no-check', action='store_true',\n                            help='No checking package name in PyPI.')\n    create_cmd.add_argument('--with-samples', action='store_true',\n                            help='Generate package with sample code.')\n    group = create_cmd.add_mutually_exclusive_group(required=True)\n    group.add_argument('-U', dest='username', action='store',\n                       help='Specify GitHub username.')\n    group.add_argument('-u', dest='url', action='store', type=valid_url,\n                       help='Python package homepage url.')\n    create_cmd.add_argument('-o', dest='outdir', action='store',\n                            default=os.path.abspath(os.path.curdir),\n                            help='Specify output directory. (default: $PWD)')\n    list_cmd = subparsers.add_parser('list')\n    list_cmd.add_argument('-l', dest='licenses', action='store_true',\n                          help='show license choices.')", "code_tokens": ["def", "setoption", "(", "parser", ",", "metadata", "=", "None", ")", ":", "parser", ".", "add_argument", "(", "'-v'", ",", "action", "=", "'version'", ",", "version", "=", "__version__", ")", "subparsers", "=", "parser", ".", "add_subparsers", "(", "help", "=", "'sub commands help'", ")", "create_cmd", "=", "subparsers", ".", "add_parser", "(", "'create'", ")", "create_cmd", ".", "add_argument", "(", "'name'", ",", "help", "=", "'Specify Python package name.'", ")", "create_cmd", ".", "add_argument", "(", "'-d'", ",", "dest", "=", "'description'", ",", "action", "=", "'store'", ",", "help", "=", "'Short description about your package.'", ")", "create_cmd", ".", "add_argument", "(", "'-a'", ",", "dest", "=", "'author'", ",", "action", "=", "'store'", ",", "required", "=", "True", ",", "help", "=", "'Python package author name.'", ")", "create_cmd", ".", "add_argument", "(", "'-e'", ",", "dest", "=", "'email'", ",", "action", "=", "'store'", ",", "required", "=", "True", ",", "help", "=", "'Python package author email address.'", ")", "create_cmd", ".", "add_argument", "(", "'-l'", ",", "dest", "=", "'license'", ",", "choices", "=", "metadata", ".", "licenses", "(", ")", ".", "keys", "(", ")", ",", "default", "=", "'GPLv3+'", ",", "help", "=", "'Specify license. (default: %(default)s)'", ")", "create_cmd", ".", "add_argument", "(", "'-s'", ",", "dest", "=", "'status'", ",", "choices", "=", "metadata", ".", "status", "(", ")", ".", "keys", "(", ")", ",", "default", "=", "'Alpha'", ",", "help", "=", "(", "'Specify development status. '", "'(default: %(default)s)'", ")", ")", "create_cmd", ".", "add_argument", "(", "'--no-check'", ",", "action", "=", "'store_true'", ",", "help", "=", "'No checking package name in PyPI.'", ")", "create_cmd", ".", "add_argument", "(", "'--with-samples'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Generate package with sample code.'", ")", "group", "=", "create_cmd", ".", "add_mutually_exclusive_group", "(", "required", "=", "True", ")", "group", ".", "add_argument", "(", "'-U'", ",", "dest", "=", "'username'", ",", "action", "=", "'store'", ",", "help", "=", "'Specify GitHub username.'", ")", "group", ".", "add_argument", "(", "'-u'", ",", "dest", "=", "'url'", ",", "action", "=", "'store'", ",", "type", "=", "valid_url", ",", "help", "=", "'Python package homepage url.'", ")", "create_cmd", ".", "add_argument", "(", "'-o'", ",", "dest", "=", "'outdir'", ",", "action", "=", "'store'", ",", "default", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "curdir", ")", ",", "help", "=", "'Specify output directory. (default: $PWD)'", ")", "list_cmd", "=", "subparsers", ".", "add_parser", "(", "'list'", ")", "list_cmd", ".", "add_argument", "(", "'-l'", ",", "dest", "=", "'licenses'", ",", "action", "=", "'store_true'", ",", "help", "=", "'show license choices.'", ")"], "docstring": "Set argument parser option.", "docstring_tokens": ["Set", "argument", "parser", "option", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/commands.py#L12-L51", "partition": "train", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.commands.\"\"\"\nimport os\nimport sys\nimport re\nimport argparse\nfrom bootstrap_py import control, __prog__, __version__\nfrom bootstrap_py.update import Update\nfrom bootstrap_py.exceptions import BackendFailure, Conflict\n\n\ndef valid_url(url):\n    \"\"\"Validate url.\n\n    :rtype: str\n    :return: url\n\n    :param str url: package homepage url.\n    \"\"\"\n    regex = re.compile(\n        r\"^(?:http)s?://\"\n        r\"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\"\n        r\"(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?))\"\n        r\"(?:/?|[/?]\\S+)$\",\n        re.IGNORECASE,\n    )\n    if not regex.match(url):\n        raise argparse.ArgumentTypeError('\"{0}\" is invalid url.'.format(url))\n    return url\n\n\ndef parse_options(metadata):\n    \"\"\"Parse argument options.\"\"\"\n    parser = argparse.ArgumentParser(description=\"%(prog)s usage:\", prog=__prog__)\n    setoption(parser, metadata=metadata)\n    return parser\n\n\ndef main():\n    \"\"\"Execute main processes.\"\"\"\n    try:\n        pkg_version = Update()\n        if pkg_version.updatable():\n            pkg_version.show_message()\n        metadata = control.retreive_metadata()\n        parser = parse_options(metadata)\n        argvs = sys.argv\n        if len(argvs) <= 1:\n            parser.print_help()\n            sys.exit(1)\n        args = parser.parse_args()\n        control.print_licences(args, metadata)\n        control.check_repository_existence(args)\n        control.check_package_existence(args)\n        control.generate_package(args)\n    except (RuntimeError, BackendFailure, Conflict) as exc:\n        sys.stderr.write(\"{0}\\n\".format(exc))\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 0, 0], "package": ["import os", "import sys", "import re", "import argparse", "from bootstrap_py import control, __prog__, __version__", "from bootstrap_py.update import Update", "from bootstrap_py.exceptions import BackendFailure, Conflict"], "function": ["def valid_url(url):\n", "def parse_options(metadata):\n", "def main():\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/commands.py", "func_name": "parse_options", "original_string": "def parse_options(metadata):\n    \"\"\"Parse argument options.\"\"\"\n    parser = argparse.ArgumentParser(description='%(prog)s usage:',\n                                     prog=__prog__)\n    setoption(parser, metadata=metadata)\n    return parser", "language": "python", "code": "def parse_options(metadata):\n    \"\"\"Parse argument options.\"\"\"\n    parser = argparse.ArgumentParser(description='%(prog)s usage:',\n                                     prog=__prog__)\n    setoption(parser, metadata=metadata)\n    return parser", "code_tokens": ["def", "parse_options", "(", "metadata", ")", ":", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'%(prog)s usage:'", ",", "prog", "=", "__prog__", ")", "setoption", "(", "parser", ",", "metadata", "=", "metadata", ")", "return", "parser"], "docstring": "Parse argument options.", "docstring_tokens": ["Parse", "argument", "options", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/commands.py#L72-L77", "partition": "train", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.commands.\"\"\"\nimport os\nimport sys\nimport re\nimport argparse\nfrom bootstrap_py import control, __prog__, __version__\nfrom bootstrap_py.update import Update\nfrom bootstrap_py.exceptions import BackendFailure, Conflict\n\n\ndef setoption(parser, metadata=None):\n    \"\"\"Set argument parser option.\"\"\"\n    parser.add_argument(\"-v\", action=\"version\", version=__version__)\n    subparsers = parser.add_subparsers(help=\"sub commands help\")\n    create_cmd = subparsers.add_parser(\"create\")\n    create_cmd.add_argument(\"name\", help=\"Specify Python package name.\")\n    create_cmd.add_argument(\n        \"-d\",\n        dest=\"description\",\n        action=\"store\",\n        help=\"Short description about your package.\",\n    )\n    create_cmd.add_argument(\n        \"-a\",\n        dest=\"author\",\n        action=\"store\",\n        required=True,\n        help=\"Python package author name.\",\n    )\n    create_cmd.add_argument(\n        \"-e\",\n        dest=\"email\",\n        action=\"store\",\n        required=True,\n        help=\"Python package author email address.\",\n    )\n    create_cmd.add_argument(\n        \"-l\",\n        dest=\"license\",\n        choices=metadata.licenses().keys(),\n        default=\"GPLv3+\",\n        help=\"Specify license. (default: %(default)s)\",\n    )\n    create_cmd.add_argument(\n        \"-s\",\n        dest=\"status\",\n        choices=metadata.status().keys(),\n        default=\"Alpha\",\n        help=(\"Specify development status. \" \"(default: %(default)s)\"),\n    )\n    create_cmd.add_argument(\n        \"--no-check\", action=\"store_true\", help=\"No checking package name in PyPI.\"\n    )\n    create_cmd.add_argument(\n        \"--with-samples\", action=\"store_true\", help=\"Generate package with sample code.\"\n    )\n    group = create_cmd.add_mutually_exclusive_group(required=True)\n    group.add_argument(\n        \"-U\", dest=\"username\", action=\"store\", help=\"Specify GitHub username.\"\n    )\n    group.add_argument(\n        \"-u\",\n        dest=\"url\",\n        action=\"store\",\n        type=valid_url,\n        help=\"Python package homepage url.\",\n    )\n    create_cmd.add_argument(\n        \"-o\",\n        dest=\"outdir\",\n        action=\"store\",\n        default=os.path.abspath(os.path.curdir),\n        help=\"Specify output directory. (default: $PWD)\",\n    )\n    list_cmd = subparsers.add_parser(\"list\")\n    list_cmd.add_argument(\n        \"-l\", dest=\"licenses\", action=\"store_true\", help=\"show license choices.\"\n    )\n\n\ndef valid_url(url):\n    \"\"\"Validate url.\n\n    :rtype: str\n    :return: url\n\n    :param str url: package homepage url.\n    \"\"\"\n    regex = re.compile(\n        r\"^(?:http)s?://\"\n        r\"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\"\n        r\"(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?))\"\n        r\"(?:/?|[/?]\\S+)$\",\n        re.IGNORECASE,\n    )\n    if not regex.match(url):\n        raise argparse.ArgumentTypeError('\"{0}\" is invalid url.'.format(url))\n    return url\n\n\ndef main():\n    \"\"\"Execute main processes.\"\"\"\n    try:\n        pkg_version = Update()\n        if pkg_version.updatable():\n            pkg_version.show_message()\n        metadata = control.retreive_metadata()\n        parser = parse_options(metadata)\n        argvs = sys.argv\n        if len(argvs) <= 1:\n            parser.print_help()\n            sys.exit(1)\n        args = parser.parse_args()\n        control.print_licences(args, metadata)\n        control.check_repository_existence(args)\n        control.check_package_existence(args)\n        control.generate_package(args)\n    except (RuntimeError, BackendFailure, Conflict) as exc:\n        sys.stderr.write(\"{0}\\n\".format(exc))\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 0, 0], "package": ["import os", "import sys", "import re", "import argparse", "from bootstrap_py import control, __prog__, __version__", "from bootstrap_py.update import Update", "from bootstrap_py.exceptions import BackendFailure, Conflict"], "function": ["def setoption(parser, metadata=None):\n", "def valid_url(url):\n", "def main():\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/commands.py", "func_name": "main", "original_string": "def main():\n    \"\"\"Execute main processes.\"\"\"\n    try:\n        pkg_version = Update()\n        if pkg_version.updatable():\n            pkg_version.show_message()\n        metadata = control.retreive_metadata()\n        parser = parse_options(metadata)\n        argvs = sys.argv\n        if len(argvs) <= 1:\n            parser.print_help()\n            sys.exit(1)\n        args = parser.parse_args()\n        control.print_licences(args, metadata)\n        control.check_repository_existence(args)\n        control.check_package_existence(args)\n        control.generate_package(args)\n    except (RuntimeError, BackendFailure, Conflict) as exc:\n        sys.stderr.write('{0}\\n'.format(exc))\n        sys.exit(1)", "language": "python", "code": "def main():\n    \"\"\"Execute main processes.\"\"\"\n    try:\n        pkg_version = Update()\n        if pkg_version.updatable():\n            pkg_version.show_message()\n        metadata = control.retreive_metadata()\n        parser = parse_options(metadata)\n        argvs = sys.argv\n        if len(argvs) <= 1:\n            parser.print_help()\n            sys.exit(1)\n        args = parser.parse_args()\n        control.print_licences(args, metadata)\n        control.check_repository_existence(args)\n        control.check_package_existence(args)\n        control.generate_package(args)\n    except (RuntimeError, BackendFailure, Conflict) as exc:\n        sys.stderr.write('{0}\\n'.format(exc))\n        sys.exit(1)", "code_tokens": ["def", "main", "(", ")", ":", "try", ":", "pkg_version", "=", "Update", "(", ")", "if", "pkg_version", ".", "updatable", "(", ")", ":", "pkg_version", ".", "show_message", "(", ")", "metadata", "=", "control", ".", "retreive_metadata", "(", ")", "parser", "=", "parse_options", "(", "metadata", ")", "argvs", "=", "sys", ".", "argv", "if", "len", "(", "argvs", ")", "<=", "1", ":", "parser", ".", "print_help", "(", ")", "sys", ".", "exit", "(", "1", ")", "args", "=", "parser", ".", "parse_args", "(", ")", "control", ".", "print_licences", "(", "args", ",", "metadata", ")", "control", ".", "check_repository_existence", "(", "args", ")", "control", ".", "check_package_existence", "(", "args", ")", "control", ".", "generate_package", "(", "args", ")", "except", "(", "RuntimeError", ",", "BackendFailure", ",", "Conflict", ")", "as", "exc", ":", "sys", ".", "stderr", ".", "write", "(", "'{0}\\n'", ".", "format", "(", "exc", ")", ")", "sys", ".", "exit", "(", "1", ")"], "docstring": "Execute main processes.", "docstring_tokens": ["Execute", "main", "processes", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/commands.py#L80-L99", "partition": "train", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.commands.\"\"\"\nimport os\nimport sys\nimport re\nimport argparse\nfrom bootstrap_py import control, __prog__, __version__\nfrom bootstrap_py.update import Update\nfrom bootstrap_py.exceptions import BackendFailure, Conflict\n\n\ndef setoption(parser, metadata=None):\n    \"\"\"Set argument parser option.\"\"\"\n    parser.add_argument(\"-v\", action=\"version\", version=__version__)\n    subparsers = parser.add_subparsers(help=\"sub commands help\")\n    create_cmd = subparsers.add_parser(\"create\")\n    create_cmd.add_argument(\"name\", help=\"Specify Python package name.\")\n    create_cmd.add_argument(\n        \"-d\",\n        dest=\"description\",\n        action=\"store\",\n        help=\"Short description about your package.\",\n    )\n    create_cmd.add_argument(\n        \"-a\",\n        dest=\"author\",\n        action=\"store\",\n        required=True,\n        help=\"Python package author name.\",\n    )\n    create_cmd.add_argument(\n        \"-e\",\n        dest=\"email\",\n        action=\"store\",\n        required=True,\n        help=\"Python package author email address.\",\n    )\n    create_cmd.add_argument(\n        \"-l\",\n        dest=\"license\",\n        choices=metadata.licenses().keys(),\n        default=\"GPLv3+\",\n        help=\"Specify license. (default: %(default)s)\",\n    )\n    create_cmd.add_argument(\n        \"-s\",\n        dest=\"status\",\n        choices=metadata.status().keys(),\n        default=\"Alpha\",\n        help=(\"Specify development status. \" \"(default: %(default)s)\"),\n    )\n    create_cmd.add_argument(\n        \"--no-check\", action=\"store_true\", help=\"No checking package name in PyPI.\"\n    )\n    create_cmd.add_argument(\n        \"--with-samples\", action=\"store_true\", help=\"Generate package with sample code.\"\n    )\n    group = create_cmd.add_mutually_exclusive_group(required=True)\n    group.add_argument(\n        \"-U\", dest=\"username\", action=\"store\", help=\"Specify GitHub username.\"\n    )\n    group.add_argument(\n        \"-u\",\n        dest=\"url\",\n        action=\"store\",\n        type=valid_url,\n        help=\"Python package homepage url.\",\n    )\n    create_cmd.add_argument(\n        \"-o\",\n        dest=\"outdir\",\n        action=\"store\",\n        default=os.path.abspath(os.path.curdir),\n        help=\"Specify output directory. (default: $PWD)\",\n    )\n    list_cmd = subparsers.add_parser(\"list\")\n    list_cmd.add_argument(\n        \"-l\", dest=\"licenses\", action=\"store_true\", help=\"show license choices.\"\n    )\n\n\ndef valid_url(url):\n    \"\"\"Validate url.\n\n    :rtype: str\n    :return: url\n\n    :param str url: package homepage url.\n    \"\"\"\n    regex = re.compile(\n        r\"^(?:http)s?://\"\n        r\"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\"\n        r\"(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?))\"\n        r\"(?:/?|[/?]\\S+)$\",\n        re.IGNORECASE,\n    )\n    if not regex.match(url):\n        raise argparse.ArgumentTypeError('\"{0}\" is invalid url.'.format(url))\n    return url\n\n\ndef parse_options(metadata):\n    \"\"\"Parse argument options.\"\"\"\n    parser = argparse.ArgumentParser(description=\"%(prog)s usage:\", prog=__prog__)\n    setoption(parser, metadata=metadata)\n    return parser\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 0, 0], "package": ["import os", "import sys", "import re", "import argparse", "from bootstrap_py import control, __prog__, __version__", "from bootstrap_py.update import Update", "from bootstrap_py.exceptions import BackendFailure, Conflict"], "function": ["def setoption(parser, metadata=None):\n", "def valid_url(url):\n", "def parse_options(metadata):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/package.py", "func_name": "PackageData._check_or_set_default_params", "original_string": "def _check_or_set_default_params(self):\n        \"\"\"Check key and set default vaule when it does not exists.\"\"\"\n        if not hasattr(self, 'date'):\n            self._set_param('date', datetime.utcnow().strftime('%Y-%m-%d'))\n        if not hasattr(self, 'version'):\n            self._set_param('version', self.default_version)\n        # pylint: disable=no-member\n        if not hasattr(self, 'description') or self.description is None:\n            getattr(self, '_set_param')('description', self.warning_message)", "language": "python", "code": "def _check_or_set_default_params(self):\n        \"\"\"Check key and set default vaule when it does not exists.\"\"\"\n        if not hasattr(self, 'date'):\n            self._set_param('date', datetime.utcnow().strftime('%Y-%m-%d'))\n        if not hasattr(self, 'version'):\n            self._set_param('version', self.default_version)\n        # pylint: disable=no-member\n        if not hasattr(self, 'description') or self.description is None:\n            getattr(self, '_set_param')('description', self.warning_message)", "code_tokens": ["def", "_check_or_set_default_params", "(", "self", ")", ":", "if", "not", "hasattr", "(", "self", ",", "'date'", ")", ":", "self", ".", "_set_param", "(", "'date'", ",", "datetime", ".", "utcnow", "(", ")", ".", "strftime", "(", "'%Y-%m-%d'", ")", ")", "if", "not", "hasattr", "(", "self", ",", "'version'", ")", ":", "self", ".", "_set_param", "(", "'version'", ",", "self", ".", "default_version", ")", "# pylint: disable=no-member", "if", "not", "hasattr", "(", "self", ",", "'description'", ")", "or", "self", ".", "description", "is", "None", ":", "getattr", "(", "self", ",", "'_set_param'", ")", "(", "'description'", ",", "self", ".", "warning_message", ")"], "docstring": "Check key and set default vaule when it does not exists.", "docstring_tokens": ["Check", "key", "and", "set", "default", "vaule", "when", "it", "does", "not", "exists", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/package.py#L44-L52", "partition": "train", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.package.\"\"\"\nimport os\nimport shutil\nimport tempfile\nfrom datetime import datetime\nfrom jinja2 import PackageLoader, Environment\nfrom pguard import guard\nfrom pguard import guard_cl as g\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.docs import build_sphinx\n\n\n# pylint: disable=too-few-public-methods\nclass PackageData:\n    \"\"\"Package meta data class.\"\"\"\n\n    #: Configured the default \"version\" of setup.setup().\n    default_version = \"0.1.0\"\n    #: Users should rewrite parameters after they generate Python package.\n    warning_message = \"##### ToDo: Rewrite me #####\"  # pylint: disable=fixme\n\n    def __init__(self, args):\n        \"\"\"Initialize Package.\"\"\"\n        self.metadata = Classifiers()\n        if hasattr(args, \"__dict__\"):\n            for name, value in vars(args).items():\n                self._set_param(name, value)\n        self._check_or_set_default_params()\n\n    def _set_param(self, name, value):\n        \"\"\"Set name:value property to Package object.\"\"\"\n        if name == \"status\":\n            setattr(self, name, self.metadata.status().get(value))\n        elif name == \"license\":\n            setattr(self, name, self.metadata.licenses().get(value))\n        elif name == \"name\":\n            setattr(self, name, value)\n            setattr(self, \"module_name\", value.replace(\"-\", \"_\"))\n        else:\n            setattr(self, name, value)\n\n    def to_dict(self):\n        \"\"\"Convert the package data to dict.\"\"\"\n        return self.__dict__\n\n\nclass PackageTree:\n    \"\"\"Package directory tree class.\"\"\"\n\n    #: Jinja2 template name\n    template_name = \"bootstrap_py\"\n    #: the suffix name of working directory for generating\n    suffix = \"-bootstrap-py\"\n    #: init filename\n    init = \"__init__.py\"\n    #: default permission\n    exec_perm = 0o755\n    #: include directories to packages\n    pkg_dirs = [\"{module_name}\", \"{module_name}/tests\"]\n\n    def __init__(self, pkg_data):\n        \"\"\"Initialize.\"\"\"\n        self.cwd = os.getcwd()\n        self.name = pkg_data.name\n        self.outdir = os.path.abspath(pkg_data.outdir)\n        self.tmpdir = tempfile.mkdtemp(suffix=self.suffix)\n        self.templates = Environment(loader=PackageLoader(self.template_name))\n        self.pkg_data = pkg_data\n\n    def _init_py(self, dir_path):\n        return os.path.join(\n            self.tmpdir, dir_path.format(**self.pkg_data.to_dict()), self.init\n        )\n\n    def _sample_py(self, file_path):\n        return os.path.join(\n            self.tmpdir, self.pkg_data.module_name, os.path.splitext(file_path)[0]\n        )\n\n    def _tmpl_path(self, file_path):\n        return os.path.join(self.tmpdir, os.path.splitext(file_path)[0])\n\n    def _generate_dirs(self):\n        dirs = [\n            os.path.dirname(tmpl)\n            for tmpl in self.templates.list_templates()\n            if tmpl.find(\"/\") > -1\n        ] + self.pkg_dirs\n        for dir_path in dirs:\n            if not os.path.isdir(\n                os.path.join(self.tmpdir, dir_path.format(**self.pkg_data.to_dict()))\n            ):\n                os.makedirs(\n                    os.path.join(\n                        self.tmpdir, dir_path.format(**self.pkg_data.to_dict())\n                    ),\n                    self.exec_perm,\n                )\n\n    def _generate_docs(self):\n        docs_path = os.path.join(self.tmpdir, \"docs\")\n        os.makedirs(docs_path)\n        build_sphinx(self.pkg_data, docs_path)\n\n    def _list_module_dirs(self):\n        return [\n            dir_path\n            for dir_path in self.pkg_dirs\n            if dir_path.find(\"{module_name}\") == 0\n        ]\n\n    def _generate_init(self):\n        tmpl = self.templates.get_template(\"__init__.py.j2\")\n\n        for dir_path in self._list_module_dirs():\n            if not os.path.isfile(self._init_py(dir_path)):\n                with open(self._init_py(dir_path), \"w\") as fobj:\n                    # pylint: disable=no-member\n                    fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_file(self, file_path):\n        tmpl = self.templates.get_template(file_path)\n        with open(self._tmpl_path(file_path), \"w\") as fobj:\n            fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_exec_file(self, file_path):\n        self._generate_file(file_path)\n        os.chmod(self._tmpl_path(file_path), self.exec_perm)\n        return True\n\n    def _generate_samples(self, file_path):\n        if not self.pkg_data.with_samples:\n            return False\n        tmpl = self.templates.get_template(file_path)\n        if file_path == \"sample.py.j2\":\n            with open(self._sample_py(file_path), \"w\") as fobj:\n                fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        elif file_path == \"test_sample.py.j2\":\n            with open(self._sample_py(os.path.join(\"tests\", file_path)), \"w\") as fobj:\n                fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_files(self):\n        generator = lambda f: guard(\n            g(self._generate_init, f == \"__init__.py.j2\"),\n            g(self._generate_exec_file, f == \"utils/pre-commit.j2\", (f,)),\n            g(self._generate_samples, f.endswith(\"sample.py.j2\"), (f,)),\n            g(self._generate_file, params=(f,)),\n        )\n        for file_path in self.templates.list_templates():\n            generator(file_path)\n        os.chdir(self.tmpdir)\n        os.symlink(\"../../README.rst\", \"docs/source/README.rst\")\n        os.chdir(self.cwd)\n\n    def move(self):\n        \"\"\"Move directory from working directory to output directory.\"\"\"\n        if not os.path.isdir(self.outdir):\n            os.makedirs(self.outdir)\n        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))\n\n    def clean(self):\n        \"\"\"Clean up working directory.\"\"\"\n        shutil.rmtree(self.tmpdir)\n\n    def generate(self):\n        \"\"\"Generate package directory tree.\"\"\"\n        self._generate_docs()\n        self._generate_dirs()\n        self._generate_files()\n\n    def vcs_init(self):\n        \"\"\"Initialize VCS repository.\"\"\"\n        VCS(os.path.join(self.outdir, self.name), self.pkg_data)\n", "levels": [0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import os", "import shutil", "import tempfile", "from datetime import datetime", "from jinja2 import PackageLoader, Environment", "from pguard import guard", "from pguard import guard_cl as g", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.docs import build_sphinx"], "function": ["class PackageData:\n", "    def __init__(self, args):\n", "    def _set_param(self, name, value):\n", "    def to_dict(self):\n", "class PackageTree:\n", "    def __init__(self, pkg_data):\n", "    def _init_py(self, dir_path):\n", "    def _sample_py(self, file_path):\n", "    def _tmpl_path(self, file_path):\n", "    def _generate_dirs(self):\n", "    def _generate_docs(self):\n", "    def _list_module_dirs(self):\n", "    def _generate_init(self):\n", "    def _generate_file(self, file_path):\n", "    def _generate_exec_file(self, file_path):\n", "    def _generate_samples(self, file_path):\n", "    def _generate_files(self):\n", "    def move(self):\n", "    def clean(self):\n", "    def generate(self):\n", "    def vcs_init(self):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/package.py", "func_name": "PackageTree.move", "original_string": "def move(self):\n        \"\"\"Move directory from working directory to output directory.\"\"\"\n        if not os.path.isdir(self.outdir):\n            os.makedirs(self.outdir)\n        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))", "language": "python", "code": "def move(self):\n        \"\"\"Move directory from working directory to output directory.\"\"\"\n        if not os.path.isdir(self.outdir):\n            os.makedirs(self.outdir)\n        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))", "code_tokens": ["def", "move", "(", "self", ")", ":", "if", "not", "os", ".", "path", ".", "isdir", "(", "self", ".", "outdir", ")", ":", "os", ".", "makedirs", "(", "self", ".", "outdir", ")", "shutil", ".", "move", "(", "self", ".", "tmpdir", ",", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "self", ".", "name", ")", ")"], "docstring": "Move directory from working directory to output directory.", "docstring_tokens": ["Move", "directory", "from", "working", "directory", "to", "output", "directory", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/package.py#L169-L173", "partition": "train", "up_fun_num": 18, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.package.\"\"\"\nimport os\nimport shutil\nimport tempfile\nfrom datetime import datetime\nfrom jinja2 import PackageLoader, Environment\nfrom pguard import guard\nfrom pguard import guard_cl as g\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.docs import build_sphinx\n\n\n# pylint: disable=too-few-public-methods\nclass PackageData:\n    \"\"\"Package meta data class.\"\"\"\n\n    #: Configured the default \"version\" of setup.setup().\n    default_version = \"0.1.0\"\n    #: Users should rewrite parameters after they generate Python package.\n    warning_message = \"##### ToDo: Rewrite me #####\"  # pylint: disable=fixme\n\n    def __init__(self, args):\n        \"\"\"Initialize Package.\"\"\"\n        self.metadata = Classifiers()\n        if hasattr(args, \"__dict__\"):\n            for name, value in vars(args).items():\n                self._set_param(name, value)\n        self._check_or_set_default_params()\n\n    def _set_param(self, name, value):\n        \"\"\"Set name:value property to Package object.\"\"\"\n        if name == \"status\":\n            setattr(self, name, self.metadata.status().get(value))\n        elif name == \"license\":\n            setattr(self, name, self.metadata.licenses().get(value))\n        elif name == \"name\":\n            setattr(self, name, value)\n            setattr(self, \"module_name\", value.replace(\"-\", \"_\"))\n        else:\n            setattr(self, name, value)\n\n    def _check_or_set_default_params(self):\n        \"\"\"Check key and set default vaule when it does not exists.\"\"\"\n        if not hasattr(self, \"date\"):\n            self._set_param(\"date\", datetime.utcnow().strftime(\"%Y-%m-%d\"))\n        if not hasattr(self, \"version\"):\n            self._set_param(\"version\", self.default_version)\n        # pylint: disable=no-member\n        if not hasattr(self, \"description\") or self.description is None:\n            getattr(self, \"_set_param\")(\"description\", self.warning_message)\n\n    def to_dict(self):\n        \"\"\"Convert the package data to dict.\"\"\"\n        return self.__dict__\n\n\nclass PackageTree:\n    \"\"\"Package directory tree class.\"\"\"\n\n    #: Jinja2 template name\n    template_name = \"bootstrap_py\"\n    #: the suffix name of working directory for generating\n    suffix = \"-bootstrap-py\"\n    #: init filename\n    init = \"__init__.py\"\n    #: default permission\n    exec_perm = 0o755\n    #: include directories to packages\n    pkg_dirs = [\"{module_name}\", \"{module_name}/tests\"]\n\n    def __init__(self, pkg_data):\n        \"\"\"Initialize.\"\"\"\n        self.cwd = os.getcwd()\n        self.name = pkg_data.name\n        self.outdir = os.path.abspath(pkg_data.outdir)\n        self.tmpdir = tempfile.mkdtemp(suffix=self.suffix)\n        self.templates = Environment(loader=PackageLoader(self.template_name))\n        self.pkg_data = pkg_data\n\n    def _init_py(self, dir_path):\n        return os.path.join(\n            self.tmpdir, dir_path.format(**self.pkg_data.to_dict()), self.init\n        )\n\n    def _sample_py(self, file_path):\n        return os.path.join(\n            self.tmpdir, self.pkg_data.module_name, os.path.splitext(file_path)[0]\n        )\n\n    def _tmpl_path(self, file_path):\n        return os.path.join(self.tmpdir, os.path.splitext(file_path)[0])\n\n    def _generate_dirs(self):\n        dirs = [\n            os.path.dirname(tmpl)\n            for tmpl in self.templates.list_templates()\n            if tmpl.find(\"/\") > -1\n        ] + self.pkg_dirs\n        for dir_path in dirs:\n            if not os.path.isdir(\n                os.path.join(self.tmpdir, dir_path.format(**self.pkg_data.to_dict()))\n            ):\n                os.makedirs(\n                    os.path.join(\n                        self.tmpdir, dir_path.format(**self.pkg_data.to_dict())\n                    ),\n                    self.exec_perm,\n                )\n\n    def _generate_docs(self):\n        docs_path = os.path.join(self.tmpdir, \"docs\")\n        os.makedirs(docs_path)\n        build_sphinx(self.pkg_data, docs_path)\n\n    def _list_module_dirs(self):\n        return [\n            dir_path\n            for dir_path in self.pkg_dirs\n            if dir_path.find(\"{module_name}\") == 0\n        ]\n\n    def _generate_init(self):\n        tmpl = self.templates.get_template(\"__init__.py.j2\")\n\n        for dir_path in self._list_module_dirs():\n            if not os.path.isfile(self._init_py(dir_path)):\n                with open(self._init_py(dir_path), \"w\") as fobj:\n                    # pylint: disable=no-member\n                    fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_file(self, file_path):\n        tmpl = self.templates.get_template(file_path)\n        with open(self._tmpl_path(file_path), \"w\") as fobj:\n            fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_exec_file(self, file_path):\n        self._generate_file(file_path)\n        os.chmod(self._tmpl_path(file_path), self.exec_perm)\n        return True\n\n    def _generate_samples(self, file_path):\n        if not self.pkg_data.with_samples:\n            return False\n        tmpl = self.templates.get_template(file_path)\n        if file_path == \"sample.py.j2\":\n            with open(self._sample_py(file_path), \"w\") as fobj:\n                fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        elif file_path == \"test_sample.py.j2\":\n            with open(self._sample_py(os.path.join(\"tests\", file_path)), \"w\") as fobj:\n                fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_files(self):\n        generator = lambda f: guard(\n            g(self._generate_init, f == \"__init__.py.j2\"),\n            g(self._generate_exec_file, f == \"utils/pre-commit.j2\", (f,)),\n            g(self._generate_samples, f.endswith(\"sample.py.j2\"), (f,)),\n            g(self._generate_file, params=(f,)),\n        )\n        for file_path in self.templates.list_templates():\n            generator(file_path)\n        os.chdir(self.tmpdir)\n        os.symlink(\"../../README.rst\", \"docs/source/README.rst\")\n        os.chdir(self.cwd)\n\n    def clean(self):\n        \"\"\"Clean up working directory.\"\"\"\n        shutil.rmtree(self.tmpdir)\n\n    def generate(self):\n        \"\"\"Generate package directory tree.\"\"\"\n        self._generate_docs()\n        self._generate_dirs()\n        self._generate_files()\n\n    def vcs_init(self):\n        \"\"\"Initialize VCS repository.\"\"\"\n        VCS(os.path.join(self.outdir, self.name), self.pkg_data)\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import os", "import shutil", "import tempfile", "from datetime import datetime", "from jinja2 import PackageLoader, Environment", "from pguard import guard", "from pguard import guard_cl as g", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.docs import build_sphinx"], "function": ["class PackageData:\n", "    def __init__(self, args):\n", "    def _set_param(self, name, value):\n", "    def _check_or_set_default_params(self):\n", "    def to_dict(self):\n", "class PackageTree:\n", "    def __init__(self, pkg_data):\n", "    def _init_py(self, dir_path):\n", "    def _sample_py(self, file_path):\n", "    def _tmpl_path(self, file_path):\n", "    def _generate_dirs(self):\n", "    def _generate_docs(self):\n", "    def _list_module_dirs(self):\n", "    def _generate_init(self):\n", "    def _generate_file(self, file_path):\n", "    def _generate_exec_file(self, file_path):\n", "    def _generate_samples(self, file_path):\n", "    def _generate_files(self):\n", "    def clean(self):\n", "    def generate(self):\n", "    def vcs_init(self):\n"]}
{"repo": "mkouhei/bootstrap-py", "path": "bootstrap_py/package.py", "func_name": "PackageTree.vcs_init", "original_string": "def vcs_init(self):\n        \"\"\"Initialize VCS repository.\"\"\"\n        VCS(os.path.join(self.outdir, self.name), self.pkg_data)", "language": "python", "code": "def vcs_init(self):\n        \"\"\"Initialize VCS repository.\"\"\"\n        VCS(os.path.join(self.outdir, self.name), self.pkg_data)", "code_tokens": ["def", "vcs_init", "(", "self", ")", ":", "VCS", "(", "os", ".", "path", ".", "join", "(", "self", ".", "outdir", ",", "self", ".", "name", ")", ",", "self", ".", "pkg_data", ")"], "docstring": "Initialize VCS repository.", "docstring_tokens": ["Initialize", "VCS", "repository", "."], "sha": "95d56ed98ef409fd9f019dc352fd1c3711533275", "url": "https://github.com/mkouhei/bootstrap-py/blob/95d56ed98ef409fd9f019dc352fd1c3711533275/bootstrap_py/package.py#L185-L187", "partition": "train", "up_fun_num": 21, "context": "# -*- coding: utf-8 -*-\n\"\"\"bootstrap_py.package.\"\"\"\nimport os\nimport shutil\nimport tempfile\nfrom datetime import datetime\nfrom jinja2 import PackageLoader, Environment\nfrom pguard import guard\nfrom pguard import guard_cl as g\nfrom bootstrap_py.classifiers import Classifiers\nfrom bootstrap_py.vcs import VCS\nfrom bootstrap_py.docs import build_sphinx\n\n\n# pylint: disable=too-few-public-methods\nclass PackageData:\n    \"\"\"Package meta data class.\"\"\"\n\n    #: Configured the default \"version\" of setup.setup().\n    default_version = \"0.1.0\"\n    #: Users should rewrite parameters after they generate Python package.\n    warning_message = \"##### ToDo: Rewrite me #####\"  # pylint: disable=fixme\n\n    def __init__(self, args):\n        \"\"\"Initialize Package.\"\"\"\n        self.metadata = Classifiers()\n        if hasattr(args, \"__dict__\"):\n            for name, value in vars(args).items():\n                self._set_param(name, value)\n        self._check_or_set_default_params()\n\n    def _set_param(self, name, value):\n        \"\"\"Set name:value property to Package object.\"\"\"\n        if name == \"status\":\n            setattr(self, name, self.metadata.status().get(value))\n        elif name == \"license\":\n            setattr(self, name, self.metadata.licenses().get(value))\n        elif name == \"name\":\n            setattr(self, name, value)\n            setattr(self, \"module_name\", value.replace(\"-\", \"_\"))\n        else:\n            setattr(self, name, value)\n\n    def _check_or_set_default_params(self):\n        \"\"\"Check key and set default vaule when it does not exists.\"\"\"\n        if not hasattr(self, \"date\"):\n            self._set_param(\"date\", datetime.utcnow().strftime(\"%Y-%m-%d\"))\n        if not hasattr(self, \"version\"):\n            self._set_param(\"version\", self.default_version)\n        # pylint: disable=no-member\n        if not hasattr(self, \"description\") or self.description is None:\n            getattr(self, \"_set_param\")(\"description\", self.warning_message)\n\n    def to_dict(self):\n        \"\"\"Convert the package data to dict.\"\"\"\n        return self.__dict__\n\n\nclass PackageTree:\n    \"\"\"Package directory tree class.\"\"\"\n\n    #: Jinja2 template name\n    template_name = \"bootstrap_py\"\n    #: the suffix name of working directory for generating\n    suffix = \"-bootstrap-py\"\n    #: init filename\n    init = \"__init__.py\"\n    #: default permission\n    exec_perm = 0o755\n    #: include directories to packages\n    pkg_dirs = [\"{module_name}\", \"{module_name}/tests\"]\n\n    def __init__(self, pkg_data):\n        \"\"\"Initialize.\"\"\"\n        self.cwd = os.getcwd()\n        self.name = pkg_data.name\n        self.outdir = os.path.abspath(pkg_data.outdir)\n        self.tmpdir = tempfile.mkdtemp(suffix=self.suffix)\n        self.templates = Environment(loader=PackageLoader(self.template_name))\n        self.pkg_data = pkg_data\n\n    def _init_py(self, dir_path):\n        return os.path.join(\n            self.tmpdir, dir_path.format(**self.pkg_data.to_dict()), self.init\n        )\n\n    def _sample_py(self, file_path):\n        return os.path.join(\n            self.tmpdir, self.pkg_data.module_name, os.path.splitext(file_path)[0]\n        )\n\n    def _tmpl_path(self, file_path):\n        return os.path.join(self.tmpdir, os.path.splitext(file_path)[0])\n\n    def _generate_dirs(self):\n        dirs = [\n            os.path.dirname(tmpl)\n            for tmpl in self.templates.list_templates()\n            if tmpl.find(\"/\") > -1\n        ] + self.pkg_dirs\n        for dir_path in dirs:\n            if not os.path.isdir(\n                os.path.join(self.tmpdir, dir_path.format(**self.pkg_data.to_dict()))\n            ):\n                os.makedirs(\n                    os.path.join(\n                        self.tmpdir, dir_path.format(**self.pkg_data.to_dict())\n                    ),\n                    self.exec_perm,\n                )\n\n    def _generate_docs(self):\n        docs_path = os.path.join(self.tmpdir, \"docs\")\n        os.makedirs(docs_path)\n        build_sphinx(self.pkg_data, docs_path)\n\n    def _list_module_dirs(self):\n        return [\n            dir_path\n            for dir_path in self.pkg_dirs\n            if dir_path.find(\"{module_name}\") == 0\n        ]\n\n    def _generate_init(self):\n        tmpl = self.templates.get_template(\"__init__.py.j2\")\n\n        for dir_path in self._list_module_dirs():\n            if not os.path.isfile(self._init_py(dir_path)):\n                with open(self._init_py(dir_path), \"w\") as fobj:\n                    # pylint: disable=no-member\n                    fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_file(self, file_path):\n        tmpl = self.templates.get_template(file_path)\n        with open(self._tmpl_path(file_path), \"w\") as fobj:\n            fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_exec_file(self, file_path):\n        self._generate_file(file_path)\n        os.chmod(self._tmpl_path(file_path), self.exec_perm)\n        return True\n\n    def _generate_samples(self, file_path):\n        if not self.pkg_data.with_samples:\n            return False\n        tmpl = self.templates.get_template(file_path)\n        if file_path == \"sample.py.j2\":\n            with open(self._sample_py(file_path), \"w\") as fobj:\n                fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        elif file_path == \"test_sample.py.j2\":\n            with open(self._sample_py(os.path.join(\"tests\", file_path)), \"w\") as fobj:\n                fobj.write(tmpl.render(**self.pkg_data.to_dict()) + \"\\n\")\n        return True\n\n    def _generate_files(self):\n        generator = lambda f: guard(\n            g(self._generate_init, f == \"__init__.py.j2\"),\n            g(self._generate_exec_file, f == \"utils/pre-commit.j2\", (f,)),\n            g(self._generate_samples, f.endswith(\"sample.py.j2\"), (f,)),\n            g(self._generate_file, params=(f,)),\n        )\n        for file_path in self.templates.list_templates():\n            generator(file_path)\n        os.chdir(self.tmpdir)\n        os.symlink(\"../../README.rst\", \"docs/source/README.rst\")\n        os.chdir(self.cwd)\n\n    def move(self):\n        \"\"\"Move directory from working directory to output directory.\"\"\"\n        if not os.path.isdir(self.outdir):\n            os.makedirs(self.outdir)\n        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))\n\n    def clean(self):\n        \"\"\"Clean up working directory.\"\"\"\n        shutil.rmtree(self.tmpdir)\n\n    def generate(self):\n        \"\"\"Generate package directory tree.\"\"\"\n        self._generate_docs()\n        self._generate_dirs()\n        self._generate_files()\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import os", "import shutil", "import tempfile", "from datetime import datetime", "from jinja2 import PackageLoader, Environment", "from pguard import guard", "from pguard import guard_cl as g", "from bootstrap_py.classifiers import Classifiers", "from bootstrap_py.vcs import VCS", "from bootstrap_py.docs import build_sphinx"], "function": ["class PackageData:\n", "    def __init__(self, args):\n", "    def _set_param(self, name, value):\n", "    def _check_or_set_default_params(self):\n", "    def to_dict(self):\n", "class PackageTree:\n", "    def __init__(self, pkg_data):\n", "    def _init_py(self, dir_path):\n", "    def _sample_py(self, file_path):\n", "    def _tmpl_path(self, file_path):\n", "    def _generate_dirs(self):\n", "    def _generate_docs(self):\n", "    def _list_module_dirs(self):\n", "    def _generate_init(self):\n", "    def _generate_file(self, file_path):\n", "    def _generate_exec_file(self, file_path):\n", "    def _generate_samples(self, file_path):\n", "    def _generate_files(self):\n", "    def move(self):\n", "    def clean(self):\n", "    def generate(self):\n"]}
{"repo": "scottrice/pysteam", "path": "pysteam/winutils.py", "func_name": "find_steam_location", "original_string": "def find_steam_location():\n  \"\"\"\n  Finds the location of the current Steam installation on Windows machines.\n  Returns None for any non-Windows machines, or for Windows machines where\n  Steam is not installed.\n  \"\"\"\n  if registry is None:\n    return None\n\n  key = registry.CreateKey(registry.HKEY_CURRENT_USER,\"Software\\Valve\\Steam\")\n  return registry.QueryValueEx(key,\"SteamPath\")[0]", "language": "python", "code": "def find_steam_location():\n  \"\"\"\n  Finds the location of the current Steam installation on Windows machines.\n  Returns None for any non-Windows machines, or for Windows machines where\n  Steam is not installed.\n  \"\"\"\n  if registry is None:\n    return None\n\n  key = registry.CreateKey(registry.HKEY_CURRENT_USER,\"Software\\Valve\\Steam\")\n  return registry.QueryValueEx(key,\"SteamPath\")[0]", "code_tokens": ["def", "find_steam_location", "(", ")", ":", "if", "registry", "is", "None", ":", "return", "None", "key", "=", "registry", ".", "CreateKey", "(", "registry", ".", "HKEY_CURRENT_USER", ",", "\"Software\\Valve\\Steam\"", ")", "return", "registry", ".", "QueryValueEx", "(", "key", ",", "\"SteamPath\"", ")", "[", "0", "]"], "docstring": "Finds the location of the current Steam installation on Windows machines.\n  Returns None for any non-Windows machines, or for Windows machines where\n  Steam is not installed.", "docstring_tokens": ["Finds", "the", "location", "of", "the", "current", "Steam", "installation", "on", "Windows", "machines", ".", "Returns", "None", "for", "any", "non", "-", "Windows", "machines", "or", "for", "Windows", "machines", "where", "Steam", "is", "not", "installed", "."], "sha": "1eb2254b5235a053a953e596fa7602d0b110245d", "url": "https://github.com/scottrice/pysteam/blob/1eb2254b5235a053a953e596fa7602d0b110245d/pysteam/winutils.py#L10-L20", "partition": "train", "up_fun_num": 0, "context": "# encoding: utf-8\n\nimport os\n\ntry:\n    import _winreg as registry\nexcept ImportError:\n    registry = None\n\n\ndef find_userdata_directory():\n    \"\"\"\n    Finds the location of the userdata directory on Windows machines. Returns\n    None for any non-Windows machines, or for Windows machines where Steam is\n    not installed.\n\n    Since on Windows the userdata directory is simply a subdirectory of the\n    installation directory, this function is really a thin wrapper over\n    `find_steam_location`\n    \"\"\"\n    install_location = find_steam_location()\n    if install_location is None:\n        return None\n\n    return os.path.join(install_location, \"userdata\")\n", "levels": [0], "package": ["import os", "import _winreg as registry"], "function": ["def find_userdata_directory():\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/PCoA_bubble.py", "func_name": "plot_PCoA", "original_string": "def plot_PCoA(cat_data, otu_name, unifrac, names, colors, xr, yr, outDir,\n              save_as, plot_style):\n    \"\"\"\n    Plot PCoA principal coordinates scaled by the relative abundances of\n    otu_name.\n    \"\"\"\n    fig = plt.figure(figsize=(14, 8))\n    ax = fig.add_subplot(111)\n\n    for i, cat in enumerate(cat_data):\n        plt.scatter(cat_data[cat][\"pc1\"], cat_data[cat][\"pc2\"], cat_data[cat][\"size\"],\n                    color=colors[cat], alpha=0.85, marker=\"o\", edgecolor=\"black\",\n                    label=cat)\n    lgnd = plt.legend(loc=\"best\", scatterpoints=3, fontsize=13)\n    for i in range(len(colors.keys())):\n        lgnd.legendHandles[i]._sizes = [80]  # Change the legend marker size manually\n    plt.title(\" \".join(otu_name.split(\"_\")), style=\"italic\")\n    plt.ylabel(\"PC2 (Percent Explained Variance {:.3f}%)\".format(float(unifrac[\"varexp\"][1])))\n    plt.xlabel(\"PC1 (Percent Explained Variance {:.3f}%)\".format(float(unifrac[\"varexp\"][0])))\n    plt.xlim(round(xr[0]*1.5, 1), round(xr[1]*1.5, 1))\n    plt.ylim(round(yr[0]*1.5, 1), round(yr[1]*1.5, 1))\n    if plot_style:\n        gu.ggplot2_style(ax)\n        fc = \"0.8\"\n    else:\n        fc = \"none\"\n    fig.savefig(os.path.join(outDir, \"_\".join(otu_name.split())) + \".\" + save_as,\n                facecolor=fc, edgecolor=\"none\", format=save_as,\n                bbox_inches=\"tight\", pad_inches=0.2)\n    plt.close(fig)", "language": "python", "code": "def plot_PCoA(cat_data, otu_name, unifrac, names, colors, xr, yr, outDir,\n              save_as, plot_style):\n    \"\"\"\n    Plot PCoA principal coordinates scaled by the relative abundances of\n    otu_name.\n    \"\"\"\n    fig = plt.figure(figsize=(14, 8))\n    ax = fig.add_subplot(111)\n\n    for i, cat in enumerate(cat_data):\n        plt.scatter(cat_data[cat][\"pc1\"], cat_data[cat][\"pc2\"], cat_data[cat][\"size\"],\n                    color=colors[cat], alpha=0.85, marker=\"o\", edgecolor=\"black\",\n                    label=cat)\n    lgnd = plt.legend(loc=\"best\", scatterpoints=3, fontsize=13)\n    for i in range(len(colors.keys())):\n        lgnd.legendHandles[i]._sizes = [80]  # Change the legend marker size manually\n    plt.title(\" \".join(otu_name.split(\"_\")), style=\"italic\")\n    plt.ylabel(\"PC2 (Percent Explained Variance {:.3f}%)\".format(float(unifrac[\"varexp\"][1])))\n    plt.xlabel(\"PC1 (Percent Explained Variance {:.3f}%)\".format(float(unifrac[\"varexp\"][0])))\n    plt.xlim(round(xr[0]*1.5, 1), round(xr[1]*1.5, 1))\n    plt.ylim(round(yr[0]*1.5, 1), round(yr[1]*1.5, 1))\n    if plot_style:\n        gu.ggplot2_style(ax)\n        fc = \"0.8\"\n    else:\n        fc = \"none\"\n    fig.savefig(os.path.join(outDir, \"_\".join(otu_name.split())) + \".\" + save_as,\n                facecolor=fc, edgecolor=\"none\", format=save_as,\n                bbox_inches=\"tight\", pad_inches=0.2)\n    plt.close(fig)", "code_tokens": ["def", "plot_PCoA", "(", "cat_data", ",", "otu_name", ",", "unifrac", ",", "names", ",", "colors", ",", "xr", ",", "yr", ",", "outDir", ",", "save_as", ",", "plot_style", ")", ":", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "14", ",", "8", ")", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "for", "i", ",", "cat", "in", "enumerate", "(", "cat_data", ")", ":", "plt", ".", "scatter", "(", "cat_data", "[", "cat", "]", "[", "\"pc1\"", "]", ",", "cat_data", "[", "cat", "]", "[", "\"pc2\"", "]", ",", "cat_data", "[", "cat", "]", "[", "\"size\"", "]", ",", "color", "=", "colors", "[", "cat", "]", ",", "alpha", "=", "0.85", ",", "marker", "=", "\"o\"", ",", "edgecolor", "=", "\"black\"", ",", "label", "=", "cat", ")", "lgnd", "=", "plt", ".", "legend", "(", "loc", "=", "\"best\"", ",", "scatterpoints", "=", "3", ",", "fontsize", "=", "13", ")", "for", "i", "in", "range", "(", "len", "(", "colors", ".", "keys", "(", ")", ")", ")", ":", "lgnd", ".", "legendHandles", "[", "i", "]", ".", "_sizes", "=", "[", "80", "]", "# Change the legend marker size manually", "plt", ".", "title", "(", "\" \"", ".", "join", "(", "otu_name", ".", "split", "(", "\"_\"", ")", ")", ",", "style", "=", "\"italic\"", ")", "plt", ".", "ylabel", "(", "\"PC2 (Percent Explained Variance {:.3f}%)\"", ".", "format", "(", "float", "(", "unifrac", "[", "\"varexp\"", "]", "[", "1", "]", ")", ")", ")", "plt", ".", "xlabel", "(", "\"PC1 (Percent Explained Variance {:.3f}%)\"", ".", "format", "(", "float", "(", "unifrac", "[", "\"varexp\"", "]", "[", "0", "]", ")", ")", ")", "plt", ".", "xlim", "(", "round", "(", "xr", "[", "0", "]", "*", "1.5", ",", "1", ")", ",", "round", "(", "xr", "[", "1", "]", "*", "1.5", ",", "1", ")", ")", "plt", ".", "ylim", "(", "round", "(", "yr", "[", "0", "]", "*", "1.5", ",", "1", ")", ",", "round", "(", "yr", "[", "1", "]", "*", "1.5", ",", "1", ")", ")", "if", "plot_style", ":", "gu", ".", "ggplot2_style", "(", "ax", ")", "fc", "=", "\"0.8\"", "else", ":", "fc", "=", "\"none\"", "fig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "outDir", ",", "\"_\"", ".", "join", "(", "otu_name", ".", "split", "(", ")", ")", ")", "+", "\".\"", "+", "save_as", ",", "facecolor", "=", "fc", ",", "edgecolor", "=", "\"none\"", ",", "format", "=", "save_as", ",", "bbox_inches", "=", "\"tight\"", ",", "pad_inches", "=", "0.2", ")", "plt", ".", "close", "(", "fig", ")"], "docstring": "Plot PCoA principal coordinates scaled by the relative abundances of\n    otu_name.", "docstring_tokens": ["Plot", "PCoA", "principal", "coordinates", "scaled", "by", "the", "relative", "abundances", "of", "otu_name", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/PCoA_bubble.py#L36-L65", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python\n\"\"\"\nCreate a series of PCoA plots where the marker size varies by relative\nabundance of a particular OTU\n\nAuthor: Shareef M Dabdoub\n\"\"\"\nfrom __future__ import division\nimport os\nimport sys\nimport biom\nimport argparse\n\ntry:\n    import matplotlib.pyplot as plt\nexcept ImportError as ie:\n    sys.exit(\"Import Error. Please install missing module: {}\".format(ie))\nfrom phylotoast import util, graph_util as gu, biom_calc as bc, otu_calc as oc\n\n\ndef calculate_xy_range(data):\n    xr = [float(\"inf\"), float(\"-inf\")]\n    yr = [float(\"inf\"), float(\"-inf\")]\n\n    for cat in data:\n        pc1, pc2 = data[cat][\"pc1\"], data[cat][\"pc2\"]\n        if pc1:\n            xr[0] = min(min(pc1), xr[0])\n            xr[1] = max(max(pc1), xr[1])\n        if pc2:\n            yr[0] = min(min(pc2), yr[0])\n            yr[1] = max(max(pc2), yr[1])\n\n    return xr, yr\n\n\ndef handle_program_options():\n    parser = argparse.ArgumentParser(\n        description=\"Create a series of Principal\\\n                                     Coordinate plots for each OTU in an \\\n                                     input list where the plot points are \\\n                                     varied in size by the relative abundance \\\n                                     of the OTU relative to either Sample or\\\n                                     the total contribution of the OTU to the \\\n                                     data set.\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--otu_table\",\n        required=True,\n        help=\"The biom-format file with OTU-Sample abundance \\\n                              data.\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--mapping\",\n        required=True,\n        help=\"The mapping file specifying group information \\\n                              for each sample.\",\n    )\n    parser.add_argument(\n        \"-pc\",\n        \"--pcoa_fp\",\n        required=True,\n        help=\"Principal Coordinates Analysis file. \\\n                              Eg. unweighted_unifrac_pc.txt, or any other\\\n                              output from principal_coordinates.py.\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--group_by\",\n        required=True,\n        help=\"Column name in mapping file specifying group\\\n                              information.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--colors\",\n        default=None,\n        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\",\n    )\n    parser.add_argument(\n        \"-ids\",\n        \"--otu_ids_fp\",\n        required=True,\n        help=\"Path to a file containing one OTU ID per line.\\\n                              One plot will be created for each OTU.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        default=\".\",\n        help=\"The directory to output the PCoA plots to.\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--save_as\",\n        default=\"svg\",\n        help=\"The type of image file for PCoA plots. By\\\n                              default, files will be saved in SVG format.\",\n    )\n    parser.add_argument(\n        \"--scale_by\",\n        default=1000,\n        type=float,\n        help=\"Species relative abundance is multiplied by this \\\n                              factor in order to make appropriate visible \\\n                              bubbles in the output plots. Default is 1000.\",\n    )\n    parser.add_argument(\n        \"--ggplot2_style\",\n        action=\"store_true\",\n        help=\"Apply ggplot2 styling to the figure.\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Displays species name as each is being plotted \\\n                              and stored to disk.\",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    try:\n        with open(args.otu_table):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with BIOM format file:{}\\n\".format(ioe))\n\n    try:\n        with open(args.pcoa_fp):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with principal coordinates file:{}\\n\".format(ioe))\n\n    try:\n        with open(args.mapping):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with mapping file:{}\\n\".format(ioe))\n\n    # check that the output dir exists, create it if not\n    util.ensure_dir(args.output_dir)\n\n    # load the BIOM table\n    biomtbl = biom.load_table(args.otu_table)\n\n    # Read unifrac principal coordinates file\n    unifrac = util.parse_unifrac(args.pcoa_fp)\n\n    # Read otu data file\n    otus = set()\n    with open(args.otu_ids_fp, \"rU\") as nciF:\n        for line in nciF.readlines():\n            line = line.strip()\n            otus.add(line)\n\n    # Gather categories from mapping file\n    header, imap = util.parse_map_file(args.mapping)\n    try:\n        category_idx = header.index(args.group_by)\n    except ValueError:\n        msg = \"Error: Specified mapping category '{}' not found.\"\n        sys.exit(msg.format(args.group_by))\n    category_ids = util.gather_categories(imap, header, [args.group_by])\n    color_map = util.color_mapping(imap, header, args.group_by, args.colors)\n    rel_abd = bc.relative_abundance(biomtbl)\n    rel_abd = bc.arcsine_sqrt_transform(rel_abd)\n\n    # plot samples based on relative abundance of some OTU ID\n    for otuid in otus:\n        otuname = oc.otu_name(biomtbl.metadata(otuid, axis=\"observation\")[\"taxonomy\"])\n        cat_data = {cat: {\"pc1\": [], \"pc2\": [], \"size\": []} for cat in category_ids}\n\n        for sid in unifrac[\"pcd\"]:\n            category = cat_data[imap[sid][category_idx]]\n            try:\n                size = rel_abd[sid][otuid] * args.scale_by\n            except KeyError as ke:\n                print(\"{} not found in {} sample.\".format(ke, sid))\n                continue\n            category[\"pc1\"].append(float(unifrac[\"pcd\"][sid][0]))\n            category[\"pc2\"].append(float(unifrac[\"pcd\"][sid][1]))\n            category[\"size\"].append(size)\n\n        if args.verbose:\n            print(\"Saving chart for {}\".format(\" \".join(otuname.split(\"_\"))))\n        xr, yr = calculate_xy_range(cat_data)\n        plot_PCoA(\n            cat_data,\n            otuname,\n            unifrac,\n            color_map.keys(),\n            color_map,\n            xr,\n            yr,\n            args.output_dir,\n            args.save_as,\n            args.ggplot2_style,\n        )\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "levels": [0, 0, 0], "package": ["from __future__ import division", "import os", "import sys", "import biom", "import argparse", "import matplotlib.pyplot as plt", "from phylotoast import util, graph_util as gu, biom_calc as bc, otu_calc as oc"], "function": ["def calculate_xy_range(data):\n", "def handle_program_options():\n", "def main():\n"]}
{"repo": "smdabdoub/phylotoast", "path": "bin/transpose_biom.py", "func_name": "split_by_category", "original_string": "def split_by_category(biom_cols, mapping, category_id):\n    \"\"\"\n    Split up the column data in a biom table by mapping category value.\n    \"\"\"\n    columns = defaultdict(list)\n    for i, col in enumerate(biom_cols):\n        columns[mapping[col['id']][category_id]].append((i, col))\n\n    return columns", "language": "python", "code": "def split_by_category(biom_cols, mapping, category_id):\n    \"\"\"\n    Split up the column data in a biom table by mapping category value.\n    \"\"\"\n    columns = defaultdict(list)\n    for i, col in enumerate(biom_cols):\n        columns[mapping[col['id']][category_id]].append((i, col))\n\n    return columns", "code_tokens": ["def", "split_by_category", "(", "biom_cols", ",", "mapping", ",", "category_id", ")", ":", "columns", "=", "defaultdict", "(", "list", ")", "for", "i", ",", "col", "in", "enumerate", "(", "biom_cols", ")", ":", "columns", "[", "mapping", "[", "col", "[", "'id'", "]", "]", "[", "category_id", "]", "]", ".", "append", "(", "(", "i", ",", "col", ")", ")", "return", "columns"], "docstring": "Split up the column data in a biom table by mapping category value.", "docstring_tokens": ["Split", "up", "the", "column", "data", "in", "a", "biom", "table", "by", "mapping", "category", "value", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/transpose_biom.py#L17-L25", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\n\"\"\"\nCreated on Jul 1, 2013\n\nAuthor: Shareef Dabdoub\n\"\"\"\nimport argparse\nimport copy\nimport json\nimport os.path as osp\nimport sys\nfrom collections import defaultdict\nfrom phylotoast import util\n\n\ndef handle_program_options():\n    parser = argparse.ArgumentParser(\n        description=\"Transpose a BIOM-format file\\\n                                     so that the matrix is sample by species.\"\n    )\n    parser.add_argument(\n        \"-i\", \"--input_biom_fp\", required=True, help=\"The BIOM-format file.\"\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--mapping\",\n        required=True,\n        help=\"The mapping file specifying group information \\\n                              for each sample.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--map_category\",\n        default=None,\n        help=\"A mapping category, such as TreatmentType, \\\n                              that will be used to split the data into \\\n                              separate BIOM files; one for each value found\\\n                              in the category.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_biom_fp\",\n        default=\"transposed.biom\",\n        required=True,\n        help=\"The BIOM-format file to write.\",\n    )\n\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n\n    return parser.parse_args()\n\n\ndef main():\n    args = handle_program_options()\n\n    try:\n        with open(args.input_biom_fp):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with input BIOM-format file:{}\\n\".format(ioe))\n\n    try:\n        with open(args.mapping):\n            pass\n    except IOError as ioe:\n        sys.exit(\"\\nError with mapping file:{}\\n\".format(ioe))\n\n    out_fp, ext = osp.splitext(args.output_biom_fp)\n\n    with open(args.input_biom_fp) as bF:\n        biom = json.loads(bF.readline())\n\n    header, mapping = util.parse_map_file(args.mapping)\n\n    try:\n        category_id = header.index(args.map_category)\n    except ValueError:\n        sys.exit(\n            \"Category {} not found in supplied mapping file.\".format(args.map_category)\n        )\n\n    values = {mapping[sid][category_id] for sid in mapping}\n    biom_copies = {value: copy.deepcopy(biom) for value in values}\n    split_samples = split_by_category(biom[\"columns\"], mapping, category_id)\n    for cat_val in biom_copies:\n        biom_copies[cat_val][\"data\"] = []\n        biom_copies[cat_val][\"rows\"], biom_copies[cat_val][\"columns\"] = [\n            item[1] for item in split_samples[cat_val]\n        ], biom_copies[cat_val][\"rows\"]\n        sample_ids = [item[0] for item in split_samples[cat_val]]\n\n        for i in xrange(len(biom[\"data\"])):\n            if biom[\"data\"][i][1] in sample_ids:\n                row, col, amt = biom[\"data\"][i]\n                biom_copies[cat_val][\"data\"].append([sample_ids.index(col), row, amt])\n\n        biom_copies[cat_val][\"shape\"] = [\n            len(biom_copies[cat_val][\"rows\"]),\n            len(biom_copies[cat_val][\"columns\"]),\n        ]\n\n        with open(out_fp + \"_\" + cat_val + ext, \"w\") as outF:\n            outF.write(json.dumps(biom_copies[cat_val]))\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 0], "package": ["import argparse", "import copy", "import json", "import os.path as osp", "import sys", "from collections import defaultdict", "from phylotoast import util"], "function": ["def handle_program_options():\n", "def main():\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/stockholm2oneline.py", "func_name": "print_line", "original_string": "def print_line(l):\n    \"\"\"\n    print line if starts with ...\n    \"\"\"\n    print_lines = ['# STOCKHOLM', '#=GF', '#=GS', ' ']\n    if len(l.split()) == 0:\n        return True\n    for start in print_lines:\n        if l.startswith(start):\n            return True\n    return False", "language": "python", "code": "def print_line(l):\n    \"\"\"\n    print line if starts with ...\n    \"\"\"\n    print_lines = ['# STOCKHOLM', '#=GF', '#=GS', ' ']\n    if len(l.split()) == 0:\n        return True\n    for start in print_lines:\n        if l.startswith(start):\n            return True\n    return False", "code_tokens": ["def", "print_line", "(", "l", ")", ":", "print_lines", "=", "[", "'# STOCKHOLM'", ",", "'#=GF'", ",", "'#=GS'", ",", "' '", "]", "if", "len", "(", "l", ".", "split", "(", ")", ")", "==", "0", ":", "return", "True", "for", "start", "in", "print_lines", ":", "if", "l", ".", "startswith", "(", "start", ")", ":", "return", "True", "return", "False"], "docstring": "print line if starts with ...", "docstring_tokens": ["print", "line", "if", "starts", "with", "..."], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/stockholm2oneline.py#L11-L21", "partition": "train", "up_fun_num": 0, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for converting a stockholm formatted alignment to single line\n\"\"\"\n\nimport os\nimport re\nimport sys\n\n\ndef stock2one(stock):\n    \"\"\"\n    convert stockholm to single line format\n    \"\"\"\n    lines = {}\n    for line in stock:\n        line = line.strip()\n        if print_line(line) is True:\n            yield line\n            continue\n        if line.startswith(\"//\"):\n            continue\n        ID, seq = line.rsplit(\" \", 1)\n        if ID not in lines:\n            lines[ID] = \"\"\n        else:\n            # remove preceding white space\n            seq = seq.strip()\n        lines[ID] += seq\n    for ID, line in lines.items():\n        yield \"\\t\".join([ID, line])\n    yield \"\\n//\"\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"convert to single line stockholm formatted alignment\")\n        exit()\n    stock = sys.argv[1]\n    if stock == \"-\":\n        stock = sys.stdin\n    else:\n        stock = open(stock)\n    for line in stock2one(stock):\n        print(line)\n", "levels": [0], "package": ["import os", "import re", "import sys"], "function": ["def stock2one(stock):\n"]}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/stockholm2oneline.py", "func_name": "stock2one", "original_string": "def stock2one(stock):\n    \"\"\"\n    convert stockholm to single line format\n    \"\"\"\n    lines = {}\n    for line in stock:\n        line = line.strip()\n        if print_line(line) is True:\n            yield line\n            continue\n        if line.startswith('//'):\n            continue\n        ID, seq = line.rsplit(' ', 1)\n        if ID not in lines:\n            lines[ID] = ''\n        else:\n            # remove preceding white space\n            seq = seq.strip()\n        lines[ID] += seq\n    for ID, line in lines.items():\n        yield '\\t'.join([ID, line])\n    yield '\\n//'", "language": "python", "code": "def stock2one(stock):\n    \"\"\"\n    convert stockholm to single line format\n    \"\"\"\n    lines = {}\n    for line in stock:\n        line = line.strip()\n        if print_line(line) is True:\n            yield line\n            continue\n        if line.startswith('//'):\n            continue\n        ID, seq = line.rsplit(' ', 1)\n        if ID not in lines:\n            lines[ID] = ''\n        else:\n            # remove preceding white space\n            seq = seq.strip()\n        lines[ID] += seq\n    for ID, line in lines.items():\n        yield '\\t'.join([ID, line])\n    yield '\\n//'", "code_tokens": ["def", "stock2one", "(", "stock", ")", ":", "lines", "=", "{", "}", "for", "line", "in", "stock", ":", "line", "=", "line", ".", "strip", "(", ")", "if", "print_line", "(", "line", ")", "is", "True", ":", "yield", "line", "continue", "if", "line", ".", "startswith", "(", "'//'", ")", ":", "continue", "ID", ",", "seq", "=", "line", ".", "rsplit", "(", "' '", ",", "1", ")", "if", "ID", "not", "in", "lines", ":", "lines", "[", "ID", "]", "=", "''", "else", ":", "# remove preceding white space", "seq", "=", "seq", ".", "strip", "(", ")", "lines", "[", "ID", "]", "+=", "seq", "for", "ID", ",", "line", "in", "lines", ".", "items", "(", ")", ":", "yield", "'\\t'", ".", "join", "(", "[", "ID", ",", "line", "]", ")", "yield", "'\\n//'"], "docstring": "convert stockholm to single line format", "docstring_tokens": ["convert", "stockholm", "to", "single", "line", "format"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/stockholm2oneline.py#L23-L44", "partition": "train", "up_fun_num": 1, "context": "#!/usr/bin/env python3\n\n\"\"\"\nscript for converting a stockholm formatted alignment to single line\n\"\"\"\n\nimport os\nimport re\nimport sys\n\n\ndef print_line(l):\n    \"\"\"\n    print line if starts with ...\n    \"\"\"\n    print_lines = [\"# STOCKHOLM\", \"#=GF\", \"#=GS\", \" \"]\n    if len(l.split()) == 0:\n        return True\n    for start in print_lines:\n        if l.startswith(start):\n            return True\n    return False\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"convert to single line stockholm formatted alignment\")\n        exit()\n    stock = sys.argv[1]\n    if stock == \"-\":\n        stock = sys.stdin\n    else:\n        stock = open(stock)\n    for line in stock2one(stock):\n        print(line)\n", "levels": [0], "package": ["import os", "import re", "import sys"], "function": ["def print_line(l):\n"]}
{"repo": "elbow-jason/Uno-deprecated", "path": "uno/helpers.py", "func_name": "math_func", "original_string": "def math_func(f):\n    \"\"\"\n    Statics the methods. wut.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if len(args) > 0:\n            return_type = type(args[0])\n        if kwargs.has_key('return_type'):\n            return_type = kwargs['return_type']\n            kwargs.pop('return_type')\n            return return_type(f(*args, **kwargs))\n        args = list((setify(x) for x in args))\n        return return_type(f(*args, **kwargs))\n    return wrapper", "language": "python", "code": "def math_func(f):\n    \"\"\"\n    Statics the methods. wut.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if len(args) > 0:\n            return_type = type(args[0])\n        if kwargs.has_key('return_type'):\n            return_type = kwargs['return_type']\n            kwargs.pop('return_type')\n            return return_type(f(*args, **kwargs))\n        args = list((setify(x) for x in args))\n        return return_type(f(*args, **kwargs))\n    return wrapper", "code_tokens": ["def", "math_func", "(", "f", ")", ":", "@", "wraps", "(", "f", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "len", "(", "args", ")", ">", "0", ":", "return_type", "=", "type", "(", "args", "[", "0", "]", ")", "if", "kwargs", ".", "has_key", "(", "'return_type'", ")", ":", "return_type", "=", "kwargs", "[", "'return_type'", "]", "kwargs", ".", "pop", "(", "'return_type'", ")", "return", "return_type", "(", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ")", "args", "=", "list", "(", "(", "setify", "(", "x", ")", "for", "x", "in", "args", ")", ")", "return", "return_type", "(", "f", "(", "*", "args", ",", "*", "*", "kwargs", ")", ")", "return", "wrapper"], "docstring": "Statics the methods. wut.", "docstring_tokens": ["Statics", "the", "methods", ".", "wut", "."], "sha": "4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4", "url": "https://github.com/elbow-jason/Uno-deprecated/blob/4ad07d7b84e5b6e3e2b2c89db69448906f24b4e4/uno/helpers.py#L8-L22", "partition": "train", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n\nimport itertools\nfrom functools import wraps\nfrom collections import Iterable\n\n\ndef listify(i):\n    \"\"\"\n    Iterable to list.\n    \"\"\"\n    return list(i)\n\n\ndef tuplify(i):\n    \"\"\"\n    Iterable to tuple.\n    \"\"\"\n    return tuple(i)\n\n\ndef setify(i):\n    \"\"\"\n    Iterable to set.\n    \"\"\"\n    return set(i)\n\n\n@math_func\ndef intersection(a, b):\n    \"\"\"\n    Returns the intersection of sets a and b.\n\n    In plain english:\n        Returns all the items that are in both a and b.\n    \"\"\"\n    return a.intersection(b)\n\n\n@math_func\ndef union(a, b):\n    \"\"\"\n    Returns the union of sets a and b.\n\n    In plain english:\n        Returns all the items of a and b combined with duplications removed.\n    \"\"\"\n    return a.union(b)\n\n\n@math_func\ndef minus(a, b):\n    \"\"\"\n    Returns the assymetrical difference of set 'a' to set 'b' (a minus b).\n\n    In plain english:\n        Remove all the items in 'a' from 'b'. Return 'a'. (Order matters.)\n\n    Minus is set_a.difference(set_b). The nomenclature 'difference\n    is not linguistically descriptive (at least to a layman) so the\n    method 'minus' was used, as the meaning of 'minus' conveys the\n    result of the function more properly (once again... at least to\n    the layman).\n    \"\"\"\n    return a.difference(b)\n\n\n@math_func\ndef difference(a, b):\n    \"\"\"\n    Returns the symmetric difference of sets 'a' and 'b'.\n\n    In plain english:\n        Removes all items that occur in both 'a' and 'b'\n\n    Difference is actually set_a.symmetric_difference(set_b), not\n    set.difference(). See 'minus' for set.difference().\n    \"\"\"\n    return a.symmetric_difference(b)\n\n\ndef flatten(l):\n    return list(chain.from_iterable(l))\n\n\ndef combine_dicts(a, b):\n    return dict(a.items() + b.items())\n\n\ndef namestr(obj, namespace):\n    \"\"\"\n    called via:\n        >>> a = 'some var'\n        >>> b = namestr(a, globals())\n        assert b == ['a'] #for test\n    \"\"\"\n    return [name for name in namespace if namespace[name] is obj]\n\n\ndef get_all_parent_methods(cls):\n    num_of_dicts = len(cls.mro())\n    method_dict = {}\n    index_list = list(xrange(num_of_dicts))\n    index_list.reverse()\n    for i in index_list:\n        method_dict = dict(method_dict.items() + cls.mro()[i].__dict__.items())\n    return method_dict\n\n\ndef remove_double_underscores(dictthing):\n    for key in dictthing.keys():\n        if key.startswith(\"__\"):\n            del dictthing[key]\n    return dictthing\n\n\ndef startswith_underscore(dictthing):\n    return [x for x in dictthing.keys() if x.startswith(\"_\")]\n\n\ndef startswith_double_underscore(dictthing):\n    return [x for x in dictthing.keys() if x.startswith(\"__\")]\n\n\ndef subtract_lists(a, b):\n    return list(set(a) - set(b))\n\n\ndef subtract_dicts(a, b):\n    a_b = subtract_lists(a.keys(), b.keys())\n    new_dict = {}\n    for i in a_b:\n        new_dict[i] = a[i]\n    return new_dict\n\n\ndef remove_list_items_from_dict(list_items, dictthing):\n    for x in list_items:\n        try:\n            del dictthing[x]\n        except:\n            pass\n    return dictthing\n\n\ndef set_kwargs(obj, kwargs):\n    if kwargs is not None:\n        for key, val in kwargs:\n            setattr(obj, key, val)\n\n\ndef set_args_blank(obj, args):\n    if args is not None:\n        for arg in args:\n            setattr(obj, arg, \"\")\n\n\ndef bi_tuple_to_dict(tup):\n    return dict((y, x) for x, y in tup)\n\n\ndef remove_newlines(string):\n    return string.replace(\"\\n\", \"\")\n\n\ndef joiner(sep, listthing):\n    return sep.join([str(x) for x in listthing])\n\n\ndef join_under(listthing):\n    return joiner(\"_\", listthing)\n\n\ndef join_dot(listthing):\n    return joiner(\".\", listthing)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import itertools", "from functools import wraps", "from collections import Iterable"], "function": ["def listify(i):\n", "def tuplify(i):\n", "def setify(i):\n", "def intersection(a, b):\n", "def union(a, b):\n", "def minus(a, b):\n", "def difference(a, b):\n", "def flatten(l):\n", "def combine_dicts(a, b):\n", "def namestr(obj, namespace):\n", "def get_all_parent_methods(cls):\n", "def remove_double_underscores(dictthing):\n", "def startswith_underscore(dictthing):\n", "def startswith_double_underscore(dictthing):\n", "def subtract_lists(a, b):\n", "def subtract_dicts(a, b):\n", "def remove_list_items_from_dict(list_items, dictthing):\n", "def set_kwargs(obj, kwargs):\n", "def set_args_blank(obj, args):\n", "def bi_tuple_to_dict(tup):\n", "def remove_newlines(string):\n", "def joiner(sep, listthing):\n", "def join_under(listthing):\n", "def join_dot(listthing):\n"]}
