{"repo": "soimort/you-get", "path": "src/you_get/extractors/miomio.py", "func_name": "sina_xml_to_url_list", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "str->list\n    Convert XML to URL List.\n    From Biligrab.", "docstring_tokens": ["str", "-", ">", "list", "Convert", "XML", "to", "URL", "List", ".", "From", "Biligrab", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51", "partition": "test", "up_fun_num": 1, "context": "#!/usr/bin/env python\n\n__all__ = [\"miomio_download\"]\n\nfrom ..common import *\n\nfrom .tudou import tudou_download_by_id\nfrom .youku import youku_download_by_vid\nfrom xml.dom.minidom import parseString\n\n\ndef miomio_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n    html = get_html(url)\n\n    title = r1(r'<meta name=\"description\" content=\"([^\"]*)\"', html)\n    flashvars = r1(r'flashvars=\"(type=[^\"]*)\"', html)\n\n    t = r1(r\"type=(\\w+)\", flashvars)\n    id = r1(r'vid=([^\"]+)', flashvars)\n    if t == \"youku\":\n        youku_download_by_vid(\n            id, title=title, output_dir=output_dir, merge=merge, info_only=info_only\n        )\n    elif t == \"tudou\":\n        tudou_download_by_id(\n            id, title, output_dir=output_dir, merge=merge, info_only=info_only\n        )\n    elif t == \"sina\" or t == \"video\":\n        fake_headers[\"Referer\"] = url\n        url = \"http://www.miomio.tv/mioplayer/mioplayerconfigfiles/sina.php?vid=\" + id\n        xml_data = get_content(url, headers=fake_headers, decoded=True)\n        url_list = sina_xml_to_url_list(xml_data)\n\n        size_full = 0\n        for url in url_list:\n            type_, ext, size = url_info(url, headers=fake_headers)\n            size_full += size\n\n        print_info(site_info, title, type_, size_full)\n        if not info_only:\n            download_urls(\n                url_list,\n                title,\n                ext,\n                total_size=size_full,\n                output_dir=output_dir,\n                merge=merge,\n                headers=fake_headers,\n            )\n    else:\n        raise NotImplementedError(flashvars)\n\n\n# ----------------------------------------------------------------------\n\nsite_info = \"MioMio.tv\"\ndownload = miomio_download\ndownload_playlist = playlist_not_supported(\"miomio\")\n", "levels": [0], "package": ["from ..common import *", "from .tudou import tudou_download_by_id", "from .youku import youku_download_by_vid", "from xml.dom.minidom import parseString"], "function": ["def miomio_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/dailymotion.py", "func_name": "dailymotion_download", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Downloads Dailymotion videos by URL.", "docstring_tokens": ["Downloads", "Dailymotion", "videos", "by", "URL", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35", "partition": "test", "up_fun_num": 1, "context": "#!/usr/bin/env python\n\n__all__ = [\"dailymotion_download\"]\n\nfrom ..common import *\nimport urllib.parse\n\n\ndef rebuilt_url(url):\n    path = urllib.parse.urlparse(url).path\n    aid = path.split(\"/\")[-1].split(\"_\")[0]\n    return \"http://www.dailymotion.com/embed/video/{}?autoplay=1\".format(aid)\n\n\nsite_info = \"Dailymotion.com\"\ndownload = dailymotion_download\ndownload_playlist = playlist_not_supported(\"dailymotion\")\n", "levels": [0], "package": ["from ..common import *", "import urllib.parse"], "function": ["def rebuilt_url(url):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/sina.py", "func_name": "sina_download", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Downloads Sina videos by URL.", "docstring_tokens": ["Downloads", "Sina", "videos", "by", "URL", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121", "partition": "test", "up_fun_num": 5, "context": "#!/usr/bin/env python\n\n__all__ = [\"sina_download\", \"sina_download_by_vid\", \"sina_download_by_vkey\"]\n\nfrom ..common import *\nfrom ..util.log import *\n\nfrom hashlib import md5\nfrom random import randint\nfrom time import time\nfrom xml.dom.minidom import parseString\nimport urllib.parse\n\n\ndef api_req(vid):\n    rand = \"0.{0}{1}\".format(randint(10000, 10000000), randint(10000, 10000000))\n    t = str(int(\"{0:b}\".format(int(time()))[:-6], 2))\n    k = (\n        md5((vid + \"Z6prk18aWxP278cVAH\" + t + rand).encode(\"utf-8\")).hexdigest()[:16]\n        + t\n    )\n    url = \"http://ask.ivideo.sina.com.cn/v_play.php?vid={0}&ran={1}&p=i&k={2}\".format(\n        vid, rand, k\n    )\n    xml = get_content(url, headers=fake_headers)\n    return xml\n\n\ndef video_info(xml):\n    video = parseString(xml).getElementsByTagName(\"video\")[0]\n    result = video.getElementsByTagName(\"result\")[0]\n    if result.firstChild.nodeValue == \"error\":\n        message = video.getElementsByTagName(\"message\")[0]\n        return None, message.firstChild.nodeValue, None\n    vname = video.getElementsByTagName(\"vname\")[0].firstChild.nodeValue\n    durls = video.getElementsByTagName(\"durl\")\n\n    urls = []\n    size = 0\n    for durl in durls:\n        url = durl.getElementsByTagName(\"url\")[0].firstChild.nodeValue\n        seg_size = durl.getElementsByTagName(\"filesize\")[0].firstChild.nodeValue\n        urls.append(url)\n        size += int(seg_size)\n\n    return urls, vname, size\n\n\ndef sina_download_by_vid(vid, title=None, output_dir=\".\", merge=True, info_only=False):\n    \"\"\"Downloads a Sina video by its unique vid.\n    http://video.sina.com.cn/\n    \"\"\"\n    xml = api_req(vid)\n    urls, name, size = video_info(xml)\n    if urls is None:\n        log.wtf(name)\n    title = name\n    print_info(site_info, title, \"flv\", size)\n    if not info_only:\n        download_urls(urls, title, \"flv\", size, output_dir=output_dir, merge=merge)\n\n\ndef sina_download_by_vkey(\n    vkey, title=None, output_dir=\".\", merge=True, info_only=False\n):\n    \"\"\"Downloads a Sina video by its unique vkey.\n    http://video.sina.com/\n    \"\"\"\n\n    url = \"http://video.sina.com/v/flvideo/%s_0.flv\" % vkey\n    type, ext, size = url_info(url)\n\n    print_info(site_info, title, \"flv\", size)\n    if not info_only:\n        download_urls([url], title, \"flv\", size, output_dir=output_dir, merge=merge)\n\n\ndef sina_zxt(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n    ep = \"http://s.video.sina.com.cn/video/play?video_id=\"\n    frag = urllib.parse.urlparse(url).fragment\n    if not frag:\n        log.wtf(\"No video specified with fragment\")\n    meta = json.loads(get_content(ep + frag))\n    if meta[\"code\"] != 1:\n        # Yes they use 1 for success.\n        log.wtf(meta[\"message\"])\n    title = meta[\"data\"][\"title\"]\n    videos = sorted(meta[\"data\"][\"videos\"], key=lambda i: int(i[\"size\"]))\n\n    if len(videos) == 0:\n        log.wtf(\"No video file returned by API server\")\n\n    vid = videos[-1][\"file_id\"]\n    container = videos[-1][\"type\"]\n    size = int(videos[-1][\"size\"])\n\n    if container == \"hlv\":\n        container = \"flv\"\n\n    urls, _, _ = video_info(api_req(vid))\n    print_info(site_info, title, container, size)\n    if not info_only:\n        download_urls(\n            urls, title, container, size, output_dir=output_dir, merge=merge, **kwargs\n        )\n    return\n\n\nsite_info = \"Sina.com\"\ndownload = sina_download\ndownload_playlist = playlist_not_supported(\"sina\")\n", "levels": [0, 0, 0, 0], "package": ["from ..common import *", "from ..util.log import *", "from hashlib import md5", "from random import randint", "from time import time", "from xml.dom.minidom import parseString", "import urllib.parse"], "function": ["def api_req(vid):\n", "def video_info(xml):\n", "def sina_download_by_vid(vid, title=None, output_dir=\".\", merge=True, info_only=False):\n", "def sina_zxt(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "sprint", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Format text with color or other effects into ANSI escaped string.", "docstring_tokens": ["Format", "text", "with", "color", "or", "other", "effects", "into", "ANSI", "escaped", "string", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n# This file is Python 2 compliant.\n\nfrom ..version import script_name\n\nimport os, sys\n\nTERM = os.getenv(\"TERM\", \"\")\nIS_ANSI_TERMINAL = (\n    TERM\n    in (\n        \"eterm-color\",\n        \"linux\",\n        \"screen\",\n        \"vt100\",\n    )\n    or TERM.startswith(\"xterm\")\n)\n\n# ANSI escape code\n# See <http://en.wikipedia.org/wiki/ANSI_escape_code>\nRESET = 0\nBOLD = 1\nUNDERLINE = 4\nNEGATIVE = 7\nNO_BOLD = 21\nNO_UNDERLINE = 24\nPOSITIVE = 27\nBLACK = 30\nRED = 31\nGREEN = 32\nYELLOW = 33\nBLUE = 34\nMAGENTA = 35\nCYAN = 36\nLIGHT_GRAY = 37\nDEFAULT = 39\nBLACK_BACKGROUND = 40\nRED_BACKGROUND = 41\nGREEN_BACKGROUND = 42\nYELLOW_BACKGROUND = 43\nBLUE_BACKGROUND = 44\nMAGENTA_BACKGROUND = 45\nCYAN_BACKGROUND = 46\nLIGHT_GRAY_BACKGROUND = 47\nDEFAULT_BACKGROUND = 49\nDARK_GRAY = 90  # xterm\nLIGHT_RED = 91  # xterm\nLIGHT_GREEN = 92  # xterm\nLIGHT_YELLOW = 93  # xterm\nLIGHT_BLUE = 94  # xterm\nLIGHT_MAGENTA = 95  # xterm\nLIGHT_CYAN = 96  # xterm\nWHITE = 97  # xterm\nDARK_GRAY_BACKGROUND = 100  # xterm\nLIGHT_RED_BACKGROUND = 101  # xterm\nLIGHT_GREEN_BACKGROUND = 102  # xterm\nLIGHT_YELLOW_BACKGROUND = 103  # xterm\nLIGHT_BLUE_BACKGROUND = 104  # xterm\nLIGHT_MAGENTA_BACKGROUND = 105  # xterm\nLIGHT_CYAN_BACKGROUND = 106  # xterm\nWHITE_BACKGROUND = 107  # xterm\n\n\ndef println(text, *colors):\n    \"\"\"Print text to standard output.\"\"\"\n    sys.stdout.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_err(text, *colors):\n    \"\"\"Print text to standard error.\"\"\"\n    sys.stderr.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")\n\n\ndef i(message):\n    \"\"\"Print a normal log message.\"\"\"\n    print_log(message)\n\n\ndef d(message):\n    \"\"\"Print a debug log message.\"\"\"\n    print_log(message, BLUE)\n\n\ndef w(message):\n    \"\"\"Print a warning log message.\"\"\"\n    print_log(message, YELLOW)\n\n\ndef e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)\n\n\ndef wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)\n\n\ndef yes_or_no(message):\n    ans = str(input(\"%s (y/N) \" % message)).lower().strip()\n    if ans == \"y\":\n        return True\n    return False\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["from ..version import script_name", "import os, sys"], "function": ["def println(text, *colors):\n", "def print_err(text, *colors):\n", "def print_log(text, *colors):\n", "def i(message):\n", "def d(message):\n", "def w(message):\n", "def e(message, exit_code=None):\n", "def wtf(message, exit_code=1):\n", "def yes_or_no(message):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "print_log", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Print a log message to standard error.", "docstring_tokens": ["Print", "a", "log", "message", "to", "standard", "error", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74", "partition": "test", "up_fun_num": 3, "context": "#!/usr/bin/env python\n# This file is Python 2 compliant.\n\nfrom ..version import script_name\n\nimport os, sys\n\nTERM = os.getenv(\"TERM\", \"\")\nIS_ANSI_TERMINAL = (\n    TERM\n    in (\n        \"eterm-color\",\n        \"linux\",\n        \"screen\",\n        \"vt100\",\n    )\n    or TERM.startswith(\"xterm\")\n)\n\n# ANSI escape code\n# See <http://en.wikipedia.org/wiki/ANSI_escape_code>\nRESET = 0\nBOLD = 1\nUNDERLINE = 4\nNEGATIVE = 7\nNO_BOLD = 21\nNO_UNDERLINE = 24\nPOSITIVE = 27\nBLACK = 30\nRED = 31\nGREEN = 32\nYELLOW = 33\nBLUE = 34\nMAGENTA = 35\nCYAN = 36\nLIGHT_GRAY = 37\nDEFAULT = 39\nBLACK_BACKGROUND = 40\nRED_BACKGROUND = 41\nGREEN_BACKGROUND = 42\nYELLOW_BACKGROUND = 43\nBLUE_BACKGROUND = 44\nMAGENTA_BACKGROUND = 45\nCYAN_BACKGROUND = 46\nLIGHT_GRAY_BACKGROUND = 47\nDEFAULT_BACKGROUND = 49\nDARK_GRAY = 90  # xterm\nLIGHT_RED = 91  # xterm\nLIGHT_GREEN = 92  # xterm\nLIGHT_YELLOW = 93  # xterm\nLIGHT_BLUE = 94  # xterm\nLIGHT_MAGENTA = 95  # xterm\nLIGHT_CYAN = 96  # xterm\nWHITE = 97  # xterm\nDARK_GRAY_BACKGROUND = 100  # xterm\nLIGHT_RED_BACKGROUND = 101  # xterm\nLIGHT_GREEN_BACKGROUND = 102  # xterm\nLIGHT_YELLOW_BACKGROUND = 103  # xterm\nLIGHT_BLUE_BACKGROUND = 104  # xterm\nLIGHT_MAGENTA_BACKGROUND = 105  # xterm\nLIGHT_CYAN_BACKGROUND = 106  # xterm\nWHITE_BACKGROUND = 107  # xterm\n\n\ndef sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return (\n        \"\\33[{}m{content}\\33[{}m\".format(\n            \";\".join([str(color) for color in colors]), RESET, content=text\n        )\n        if IS_ANSI_TERMINAL and colors\n        else text\n    )\n\n\ndef println(text, *colors):\n    \"\"\"Print text to standard output.\"\"\"\n    sys.stdout.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_err(text, *colors):\n    \"\"\"Print text to standard error.\"\"\"\n    sys.stderr.write(sprint(text, *colors) + \"\\n\")\n\n\ndef i(message):\n    \"\"\"Print a normal log message.\"\"\"\n    print_log(message)\n\n\ndef d(message):\n    \"\"\"Print a debug log message.\"\"\"\n    print_log(message, BLUE)\n\n\ndef w(message):\n    \"\"\"Print a warning log message.\"\"\"\n    print_log(message, YELLOW)\n\n\ndef e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)\n\n\ndef wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)\n\n\ndef yes_or_no(message):\n    ans = str(input(\"%s (y/N) \" % message)).lower().strip()\n    if ans == \"y\":\n        return True\n    return False\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["from ..version import script_name", "import os, sys"], "function": ["def sprint(text, *colors):\n", "def println(text, *colors):\n", "def print_err(text, *colors):\n", "def i(message):\n", "def d(message):\n", "def w(message):\n", "def e(message, exit_code=None):\n", "def wtf(message, exit_code=1):\n", "def yes_or_no(message):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "e", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Print an error log message.", "docstring_tokens": ["Print", "an", "error", "log", "message", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92", "partition": "test", "up_fun_num": 7, "context": "#!/usr/bin/env python\n# This file is Python 2 compliant.\n\nfrom ..version import script_name\n\nimport os, sys\n\nTERM = os.getenv(\"TERM\", \"\")\nIS_ANSI_TERMINAL = (\n    TERM\n    in (\n        \"eterm-color\",\n        \"linux\",\n        \"screen\",\n        \"vt100\",\n    )\n    or TERM.startswith(\"xterm\")\n)\n\n# ANSI escape code\n# See <http://en.wikipedia.org/wiki/ANSI_escape_code>\nRESET = 0\nBOLD = 1\nUNDERLINE = 4\nNEGATIVE = 7\nNO_BOLD = 21\nNO_UNDERLINE = 24\nPOSITIVE = 27\nBLACK = 30\nRED = 31\nGREEN = 32\nYELLOW = 33\nBLUE = 34\nMAGENTA = 35\nCYAN = 36\nLIGHT_GRAY = 37\nDEFAULT = 39\nBLACK_BACKGROUND = 40\nRED_BACKGROUND = 41\nGREEN_BACKGROUND = 42\nYELLOW_BACKGROUND = 43\nBLUE_BACKGROUND = 44\nMAGENTA_BACKGROUND = 45\nCYAN_BACKGROUND = 46\nLIGHT_GRAY_BACKGROUND = 47\nDEFAULT_BACKGROUND = 49\nDARK_GRAY = 90  # xterm\nLIGHT_RED = 91  # xterm\nLIGHT_GREEN = 92  # xterm\nLIGHT_YELLOW = 93  # xterm\nLIGHT_BLUE = 94  # xterm\nLIGHT_MAGENTA = 95  # xterm\nLIGHT_CYAN = 96  # xterm\nWHITE = 97  # xterm\nDARK_GRAY_BACKGROUND = 100  # xterm\nLIGHT_RED_BACKGROUND = 101  # xterm\nLIGHT_GREEN_BACKGROUND = 102  # xterm\nLIGHT_YELLOW_BACKGROUND = 103  # xterm\nLIGHT_BLUE_BACKGROUND = 104  # xterm\nLIGHT_MAGENTA_BACKGROUND = 105  # xterm\nLIGHT_CYAN_BACKGROUND = 106  # xterm\nWHITE_BACKGROUND = 107  # xterm\n\n\ndef sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return (\n        \"\\33[{}m{content}\\33[{}m\".format(\n            \";\".join([str(color) for color in colors]), RESET, content=text\n        )\n        if IS_ANSI_TERMINAL and colors\n        else text\n    )\n\n\ndef println(text, *colors):\n    \"\"\"Print text to standard output.\"\"\"\n    sys.stdout.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_err(text, *colors):\n    \"\"\"Print text to standard error.\"\"\"\n    sys.stderr.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")\n\n\ndef i(message):\n    \"\"\"Print a normal log message.\"\"\"\n    print_log(message)\n\n\ndef d(message):\n    \"\"\"Print a debug log message.\"\"\"\n    print_log(message, BLUE)\n\n\ndef w(message):\n    \"\"\"Print a warning log message.\"\"\"\n    print_log(message, YELLOW)\n\n\ndef wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)\n\n\ndef yes_or_no(message):\n    ans = str(input(\"%s (y/N) \" % message)).lower().strip()\n    if ans == \"y\":\n        return True\n    return False\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["from ..version import script_name", "import os, sys"], "function": ["def sprint(text, *colors):\n", "def println(text, *colors):\n", "def print_err(text, *colors):\n", "def print_log(text, *colors):\n", "def i(message):\n", "def d(message):\n", "def w(message):\n", "def wtf(message, exit_code=1):\n", "def yes_or_no(message):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "wtf", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "What a Terrible Failure!", "docstring_tokens": ["What", "a", "Terrible", "Failure!"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98", "partition": "test", "up_fun_num": 8, "context": "#!/usr/bin/env python\n# This file is Python 2 compliant.\n\nfrom ..version import script_name\n\nimport os, sys\n\nTERM = os.getenv(\"TERM\", \"\")\nIS_ANSI_TERMINAL = (\n    TERM\n    in (\n        \"eterm-color\",\n        \"linux\",\n        \"screen\",\n        \"vt100\",\n    )\n    or TERM.startswith(\"xterm\")\n)\n\n# ANSI escape code\n# See <http://en.wikipedia.org/wiki/ANSI_escape_code>\nRESET = 0\nBOLD = 1\nUNDERLINE = 4\nNEGATIVE = 7\nNO_BOLD = 21\nNO_UNDERLINE = 24\nPOSITIVE = 27\nBLACK = 30\nRED = 31\nGREEN = 32\nYELLOW = 33\nBLUE = 34\nMAGENTA = 35\nCYAN = 36\nLIGHT_GRAY = 37\nDEFAULT = 39\nBLACK_BACKGROUND = 40\nRED_BACKGROUND = 41\nGREEN_BACKGROUND = 42\nYELLOW_BACKGROUND = 43\nBLUE_BACKGROUND = 44\nMAGENTA_BACKGROUND = 45\nCYAN_BACKGROUND = 46\nLIGHT_GRAY_BACKGROUND = 47\nDEFAULT_BACKGROUND = 49\nDARK_GRAY = 90  # xterm\nLIGHT_RED = 91  # xterm\nLIGHT_GREEN = 92  # xterm\nLIGHT_YELLOW = 93  # xterm\nLIGHT_BLUE = 94  # xterm\nLIGHT_MAGENTA = 95  # xterm\nLIGHT_CYAN = 96  # xterm\nWHITE = 97  # xterm\nDARK_GRAY_BACKGROUND = 100  # xterm\nLIGHT_RED_BACKGROUND = 101  # xterm\nLIGHT_GREEN_BACKGROUND = 102  # xterm\nLIGHT_YELLOW_BACKGROUND = 103  # xterm\nLIGHT_BLUE_BACKGROUND = 104  # xterm\nLIGHT_MAGENTA_BACKGROUND = 105  # xterm\nLIGHT_CYAN_BACKGROUND = 106  # xterm\nWHITE_BACKGROUND = 107  # xterm\n\n\ndef sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return (\n        \"\\33[{}m{content}\\33[{}m\".format(\n            \";\".join([str(color) for color in colors]), RESET, content=text\n        )\n        if IS_ANSI_TERMINAL and colors\n        else text\n    )\n\n\ndef println(text, *colors):\n    \"\"\"Print text to standard output.\"\"\"\n    sys.stdout.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_err(text, *colors):\n    \"\"\"Print text to standard error.\"\"\"\n    sys.stderr.write(sprint(text, *colors) + \"\\n\")\n\n\ndef print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")\n\n\ndef i(message):\n    \"\"\"Print a normal log message.\"\"\"\n    print_log(message)\n\n\ndef d(message):\n    \"\"\"Print a debug log message.\"\"\"\n    print_log(message, BLUE)\n\n\ndef w(message):\n    \"\"\"Print a warning log message.\"\"\"\n    print_log(message, YELLOW)\n\n\ndef e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)\n\n\ndef yes_or_no(message):\n    ans = str(input(\"%s (y/N) \" % message)).lower().strip()\n    if ans == \"y\":\n        return True\n    return False\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["from ..version import script_name", "import os, sys"], "function": ["def sprint(text, *colors):\n", "def println(text, *colors):\n", "def print_err(text, *colors):\n", "def print_log(text, *colors):\n", "def i(message):\n", "def d(message):\n", "def w(message):\n", "def e(message, exit_code=None):\n", "def yes_or_no(message):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/util/os.py", "func_name": "detect_os", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Detect operating system.", "docstring_tokens": ["Detect", "operating", "system", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\nfrom platform import system\n", "levels": [], "package": ["from platform import system"], "function": []}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/vimeo.py", "func_name": "vimeo_download_by_channel", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "str->None", "docstring_tokens": ["str", "-", ">", "None"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\n__all__ = [\n    \"vimeo_download\",\n    \"vimeo_download_by_id\",\n    \"vimeo_download_by_channel\",\n    \"vimeo_download_by_channel_id\",\n]\n\nfrom ..common import *\nfrom ..util.log import *\nfrom ..extractor import VideoExtractor\nfrom json import loads\nimport urllib.error\nimport urllib.parse\n\naccess_token = \"f6785418277b72c7c87d3132c79eec24\"  # By Beining\n\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\ndef vimeo_download_by_channel_id(\n    channel_id, output_dir=\".\", merge=False, info_only=False, **kwargs\n):\n    \"\"\"str/int->None\"\"\"\n    html = get_content(\n        \"https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}\".format(\n            channel_id=channel_id, access_token=access_token\n        )\n    )\n    data = loads(html)\n    id_list = []\n\n    # print(data)\n    for i in data[\"data\"]:\n        id_list.append(match1(i[\"uri\"], r\"/videos/(\\w+)\"))\n\n    for id in id_list:\n        try:\n            vimeo_download_by_id(id, None, output_dir, merge, info_only, **kwargs)\n        except urllib.error.URLError as e:\n            log.w(\"{} failed with {}\".format(id, e))\n\n\nclass VimeoExtractor(VideoExtractor):\n    stream_types = [\n        {\"id\": \"2160p\", \"video_profile\": \"3840x2160\"},\n        {\"id\": \"1440p\", \"video_profile\": \"2560x1440\"},\n        {\"id\": \"1080p\", \"video_profile\": \"1920x1080\"},\n        {\"id\": \"720p\", \"video_profile\": \"1280x720\"},\n        {\"id\": \"540p\", \"video_profile\": \"960x540\"},\n        {\"id\": \"360p\", \"video_profile\": \"640x360\"},\n    ]\n    name = \"Vimeo\"\n\n    def prepare(self, **kwargs):\n        headers = fake_headers.copy()\n        if \"referer\" in kwargs:\n            headers[\"Referer\"] = kwargs[\"referer\"]\n\n        try:\n            page = get_content(\"https://vimeo.com/{}\".format(self.vid))\n            cfg_patt = r\"clip_page_config\\s*=\\s*(\\{.+?\\});\"\n            cfg = json.loads(match1(page, cfg_patt))\n            video_page = get_content(cfg[\"player\"][\"config_url\"], headers=headers)\n            self.title = cfg[\"clip\"][\"title\"]\n            info = json.loads(video_page)\n        except Exception as e:\n            page = get_content(\"https://player.vimeo.com/video/{}\".format(self.vid))\n            self.title = r1(r\"<title>([^<]+)</title>\", page)\n            info = json.loads(match1(page, r\"var t=(\\{.+?\\});\"))\n\n        plain = info[\"request\"][\"files\"][\"progressive\"]\n        for s in plain:\n            meta = dict(src=[s[\"url\"]], container=\"mp4\")\n            meta[\"video_profile\"] = \"{}x{}\".format(s[\"width\"], s[\"height\"])\n            for stream in self.__class__.stream_types:\n                if s[\"quality\"] == stream[\"id\"]:\n                    self.streams[s[\"quality\"]] = meta\n        self.master_m3u8 = info[\"request\"][\"files\"][\"hls\"][\"cdns\"]\n\n    def extract(self, **kwargs):\n        for s in self.streams:\n            self.streams[s][\"size\"] = urls_size(self.streams[s][\"src\"])\n\n        master_m3u8s = []\n        for m in self.master_m3u8:\n            master_m3u8s.append(self.master_m3u8[m][\"url\"])\n\n        master_content = None\n        master_url = None\n\n        for master_u in master_m3u8s:\n            try:\n                master_content = get_content(master_u).split(\"\\n\")\n            except urllib.error.URLError:\n                continue\n            else:\n                master_url = master_u\n\n        if master_content is None:\n            return\n\n        lines = []\n        for line in master_content:\n            if len(line.strip()) > 0:\n                lines.append(line.strip())\n\n        pos = 0\n        while pos < len(lines):\n            if lines[pos].startswith(\"#EXT-X-STREAM-INF\"):\n                patt = \"RESOLUTION=(\\d+)x(\\d+)\"\n                hit = re.search(patt, lines[pos])\n                if hit is None:\n                    continue\n                width = hit.group(1)\n                height = hit.group(2)\n\n                if height in (\"2160\", \"1440\"):\n                    m3u8_url = urllib.parse.urljoin(master_url, lines[pos + 1])\n                    meta = dict(m3u8_url=m3u8_url, container=\"m3u8\")\n                    if height == \"1440\":\n                        meta[\"video_profile\"] = \"2560x1440\"\n                    else:\n                        meta[\"video_profile\"] = \"3840x2160\"\n                    meta[\"size\"] = 0\n                    meta[\"src\"] = general_m3u8_extractor(m3u8_url)\n                    self.streams[height + \"p\"] = meta\n\n                pos += 2\n            else:\n                pos += 1\n        self.streams_sorted = []\n        for stream_type in self.stream_types:\n            if stream_type[\"id\"] in self.streams:\n                item = [(\"id\", stream_type[\"id\"])] + list(\n                    self.streams[stream_type[\"id\"]].items()\n                )\n                self.streams_sorted.append(dict(item))\n\n\ndef vimeo_download_by_id(\n    id, title=None, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    \"\"\"\n    try:\n        # normal Vimeo video\n        html = get_content('https://vimeo.com/' + id)\n        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n        cfg = json.loads(match1(html, cfg_patt))\n        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)\n        title = cfg['clip']['title']\n        info = loads(video_page)\n    except:\n        # embedded player - referer may be required\n        if 'referer' in kwargs:\n            fake_headers['Referer'] = kwargs['referer']\n\n        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)\n        title = r1(r'<title>([^<]+)</title>', video_page)\n        info = loads(match1(video_page, r'var t=(\\{.+?\\});'))\n\n    streams = info['request']['files']['progressive']\n    streams = sorted(streams, key=lambda i: i['height'])\n    url = streams[-1]['url']\n\n    type, ext, size = url_info(url, faker=True)\n\n    print_info(site_info, title, type, size)\n    if not info_only:\n        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)\n    \"\"\"\n    site = VimeoExtractor()\n    site.download_by_vid(\n        id, info_only=info_only, output_dir=output_dir, merge=merge, **kwargs\n    )\n\n\ndef vimeo_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n    if re.match(r\"https?://vimeo.com/channels/\\w+\", url):\n        vimeo_download_by_channel(url, output_dir, merge, info_only)\n    else:\n        id = r1(r\"https?://[\\w.]*vimeo.com[/\\w]*/(\\d+)\", url)\n        if id is None:\n            video_page = get_content(url, headers=fake_headers)\n            id = r1(r'\"clip_id\":(\\d+)', video_page)\n        assert id\n\n        vimeo_download_by_id(\n            id, None, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs\n        )\n\n\nsite_info = \"Vimeo.com\"\ndownload = vimeo_download\ndownload_playlist = vimeo_download_by_channel\n", "levels": [0, 1, 1, 0], "package": ["from ..common import *", "from ..util.log import *", "from ..extractor import VideoExtractor", "from json import loads", "import urllib.error", "import urllib.parse"], "function": ["class VimeoExtractor(VideoExtractor):\n", "    def prepare(self, **kwargs):\n", "    def extract(self, **kwargs):\n", "def vimeo_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/ckplayer.py", "func_name": "ckplayer_get_info_by_xml", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "str->dict\n    Information for CKPlayer API content.", "docstring_tokens": ["str", "-", ">", "dict", "Information", "for", "CKPlayer", "API", "content", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n# coding:utf-8\n# Author:  Beining --<i@cnbeining.com>\n# Purpose: A general extractor for CKPlayer\n# Created: 03/15/2016\n\n__all__ = [\"ckplayer_download\"]\n\nfrom xml.etree import cElementTree as ET\nfrom copy import copy\nfrom ..common import *\n\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\n# helper\n# https://stackoverflow.com/questions/2148119/how-to-convert-an-xml-string-to-a-dictionary-in-python\ndef dictify(r, root=True):\n    if root:\n        return {r.tag: dictify(r, False)}\n    d = copy(r.attrib)\n    if r.text:\n        d[\"_text\"] = r.text\n    for x in r.findall(\"./*\"):\n        if x.tag not in d:\n            d[x.tag] = []\n        d[x.tag].append(dictify(x, False))\n    return d\n\n\n# ----------------------------------------------------------------------\ndef ckplayer_download_by_xml(\n    ckinfo, output_dir=\".\", merge=False, info_only=False, **kwargs\n):\n    # Info XML\n    video_info = ckplayer_get_info_by_xml(ckinfo)\n\n    try:\n        title = kwargs[\"title\"]\n    except:\n        title = \"\"\n    type_ = \"\"\n    size = 0\n\n    if len(video_info[\"links\"]) > 0:  # has link\n        type_, _ext, size = url_info(\n            video_info[\"links\"][0]\n        )  # use 1st to determine type, ext\n\n    if \"size\" in video_info:\n        size = int(video_info[\"size\"])\n    else:\n        for i in video_info[\"links\"][1:]:  # save 1st one\n            size += url_info(i)[2]\n\n    print_info(site_info, title, type_, size)\n    if not info_only:\n        download_urls(\n            video_info[\"links\"], title, _ext, size, output_dir=output_dir, merge=merge\n        )\n\n\n# ----------------------------------------------------------------------\ndef ckplayer_download(\n    url, output_dir=\".\", merge=False, info_only=False, is_xml=True, **kwargs\n):\n    if is_xml:  # URL is XML URL\n        try:\n            title = kwargs[\"title\"]\n        except:\n            title = \"\"\n        try:\n            headers = kwargs[\"headers\"]  # headers provided\n            ckinfo = get_content(url, headers=headers)\n        except NameError:\n            ckinfo = get_content(url)\n\n        ckplayer_download_by_xml(ckinfo, output_dir, merge, info_only, title=title)\n\n\nsite_info = \"CKPlayer General\"\ndownload = ckplayer_download\ndownload_playlist = playlist_not_supported(\"ckplayer\")\n", "levels": [0], "package": ["from xml.etree import cElementTree as ET", "from copy import copy", "from ..common import *"], "function": ["def dictify(r, root=True):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/ixigua.py", "func_name": "get_video_url_from_video_id", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Splicing URLs according to video ID to get video details", "docstring_tokens": ["Splicing", "URLs", "according", "to", "video", "ID", "to", "get", "video", "details"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78", "partition": "test", "up_fun_num": 2, "context": "#!/usr/bin/env python\nimport base64\n\nimport binascii\n\nfrom ..common import *\nimport random\nimport ctypes\nfrom json import loads\n\n__all__ = [\"ixigua_download\", \"ixigua_download_playlist_by_url\"]\n\nheaders = {\n    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 \"\n    \"Safari/537.36\",\n}\n\n\ndef int_overflow(val):\n    maxint = 2147483647\n    if not -maxint - 1 <= val <= maxint:\n        val = (val + (maxint + 1)) % (2 * (maxint + 1)) - maxint - 1\n    return val\n\n\ndef unsigned_right_shitf(n, i):\n    if n < 0:\n        n = ctypes.c_uint32(n).value\n    if i < 0:\n        return -int_overflow(n << abs(i))\n    return int_overflow(n >> i)\n\n\ndef ixigua_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n    # example url: https://www.ixigua.com/i6631065141750268420/#mid=63024814422\n    html = get_html(url, faker=True)\n    video_id = match1(html, r\"videoId\\s*:\\s*'([^']+)'\")\n    title = match1(html, r\"title: '(\\S+)',\")\n    if not video_id:\n        log.e(\"video_id not found, url:{}\".format(url))\n        return\n    video_info_url = get_video_url_from_video_id(video_id)\n    video_info = loads(get_content(video_info_url))\n    if video_info.get(\"code\", 1) != 0:\n        log.e(\n            \"Get video info from {} error: server return code {}\".format(\n                video_info_url, video_info.get(\"code\", 1)\n            )\n        )\n        return\n    if not video_info.get(\"data\", None):\n        log.e(\n            \"Get video info from {} error: The server returns JSON value\"\n            \" without data or data is empty\".format(video_info_url)\n        )\n        return\n    if not video_info[\"data\"].get(\"video_list\", None):\n        log.e(\n            \"Get video info from {} error: The server returns JSON value\"\n            \" without data.video_list or data.video_list is empty\".format(\n                video_info_url\n            )\n        )\n        return\n    if not video_info[\"data\"][\"video_list\"].get(\"video_1\", None):\n        log.e(\n            \"Get video info from {} error: The server returns JSON value\"\n            \" without data.video_list.video_1 or data.video_list.video_1 is empty\".format(\n                video_info_url\n            )\n        )\n        return\n    size = int(video_info[\"data\"][\"video_list\"][\"video_1\"][\"size\"])\n    print_info(site_info=site_info, title=title, type=\"mp4\", size=size)  # \u8be5\u7f51\u7ad9\u53ea\u6709mp4\u7c7b\u578b\u6587\u4ef6\n    if not info_only:\n        video_url = base64.b64decode(\n            video_info[\"data\"][\"video_list\"][\"video_1\"][\"main_url\"].encode(\"utf-8\")\n        )\n        download_urls(\n            [video_url.decode(\"utf-8\")],\n            title,\n            \"mp4\",\n            size,\n            output_dir,\n            merge=merge,\n            headers=headers,\n            **kwargs\n        )\n\n\ndef ixigua_download_playlist_by_url(\n    url, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    assert \"user\" in url, (\n        \"Only support users to publish video list,Please provide a similar url:\"\n        \"https://www.ixigua.com/c/user/6907091136/\"\n    )\n\n    user_id = url.split(\"/\")[-2] if url[-1] == \"/\" else url.split(\"/\")[-1]\n    params = {\n        \"max_behot_time\": \"0\",\n        \"max_repin_time\": \"0\",\n        \"count\": \"20\",\n        \"page_type\": \"0\",\n        \"user_id\": user_id,\n    }\n    while 1:\n        url = \"https://www.ixigua.com/c/user/article/?\" + \"&\".join(\n            [\"{}={}\".format(k, v) for k, v in params.items()]\n        )\n        video_list = loads(get_content(url, headers=headers))\n        params[\"max_behot_time\"] = video_list[\"next\"][\"max_behot_time\"]\n        for video in video_list[\"data\"]:\n            ixigua_download(\n                \"https://www.ixigua.com/i{}/\".format(video[\"item_id\"]),\n                output_dir,\n                merge,\n                info_only,\n                **kwargs\n            )\n        if video_list[\"next\"][\"max_behot_time\"] == 0:\n            break\n\n\nsite_info = \"ixigua.com\"\ndownload = ixigua_download\ndownload_playlist = ixigua_download_playlist_by_url\n", "levels": [0, 0, 0], "package": ["import base64", "import binascii", "from ..common import *", "import random", "import ctypes", "from json import loads"], "function": ["def int_overflow(val):\n", "def unsigned_right_shitf(n, i):\n", "def ixigua_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/mgtv.py", "func_name": "MGTV.get_mgtv_real_url", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "str->list of str\n        Give you the real URLs.", "docstring_tokens": ["str", "-", ">", "list", "of", "str", "Give", "you", "the", "real", "URLs", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58", "partition": "test", "up_fun_num": 2, "context": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom ..common import *\nfrom ..extractor import VideoExtractor\n\nfrom json import loads\nfrom urllib.parse import urlsplit\nfrom os.path import dirname\nimport re\n\n\nclass MGTV(VideoExtractor):\n    name = \"\u8292\u679c (MGTV)\"\n\n    # Last updated: 2016-11-13\n    stream_types = [\n        {\"id\": \"hd\", \"container\": \"ts\", \"video_profile\": \"\u8d85\u6e05\"},\n        {\"id\": \"sd\", \"container\": \"ts\", \"video_profile\": \"\u9ad8\u6e05\"},\n        {\"id\": \"ld\", \"container\": \"ts\", \"video_profile\": \"\u6807\u6e05\"},\n    ]\n\n    id_dic = {i[\"video_profile\"]: (i[\"id\"]) for i in stream_types}\n\n    api_endpoint = \"http://pcweb.api.mgtv.com/player/video?video_id={video_id}\"\n\n    @staticmethod\n    def get_vid_from_url(url):\n        \"\"\"Extracts video ID from URL.\"\"\"\n        vid = match1(url, \"https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html\")\n        if not vid:\n            vid = match1(url, \"https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html\")\n        return vid\n\n    # ----------------------------------------------------------------------\n    @staticmethod\n    def download_playlist_by_url(self, url, **kwargs):\n        pass\n\n    def prepare(self, **kwargs):\n        if self.url:\n            self.vid = self.get_vid_from_url(self.url)\n        content = get_content(self.api_endpoint.format(video_id=self.vid))\n        content = loads(content)\n        self.title = content[\"data\"][\"info\"][\"title\"]\n        domain = content[\"data\"][\"stream_domain\"][0]\n\n        # stream_avalable = [i['name'] for i in content['data']['stream']]\n        stream_available = {}\n        for i in content[\"data\"][\"stream\"]:\n            stream_available[i[\"name\"]] = i[\"url\"]\n\n        for s in self.stream_types:\n            if s[\"video_profile\"] in stream_available.keys():\n                quality_id = self.id_dic[s[\"video_profile\"]]\n                url = stream_available[s[\"video_profile\"]]\n                url = domain + re.sub(r\"(\\&arange\\=\\d+)\", \"\", url)  # Un-Hum\n                m3u8_url, m3u8_size, segment_list_this = self.get_mgtv_real_url(url)\n\n                stream_fileid_list = []\n                for i in segment_list_this:\n                    stream_fileid_list.append(os.path.basename(i).split(\".\")[0])\n\n            # make pieces\n            pieces = []\n            for i in zip(stream_fileid_list, segment_list_this):\n                pieces.append(\n                    {\n                        \"fileid\": i[0],\n                        \"segs\": i[1],\n                    }\n                )\n\n                self.streams[quality_id] = {\n                    \"container\": s[\"container\"],\n                    \"video_profile\": s[\"video_profile\"],\n                    \"size\": m3u8_size,\n                    \"pieces\": pieces,\n                    \"m3u8_url\": m3u8_url,\n                }\n\n            if not kwargs[\"info_only\"]:\n                self.streams[quality_id][\"src\"] = segment_list_this\n\n    def extract(self, **kwargs):\n        if \"stream_id\" in kwargs and kwargs[\"stream_id\"]:\n            # Extract the stream\n            stream_id = kwargs[\"stream_id\"]\n\n            if stream_id not in self.streams:\n                log.e(\"[Error] Invalid video format.\")\n                log.e(\n                    \"Run '-i' command with no specific video format to view all available formats.\"\n                )\n                exit(2)\n        else:\n            # Extract stream with the best quality\n            stream_id = self.streams_sorted[0][\"id\"]\n\n    def download(self, **kwargs):\n\n        if \"stream_id\" in kwargs and kwargs[\"stream_id\"]:\n            stream_id = kwargs[\"stream_id\"]\n        else:\n            stream_id = \"null\"\n\n        # print video info only\n        if \"info_only\" in kwargs and kwargs[\"info_only\"]:\n            if stream_id != \"null\":\n                if \"index\" not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if \"index\" not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = (\n                        self.streams_sorted[0][\"id\"]\n                        if \"id\" in self.streams_sorted[0]\n                        else self.streams_sorted[0][\"itag\"]\n                    )\n                    self.p_i(stream_id)\n\n        # default to use the best quality\n        if stream_id == \"null\":\n            stream_id = self.streams_sorted[0][\"id\"]\n\n        stream_info = self.streams[stream_id]\n\n        if not kwargs[\"info_only\"]:\n            if player:\n                # with m3u8 format because some video player can process urls automatically (e.g. mpv)\n                launch_player(player, [stream_info[\"m3u8_url\"]])\n            else:\n                download_urls(\n                    stream_info[\"src\"],\n                    self.title,\n                    stream_info[\"container\"],\n                    stream_info[\"size\"],\n                    output_dir=kwargs[\"output_dir\"],\n                    merge=kwargs.get(\"merge\", True),\n                )\n                # av=stream_id in self.dash_streams)\n\n\nsite = MGTV()\ndownload = site.download_by_url\ndownload_playlist = site.download_playlist_by_url\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["from ..common import *", "from ..extractor import VideoExtractor", "from json import loads", "from urllib.parse import urlsplit", "from os.path import dirname", "import re"], "function": ["class MGTV(VideoExtractor):\n", "    def get_vid_from_url(url):\n", "    def download_playlist_by_url(self, url, **kwargs):\n", "    def prepare(self, **kwargs):\n", "    def extract(self, **kwargs):\n", "    def download(self, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/util/fs.py", "func_name": "legitimize", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Converts a string to a valid filename.", "docstring_tokens": ["Converts", "a", "string", "to", "a", "valid", "filename", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\nfrom .os import detect_os\n", "levels": [], "package": ["from .os import detect_os"], "function": []}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/cbs.py", "func_name": "cbs_download", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Downloads CBS videos by URL.", "docstring_tokens": ["Downloads", "CBS", "videos", "by", "URL", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\n__all__ = [\"cbs_download\"]\n\nfrom ..common import *\n\nfrom .theplatform import theplatform_download_by_pid\n\n\nsite_info = \"CBS.com\"\ndownload = cbs_download\ndownload_playlist = playlist_not_supported(\"cbs\")\n", "levels": [], "package": ["from ..common import *", "from .theplatform import theplatform_download_by_pid"], "function": []}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/iqiyi.py", "func_name": "Iqiyi.download", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Override the original one\n        Ugly ugly dirty hack", "docstring_tokens": ["Override", "the", "original", "one", "Ugly", "ugly", "dirty", "hack"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218", "partition": "test", "up_fun_num": 8, "context": "#!/usr/bin/env python\n\nfrom ..common import *\nfrom ..common import print_more_compatible as print\nfrom ..extractor import VideoExtractor\nfrom ..util import log\nfrom .. import json_output\n\nfrom uuid import uuid4\nfrom random import random, randint\nimport json\nfrom math import floor\nfrom zlib import decompress\nimport hashlib\nimport time\n\n\"\"\"\nChangelog:\n-> http://www.iqiyi.com/common/flashplayer/20150916/MainPlayer_5_2_28_c3_3_7_4.swf\n   use @fffonion 's method in #617.\n   Add trace AVM(asasm) code in Iqiyi's encode function where the salt is put into the encode array and reassemble by RABCDasm(or WinRABCDasm),then use Fiddler to response modified file to replace the src file with its AutoResponder function ,set browser Fiddler proxy and play with !debug version! Flash Player ,finially get result in flashlog.txt(its location can be easily found in search engine).\n   Code Like (without letters after #comment:),it just do the job : trace(\"{IQIYI_SALT}:\"+salt_array.join(\"\"))\n   ```(Postion After getTimer)\n     findpropstrict      QName(PackageNamespace(\"\"), \"trace\")\n     pushstring          \"{IQIYI_SALT}:\" #comment for you to locate the salt\n     getscopeobject      1\n     getslot             17 #comment: 17 is the salt slots number defined in code\n     pushstring          \"\"\n     callproperty        QName(Namespace(\"http://adobe.com/AS3/2006/builtin\"), \"join\"), 1\n     add\n     callpropvoid        QName(PackageNamespace(\"\"), \"trace\"), 1\n   ```\n\n-> http://www.iqiyi.com/common/flashplayer/20150820/MainPlayer_5_2_27_2_c3_3_7_3.swf\n    some small changes in Zombie.bite function\n\n\"\"\"\n\n\"\"\"\ncom.qiyi.player.core.model.def.DefinitonEnum\nbid meaning for quality\n0 none\n1 standard\n2 high\n3 super\n4 suprt-high\n5 fullhd\n10 4k\n96 topspeed\n\n\"\"\"\n\"\"\"\ndef mix(tvid):\n    salt = '4a1caba4b4465345366f28da7c117d20'\n    tm = str(randint(2000,4000))\n    sc = hashlib.new('md5', bytes(salt + tm + tvid, 'utf-8')).hexdigest()\n    return tm, sc, 'eknas'\n\ndef getVRSXORCode(arg1,arg2):\n    loc3=arg2 %3\n    if loc3 == 1:\n        return arg1^121\n    if loc3 == 2:\n        return arg1^72\n    return arg1^103\n\n\ndef getVrsEncodeCode(vlink):\n    loc6=0\n    loc2=''\n    loc3=vlink.split(\"-\")\n    loc4=len(loc3)\n    # loc5=loc4-1\n    for i in range(loc4-1,-1,-1):\n        loc6=getVRSXORCode(int(loc3[loc4-i-1],16),i)\n        loc2+=chr(loc6)\n    return loc2[::-1]\n\ndef getDispathKey(rid):\n    tp=\")(*&^flash@#$%a\"  #magic from swf\n    time=json.loads(get_content(\"http://data.video.qiyi.com/t?tn=\"+str(random())))[\"t\"]\n    t=str(int(floor(int(time)/(10*60.0))))\n    return hashlib.new(\"md5\",bytes(t+tp+rid,\"utf-8\")).hexdigest()\n\"\"\"\n\n\ndef getVMS(tvid, vid):\n    t = int(time.time() * 1000)\n    src = \"76f90cbd92f94a2e925d83e8ccd22cb7\"\n    key = \"d5fb4bd9d50c4be6948c97edd7254b0e\"\n    sc = hashlib.new(\"md5\", bytes(str(t) + key + vid, \"utf-8\")).hexdigest()\n    vmsreq = url = \"http://cache.m.iqiyi.com/tmts/{0}/{1}/?t={2}&sc={3}&src={4}\".format(\n        tvid, vid, t, sc, src\n    )\n    return json.loads(get_content(vmsreq))\n\n\nclass Iqiyi(VideoExtractor):\n    name = \"\u7231\u5947\u827a (Iqiyi)\"\n\n    stream_types = [\n        {\"id\": \"4k\", \"container\": \"m3u8\", \"video_profile\": \"4k\"},\n        {\"id\": \"BD\", \"container\": \"m3u8\", \"video_profile\": \"1080p\"},\n        {\"id\": \"TD\", \"container\": \"m3u8\", \"video_profile\": \"720p\"},\n        {\"id\": \"TD_H265\", \"container\": \"m3u8\", \"video_profile\": \"720p H265\"},\n        {\"id\": \"HD\", \"container\": \"m3u8\", \"video_profile\": \"540p\"},\n        {\"id\": \"HD_H265\", \"container\": \"m3u8\", \"video_profile\": \"540p H265\"},\n        {\"id\": \"SD\", \"container\": \"m3u8\", \"video_profile\": \"360p\"},\n        {\"id\": \"LD\", \"container\": \"m3u8\", \"video_profile\": \"210p\"},\n    ]\n    \"\"\"\n    supported_stream_types = [ 'high', 'standard']\n\n\n    stream_to_bid = {  '4k': 10, 'fullhd' : 5, 'suprt-high' : 4, 'super' : 3, 'high' : 2, 'standard' :1, 'topspeed' :96}\n    \"\"\"\n    ids = [\"4k\", \"BD\", \"TD\", \"HD\", \"SD\", \"LD\"]\n    vd_2_id = {\n        10: \"4k\",\n        19: \"4k\",\n        5: \"BD\",\n        18: \"BD\",\n        21: \"HD_H265\",\n        2: \"HD\",\n        4: \"TD\",\n        17: \"TD_H265\",\n        96: \"LD\",\n        1: \"SD\",\n        14: \"TD\",\n    }\n    id_2_profile = {\n        \"4k\": \"4k\",\n        \"BD\": \"1080p\",\n        \"TD\": \"720p\",\n        \"HD\": \"540p\",\n        \"SD\": \"360p\",\n        \"LD\": \"210p\",\n        \"HD_H265\": \"540p H265\",\n        \"TD_H265\": \"720p H265\",\n    }\n\n    def download_playlist_by_url(self, url, **kwargs):\n        self.url = url\n\n        video_page = get_content(url)\n        videos = set(\n            re.findall(r'<a href=\"(http://www\\.iqiyi\\.com/v_[^\"]+)\"', video_page)\n        )\n\n        for video in videos:\n            self.__class__().download_by_url(video, **kwargs)\n\n    def prepare(self, **kwargs):\n        assert self.url or self.vid\n\n        if self.url and not self.vid:\n            html = get_html(self.url)\n            tvid = (\n                r1(r\"#curid=(.+)_\", self.url)\n                or r1(r\"tvid=([^&]+)\", self.url)\n                or r1(r'data-player-tvid=\"([^\"]+)\"', html)\n                or r1(r\"tv(?:i|I)d=(.+?)\\&\", html)\n                or r1(r'param\\[\\'tvid\\'\\]\\s*=\\s*\"(.+?)\"', html)\n            )\n            videoid = (\n                r1(r\"#curid=.+_(.*)$\", self.url)\n                or r1(r\"vid=([^&]+)\", self.url)\n                or r1(r'data-player-videoid=\"([^\"]+)\"', html)\n                or r1(r\"vid=(.+?)\\&\", html)\n                or r1(r'param\\[\\'vid\\'\\]\\s*=\\s*\"(.+?)\"', html)\n            )\n            self.vid = (tvid, videoid)\n            info_u = \"http://pcw-api.iqiyi.com/video/video/playervideoinfo?tvid=\" + tvid\n            json_res = get_content(info_u)\n            self.title = json.loads(json_res)[\"data\"][\"vn\"]\n        tvid, videoid = self.vid\n        info = getVMS(tvid, videoid)\n        assert info[\"code\"] == \"A00000\", \"can't play this video\"\n\n        for stream in info[\"data\"][\"vidl\"]:\n            try:\n                stream_id = self.vd_2_id[stream[\"vd\"]]\n                if stream_id in self.stream_types:\n                    continue\n                stream_profile = self.id_2_profile[stream_id]\n                self.streams[stream_id] = {\n                    \"video_profile\": stream_profile,\n                    \"container\": \"m3u8\",\n                    \"src\": [stream[\"m3u\"]],\n                    \"size\": 0,\n                    \"m3u8_url\": stream[\"m3u\"],\n                }\n            except Exception as e:\n                log.i(\"vd: {} is not handled\".format(stream[\"vd\"]))\n                log.i(\"info is {}\".format(stream))\n\n\n\"\"\"\n        if info[\"code\"] != \"A000000\":\n            log.e(\"[error] outdated iQIYI key\")\n            log.wtf(\"is your you-get up-to-date?\")\n\n        self.title = info[\"data\"][\"vi\"][\"vn\"]\n        self.title = self.title.replace('\\u200b', '')\n\n        # data.vp = json.data.vp\n        #  data.vi = json.data.vi\n        #  data.f4v = json.data.f4v\n        # if movieIsMember data.vp = json.data.np\n\n        #for highest qualities\n        #for http://www.iqiyi.com/v_19rrmmz5yw.html  not vp -> np\n        try:\n            if info[\"data\"]['vp'][\"tkl\"]=='' :\n                raise ValueError\n        except:\n            log.e(\"[Error] Do not support for iQIYI VIP video.\")\n            exit(-1)\n\n        vs = info[\"data\"][\"vp\"][\"tkl\"][0][\"vs\"]\n        self.baseurl=info[\"data\"][\"vp\"][\"du\"].split(\"/\")\n\n        for stream in self.stream_types:\n            for i in vs:\n                if self.stream_to_bid[stream['id']] == i['bid']:\n                    video_links=i[\"fs\"] #now in i[\"flvs\"] not in i[\"fs\"]\n                    if not i[\"fs\"][0][\"l\"].startswith(\"/\"):\n                        tmp = getVrsEncodeCode(i[\"fs\"][0][\"l\"])\n                        if tmp.endswith('mp4'):\n                             video_links = i[\"flvs\"]\n                    self.stream_urls[stream['id']] = video_links\n                    size = 0\n                    for l in video_links:\n                        size += l['b']\n                    self.streams[stream['id']] = {'container': stream['container'], 'video_profile': stream['video_profile'], 'size' : size}\n                    break\n\n    def extract(self, **kwargs):\n        if 'stream_id' in kwargs and kwargs['stream_id']:\n            # Extract the stream\n            stream_id = kwargs['stream_id']\n\n            if stream_id not in self.streams:\n                log.e('[Error] Invalid video format.')\n                log.e('Run \\'-i\\' command with no specific video format to view all available formats.')\n                exit(2)\n        else:\n            # Extract stream with the best quality\n            stream_id = self.streams_sorted[0]['id']\n\n        urls=[]\n        for i in self.stream_urls[stream_id]:\n            vlink=i[\"l\"]\n            if not vlink.startswith(\"/\"):\n                #vlink is encode\n                vlink=getVrsEncodeCode(vlink)\n            key=getDispathKey(vlink.split(\"/\")[-1].split(\".\")[0])\n            baseurl = [x for x in self.baseurl]\n            baseurl.insert(-1,key)\n            url=\"/\".join(baseurl)+vlink+'?su='+self.gen_uid+'&qyid='+uuid4().hex+'&client=&z=&bt=&ct=&tn='+str(randint(10000,20000))\n            urls.append(json.loads(get_content(url))[\"l\"])\n        #download should be complete in 10 minutes\n        #because the url is generated before start downloading\n        #and the key may be expired after 10 minutes\n        self.streams[stream_id]['src'] = urls\n\"\"\"\n\nsite = Iqiyi()\ndownload = site.download_by_url\niqiyi_download_by_vid = site.download_by_vid\ndownload_playlist = site.download_playlist_by_url\n", "levels": [0, 0, 1, 1], "package": ["from ..common import *", "from ..common import print_more_compatible as print", "from ..extractor import VideoExtractor", "from ..util import log", "from .. import json_output", "from uuid import uuid4", "from random import random, randint", "import json", "from math import floor", "from zlib import decompress", "import hashlib", "import time"], "function": ["def getVMS(tvid, vid):\n", "class Iqiyi(VideoExtractor):\n", "    def download_playlist_by_url(self, url, **kwargs):\n", "    def prepare(self, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/acfun.py", "func_name": "acfun_download_by_vid", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.", "docstring_tokens": ["str", "str", "str", "bool", "bool", "-", ">", "None"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109", "partition": "test", "up_fun_num": 2, "context": "#!/usr/bin/env python\n\n__all__ = [\"acfun_download\"]\n\nfrom ..common import *\n\nfrom .le import letvcloud_download_by_vu\nfrom .qq import qq_download_by_vid\nfrom .sina import sina_download_by_vid\nfrom .tudou import tudou_download_by_iid\nfrom .youku import youku_download_by_vid\n\nimport json\nimport re\nimport base64\nimport time\n\n\ndef get_srt_json(id):\n    url = \"http://danmu.aixifan.com/V2/%s\" % id\n    return get_content(url)\n\n\ndef youku_acfun_proxy(vid, sign, ref):\n    endpoint = \"http://player.acfun.cn/flash_data?vid={}&ct=85&ev=3&sign={}&time={}\"\n    url = endpoint.format(vid, sign, str(int(time.time() * 1000)))\n    json_data = json.loads(get_content(url, headers=dict(referer=ref)))[\"data\"]\n    enc_text = base64.b64decode(json_data)\n    dec_text = rc4(b\"8bdc7e1a\", enc_text).decode(\"utf8\")\n    youku_json = json.loads(dec_text)\n\n    yk_streams = {}\n    for stream in youku_json[\"stream\"]:\n        tp = stream[\"stream_type\"]\n        yk_streams[tp] = [], stream[\"total_size\"]\n        if stream.get(\"segs\"):\n            for seg in stream[\"segs\"]:\n                yk_streams[tp][0].append(seg[\"url\"])\n        else:\n            yk_streams[tp] = stream[\"m3u8\"], stream[\"total_size\"]\n\n    return yk_streams\n\n\ndef acfun_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n    assert re.match(r\"http://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)\", url)\n\n    if re.match(r\"http://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)\", url):\n        html = get_content(url)\n        title = r1(r'data-title=\"([^\"]+)\"', html)\n        if match1(url, r\"_(\\d+)$\"):  # current P\n            title = title + \" \" + r1(r'active\">([^<]*)', html)\n        vid = r1('data-vid=\"(\\d+)\"', html)\n        up = r1('data-name=\"([^\"]+)\"', html)\n    # bangumi\n    elif re.match(\"http://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/ab(\\d+)\", url):\n        html = get_content(url)\n        title = match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n        if match1(url, r\"_(\\d+)$\"):  # current P\n            title = title + \" \" + r1(r'active\">([^<]*)', html)\n        vid = match1(html, r'videoId=\"(\\d+)\"')\n        up = \"acfun\"\n    else:\n        raise NotImplemented\n\n    assert title and vid\n    title = unescape_html(title)\n    title = escape_file_path(title)\n    p_title = r1('active\">([^<]+)', html)\n    title = \"%s (%s)\" % (title, up)\n    if p_title:\n        title = \"%s - %s\" % (title, p_title)\n\n    acfun_download_by_vid(\n        vid, title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs\n    )\n\n\nsite_info = \"AcFun.tv\"\ndownload = acfun_download\ndownload_playlist = playlist_not_supported(\"acfun\")\n", "levels": [0, 0, 0], "package": ["from ..common import *", "from .le import letvcloud_download_by_vu", "from .qq import qq_download_by_vid", "from .sina import sina_download_by_vid", "from .tudou import tudou_download_by_iid", "from .youku import youku_download_by_vid", "import json", "import re", "import base64", "import time"], "function": ["def get_srt_json(id):\n", "def youku_acfun_proxy(vid, sign, ref):\n", "def acfun_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "matchall", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.", "docstring_tokens": ["Scans", "through", "a", "string", "for", "substrings", "matched", "some", "patterns", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268", "partition": "test", "up_fun_num": 7, "context": "#!/usr/bin/env python\n\nimport io\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport socket\nimport locale\nimport logging\nimport argparse\nimport ssl\nfrom http import cookiejar\nfrom importlib import import_module\nfrom urllib import request, parse, error\n\nfrom .version import __version__\nfrom .util import log, term\nfrom .util.git import get_version\nfrom .util.strings import get_filename, unescape_html\nfrom . import json_output as json_output_\n\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf8\")\n\nSITES = {\n    \"163\": \"netease\",\n    \"56\": \"w56\",\n    \"365yg\": \"toutiao\",\n    \"acfun\": \"acfun\",\n    \"archive\": \"archive\",\n    \"baidu\": \"baidu\",\n    \"bandcamp\": \"bandcamp\",\n    \"baomihua\": \"baomihua\",\n    \"bigthink\": \"bigthink\",\n    \"bilibili\": \"bilibili\",\n    \"cctv\": \"cntv\",\n    \"cntv\": \"cntv\",\n    \"cbs\": \"cbs\",\n    \"coub\": \"coub\",\n    \"dailymotion\": \"dailymotion\",\n    \"douban\": \"douban\",\n    \"douyin\": \"douyin\",\n    \"douyu\": \"douyutv\",\n    \"ehow\": \"ehow\",\n    \"facebook\": \"facebook\",\n    \"fc2\": \"fc2video\",\n    \"flickr\": \"flickr\",\n    \"freesound\": \"freesound\",\n    \"fun\": \"funshion\",\n    \"google\": \"google\",\n    \"giphy\": \"giphy\",\n    \"heavy-music\": \"heavymusic\",\n    \"huomao\": \"huomaotv\",\n    \"iask\": \"sina\",\n    \"icourses\": \"icourses\",\n    \"ifeng\": \"ifeng\",\n    \"imgur\": \"imgur\",\n    \"in\": \"alive\",\n    \"infoq\": \"infoq\",\n    \"instagram\": \"instagram\",\n    \"interest\": \"interest\",\n    \"iqilu\": \"iqilu\",\n    \"iqiyi\": \"iqiyi\",\n    \"ixigua\": \"ixigua\",\n    \"isuntv\": \"suntv\",\n    \"iwara\": \"iwara\",\n    \"joy\": \"joy\",\n    \"kankanews\": \"bilibili\",\n    \"khanacademy\": \"khan\",\n    \"ku6\": \"ku6\",\n    \"kuaishou\": \"kuaishou\",\n    \"kugou\": \"kugou\",\n    \"kuwo\": \"kuwo\",\n    \"le\": \"le\",\n    \"letv\": \"le\",\n    \"lizhi\": \"lizhi\",\n    \"longzhu\": \"longzhu\",\n    \"magisto\": \"magisto\",\n    \"metacafe\": \"metacafe\",\n    \"mgtv\": \"mgtv\",\n    \"miomio\": \"miomio\",\n    \"mixcloud\": \"mixcloud\",\n    \"mtv81\": \"mtv81\",\n    \"musicplayon\": \"musicplayon\",\n    \"miaopai\": \"yixia\",\n    \"naver\": \"naver\",\n    \"7gogo\": \"nanagogo\",\n    \"nicovideo\": \"nicovideo\",\n    \"panda\": \"panda\",\n    \"pinterest\": \"pinterest\",\n    \"pixnet\": \"pixnet\",\n    \"pptv\": \"pptv\",\n    \"qingting\": \"qingting\",\n    \"qq\": \"qq\",\n    \"showroom-live\": \"showroom\",\n    \"sina\": \"sina\",\n    \"smgbb\": \"bilibili\",\n    \"sohu\": \"sohu\",\n    \"soundcloud\": \"soundcloud\",\n    \"ted\": \"ted\",\n    \"theplatform\": \"theplatform\",\n    \"tiktok\": \"tiktok\",\n    \"tucao\": \"tucao\",\n    \"tudou\": \"tudou\",\n    \"tumblr\": \"tumblr\",\n    \"twimg\": \"twitter\",\n    \"twitter\": \"twitter\",\n    \"ucas\": \"ucas\",\n    \"videomega\": \"videomega\",\n    \"vidto\": \"vidto\",\n    \"vimeo\": \"vimeo\",\n    \"wanmen\": \"wanmen\",\n    \"weibo\": \"miaopai\",\n    \"veoh\": \"veoh\",\n    \"vine\": \"vine\",\n    \"vk\": \"vk\",\n    \"xiami\": \"xiami\",\n    \"xiaokaxiu\": \"yixia\",\n    \"xiaojiadianvideo\": \"fc2video\",\n    \"ximalaya\": \"ximalaya\",\n    \"yinyuetai\": \"yinyuetai\",\n    \"yizhibo\": \"yizhibo\",\n    \"youku\": \"youku\",\n    \"youtu\": \"youtube\",\n    \"youtube\": \"youtube\",\n    \"zhanqi\": \"zhanqi\",\n    \"zhibo\": \"zhibo\",\n    \"zhihu\": \"zhihu\",\n}\n\ndry_run = False\njson_output = False\nforce = False\nplayer = None\nextractor_proxy = None\ncookies = None\noutput_filename = None\nauto_rename = False\ninsecure = False\n\nfake_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",  # noqa\n    \"Accept-Charset\": \"UTF-8,*;q=0.5\",\n    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0\",  # noqa\n}\n\nif sys.stdout.isatty():\n    default_encoding = sys.stdout.encoding.lower()\nelse:\n    default_encoding = locale.getpreferredencoding().lower()\n\n\ndef rc4(key, data):\n    # all encryption algo should work on bytes\n    assert type(key) == type(data) and type(key) == type(b\"\")\n    state = list(range(256))\n    j = 0\n    for i in range(256):\n        j += state[i] + key[i % len(key)]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n\n    i = 0\n    j = 0\n    out_list = []\n    for char in data:\n        i += 1\n        i &= 0xFF\n        j += state[i]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n        prn = state[(state[i] + state[j]) & 0xFF]\n        out_list.append(char ^ prn)\n\n    return bytes(out_list)\n\n\ndef general_m3u8_extractor(url, headers={}):\n    m3u8_list = get_content(url, headers=headers).split(\"\\n\")\n    urls = []\n    for line in m3u8_list:\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            if line.startswith(\"http\"):\n                urls.append(line)\n            else:\n                seg_url = parse.urljoin(url, line)\n                urls.append(seg_url)\n    return urls\n\n\ndef maybe_print(*s):\n    try:\n        print(*s)\n    except:\n        pass\n\n\ndef tr(s):\n    if default_encoding == \"utf-8\":\n        return s\n    else:\n        return s\n        # return str(s.encode('utf-8'))[2:-1]\n\n\n# DEPRECATED in favor of match1()\ndef r1(pattern, text):\n    m = re.search(pattern, text)\n    if m:\n        return m.group(1)\n\n\n# DEPRECATED in favor of match1()\ndef r1_of(patterns, text):\n    for p in patterns:\n        x = r1(p, text)\n        if x:\n            return x\n\n\ndef match1(text, *patterns):\n    \"\"\"Scans through a string for substrings matched some patterns (first-subgroups only).\n\n    Args:\n        text: A string to be scanned.\n        patterns: Arbitrary number of regex patterns.\n\n    Returns:\n        When only one pattern is given, returns a string (None if no match found).\n        When more than one pattern are given, returns a list of strings ([] if no match found).\n    \"\"\"\n\n    if len(patterns) == 1:\n        pattern = patterns[0]\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    else:\n        ret = []\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                ret.append(match.group(1))\n        return ret\n\n\ndef launch_player(player, urls):\n    import subprocess\n    import shlex\n\n    if sys.version_info >= (3, 3):\n        import shutil\n\n        exefile = shlex.split(player)[0]\n        if shutil.which(exefile) is not None:\n            subprocess.call(shlex.split(player) + list(urls))\n        else:\n            log.wtf('[Failed] Cannot find player \"%s\"' % exefile)\n    else:\n        subprocess.call(shlex.split(player) + list(urls))\n\n\ndef parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None\n\n\ndef unicodize(text):\n    return re.sub(\n        r\"\\\\u([0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f])\",\n        lambda x: chr(int(x.group(0)[2:], 16)),\n        text,\n    )\n\n\n# DEPRECATED in favor of util.legitimize()\ndef escape_file_path(path):\n    path = path.replace(\"/\", \"-\")\n    path = path.replace(\"\\\\\", \"-\")\n    path = path.replace(\"*\", \"-\")\n    path = path.replace(\"?\", \"-\")\n    return path\n\n\ndef ungzip(data):\n    \"\"\"Decompresses data for Content-Encoding: gzip.\"\"\"\n    from io import BytesIO\n    import gzip\n\n    buffer = BytesIO(data)\n    f = gzip.GzipFile(fileobj=buffer)\n    return f.read()\n\n\ndef undeflate(data):\n    \"\"\"Decompresses data for Content-Encoding: deflate.\n    (the zlib compression is used.)\n    \"\"\"\n    import zlib\n\n    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)\n    return decompressobj.decompress(data) + decompressobj.flush()\n\n\n# DEPRECATED in favor of get_content()\ndef get_response(url, faker=False):\n    logging.debug(\"get_response: %s\" % url)\n\n    # install cookies\n    if cookies:\n        opener = request.build_opener(request.HTTPCookieProcessor(cookies))\n        request.install_opener(opener)\n\n    if faker:\n        response = request.urlopen(request.Request(url, headers=fake_headers), None)\n    else:\n        response = request.urlopen(url)\n\n    data = response.read()\n    if response.info().get(\"Content-Encoding\") == \"gzip\":\n        data = ungzip(data)\n    elif response.info().get(\"Content-Encoding\") == \"deflate\":\n        data = undeflate(data)\n    response.data = data\n    return response\n\n\n# DEPRECATED in favor of get_content()\ndef get_html(url, encoding=None, faker=False):\n    content = get_response(url, faker).data\n    return str(content, \"utf-8\", \"ignore\")\n\n\n# DEPRECATED in favor of get_content()\ndef get_decoded_html(url, faker=False):\n    response = get_response(url, faker)\n    data = response.data\n    charset = r1(r\"charset=([\\w-]+)\", response.headers[\"content-type\"])\n    if charset:\n        return data.decode(charset, \"ignore\")\n    else:\n        return data\n\n\ndef get_location(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_location: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.geturl()\n\n\ndef urlopen_with_retry(*args, **kwargs):\n    retry_time = 3\n    for i in range(retry_time):\n        try:\n            if insecure:\n                # ignore ssl errors\n                ctx = ssl.create_default_context()\n                ctx.check_hostname = False\n                ctx.verify_mode = ssl.CERT_NONE\n                return request.urlopen(*args, context=ctx, **kwargs)\n            else:\n                return request.urlopen(*args, **kwargs)\n        except socket.timeout as e:\n            logging.debug(\"request attempt %s timeout\" % str(i + 1))\n            if i + 1 == retry_time:\n                raise e\n        # try to tackle youku CDN fails\n        except error.HTTPError as http_error:\n            logging.debug(\"HTTP Error with code{}\".format(http_error.code))\n            if i + 1 == retry_time:\n                raise http_error\n\n\ndef get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug(\"get_content: %s\" % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\", \"\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset, \"ignore\")\n        else:\n            data = data.decode(\"utf-8\", \"ignore\")\n\n    return data\n\n\ndef post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get(\"post_data_raw\"):\n        logging.debug(\n            \"post_content: %s\\npost_data_raw: %s\" % (url, kwargs[\"post_data_raw\"])\n        )\n    else:\n        logging.debug(\"post_content: %s\\npost_data: %s\" % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get(\"post_data_raw\"):\n        post_data_enc = bytes(kwargs[\"post_data_raw\"], \"utf-8\")\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), \"utf-8\")\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode(\"utf-8\")\n\n    return data\n\n\ndef url_size(url, faker=False, headers={}):\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(url)\n\n    size = response.headers[\"content-length\"]\n    return int(size) if size is not None else float(\"inf\")\n\n\ndef urls_size(urls, faker=False, headers={}):\n    return sum([url_size(url, faker=faker, headers=headers) for url in urls])\n\n\ndef get_head(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_head: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.headers\n\n\ndef url_info(url, faker=False, headers={}):\n    logging.debug(\"url_info: %s\" % url)\n\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(request.Request(url))\n\n    headers = response.headers\n\n    type = headers[\"content-type\"]\n    if type == \"image/jpg; charset=UTF-8\" or type == \"image/jpg\":\n        type = \"audio/mpeg\"  # fix for netease\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/f4v\": \"flv\",\n        \"video/mp4\": \"mp4\",\n        \"video/MP2T\": \"ts\",\n        \"video/quicktime\": \"mov\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n        \"video/x-ms-asf\": \"asf\",\n        \"audio/mp4\": \"mp4\",\n        \"audio/mpeg\": \"mp3\",\n        \"audio/wav\": \"wav\",\n        \"audio/x-wav\": \"wav\",\n        \"audio/wave\": \"wav\",\n        \"image/jpeg\": \"jpg\",\n        \"image/png\": \"png\",\n        \"image/gif\": \"gif\",\n        \"application/pdf\": \"pdf\",\n    }\n    if type in mapping:\n        ext = mapping[type]\n    else:\n        type = None\n        if headers[\"content-disposition\"]:\n            try:\n                filename = parse.unquote(\n                    r1(r'filename=\"?([^\"]+)\"?', headers[\"content-disposition\"])\n                )\n                if len(filename.split(\".\")) > 1:\n                    ext = filename.split(\".\")[-1]\n                else:\n                    ext = None\n            except:\n                ext = None\n        else:\n            ext = None\n\n    if headers[\"transfer-encoding\"] != \"chunked\":\n        size = headers[\"content-length\"] and int(headers[\"content-length\"])\n    else:\n        size = None\n\n    return type, ext, size\n\n\ndef url_locations(urls, faker=False, headers={}):\n    locations = []\n    for url in urls:\n        logging.debug(\"url_locations: %s\" % url)\n\n        if faker:\n            response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n        elif headers:\n            response = urlopen_with_retry(request.Request(url, headers=headers))\n        else:\n            response = urlopen_with_retry(request.Request(url))\n\n        locations.append(response.url)\n    return locations\n\n\ndef url_save(\n    url,\n    filepath,\n    bar,\n    refer=None,\n    is_part=False,\n    faker=False,\n    headers=None,\n    timeout=None,\n    **kwargs\n):\n    tmp_headers = headers.copy() if headers is not None else {}\n    # When a referer specified with param refer,\n    # the key must be 'Referer' for the hack here\n    if refer is not None:\n        tmp_headers[\"Referer\"] = refer\n    if type(url) is list:\n        file_size = urls_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = True, url\n    else:\n        file_size = url_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = False, [url]\n\n    continue_renameing = True\n    while continue_renameing:\n        continue_renameing = False\n        if os.path.exists(filepath):\n            if not force and file_size == os.path.getsize(filepath):\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    log.w(\n                        \"Skipping {}: file already exists\".format(\n                            tr(os.path.basename(filepath))\n                        )\n                    )\n                else:\n                    if bar:\n                        bar.update_received(file_size)\n                return\n            else:\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    if not force and auto_rename:\n                        path, ext = os.path.basename(filepath).rsplit(\".\", 1)\n                        finder = re.compile(\" \\([1-9]\\d*?\\)$\")\n                        if finder.search(path) is None:\n                            thisfile = path + \" (1).\" + ext\n                        else:\n\n                            def numreturn(a):\n                                return \" (\" + str(int(a.group()[2:-1]) + 1) + \").\"\n\n                            thisfile = finder.sub(numreturn, path) + ext\n                        filepath = os.path.join(os.path.dirname(filepath), thisfile)\n                        print(\n                            \"Changing name to %s\" % tr(os.path.basename(filepath)),\n                            \"...\",\n                        )\n                        continue_renameing = True\n                        continue\n                    if log.yes_or_no(\"File with this name already exists. Overwrite?\"):\n                        log.w(\"Overwriting %s ...\" % tr(os.path.basename(filepath)))\n                    else:\n                        return\n        elif not os.path.exists(os.path.dirname(filepath)):\n            os.mkdir(os.path.dirname(filepath))\n\n    temp_filepath = filepath + \".download\" if file_size != float(\"inf\") else filepath\n    received = 0\n    if not force:\n        open_mode = \"ab\"\n\n        if os.path.exists(temp_filepath):\n            received += os.path.getsize(temp_filepath)\n            if bar:\n                bar.update_received(os.path.getsize(temp_filepath))\n    else:\n        open_mode = \"wb\"\n\n    for url in urls:\n        received_chunk = 0\n        if received < file_size:\n            if faker:\n                tmp_headers = fake_headers\n            \"\"\"\n            if parameter headers passed in, we have it copied as tmp_header\n            elif headers:\n                headers = headers\n            else:\n                headers = {}\n            \"\"\"\n            if received and not is_chunked:  # only request a range when not chunked\n                tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n            if refer:\n                tmp_headers[\"Referer\"] = refer\n\n            if timeout:\n                response = urlopen_with_retry(\n                    request.Request(url, headers=tmp_headers), timeout=timeout\n                )\n            else:\n                response = urlopen_with_retry(request.Request(url, headers=tmp_headers))\n            try:\n                range_start = int(\n                    response.headers[\"content-range\"][6:].split(\"/\")[0].split(\"-\")[0]\n                )\n                end_length = int(response.headers[\"content-range\"][6:].split(\"/\")[1])\n                range_length = end_length - range_start\n            except:\n                content_length = response.headers[\"content-length\"]\n                range_length = (\n                    int(content_length) if content_length is not None else float(\"inf\")\n                )\n\n            if is_chunked:  # always append if chunked\n                open_mode = \"ab\"\n            elif file_size != received + range_length:  # is it ever necessary?\n                received = 0\n                if bar:\n                    bar.received = 0\n                open_mode = \"wb\"\n\n            with open(temp_filepath, open_mode) as output:\n                while True:\n                    buffer = None\n                    try:\n                        buffer = response.read(1024 * 256)\n                    except socket.timeout:\n                        pass\n                    if not buffer:\n                        if is_chunked and received_chunk == range_length:\n                            break\n                        elif (\n                            not is_chunked and received == file_size\n                        ):  # Download finished\n                            break\n                        # Unexpected termination. Retry request\n                        if not is_chunked:  # when\n                            tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n                        response = urlopen_with_retry(\n                            request.Request(url, headers=tmp_headers)\n                        )\n                        continue\n                    output.write(buffer)\n                    received += len(buffer)\n                    received_chunk += len(buffer)\n                    if bar:\n                        bar.update_received(len(buffer))\n\n    assert received == os.path.getsize(temp_filepath), \"%s == %s == %s\" % (\n        received,\n        os.path.getsize(temp_filepath),\n        temp_filepath,\n    )\n\n    if os.access(filepath, os.W_OK):\n        # on Windows rename could fail if destination filepath exists\n        os.remove(filepath)\n    os.rename(temp_filepath, filepath)\n\n\nclass SimpleProgressBar:\n    term_size = term.get_terminal_size()[1]\n\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n        self.speed = \"\"\n        self.last_updated = time.time()\n\n        total_pieces_len = len(str(total_pieces))\n        # 38 is the size of all statically known size in self.bar\n        total_str = \"%5s\" % round(self.total_size / 1048576, 1)\n        total_str_width = max(len(total_str), 5)\n        self.bar_size = self.term_size - 28 - 2 * total_pieces_len - 2 * total_str_width\n        self.bar = \"{:>4}%% ({:>%s}/%sMB) \u251c{:\u2500<%s}\u2524[{:>%s}/{:>%s}] {}\" % (\n            total_str_width,\n            total_str,\n            self.bar_size,\n            total_pieces_len,\n            total_pieces_len,\n        )\n\n    def update(self):\n        self.displayed = True\n        bar_size = self.bar_size\n        percent = round(self.received * 100 / self.total_size, 1)\n        if percent >= 100:\n            percent = 100\n        dots = bar_size * int(percent) // 100\n        plus = int(percent) - dots // bar_size * 100\n        if plus > 0.8:\n            plus = \"\u2588\"\n        elif plus > 0.4:\n            plus = \">\"\n        else:\n            plus = \"\"\n        bar = \"\u2588\" * dots + plus\n        bar = self.bar.format(\n            percent,\n            round(self.received / 1048576, 1),\n            bar,\n            self.current_piece,\n            self.total_pieces,\n            self.speed,\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        time_diff = time.time() - self.last_updated\n        bytes_ps = n / time_diff if time_diff else 0\n        if bytes_ps >= 1024 ** 3:\n            self.speed = \"{:4.0f} GB/s\".format(bytes_ps / 1024 ** 3)\n        elif bytes_ps >= 1024 ** 2:\n            self.speed = \"{:4.0f} MB/s\".format(bytes_ps / 1024 ** 2)\n        elif bytes_ps >= 1024:\n            self.speed = \"{:4.0f} kB/s\".format(bytes_ps / 1024)\n        else:\n            self.speed = \"{:4.0f}  B/s\".format(bytes_ps)\n        self.last_updated = time.time()\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass PiecesProgressBar:\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n\n    def update(self):\n        self.displayed = True\n        bar = \"{0:>5}%[{1:<40}] {2}/{3}\".format(\n            \"\", \"=\" * 40, self.current_piece, self.total_pieces\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass DummyProgressBar:\n    def __init__(self, *args):\n        pass\n\n    def update_received(self, n):\n        pass\n\n    def update_piece(self, n):\n        pass\n\n    def done(self):\n        pass\n\n\ndef get_output_filename(urls, title, ext, output_dir, merge):\n    # lame hack for the --output-filename option\n    global output_filename\n    if output_filename:\n        if ext:\n            return output_filename + \".\" + ext\n        return output_filename\n\n    merged_ext = ext\n    if (len(urls) > 1) and merge:\n        from .processor.ffmpeg import has_ffmpeg_installed\n\n        if ext in [\"flv\", \"f4v\"]:\n            if has_ffmpeg_installed():\n                merged_ext = \"mp4\"\n            else:\n                merged_ext = \"flv\"\n        elif ext == \"mp4\":\n            merged_ext = \"mp4\"\n        elif ext == \"ts\":\n            if has_ffmpeg_installed():\n                merged_ext = \"mkv\"\n            else:\n                merged_ext = \"ts\"\n    return \"%s.%s\" % (title, merged_ext)\n\n\ndef print_user_agent(faker=False):\n    urllib_default_user_agent = \"Python-urllib/%d.%d\" % sys.version_info[:2]\n    user_agent = fake_headers[\"User-Agent\"] if faker else urllib_default_user_agent\n    print(\"User Agent: %s\" % user_agent)\n\n\ndef download_urls(\n    urls,\n    title,\n    ext,\n    total_size,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    headers={},\n    **kwargs\n):\n    assert urls\n    if json_output:\n        json_output_.download_urls(\n            urls=urls, title=title, ext=ext, total_size=total_size, refer=refer\n        )\n        return\n    if dry_run:\n        print_user_agent(faker=faker)\n        try:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join(urls))\n        except:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join([j for i in urls for j in i]))\n        return\n\n    if player:\n        launch_player(player, urls)\n        return\n\n    if not total_size:\n        try:\n            total_size = urls_size(urls, faker=faker, headers=headers)\n        except:\n            import traceback\n\n            traceback.print_exc(file=sys.stdout)\n            pass\n\n    title = tr(get_filename(title))\n    output_filename = get_output_filename(urls, title, ext, output_dir, merge)\n    output_filepath = os.path.join(output_dir, output_filename)\n\n    if total_size:\n        if (\n            not force\n            and os.path.exists(output_filepath)\n            and not auto_rename\n            and os.path.getsize(output_filepath) >= total_size * 0.9\n        ):\n            log.w(\"Skipping %s: file already exists\" % output_filepath)\n            print()\n            return\n        bar = SimpleProgressBar(total_size, len(urls))\n    else:\n        bar = PiecesProgressBar(total_size, len(urls))\n\n    if len(urls) == 1:\n        url = urls[0]\n        print(\"Downloading %s ...\" % tr(output_filename))\n        bar.update()\n        url_save(\n            url,\n            output_filepath,\n            bar,\n            refer=refer,\n            faker=faker,\n            headers=headers,\n            **kwargs\n        )\n        bar.done()\n    else:\n        parts = []\n        print(\"Downloading %s.%s ...\" % (tr(title), ext))\n        bar.update()\n        for i, url in enumerate(urls):\n            filename = \"%s[%02d].%s\" % (title, i, ext)\n            filepath = os.path.join(output_dir, filename)\n            parts.append(filepath)\n            # print 'Downloading %s [%s/%s]...' % (tr(filename), i + 1, len(urls))\n            bar.update_piece(i + 1)\n            url_save(\n                url,\n                filepath,\n                bar,\n                refer=refer,\n                is_part=True,\n                faker=faker,\n                headers=headers,\n                **kwargs\n            )\n        bar.done()\n\n        if not merge:\n            print()\n            return\n\n        if \"av\" in kwargs and kwargs[\"av\"]:\n            from .processor.ffmpeg import has_ffmpeg_installed\n\n            if has_ffmpeg_installed():\n                from .processor.ffmpeg import ffmpeg_concat_av\n\n                ret = ffmpeg_concat_av(parts, output_filepath, ext)\n                print(\"Merged into %s\" % output_filename)\n                if ret == 0:\n                    for part in parts:\n                        os.remove(part)\n\n        elif ext in [\"flv\", \"f4v\"]:\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_flv_to_mp4\n\n                    ffmpeg_concat_flv_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_flv import concat_flv\n\n                    concat_flv(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"mp4\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_mp4_to_mp4\n\n                    ffmpeg_concat_mp4_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_mp4 import concat_mp4\n\n                    concat_mp4(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"ts\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_ts_to_mkv\n\n                    ffmpeg_concat_ts_to_mkv(parts, output_filepath)\n                else:\n                    from .processor.join_ts import concat_ts\n\n                    concat_ts(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        else:\n            print(\"Can't merge %s files\" % ext)\n\n    print()\n\n\ndef download_rtmp_url(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset -> False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        from .processor.rtmpdump import play_rtmpdump_stream\n\n        play_rtmpdump_stream(player, url, params)\n        return\n\n    from .processor.rtmpdump import has_rtmpdump_installed, download_rtmpdump_stream\n\n    assert has_rtmpdump_installed(), \"RTMPDump not installed.\"\n    download_rtmpdump_stream(url, title, ext, params, output_dir)\n\n\ndef download_url_ffmpeg(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    stream=True,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset ->False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        launch_player(player, [url])\n        return\n\n    from .processor.ffmpeg import has_ffmpeg_installed, ffmpeg_download_stream\n\n    assert has_ffmpeg_installed(), \"FFmpeg not installed.\"\n\n    global output_filename\n    if output_filename:\n        dotPos = output_filename.rfind(\".\")\n        if dotPos > 0:\n            title = output_filename[:dotPos]\n            ext = output_filename[dotPos + 1 :]\n        else:\n            title = output_filename\n\n    title = tr(get_filename(title))\n\n    ffmpeg_download_stream(url, title, ext, params, output_dir, stream=stream)\n\n\ndef playlist_not_supported(name):\n    def f(*args, **kwargs):\n        raise NotImplementedError(\"Playlist is not supported for \" + name)\n\n    return f\n\n\ndef print_info(site_info, title, type, size, **kwargs):\n    if json_output:\n        json_output_.print_info(site_info=site_info, title=title, type=type, size=size)\n        return\n    if type:\n        type = type.lower()\n    if type in [\"3gp\"]:\n        type = \"video/3gpp\"\n    elif type in [\"asf\", \"wmv\"]:\n        type = \"video/x-ms-asf\"\n    elif type in [\"flv\", \"f4v\"]:\n        type = \"video/x-flv\"\n    elif type in [\"mkv\"]:\n        type = \"video/x-matroska\"\n    elif type in [\"mp3\"]:\n        type = \"audio/mpeg\"\n    elif type in [\"mp4\"]:\n        type = \"video/mp4\"\n    elif type in [\"mov\"]:\n        type = \"video/quicktime\"\n    elif type in [\"ts\"]:\n        type = \"video/MP2T\"\n    elif type in [\"webm\"]:\n        type = \"video/webm\"\n\n    elif type in [\"jpg\"]:\n        type = \"image/jpeg\"\n    elif type in [\"png\"]:\n        type = \"image/png\"\n    elif type in [\"gif\"]:\n        type = \"image/gif\"\n\n    if type in [\"video/3gpp\"]:\n        type_info = \"3GPP multimedia file (%s)\" % type\n    elif type in [\"video/x-flv\", \"video/f4v\"]:\n        type_info = \"Flash video (%s)\" % type\n    elif type in [\"video/mp4\", \"video/x-m4v\"]:\n        type_info = \"MPEG-4 video (%s)\" % type\n    elif type in [\"video/MP2T\"]:\n        type_info = \"MPEG-2 transport stream (%s)\" % type\n    elif type in [\"video/webm\"]:\n        type_info = \"WebM video (%s)\" % type\n    # elif type in ['video/ogg']:\n    #    type_info = 'Ogg video (%s)' % type\n    elif type in [\"video/quicktime\"]:\n        type_info = \"QuickTime video (%s)\" % type\n    elif type in [\"video/x-matroska\"]:\n        type_info = \"Matroska video (%s)\" % type\n    # elif type in ['video/x-ms-wmv']:\n    #    type_info = 'Windows Media video (%s)' % type\n    elif type in [\"video/x-ms-asf\"]:\n        type_info = \"Advanced Systems Format (%s)\" % type\n    # elif type in ['video/mpeg']:\n    #    type_info = 'MPEG video (%s)' % type\n    elif type in [\"audio/mp4\", \"audio/m4a\"]:\n        type_info = \"MPEG-4 audio (%s)\" % type\n    elif type in [\"audio/mpeg\"]:\n        type_info = \"MP3 (%s)\" % type\n    elif type in [\"audio/wav\", \"audio/wave\", \"audio/x-wav\"]:\n        type_info = \"Waveform Audio File Format ({})\".format(type)\n\n    elif type in [\"image/jpeg\"]:\n        type_info = \"JPEG Image (%s)\" % type\n    elif type in [\"image/png\"]:\n        type_info = \"Portable Network Graphics (%s)\" % type\n    elif type in [\"image/gif\"]:\n        type_info = \"Graphics Interchange Format (%s)\" % type\n    elif type in [\"m3u8\"]:\n        if \"m3u8_type\" in kwargs:\n            if kwargs[\"m3u8_type\"] == \"master\":\n                type_info = \"M3U8 Master {}\".format(type)\n        else:\n            type_info = \"M3U8 Playlist {}\".format(type)\n    else:\n        type_info = \"Unknown type (%s)\" % type\n\n    maybe_print(\"Site:      \", site_info)\n    maybe_print(\"Title:     \", unescape_html(tr(title)))\n    print(\"Type:      \", type_info)\n    if type != \"m3u8\":\n        print(\"Size:      \", round(size / 1048576, 2), \"MiB (\" + str(size) + \" Bytes)\")\n    if type == \"m3u8\" and \"m3u8_url\" in kwargs:\n        print(\"M3U8 Url:   {}\".format(kwargs[\"m3u8_url\"]))\n    print()\n\n\ndef mime_to_container(mime):\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/mp4\": \"mp4\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n    }\n    if mime in mapping:\n        return mapping[mime]\n    else:\n        return mime.split(\"/\")[1]\n\n\ndef parse_host(host):\n    \"\"\"Parses host name and port number from a string.\"\"\"\n    if re.match(r\"^(\\d+)$\", host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r\"^(\\w+)://\", host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)\n\n\ndef set_proxy(proxy):\n    proxy_handler = request.ProxyHandler(\n        {\n            \"http\": \"%s:%s\" % proxy,\n            \"https\": \"%s:%s\" % proxy,\n        }\n    )\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\ndef unset_proxy():\n    proxy_handler = request.ProxyHandler({})\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\n# DEPRECATED in favor of set_proxy() and unset_proxy()\ndef set_http_proxy(proxy):\n    if proxy is None:  # Use system default setting\n        proxy_support = request.ProxyHandler()\n    elif proxy == \"\":  # Don't use any proxy\n        proxy_support = request.ProxyHandler({})\n    else:  # Use proxy\n        proxy_support = request.ProxyHandler(\n            {\"http\": \"%s\" % proxy, \"https\": \"%s\" % proxy}\n        )\n    opener = request.build_opener(proxy_support)\n    request.install_opener(opener)\n\n\ndef print_more_compatible(*args, **kwargs):\n    import builtins as __builtin__\n\n    \"\"\"Overload default print function as py (<3.3) does not support 'flush' keyword.\n    Although the function name can be same as print to get itself overloaded automatically,\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n    \"\"\"\n    # nothing happens on py3.3 and later\n    if sys.version_info[:2] >= (3, 3):\n        return __builtin__.print(*args, **kwargs)\n\n    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested\n    doFlush = kwargs.pop(\"flush\", False)\n    ret = __builtin__.print(*args, **kwargs)\n    if doFlush:\n        kwargs.get(\"file\", sys.stdout).flush()\n    return ret\n\n\ndef download_main(download, download_playlist, urls, playlist, **kwargs):\n    for url in urls:\n        if re.match(r\"https?://\", url) is None:\n            url = \"http://\" + url\n\n        if playlist:\n            download_playlist(url, **kwargs)\n        else:\n            download(url, **kwargs)\n\n\ndef load_cookies(cookiefile):\n    global cookies\n    if cookiefile.endswith(\".txt\"):\n        # MozillaCookieJar treats prefix '#HttpOnly_' as comments incorrectly!\n        # do not use its load()\n        # see also:\n        #   - https://docs.python.org/3/library/http.cookiejar.html#http.cookiejar.MozillaCookieJar\n        #   - https://github.com/python/cpython/blob/4b219ce/Lib/http/cookiejar.py#L2014\n        #   - https://curl.haxx.se/libcurl/c/CURLOPT_COOKIELIST.html#EXAMPLE\n        # cookies = cookiejar.MozillaCookieJar(cookiefile)\n        # cookies.load()\n        from http.cookiejar import Cookie\n\n        cookies = cookiejar.MozillaCookieJar()\n        now = time.time()\n        ignore_discard, ignore_expires = False, False\n        with open(cookiefile, \"r\") as f:\n            for line in f:\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"):\n                    line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if line.strip().startswith((\"#\", \"$\")) or line.strip() == \"\":\n                    if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                        continue\n\n                (\n                    domain,\n                    domain_specified,\n                    path,\n                    secure,\n                    expires,\n                    name,\n                    value,\n                ) = line.split(\"\\t\")\n                secure = secure == \"TRUE\"\n                domain_specified = domain_specified == \"TRUE\"\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas http.cookiejar regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                    assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(\n                    0,\n                    name,\n                    value,\n                    None,\n                    False,\n                    domain,\n                    domain_specified,\n                    initial_dot,\n                    path,\n                    False,\n                    secure,\n                    expires,\n                    discard,\n                    None,\n                    None,\n                    {},\n                )\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                cookies.set_cookie(c)\n\n    elif cookiefile.endswith((\".sqlite\", \".sqlite3\")):\n        import sqlite3, shutil, tempfile\n\n        temp_dir = tempfile.gettempdir()\n        temp_cookiefile = os.path.join(temp_dir, \"temp_cookiefile.sqlite\")\n        shutil.copy2(cookiefile, temp_cookiefile)\n\n        cookies = cookiejar.MozillaCookieJar()\n        con = sqlite3.connect(temp_cookiefile)\n        cur = con.cursor()\n        cur.execute(\n            \"\"\"SELECT host, path, isSecure, expiry, name, value\n        FROM moz_cookies\"\"\"\n        )\n        for item in cur.fetchall():\n            c = cookiejar.Cookie(\n                0,\n                item[4],\n                item[5],\n                None,\n                False,\n                item[0],\n                item[0].startswith(\".\"),\n                item[0].startswith(\".\"),\n                item[1],\n                False,\n                item[2],\n                item[3],\n                item[3] == \"\",\n                None,\n                None,\n                {},\n            )\n            cookies.set_cookie(c)\n\n    else:\n        log.e(\"[error] unsupported cookies format\")\n        # TODO: Chromium Cookies\n        # SELECT host_key, path, secure, expires_utc, name, encrypted_value\n        # FROM cookies\n        # http://n8henrie.com/2013/11/use-chromes-cookies-for-easier-downloading-with-python-requests/\n\n\ndef set_socks_proxy(proxy):\n    try:\n        import socks\n\n        socks_proxy_addrs = proxy.split(\":\")\n        socks.set_default_proxy(\n            socks.SOCKS5, socks_proxy_addrs[0], int(socks_proxy_addrs[1])\n        )\n        socket.socket = socks.socksocket\n\n        def getaddrinfo(*args):\n            return [(socket.AF_INET, socket.SOCK_STREAM, 6, \"\", (args[0], args[1]))]\n\n        socket.getaddrinfo = getaddrinfo\n    except ImportError:\n        log.w(\n            \"Error importing PySocks library, socks proxy ignored.\"\n            \"In order to use use socks proxy, please install PySocks.\"\n        )\n\n\ndef script_main(download, download_playlist, **kwargs):\n    logging.basicConfig(format=\"[%(levelname)s] %(message)s\")\n\n    def print_version():\n        version = get_version(\n            kwargs[\"repo_path\"] if \"repo_path\" in kwargs else __version__\n        )\n        log.i(\"version {}, a tiny downloader that scrapes the web.\".format(version))\n\n    parser = argparse.ArgumentParser(\n        prog=\"you-get\",\n        usage=\"you-get [OPTION]... URL...\",\n        description=\"A tiny downloader that scrapes the web\",\n        add_help=False,\n    )\n    parser.add_argument(\n        \"-V\", \"--version\", action=\"store_true\", help=\"Print version and exit\"\n    )\n    parser.add_argument(\n        \"-h\", \"--help\", action=\"store_true\", help=\"Print this help message and exit\"\n    )\n\n    dry_run_grp = parser.add_argument_group(\n        \"Dry-run options\", \"(no actual downloading)\"\n    )\n    dry_run_grp = dry_run_grp.add_mutually_exclusive_group()\n    dry_run_grp.add_argument(\n        \"-i\", \"--info\", action=\"store_true\", help=\"Print extracted information\"\n    )\n    dry_run_grp.add_argument(\n        \"-u\", \"--url\", action=\"store_true\", help=\"Print extracted information with URLs\"\n    )\n    dry_run_grp.add_argument(\n        \"--json\", action=\"store_true\", help=\"Print extracted URLs in JSON format\"\n    )\n\n    download_grp = parser.add_argument_group(\"Download options\")\n    download_grp.add_argument(\n        \"-n\",\n        \"--no-merge\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not merge video parts\",\n    )\n    download_grp.add_argument(\n        \"--no-caption\",\n        action=\"store_true\",\n        help=\"Do not download captions (subtitles, lyrics, danmaku, ...)\",\n    )\n    download_grp.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force overwriting existing files\",\n    )\n    download_grp.add_argument(\n        \"-F\", \"--format\", metavar=\"STREAM_ID\", help=\"Set video format to STREAM_ID\"\n    )\n    download_grp.add_argument(\n        \"-O\", \"--output-filename\", metavar=\"FILE\", help=\"Set output filename\"\n    )\n    download_grp.add_argument(\n        \"-o\", \"--output-dir\", metavar=\"DIR\", default=\".\", help=\"Set output directory\"\n    )\n    download_grp.add_argument(\n        \"-p\", \"--player\", metavar=\"PLAYER\", help=\"Stream extracted URL to a PLAYER\"\n    )\n    download_grp.add_argument(\n        \"-c\",\n        \"--cookies\",\n        metavar=\"COOKIES_FILE\",\n        help=\"Load cookies.txt or cookies.sqlite\",\n    )\n    download_grp.add_argument(\n        \"-t\",\n        \"--timeout\",\n        metavar=\"SECONDS\",\n        type=int,\n        default=600,\n        help=\"Set socket timeout\",\n    )\n    download_grp.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show traceback and other debug info\"\n    )\n    download_grp.add_argument(\n        \"-I\",\n        \"--input-file\",\n        metavar=\"FILE\",\n        type=argparse.FileType(\"r\"),\n        help=\"Read non-playlist URLs from FILE\",\n    )\n    download_grp.add_argument(\n        \"-P\", \"--password\", help=\"Set video visit password to PASSWORD\"\n    )\n    download_grp.add_argument(\n        \"-l\", \"--playlist\", action=\"store_true\", help=\"Prefer to download a playlist\"\n    )\n    download_grp.add_argument(\n        \"-a\",\n        \"--auto-rename\",\n        action=\"store_true\",\n        default=False,\n        help=\"Auto rename same name different files\",\n    )\n\n    download_grp.add_argument(\n        \"-k\", \"--insecure\", action=\"store_true\", default=False, help=\"ignore ssl errors\"\n    )\n\n    proxy_grp = parser.add_argument_group(\"Proxy options\")\n    proxy_grp = proxy_grp.add_mutually_exclusive_group()\n    proxy_grp.add_argument(\n        \"-x\",\n        \"--http-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for downloading\",\n    )\n    proxy_grp.add_argument(\n        \"-y\",\n        \"--extractor-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for extracting only\",\n    )\n    proxy_grp.add_argument(\"--no-proxy\", action=\"store_true\", help=\"Never use a proxy\")\n    proxy_grp.add_argument(\n        \"-s\",\n        \"--socks-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an SOCKS5 proxy for downloading\",\n    )\n\n    download_grp.add_argument(\"--stream\", help=argparse.SUPPRESS)\n    download_grp.add_argument(\"--itag\", help=argparse.SUPPRESS)\n\n    parser.add_argument(\"URL\", nargs=\"*\", help=argparse.SUPPRESS)\n\n    args = parser.parse_args()\n\n    if args.help:\n        print_version()\n        parser.print_help()\n        sys.exit()\n    if args.version:\n        print_version()\n        sys.exit()\n\n    if args.debug:\n        # Set level of root logger to DEBUG\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    global force\n    global dry_run\n    global json_output\n    global player\n    global extractor_proxy\n    global output_filename\n    global auto_rename\n    global insecure\n    output_filename = args.output_filename\n    extractor_proxy = args.extractor_proxy\n\n    info_only = args.info\n    if args.force:\n        force = True\n    if args.auto_rename:\n        auto_rename = True\n    if args.url:\n        dry_run = True\n    if args.json:\n        json_output = True\n        # to fix extractors not use VideoExtractor\n        dry_run = True\n        info_only = False\n\n    if args.cookies:\n        load_cookies(args.cookies)\n\n    caption = True\n    stream_id = args.format or args.stream or args.itag\n    if args.no_caption:\n        caption = False\n    if args.player:\n        player = args.player\n        caption = False\n\n    if args.insecure:\n        # ignore ssl\n        insecure = True\n\n    if args.no_proxy:\n        set_http_proxy(\"\")\n    else:\n        set_http_proxy(args.http_proxy)\n    if args.socks_proxy:\n        set_socks_proxy(args.socks_proxy)\n\n    URLs = []\n    if args.input_file:\n        logging.debug(\"you are trying to load urls from %s\", args.input_file)\n        if args.playlist:\n            log.e(\n                \"reading playlist from a file is unsupported \"\n                \"and won't make your life easier\"\n            )\n            sys.exit(2)\n        URLs.extend(args.input_file.read().splitlines())\n        args.input_file.close()\n    URLs.extend(args.URL)\n\n    if not URLs:\n        parser.print_help()\n        sys.exit()\n\n    socket.setdefaulttimeout(args.timeout)\n\n    try:\n        extra = {}\n        if extractor_proxy:\n            extra[\"extractor_proxy\"] = extractor_proxy\n        if stream_id:\n            extra[\"stream_id\"] = stream_id\n        download_main(\n            download,\n            download_playlist,\n            URLs,\n            args.playlist,\n            output_dir=args.output_dir,\n            merge=not args.no_merge,\n            info_only=info_only,\n            json_output=json_output,\n            caption=caption,\n            password=args.password,\n            **extra\n        )\n    except KeyboardInterrupt:\n        if args.debug:\n            raise\n        else:\n            sys.exit(1)\n    except UnicodeEncodeError:\n        if args.debug:\n            raise\n        log.e(\n            \"[error] oops, the current environment does not seem to support \" \"Unicode.\"\n        )\n        log.e(\"please set it to a UTF-8-aware locale first,\")\n        log.e(\"so as to save the video (with some Unicode characters) correctly.\")\n        log.e(\"you can do it like this:\")\n        log.e(\"    (Windows)    % chcp 65001 \")\n        log.e(\"    (Linux)      $ LC_CTYPE=en_US.UTF-8\")\n        sys.exit(1)\n    except Exception:\n        if not args.debug:\n            log.e(\"[error] oops, something went wrong.\")\n            log.e(\"don't panic, c'est la vie. please try the following steps:\")\n            log.e(\"  (1) Rule out any network problem.\")\n            log.e(\"  (2) Make sure you-get is up-to-date.\")\n            log.e(\"  (3) Check if the issue is already known, on\")\n            log.e(\"        https://github.com/soimort/you-get/wiki/Known-Bugs\")\n            log.e(\"        https://github.com/soimort/you-get/issues\")\n            log.e(\"  (4) Run the command with '--debug' option,\")\n            log.e(\"      and report this issue with the full output.\")\n        else:\n            print_version()\n            log.i(args)\n            raise\n        sys.exit(1)\n\n\ndef google_search(url):\n    keywords = r1(r\"https?://(.*)\", url)\n    url = \"https://www.google.com/search?tbm=vid&q=%s\" % parse.quote(keywords)\n    page = get_content(url, headers=fake_headers)\n    videos = re.findall(\n        r'<a href=\"(https?://[^\"]+)\" onmousedown=\"[^\"]+\"><h3 class=\"[^\"]*\">([^<]+)<',\n        page,\n    )\n    vdurs = re.findall(r'<span class=\"vdur[^\"]*\">([^<]+)<', page)\n    durs = [r1(r\"(\\d+:\\d+)\", unescape_html(dur)) for dur in vdurs]\n    print(\"Google Videos search:\")\n    for v in zip(videos, durs):\n        print(\"- video:  {} [{}]\".format(unescape_html(v[0][1]), v[1] if v[1] else \"?\"))\n        print(\"# you-get %s\" % log.sprint(v[0][0], log.UNDERLINE))\n        print()\n    print(\"Best matched result:\")\n    return videos[0][0]\n\n\ndef url_to_module(url):\n    try:\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n        assert video_host and video_url\n    except AssertionError:\n        url = google_search(url)\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    if video_host.endswith(\".com.cn\") or video_host.endswith(\".ac.cn\"):\n        video_host = video_host[:-3]\n    domain = r1(r\"(\\.[^.]+\\.[^.]+)$\", video_host) or video_host\n    assert domain, \"unsupported url: \" + url\n\n    # all non-ASCII code points must be quoted (percent-encoded UTF-8)\n    url = \"\".join([ch if ord(ch) in range(128) else parse.quote(ch) for ch in url])\n    video_host = r1(r\"https?://([^/]+)/\", url)\n    video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    k = r1(r\"([^.]+)\", domain)\n    if k in SITES:\n        return (import_module(\".\".join([\"you_get\", \"extractors\", SITES[k]])), url)\n    else:\n        try:\n            location = get_location(url)  # t.co isn't happy with fake_headers\n        except:\n            location = get_location(url, headers=fake_headers)\n\n        if location and location != url and not location.startswith(\"/\"):\n            return url_to_module(location)\n        else:\n            return import_module(\"you_get.extractors.universal\"), url\n\n\ndef any_download(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download(url, **kwargs)\n\n\ndef any_download_playlist(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download_playlist(url, **kwargs)\n\n\ndef main(**kwargs):\n    script_main(any_download, any_download_playlist, **kwargs)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "package": ["import io", "import os", "import re", "import sys", "import time", "import json", "import socket", "import locale", "import logging", "import argparse", "import ssl", "from http import cookiejar", "from importlib import import_module", "from urllib import request, parse, error", "from .version import __version__", "from .util import log, term", "from .util.git import get_version", "from .util.strings import get_filename, unescape_html", "from . import json_output as json_output_", "import subprocess", "import shlex", "import shutil", "from io import BytesIO", "import gzip", "import zlib"], "function": ["def rc4(key, data):\n", "def general_m3u8_extractor(url, headers={}):\n", "def maybe_print(*s):\n", "def tr(s):\n", "def r1(pattern, text):\n", "def r1_of(patterns, text):\n", "def match1(text, *patterns):\n", "def launch_player(player, urls):\n", "def parse_query_param(url, param):\n", "def unicodize(text):\n", "def escape_file_path(path):\n", "def ungzip(data):\n", "def undeflate(data):\n", "def get_response(url, faker=False):\n", "def get_html(url, encoding=None, faker=False):\n", "def get_decoded_html(url, faker=False):\n", "def get_location(url, headers=None, get_method=\"HEAD\"):\n", "def urlopen_with_retry(*args, **kwargs):\n", "def get_content(url, headers={}, decoded=True):\n", "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n", "def url_size(url, faker=False, headers={}):\n", "def urls_size(urls, faker=False, headers={}):\n", "def get_head(url, headers=None, get_method=\"HEAD\"):\n", "def url_info(url, faker=False, headers={}):\n", "def url_locations(urls, faker=False, headers={}):\n", "class SimpleProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class PiecesProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class DummyProgressBar:\n", "    def __init__(self, *args):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "def get_output_filename(urls, title, ext, output_dir, merge):\n", "def print_user_agent(faker=False):\n", "def playlist_not_supported(name):\n", "    def f(*args, **kwargs):\n", "def print_info(site_info, title, type, size, **kwargs):\n", "def mime_to_container(mime):\n", "def parse_host(host):\n", "def set_proxy(proxy):\n", "def unset_proxy():\n", "def set_http_proxy(proxy):\n", "def print_more_compatible(*args, **kwargs):\n", "def download_main(download, download_playlist, urls, playlist, **kwargs):\n", "def load_cookies(cookiefile):\n", "def set_socks_proxy(proxy):\n", "def script_main(download, download_playlist, **kwargs):\n", "    def print_version():\n", "def google_search(url):\n", "def url_to_module(url):\n", "def any_download(url, **kwargs):\n", "def any_download_playlist(url, **kwargs):\n", "def main(**kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "parse_query_param", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.", "docstring_tokens": ["Parses", "the", "query", "string", "of", "a", "URL", "and", "returns", "the", "value", "of", "a", "parameter", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299", "partition": "test", "up_fun_num": 9, "context": "#!/usr/bin/env python\n\nimport io\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport socket\nimport locale\nimport logging\nimport argparse\nimport ssl\nfrom http import cookiejar\nfrom importlib import import_module\nfrom urllib import request, parse, error\n\nfrom .version import __version__\nfrom .util import log, term\nfrom .util.git import get_version\nfrom .util.strings import get_filename, unescape_html\nfrom . import json_output as json_output_\n\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf8\")\n\nSITES = {\n    \"163\": \"netease\",\n    \"56\": \"w56\",\n    \"365yg\": \"toutiao\",\n    \"acfun\": \"acfun\",\n    \"archive\": \"archive\",\n    \"baidu\": \"baidu\",\n    \"bandcamp\": \"bandcamp\",\n    \"baomihua\": \"baomihua\",\n    \"bigthink\": \"bigthink\",\n    \"bilibili\": \"bilibili\",\n    \"cctv\": \"cntv\",\n    \"cntv\": \"cntv\",\n    \"cbs\": \"cbs\",\n    \"coub\": \"coub\",\n    \"dailymotion\": \"dailymotion\",\n    \"douban\": \"douban\",\n    \"douyin\": \"douyin\",\n    \"douyu\": \"douyutv\",\n    \"ehow\": \"ehow\",\n    \"facebook\": \"facebook\",\n    \"fc2\": \"fc2video\",\n    \"flickr\": \"flickr\",\n    \"freesound\": \"freesound\",\n    \"fun\": \"funshion\",\n    \"google\": \"google\",\n    \"giphy\": \"giphy\",\n    \"heavy-music\": \"heavymusic\",\n    \"huomao\": \"huomaotv\",\n    \"iask\": \"sina\",\n    \"icourses\": \"icourses\",\n    \"ifeng\": \"ifeng\",\n    \"imgur\": \"imgur\",\n    \"in\": \"alive\",\n    \"infoq\": \"infoq\",\n    \"instagram\": \"instagram\",\n    \"interest\": \"interest\",\n    \"iqilu\": \"iqilu\",\n    \"iqiyi\": \"iqiyi\",\n    \"ixigua\": \"ixigua\",\n    \"isuntv\": \"suntv\",\n    \"iwara\": \"iwara\",\n    \"joy\": \"joy\",\n    \"kankanews\": \"bilibili\",\n    \"khanacademy\": \"khan\",\n    \"ku6\": \"ku6\",\n    \"kuaishou\": \"kuaishou\",\n    \"kugou\": \"kugou\",\n    \"kuwo\": \"kuwo\",\n    \"le\": \"le\",\n    \"letv\": \"le\",\n    \"lizhi\": \"lizhi\",\n    \"longzhu\": \"longzhu\",\n    \"magisto\": \"magisto\",\n    \"metacafe\": \"metacafe\",\n    \"mgtv\": \"mgtv\",\n    \"miomio\": \"miomio\",\n    \"mixcloud\": \"mixcloud\",\n    \"mtv81\": \"mtv81\",\n    \"musicplayon\": \"musicplayon\",\n    \"miaopai\": \"yixia\",\n    \"naver\": \"naver\",\n    \"7gogo\": \"nanagogo\",\n    \"nicovideo\": \"nicovideo\",\n    \"panda\": \"panda\",\n    \"pinterest\": \"pinterest\",\n    \"pixnet\": \"pixnet\",\n    \"pptv\": \"pptv\",\n    \"qingting\": \"qingting\",\n    \"qq\": \"qq\",\n    \"showroom-live\": \"showroom\",\n    \"sina\": \"sina\",\n    \"smgbb\": \"bilibili\",\n    \"sohu\": \"sohu\",\n    \"soundcloud\": \"soundcloud\",\n    \"ted\": \"ted\",\n    \"theplatform\": \"theplatform\",\n    \"tiktok\": \"tiktok\",\n    \"tucao\": \"tucao\",\n    \"tudou\": \"tudou\",\n    \"tumblr\": \"tumblr\",\n    \"twimg\": \"twitter\",\n    \"twitter\": \"twitter\",\n    \"ucas\": \"ucas\",\n    \"videomega\": \"videomega\",\n    \"vidto\": \"vidto\",\n    \"vimeo\": \"vimeo\",\n    \"wanmen\": \"wanmen\",\n    \"weibo\": \"miaopai\",\n    \"veoh\": \"veoh\",\n    \"vine\": \"vine\",\n    \"vk\": \"vk\",\n    \"xiami\": \"xiami\",\n    \"xiaokaxiu\": \"yixia\",\n    \"xiaojiadianvideo\": \"fc2video\",\n    \"ximalaya\": \"ximalaya\",\n    \"yinyuetai\": \"yinyuetai\",\n    \"yizhibo\": \"yizhibo\",\n    \"youku\": \"youku\",\n    \"youtu\": \"youtube\",\n    \"youtube\": \"youtube\",\n    \"zhanqi\": \"zhanqi\",\n    \"zhibo\": \"zhibo\",\n    \"zhihu\": \"zhihu\",\n}\n\ndry_run = False\njson_output = False\nforce = False\nplayer = None\nextractor_proxy = None\ncookies = None\noutput_filename = None\nauto_rename = False\ninsecure = False\n\nfake_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",  # noqa\n    \"Accept-Charset\": \"UTF-8,*;q=0.5\",\n    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0\",  # noqa\n}\n\nif sys.stdout.isatty():\n    default_encoding = sys.stdout.encoding.lower()\nelse:\n    default_encoding = locale.getpreferredencoding().lower()\n\n\ndef rc4(key, data):\n    # all encryption algo should work on bytes\n    assert type(key) == type(data) and type(key) == type(b\"\")\n    state = list(range(256))\n    j = 0\n    for i in range(256):\n        j += state[i] + key[i % len(key)]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n\n    i = 0\n    j = 0\n    out_list = []\n    for char in data:\n        i += 1\n        i &= 0xFF\n        j += state[i]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n        prn = state[(state[i] + state[j]) & 0xFF]\n        out_list.append(char ^ prn)\n\n    return bytes(out_list)\n\n\ndef general_m3u8_extractor(url, headers={}):\n    m3u8_list = get_content(url, headers=headers).split(\"\\n\")\n    urls = []\n    for line in m3u8_list:\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            if line.startswith(\"http\"):\n                urls.append(line)\n            else:\n                seg_url = parse.urljoin(url, line)\n                urls.append(seg_url)\n    return urls\n\n\ndef maybe_print(*s):\n    try:\n        print(*s)\n    except:\n        pass\n\n\ndef tr(s):\n    if default_encoding == \"utf-8\":\n        return s\n    else:\n        return s\n        # return str(s.encode('utf-8'))[2:-1]\n\n\n# DEPRECATED in favor of match1()\ndef r1(pattern, text):\n    m = re.search(pattern, text)\n    if m:\n        return m.group(1)\n\n\n# DEPRECATED in favor of match1()\ndef r1_of(patterns, text):\n    for p in patterns:\n        x = r1(p, text)\n        if x:\n            return x\n\n\ndef match1(text, *patterns):\n    \"\"\"Scans through a string for substrings matched some patterns (first-subgroups only).\n\n    Args:\n        text: A string to be scanned.\n        patterns: Arbitrary number of regex patterns.\n\n    Returns:\n        When only one pattern is given, returns a string (None if no match found).\n        When more than one pattern are given, returns a list of strings ([] if no match found).\n    \"\"\"\n\n    if len(patterns) == 1:\n        pattern = patterns[0]\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    else:\n        ret = []\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                ret.append(match.group(1))\n        return ret\n\n\ndef matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret\n\n\ndef launch_player(player, urls):\n    import subprocess\n    import shlex\n\n    if sys.version_info >= (3, 3):\n        import shutil\n\n        exefile = shlex.split(player)[0]\n        if shutil.which(exefile) is not None:\n            subprocess.call(shlex.split(player) + list(urls))\n        else:\n            log.wtf('[Failed] Cannot find player \"%s\"' % exefile)\n    else:\n        subprocess.call(shlex.split(player) + list(urls))\n\n\ndef unicodize(text):\n    return re.sub(\n        r\"\\\\u([0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f])\",\n        lambda x: chr(int(x.group(0)[2:], 16)),\n        text,\n    )\n\n\n# DEPRECATED in favor of util.legitimize()\ndef escape_file_path(path):\n    path = path.replace(\"/\", \"-\")\n    path = path.replace(\"\\\\\", \"-\")\n    path = path.replace(\"*\", \"-\")\n    path = path.replace(\"?\", \"-\")\n    return path\n\n\ndef ungzip(data):\n    \"\"\"Decompresses data for Content-Encoding: gzip.\"\"\"\n    from io import BytesIO\n    import gzip\n\n    buffer = BytesIO(data)\n    f = gzip.GzipFile(fileobj=buffer)\n    return f.read()\n\n\ndef undeflate(data):\n    \"\"\"Decompresses data for Content-Encoding: deflate.\n    (the zlib compression is used.)\n    \"\"\"\n    import zlib\n\n    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)\n    return decompressobj.decompress(data) + decompressobj.flush()\n\n\n# DEPRECATED in favor of get_content()\ndef get_response(url, faker=False):\n    logging.debug(\"get_response: %s\" % url)\n\n    # install cookies\n    if cookies:\n        opener = request.build_opener(request.HTTPCookieProcessor(cookies))\n        request.install_opener(opener)\n\n    if faker:\n        response = request.urlopen(request.Request(url, headers=fake_headers), None)\n    else:\n        response = request.urlopen(url)\n\n    data = response.read()\n    if response.info().get(\"Content-Encoding\") == \"gzip\":\n        data = ungzip(data)\n    elif response.info().get(\"Content-Encoding\") == \"deflate\":\n        data = undeflate(data)\n    response.data = data\n    return response\n\n\n# DEPRECATED in favor of get_content()\ndef get_html(url, encoding=None, faker=False):\n    content = get_response(url, faker).data\n    return str(content, \"utf-8\", \"ignore\")\n\n\n# DEPRECATED in favor of get_content()\ndef get_decoded_html(url, faker=False):\n    response = get_response(url, faker)\n    data = response.data\n    charset = r1(r\"charset=([\\w-]+)\", response.headers[\"content-type\"])\n    if charset:\n        return data.decode(charset, \"ignore\")\n    else:\n        return data\n\n\ndef get_location(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_location: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.geturl()\n\n\ndef urlopen_with_retry(*args, **kwargs):\n    retry_time = 3\n    for i in range(retry_time):\n        try:\n            if insecure:\n                # ignore ssl errors\n                ctx = ssl.create_default_context()\n                ctx.check_hostname = False\n                ctx.verify_mode = ssl.CERT_NONE\n                return request.urlopen(*args, context=ctx, **kwargs)\n            else:\n                return request.urlopen(*args, **kwargs)\n        except socket.timeout as e:\n            logging.debug(\"request attempt %s timeout\" % str(i + 1))\n            if i + 1 == retry_time:\n                raise e\n        # try to tackle youku CDN fails\n        except error.HTTPError as http_error:\n            logging.debug(\"HTTP Error with code{}\".format(http_error.code))\n            if i + 1 == retry_time:\n                raise http_error\n\n\ndef get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug(\"get_content: %s\" % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\", \"\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset, \"ignore\")\n        else:\n            data = data.decode(\"utf-8\", \"ignore\")\n\n    return data\n\n\ndef post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get(\"post_data_raw\"):\n        logging.debug(\n            \"post_content: %s\\npost_data_raw: %s\" % (url, kwargs[\"post_data_raw\"])\n        )\n    else:\n        logging.debug(\"post_content: %s\\npost_data: %s\" % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get(\"post_data_raw\"):\n        post_data_enc = bytes(kwargs[\"post_data_raw\"], \"utf-8\")\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), \"utf-8\")\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode(\"utf-8\")\n\n    return data\n\n\ndef url_size(url, faker=False, headers={}):\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(url)\n\n    size = response.headers[\"content-length\"]\n    return int(size) if size is not None else float(\"inf\")\n\n\ndef urls_size(urls, faker=False, headers={}):\n    return sum([url_size(url, faker=faker, headers=headers) for url in urls])\n\n\ndef get_head(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_head: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.headers\n\n\ndef url_info(url, faker=False, headers={}):\n    logging.debug(\"url_info: %s\" % url)\n\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(request.Request(url))\n\n    headers = response.headers\n\n    type = headers[\"content-type\"]\n    if type == \"image/jpg; charset=UTF-8\" or type == \"image/jpg\":\n        type = \"audio/mpeg\"  # fix for netease\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/f4v\": \"flv\",\n        \"video/mp4\": \"mp4\",\n        \"video/MP2T\": \"ts\",\n        \"video/quicktime\": \"mov\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n        \"video/x-ms-asf\": \"asf\",\n        \"audio/mp4\": \"mp4\",\n        \"audio/mpeg\": \"mp3\",\n        \"audio/wav\": \"wav\",\n        \"audio/x-wav\": \"wav\",\n        \"audio/wave\": \"wav\",\n        \"image/jpeg\": \"jpg\",\n        \"image/png\": \"png\",\n        \"image/gif\": \"gif\",\n        \"application/pdf\": \"pdf\",\n    }\n    if type in mapping:\n        ext = mapping[type]\n    else:\n        type = None\n        if headers[\"content-disposition\"]:\n            try:\n                filename = parse.unquote(\n                    r1(r'filename=\"?([^\"]+)\"?', headers[\"content-disposition\"])\n                )\n                if len(filename.split(\".\")) > 1:\n                    ext = filename.split(\".\")[-1]\n                else:\n                    ext = None\n            except:\n                ext = None\n        else:\n            ext = None\n\n    if headers[\"transfer-encoding\"] != \"chunked\":\n        size = headers[\"content-length\"] and int(headers[\"content-length\"])\n    else:\n        size = None\n\n    return type, ext, size\n\n\ndef url_locations(urls, faker=False, headers={}):\n    locations = []\n    for url in urls:\n        logging.debug(\"url_locations: %s\" % url)\n\n        if faker:\n            response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n        elif headers:\n            response = urlopen_with_retry(request.Request(url, headers=headers))\n        else:\n            response = urlopen_with_retry(request.Request(url))\n\n        locations.append(response.url)\n    return locations\n\n\ndef url_save(\n    url,\n    filepath,\n    bar,\n    refer=None,\n    is_part=False,\n    faker=False,\n    headers=None,\n    timeout=None,\n    **kwargs\n):\n    tmp_headers = headers.copy() if headers is not None else {}\n    # When a referer specified with param refer,\n    # the key must be 'Referer' for the hack here\n    if refer is not None:\n        tmp_headers[\"Referer\"] = refer\n    if type(url) is list:\n        file_size = urls_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = True, url\n    else:\n        file_size = url_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = False, [url]\n\n    continue_renameing = True\n    while continue_renameing:\n        continue_renameing = False\n        if os.path.exists(filepath):\n            if not force and file_size == os.path.getsize(filepath):\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    log.w(\n                        \"Skipping {}: file already exists\".format(\n                            tr(os.path.basename(filepath))\n                        )\n                    )\n                else:\n                    if bar:\n                        bar.update_received(file_size)\n                return\n            else:\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    if not force and auto_rename:\n                        path, ext = os.path.basename(filepath).rsplit(\".\", 1)\n                        finder = re.compile(\" \\([1-9]\\d*?\\)$\")\n                        if finder.search(path) is None:\n                            thisfile = path + \" (1).\" + ext\n                        else:\n\n                            def numreturn(a):\n                                return \" (\" + str(int(a.group()[2:-1]) + 1) + \").\"\n\n                            thisfile = finder.sub(numreturn, path) + ext\n                        filepath = os.path.join(os.path.dirname(filepath), thisfile)\n                        print(\n                            \"Changing name to %s\" % tr(os.path.basename(filepath)),\n                            \"...\",\n                        )\n                        continue_renameing = True\n                        continue\n                    if log.yes_or_no(\"File with this name already exists. Overwrite?\"):\n                        log.w(\"Overwriting %s ...\" % tr(os.path.basename(filepath)))\n                    else:\n                        return\n        elif not os.path.exists(os.path.dirname(filepath)):\n            os.mkdir(os.path.dirname(filepath))\n\n    temp_filepath = filepath + \".download\" if file_size != float(\"inf\") else filepath\n    received = 0\n    if not force:\n        open_mode = \"ab\"\n\n        if os.path.exists(temp_filepath):\n            received += os.path.getsize(temp_filepath)\n            if bar:\n                bar.update_received(os.path.getsize(temp_filepath))\n    else:\n        open_mode = \"wb\"\n\n    for url in urls:\n        received_chunk = 0\n        if received < file_size:\n            if faker:\n                tmp_headers = fake_headers\n            \"\"\"\n            if parameter headers passed in, we have it copied as tmp_header\n            elif headers:\n                headers = headers\n            else:\n                headers = {}\n            \"\"\"\n            if received and not is_chunked:  # only request a range when not chunked\n                tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n            if refer:\n                tmp_headers[\"Referer\"] = refer\n\n            if timeout:\n                response = urlopen_with_retry(\n                    request.Request(url, headers=tmp_headers), timeout=timeout\n                )\n            else:\n                response = urlopen_with_retry(request.Request(url, headers=tmp_headers))\n            try:\n                range_start = int(\n                    response.headers[\"content-range\"][6:].split(\"/\")[0].split(\"-\")[0]\n                )\n                end_length = int(response.headers[\"content-range\"][6:].split(\"/\")[1])\n                range_length = end_length - range_start\n            except:\n                content_length = response.headers[\"content-length\"]\n                range_length = (\n                    int(content_length) if content_length is not None else float(\"inf\")\n                )\n\n            if is_chunked:  # always append if chunked\n                open_mode = \"ab\"\n            elif file_size != received + range_length:  # is it ever necessary?\n                received = 0\n                if bar:\n                    bar.received = 0\n                open_mode = \"wb\"\n\n            with open(temp_filepath, open_mode) as output:\n                while True:\n                    buffer = None\n                    try:\n                        buffer = response.read(1024 * 256)\n                    except socket.timeout:\n                        pass\n                    if not buffer:\n                        if is_chunked and received_chunk == range_length:\n                            break\n                        elif (\n                            not is_chunked and received == file_size\n                        ):  # Download finished\n                            break\n                        # Unexpected termination. Retry request\n                        if not is_chunked:  # when\n                            tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n                        response = urlopen_with_retry(\n                            request.Request(url, headers=tmp_headers)\n                        )\n                        continue\n                    output.write(buffer)\n                    received += len(buffer)\n                    received_chunk += len(buffer)\n                    if bar:\n                        bar.update_received(len(buffer))\n\n    assert received == os.path.getsize(temp_filepath), \"%s == %s == %s\" % (\n        received,\n        os.path.getsize(temp_filepath),\n        temp_filepath,\n    )\n\n    if os.access(filepath, os.W_OK):\n        # on Windows rename could fail if destination filepath exists\n        os.remove(filepath)\n    os.rename(temp_filepath, filepath)\n\n\nclass SimpleProgressBar:\n    term_size = term.get_terminal_size()[1]\n\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n        self.speed = \"\"\n        self.last_updated = time.time()\n\n        total_pieces_len = len(str(total_pieces))\n        # 38 is the size of all statically known size in self.bar\n        total_str = \"%5s\" % round(self.total_size / 1048576, 1)\n        total_str_width = max(len(total_str), 5)\n        self.bar_size = self.term_size - 28 - 2 * total_pieces_len - 2 * total_str_width\n        self.bar = \"{:>4}%% ({:>%s}/%sMB) \u251c{:\u2500<%s}\u2524[{:>%s}/{:>%s}] {}\" % (\n            total_str_width,\n            total_str,\n            self.bar_size,\n            total_pieces_len,\n            total_pieces_len,\n        )\n\n    def update(self):\n        self.displayed = True\n        bar_size = self.bar_size\n        percent = round(self.received * 100 / self.total_size, 1)\n        if percent >= 100:\n            percent = 100\n        dots = bar_size * int(percent) // 100\n        plus = int(percent) - dots // bar_size * 100\n        if plus > 0.8:\n            plus = \"\u2588\"\n        elif plus > 0.4:\n            plus = \">\"\n        else:\n            plus = \"\"\n        bar = \"\u2588\" * dots + plus\n        bar = self.bar.format(\n            percent,\n            round(self.received / 1048576, 1),\n            bar,\n            self.current_piece,\n            self.total_pieces,\n            self.speed,\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        time_diff = time.time() - self.last_updated\n        bytes_ps = n / time_diff if time_diff else 0\n        if bytes_ps >= 1024 ** 3:\n            self.speed = \"{:4.0f} GB/s\".format(bytes_ps / 1024 ** 3)\n        elif bytes_ps >= 1024 ** 2:\n            self.speed = \"{:4.0f} MB/s\".format(bytes_ps / 1024 ** 2)\n        elif bytes_ps >= 1024:\n            self.speed = \"{:4.0f} kB/s\".format(bytes_ps / 1024)\n        else:\n            self.speed = \"{:4.0f}  B/s\".format(bytes_ps)\n        self.last_updated = time.time()\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass PiecesProgressBar:\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n\n    def update(self):\n        self.displayed = True\n        bar = \"{0:>5}%[{1:<40}] {2}/{3}\".format(\n            \"\", \"=\" * 40, self.current_piece, self.total_pieces\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass DummyProgressBar:\n    def __init__(self, *args):\n        pass\n\n    def update_received(self, n):\n        pass\n\n    def update_piece(self, n):\n        pass\n\n    def done(self):\n        pass\n\n\ndef get_output_filename(urls, title, ext, output_dir, merge):\n    # lame hack for the --output-filename option\n    global output_filename\n    if output_filename:\n        if ext:\n            return output_filename + \".\" + ext\n        return output_filename\n\n    merged_ext = ext\n    if (len(urls) > 1) and merge:\n        from .processor.ffmpeg import has_ffmpeg_installed\n\n        if ext in [\"flv\", \"f4v\"]:\n            if has_ffmpeg_installed():\n                merged_ext = \"mp4\"\n            else:\n                merged_ext = \"flv\"\n        elif ext == \"mp4\":\n            merged_ext = \"mp4\"\n        elif ext == \"ts\":\n            if has_ffmpeg_installed():\n                merged_ext = \"mkv\"\n            else:\n                merged_ext = \"ts\"\n    return \"%s.%s\" % (title, merged_ext)\n\n\ndef print_user_agent(faker=False):\n    urllib_default_user_agent = \"Python-urllib/%d.%d\" % sys.version_info[:2]\n    user_agent = fake_headers[\"User-Agent\"] if faker else urllib_default_user_agent\n    print(\"User Agent: %s\" % user_agent)\n\n\ndef download_urls(\n    urls,\n    title,\n    ext,\n    total_size,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    headers={},\n    **kwargs\n):\n    assert urls\n    if json_output:\n        json_output_.download_urls(\n            urls=urls, title=title, ext=ext, total_size=total_size, refer=refer\n        )\n        return\n    if dry_run:\n        print_user_agent(faker=faker)\n        try:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join(urls))\n        except:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join([j for i in urls for j in i]))\n        return\n\n    if player:\n        launch_player(player, urls)\n        return\n\n    if not total_size:\n        try:\n            total_size = urls_size(urls, faker=faker, headers=headers)\n        except:\n            import traceback\n\n            traceback.print_exc(file=sys.stdout)\n            pass\n\n    title = tr(get_filename(title))\n    output_filename = get_output_filename(urls, title, ext, output_dir, merge)\n    output_filepath = os.path.join(output_dir, output_filename)\n\n    if total_size:\n        if (\n            not force\n            and os.path.exists(output_filepath)\n            and not auto_rename\n            and os.path.getsize(output_filepath) >= total_size * 0.9\n        ):\n            log.w(\"Skipping %s: file already exists\" % output_filepath)\n            print()\n            return\n        bar = SimpleProgressBar(total_size, len(urls))\n    else:\n        bar = PiecesProgressBar(total_size, len(urls))\n\n    if len(urls) == 1:\n        url = urls[0]\n        print(\"Downloading %s ...\" % tr(output_filename))\n        bar.update()\n        url_save(\n            url,\n            output_filepath,\n            bar,\n            refer=refer,\n            faker=faker,\n            headers=headers,\n            **kwargs\n        )\n        bar.done()\n    else:\n        parts = []\n        print(\"Downloading %s.%s ...\" % (tr(title), ext))\n        bar.update()\n        for i, url in enumerate(urls):\n            filename = \"%s[%02d].%s\" % (title, i, ext)\n            filepath = os.path.join(output_dir, filename)\n            parts.append(filepath)\n            # print 'Downloading %s [%s/%s]...' % (tr(filename), i + 1, len(urls))\n            bar.update_piece(i + 1)\n            url_save(\n                url,\n                filepath,\n                bar,\n                refer=refer,\n                is_part=True,\n                faker=faker,\n                headers=headers,\n                **kwargs\n            )\n        bar.done()\n\n        if not merge:\n            print()\n            return\n\n        if \"av\" in kwargs and kwargs[\"av\"]:\n            from .processor.ffmpeg import has_ffmpeg_installed\n\n            if has_ffmpeg_installed():\n                from .processor.ffmpeg import ffmpeg_concat_av\n\n                ret = ffmpeg_concat_av(parts, output_filepath, ext)\n                print(\"Merged into %s\" % output_filename)\n                if ret == 0:\n                    for part in parts:\n                        os.remove(part)\n\n        elif ext in [\"flv\", \"f4v\"]:\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_flv_to_mp4\n\n                    ffmpeg_concat_flv_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_flv import concat_flv\n\n                    concat_flv(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"mp4\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_mp4_to_mp4\n\n                    ffmpeg_concat_mp4_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_mp4 import concat_mp4\n\n                    concat_mp4(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"ts\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_ts_to_mkv\n\n                    ffmpeg_concat_ts_to_mkv(parts, output_filepath)\n                else:\n                    from .processor.join_ts import concat_ts\n\n                    concat_ts(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        else:\n            print(\"Can't merge %s files\" % ext)\n\n    print()\n\n\ndef download_rtmp_url(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset -> False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        from .processor.rtmpdump import play_rtmpdump_stream\n\n        play_rtmpdump_stream(player, url, params)\n        return\n\n    from .processor.rtmpdump import has_rtmpdump_installed, download_rtmpdump_stream\n\n    assert has_rtmpdump_installed(), \"RTMPDump not installed.\"\n    download_rtmpdump_stream(url, title, ext, params, output_dir)\n\n\ndef download_url_ffmpeg(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    stream=True,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset ->False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        launch_player(player, [url])\n        return\n\n    from .processor.ffmpeg import has_ffmpeg_installed, ffmpeg_download_stream\n\n    assert has_ffmpeg_installed(), \"FFmpeg not installed.\"\n\n    global output_filename\n    if output_filename:\n        dotPos = output_filename.rfind(\".\")\n        if dotPos > 0:\n            title = output_filename[:dotPos]\n            ext = output_filename[dotPos + 1 :]\n        else:\n            title = output_filename\n\n    title = tr(get_filename(title))\n\n    ffmpeg_download_stream(url, title, ext, params, output_dir, stream=stream)\n\n\ndef playlist_not_supported(name):\n    def f(*args, **kwargs):\n        raise NotImplementedError(\"Playlist is not supported for \" + name)\n\n    return f\n\n\ndef print_info(site_info, title, type, size, **kwargs):\n    if json_output:\n        json_output_.print_info(site_info=site_info, title=title, type=type, size=size)\n        return\n    if type:\n        type = type.lower()\n    if type in [\"3gp\"]:\n        type = \"video/3gpp\"\n    elif type in [\"asf\", \"wmv\"]:\n        type = \"video/x-ms-asf\"\n    elif type in [\"flv\", \"f4v\"]:\n        type = \"video/x-flv\"\n    elif type in [\"mkv\"]:\n        type = \"video/x-matroska\"\n    elif type in [\"mp3\"]:\n        type = \"audio/mpeg\"\n    elif type in [\"mp4\"]:\n        type = \"video/mp4\"\n    elif type in [\"mov\"]:\n        type = \"video/quicktime\"\n    elif type in [\"ts\"]:\n        type = \"video/MP2T\"\n    elif type in [\"webm\"]:\n        type = \"video/webm\"\n\n    elif type in [\"jpg\"]:\n        type = \"image/jpeg\"\n    elif type in [\"png\"]:\n        type = \"image/png\"\n    elif type in [\"gif\"]:\n        type = \"image/gif\"\n\n    if type in [\"video/3gpp\"]:\n        type_info = \"3GPP multimedia file (%s)\" % type\n    elif type in [\"video/x-flv\", \"video/f4v\"]:\n        type_info = \"Flash video (%s)\" % type\n    elif type in [\"video/mp4\", \"video/x-m4v\"]:\n        type_info = \"MPEG-4 video (%s)\" % type\n    elif type in [\"video/MP2T\"]:\n        type_info = \"MPEG-2 transport stream (%s)\" % type\n    elif type in [\"video/webm\"]:\n        type_info = \"WebM video (%s)\" % type\n    # elif type in ['video/ogg']:\n    #    type_info = 'Ogg video (%s)' % type\n    elif type in [\"video/quicktime\"]:\n        type_info = \"QuickTime video (%s)\" % type\n    elif type in [\"video/x-matroska\"]:\n        type_info = \"Matroska video (%s)\" % type\n    # elif type in ['video/x-ms-wmv']:\n    #    type_info = 'Windows Media video (%s)' % type\n    elif type in [\"video/x-ms-asf\"]:\n        type_info = \"Advanced Systems Format (%s)\" % type\n    # elif type in ['video/mpeg']:\n    #    type_info = 'MPEG video (%s)' % type\n    elif type in [\"audio/mp4\", \"audio/m4a\"]:\n        type_info = \"MPEG-4 audio (%s)\" % type\n    elif type in [\"audio/mpeg\"]:\n        type_info = \"MP3 (%s)\" % type\n    elif type in [\"audio/wav\", \"audio/wave\", \"audio/x-wav\"]:\n        type_info = \"Waveform Audio File Format ({})\".format(type)\n\n    elif type in [\"image/jpeg\"]:\n        type_info = \"JPEG Image (%s)\" % type\n    elif type in [\"image/png\"]:\n        type_info = \"Portable Network Graphics (%s)\" % type\n    elif type in [\"image/gif\"]:\n        type_info = \"Graphics Interchange Format (%s)\" % type\n    elif type in [\"m3u8\"]:\n        if \"m3u8_type\" in kwargs:\n            if kwargs[\"m3u8_type\"] == \"master\":\n                type_info = \"M3U8 Master {}\".format(type)\n        else:\n            type_info = \"M3U8 Playlist {}\".format(type)\n    else:\n        type_info = \"Unknown type (%s)\" % type\n\n    maybe_print(\"Site:      \", site_info)\n    maybe_print(\"Title:     \", unescape_html(tr(title)))\n    print(\"Type:      \", type_info)\n    if type != \"m3u8\":\n        print(\"Size:      \", round(size / 1048576, 2), \"MiB (\" + str(size) + \" Bytes)\")\n    if type == \"m3u8\" and \"m3u8_url\" in kwargs:\n        print(\"M3U8 Url:   {}\".format(kwargs[\"m3u8_url\"]))\n    print()\n\n\ndef mime_to_container(mime):\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/mp4\": \"mp4\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n    }\n    if mime in mapping:\n        return mapping[mime]\n    else:\n        return mime.split(\"/\")[1]\n\n\ndef parse_host(host):\n    \"\"\"Parses host name and port number from a string.\"\"\"\n    if re.match(r\"^(\\d+)$\", host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r\"^(\\w+)://\", host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)\n\n\ndef set_proxy(proxy):\n    proxy_handler = request.ProxyHandler(\n        {\n            \"http\": \"%s:%s\" % proxy,\n            \"https\": \"%s:%s\" % proxy,\n        }\n    )\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\ndef unset_proxy():\n    proxy_handler = request.ProxyHandler({})\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\n# DEPRECATED in favor of set_proxy() and unset_proxy()\ndef set_http_proxy(proxy):\n    if proxy is None:  # Use system default setting\n        proxy_support = request.ProxyHandler()\n    elif proxy == \"\":  # Don't use any proxy\n        proxy_support = request.ProxyHandler({})\n    else:  # Use proxy\n        proxy_support = request.ProxyHandler(\n            {\"http\": \"%s\" % proxy, \"https\": \"%s\" % proxy}\n        )\n    opener = request.build_opener(proxy_support)\n    request.install_opener(opener)\n\n\ndef print_more_compatible(*args, **kwargs):\n    import builtins as __builtin__\n\n    \"\"\"Overload default print function as py (<3.3) does not support 'flush' keyword.\n    Although the function name can be same as print to get itself overloaded automatically,\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n    \"\"\"\n    # nothing happens on py3.3 and later\n    if sys.version_info[:2] >= (3, 3):\n        return __builtin__.print(*args, **kwargs)\n\n    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested\n    doFlush = kwargs.pop(\"flush\", False)\n    ret = __builtin__.print(*args, **kwargs)\n    if doFlush:\n        kwargs.get(\"file\", sys.stdout).flush()\n    return ret\n\n\ndef download_main(download, download_playlist, urls, playlist, **kwargs):\n    for url in urls:\n        if re.match(r\"https?://\", url) is None:\n            url = \"http://\" + url\n\n        if playlist:\n            download_playlist(url, **kwargs)\n        else:\n            download(url, **kwargs)\n\n\ndef load_cookies(cookiefile):\n    global cookies\n    if cookiefile.endswith(\".txt\"):\n        # MozillaCookieJar treats prefix '#HttpOnly_' as comments incorrectly!\n        # do not use its load()\n        # see also:\n        #   - https://docs.python.org/3/library/http.cookiejar.html#http.cookiejar.MozillaCookieJar\n        #   - https://github.com/python/cpython/blob/4b219ce/Lib/http/cookiejar.py#L2014\n        #   - https://curl.haxx.se/libcurl/c/CURLOPT_COOKIELIST.html#EXAMPLE\n        # cookies = cookiejar.MozillaCookieJar(cookiefile)\n        # cookies.load()\n        from http.cookiejar import Cookie\n\n        cookies = cookiejar.MozillaCookieJar()\n        now = time.time()\n        ignore_discard, ignore_expires = False, False\n        with open(cookiefile, \"r\") as f:\n            for line in f:\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"):\n                    line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if line.strip().startswith((\"#\", \"$\")) or line.strip() == \"\":\n                    if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                        continue\n\n                (\n                    domain,\n                    domain_specified,\n                    path,\n                    secure,\n                    expires,\n                    name,\n                    value,\n                ) = line.split(\"\\t\")\n                secure = secure == \"TRUE\"\n                domain_specified = domain_specified == \"TRUE\"\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas http.cookiejar regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                    assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(\n                    0,\n                    name,\n                    value,\n                    None,\n                    False,\n                    domain,\n                    domain_specified,\n                    initial_dot,\n                    path,\n                    False,\n                    secure,\n                    expires,\n                    discard,\n                    None,\n                    None,\n                    {},\n                )\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                cookies.set_cookie(c)\n\n    elif cookiefile.endswith((\".sqlite\", \".sqlite3\")):\n        import sqlite3, shutil, tempfile\n\n        temp_dir = tempfile.gettempdir()\n        temp_cookiefile = os.path.join(temp_dir, \"temp_cookiefile.sqlite\")\n        shutil.copy2(cookiefile, temp_cookiefile)\n\n        cookies = cookiejar.MozillaCookieJar()\n        con = sqlite3.connect(temp_cookiefile)\n        cur = con.cursor()\n        cur.execute(\n            \"\"\"SELECT host, path, isSecure, expiry, name, value\n        FROM moz_cookies\"\"\"\n        )\n        for item in cur.fetchall():\n            c = cookiejar.Cookie(\n                0,\n                item[4],\n                item[5],\n                None,\n                False,\n                item[0],\n                item[0].startswith(\".\"),\n                item[0].startswith(\".\"),\n                item[1],\n                False,\n                item[2],\n                item[3],\n                item[3] == \"\",\n                None,\n                None,\n                {},\n            )\n            cookies.set_cookie(c)\n\n    else:\n        log.e(\"[error] unsupported cookies format\")\n        # TODO: Chromium Cookies\n        # SELECT host_key, path, secure, expires_utc, name, encrypted_value\n        # FROM cookies\n        # http://n8henrie.com/2013/11/use-chromes-cookies-for-easier-downloading-with-python-requests/\n\n\ndef set_socks_proxy(proxy):\n    try:\n        import socks\n\n        socks_proxy_addrs = proxy.split(\":\")\n        socks.set_default_proxy(\n            socks.SOCKS5, socks_proxy_addrs[0], int(socks_proxy_addrs[1])\n        )\n        socket.socket = socks.socksocket\n\n        def getaddrinfo(*args):\n            return [(socket.AF_INET, socket.SOCK_STREAM, 6, \"\", (args[0], args[1]))]\n\n        socket.getaddrinfo = getaddrinfo\n    except ImportError:\n        log.w(\n            \"Error importing PySocks library, socks proxy ignored.\"\n            \"In order to use use socks proxy, please install PySocks.\"\n        )\n\n\ndef script_main(download, download_playlist, **kwargs):\n    logging.basicConfig(format=\"[%(levelname)s] %(message)s\")\n\n    def print_version():\n        version = get_version(\n            kwargs[\"repo_path\"] if \"repo_path\" in kwargs else __version__\n        )\n        log.i(\"version {}, a tiny downloader that scrapes the web.\".format(version))\n\n    parser = argparse.ArgumentParser(\n        prog=\"you-get\",\n        usage=\"you-get [OPTION]... URL...\",\n        description=\"A tiny downloader that scrapes the web\",\n        add_help=False,\n    )\n    parser.add_argument(\n        \"-V\", \"--version\", action=\"store_true\", help=\"Print version and exit\"\n    )\n    parser.add_argument(\n        \"-h\", \"--help\", action=\"store_true\", help=\"Print this help message and exit\"\n    )\n\n    dry_run_grp = parser.add_argument_group(\n        \"Dry-run options\", \"(no actual downloading)\"\n    )\n    dry_run_grp = dry_run_grp.add_mutually_exclusive_group()\n    dry_run_grp.add_argument(\n        \"-i\", \"--info\", action=\"store_true\", help=\"Print extracted information\"\n    )\n    dry_run_grp.add_argument(\n        \"-u\", \"--url\", action=\"store_true\", help=\"Print extracted information with URLs\"\n    )\n    dry_run_grp.add_argument(\n        \"--json\", action=\"store_true\", help=\"Print extracted URLs in JSON format\"\n    )\n\n    download_grp = parser.add_argument_group(\"Download options\")\n    download_grp.add_argument(\n        \"-n\",\n        \"--no-merge\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not merge video parts\",\n    )\n    download_grp.add_argument(\n        \"--no-caption\",\n        action=\"store_true\",\n        help=\"Do not download captions (subtitles, lyrics, danmaku, ...)\",\n    )\n    download_grp.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force overwriting existing files\",\n    )\n    download_grp.add_argument(\n        \"-F\", \"--format\", metavar=\"STREAM_ID\", help=\"Set video format to STREAM_ID\"\n    )\n    download_grp.add_argument(\n        \"-O\", \"--output-filename\", metavar=\"FILE\", help=\"Set output filename\"\n    )\n    download_grp.add_argument(\n        \"-o\", \"--output-dir\", metavar=\"DIR\", default=\".\", help=\"Set output directory\"\n    )\n    download_grp.add_argument(\n        \"-p\", \"--player\", metavar=\"PLAYER\", help=\"Stream extracted URL to a PLAYER\"\n    )\n    download_grp.add_argument(\n        \"-c\",\n        \"--cookies\",\n        metavar=\"COOKIES_FILE\",\n        help=\"Load cookies.txt or cookies.sqlite\",\n    )\n    download_grp.add_argument(\n        \"-t\",\n        \"--timeout\",\n        metavar=\"SECONDS\",\n        type=int,\n        default=600,\n        help=\"Set socket timeout\",\n    )\n    download_grp.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show traceback and other debug info\"\n    )\n    download_grp.add_argument(\n        \"-I\",\n        \"--input-file\",\n        metavar=\"FILE\",\n        type=argparse.FileType(\"r\"),\n        help=\"Read non-playlist URLs from FILE\",\n    )\n    download_grp.add_argument(\n        \"-P\", \"--password\", help=\"Set video visit password to PASSWORD\"\n    )\n    download_grp.add_argument(\n        \"-l\", \"--playlist\", action=\"store_true\", help=\"Prefer to download a playlist\"\n    )\n    download_grp.add_argument(\n        \"-a\",\n        \"--auto-rename\",\n        action=\"store_true\",\n        default=False,\n        help=\"Auto rename same name different files\",\n    )\n\n    download_grp.add_argument(\n        \"-k\", \"--insecure\", action=\"store_true\", default=False, help=\"ignore ssl errors\"\n    )\n\n    proxy_grp = parser.add_argument_group(\"Proxy options\")\n    proxy_grp = proxy_grp.add_mutually_exclusive_group()\n    proxy_grp.add_argument(\n        \"-x\",\n        \"--http-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for downloading\",\n    )\n    proxy_grp.add_argument(\n        \"-y\",\n        \"--extractor-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for extracting only\",\n    )\n    proxy_grp.add_argument(\"--no-proxy\", action=\"store_true\", help=\"Never use a proxy\")\n    proxy_grp.add_argument(\n        \"-s\",\n        \"--socks-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an SOCKS5 proxy for downloading\",\n    )\n\n    download_grp.add_argument(\"--stream\", help=argparse.SUPPRESS)\n    download_grp.add_argument(\"--itag\", help=argparse.SUPPRESS)\n\n    parser.add_argument(\"URL\", nargs=\"*\", help=argparse.SUPPRESS)\n\n    args = parser.parse_args()\n\n    if args.help:\n        print_version()\n        parser.print_help()\n        sys.exit()\n    if args.version:\n        print_version()\n        sys.exit()\n\n    if args.debug:\n        # Set level of root logger to DEBUG\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    global force\n    global dry_run\n    global json_output\n    global player\n    global extractor_proxy\n    global output_filename\n    global auto_rename\n    global insecure\n    output_filename = args.output_filename\n    extractor_proxy = args.extractor_proxy\n\n    info_only = args.info\n    if args.force:\n        force = True\n    if args.auto_rename:\n        auto_rename = True\n    if args.url:\n        dry_run = True\n    if args.json:\n        json_output = True\n        # to fix extractors not use VideoExtractor\n        dry_run = True\n        info_only = False\n\n    if args.cookies:\n        load_cookies(args.cookies)\n\n    caption = True\n    stream_id = args.format or args.stream or args.itag\n    if args.no_caption:\n        caption = False\n    if args.player:\n        player = args.player\n        caption = False\n\n    if args.insecure:\n        # ignore ssl\n        insecure = True\n\n    if args.no_proxy:\n        set_http_proxy(\"\")\n    else:\n        set_http_proxy(args.http_proxy)\n    if args.socks_proxy:\n        set_socks_proxy(args.socks_proxy)\n\n    URLs = []\n    if args.input_file:\n        logging.debug(\"you are trying to load urls from %s\", args.input_file)\n        if args.playlist:\n            log.e(\n                \"reading playlist from a file is unsupported \"\n                \"and won't make your life easier\"\n            )\n            sys.exit(2)\n        URLs.extend(args.input_file.read().splitlines())\n        args.input_file.close()\n    URLs.extend(args.URL)\n\n    if not URLs:\n        parser.print_help()\n        sys.exit()\n\n    socket.setdefaulttimeout(args.timeout)\n\n    try:\n        extra = {}\n        if extractor_proxy:\n            extra[\"extractor_proxy\"] = extractor_proxy\n        if stream_id:\n            extra[\"stream_id\"] = stream_id\n        download_main(\n            download,\n            download_playlist,\n            URLs,\n            args.playlist,\n            output_dir=args.output_dir,\n            merge=not args.no_merge,\n            info_only=info_only,\n            json_output=json_output,\n            caption=caption,\n            password=args.password,\n            **extra\n        )\n    except KeyboardInterrupt:\n        if args.debug:\n            raise\n        else:\n            sys.exit(1)\n    except UnicodeEncodeError:\n        if args.debug:\n            raise\n        log.e(\n            \"[error] oops, the current environment does not seem to support \" \"Unicode.\"\n        )\n        log.e(\"please set it to a UTF-8-aware locale first,\")\n        log.e(\"so as to save the video (with some Unicode characters) correctly.\")\n        log.e(\"you can do it like this:\")\n        log.e(\"    (Windows)    % chcp 65001 \")\n        log.e(\"    (Linux)      $ LC_CTYPE=en_US.UTF-8\")\n        sys.exit(1)\n    except Exception:\n        if not args.debug:\n            log.e(\"[error] oops, something went wrong.\")\n            log.e(\"don't panic, c'est la vie. please try the following steps:\")\n            log.e(\"  (1) Rule out any network problem.\")\n            log.e(\"  (2) Make sure you-get is up-to-date.\")\n            log.e(\"  (3) Check if the issue is already known, on\")\n            log.e(\"        https://github.com/soimort/you-get/wiki/Known-Bugs\")\n            log.e(\"        https://github.com/soimort/you-get/issues\")\n            log.e(\"  (4) Run the command with '--debug' option,\")\n            log.e(\"      and report this issue with the full output.\")\n        else:\n            print_version()\n            log.i(args)\n            raise\n        sys.exit(1)\n\n\ndef google_search(url):\n    keywords = r1(r\"https?://(.*)\", url)\n    url = \"https://www.google.com/search?tbm=vid&q=%s\" % parse.quote(keywords)\n    page = get_content(url, headers=fake_headers)\n    videos = re.findall(\n        r'<a href=\"(https?://[^\"]+)\" onmousedown=\"[^\"]+\"><h3 class=\"[^\"]*\">([^<]+)<',\n        page,\n    )\n    vdurs = re.findall(r'<span class=\"vdur[^\"]*\">([^<]+)<', page)\n    durs = [r1(r\"(\\d+:\\d+)\", unescape_html(dur)) for dur in vdurs]\n    print(\"Google Videos search:\")\n    for v in zip(videos, durs):\n        print(\"- video:  {} [{}]\".format(unescape_html(v[0][1]), v[1] if v[1] else \"?\"))\n        print(\"# you-get %s\" % log.sprint(v[0][0], log.UNDERLINE))\n        print()\n    print(\"Best matched result:\")\n    return videos[0][0]\n\n\ndef url_to_module(url):\n    try:\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n        assert video_host and video_url\n    except AssertionError:\n        url = google_search(url)\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    if video_host.endswith(\".com.cn\") or video_host.endswith(\".ac.cn\"):\n        video_host = video_host[:-3]\n    domain = r1(r\"(\\.[^.]+\\.[^.]+)$\", video_host) or video_host\n    assert domain, \"unsupported url: \" + url\n\n    # all non-ASCII code points must be quoted (percent-encoded UTF-8)\n    url = \"\".join([ch if ord(ch) in range(128) else parse.quote(ch) for ch in url])\n    video_host = r1(r\"https?://([^/]+)/\", url)\n    video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    k = r1(r\"([^.]+)\", domain)\n    if k in SITES:\n        return (import_module(\".\".join([\"you_get\", \"extractors\", SITES[k]])), url)\n    else:\n        try:\n            location = get_location(url)  # t.co isn't happy with fake_headers\n        except:\n            location = get_location(url, headers=fake_headers)\n\n        if location and location != url and not location.startswith(\"/\"):\n            return url_to_module(location)\n        else:\n            return import_module(\"you_get.extractors.universal\"), url\n\n\ndef any_download(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download(url, **kwargs)\n\n\ndef any_download_playlist(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download_playlist(url, **kwargs)\n\n\ndef main(**kwargs):\n    script_main(any_download, any_download_playlist, **kwargs)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "package": ["import io", "import os", "import re", "import sys", "import time", "import json", "import socket", "import locale", "import logging", "import argparse", "import ssl", "from http import cookiejar", "from importlib import import_module", "from urllib import request, parse, error", "from .version import __version__", "from .util import log, term", "from .util.git import get_version", "from .util.strings import get_filename, unescape_html", "from . import json_output as json_output_", "import subprocess", "import shlex", "import shutil", "from io import BytesIO", "import gzip", "import zlib"], "function": ["def rc4(key, data):\n", "def general_m3u8_extractor(url, headers={}):\n", "def maybe_print(*s):\n", "def tr(s):\n", "def r1(pattern, text):\n", "def r1_of(patterns, text):\n", "def match1(text, *patterns):\n", "def matchall(text, patterns):\n", "def launch_player(player, urls):\n", "def unicodize(text):\n", "def escape_file_path(path):\n", "def ungzip(data):\n", "def undeflate(data):\n", "def get_response(url, faker=False):\n", "def get_html(url, encoding=None, faker=False):\n", "def get_decoded_html(url, faker=False):\n", "def get_location(url, headers=None, get_method=\"HEAD\"):\n", "def urlopen_with_retry(*args, **kwargs):\n", "def get_content(url, headers={}, decoded=True):\n", "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n", "def url_size(url, faker=False, headers={}):\n", "def urls_size(urls, faker=False, headers={}):\n", "def get_head(url, headers=None, get_method=\"HEAD\"):\n", "def url_info(url, faker=False, headers={}):\n", "def url_locations(urls, faker=False, headers={}):\n", "class SimpleProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class PiecesProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class DummyProgressBar:\n", "    def __init__(self, *args):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "def get_output_filename(urls, title, ext, output_dir, merge):\n", "def print_user_agent(faker=False):\n", "def playlist_not_supported(name):\n", "    def f(*args, **kwargs):\n", "def print_info(site_info, title, type, size, **kwargs):\n", "def mime_to_container(mime):\n", "def parse_host(host):\n", "def set_proxy(proxy):\n", "def unset_proxy():\n", "def set_http_proxy(proxy):\n", "def print_more_compatible(*args, **kwargs):\n", "def download_main(download, download_playlist, urls, playlist, **kwargs):\n", "def load_cookies(cookiefile):\n", "def set_socks_proxy(proxy):\n", "def script_main(download, download_playlist, **kwargs):\n", "    def print_version():\n", "def google_search(url):\n", "def url_to_module(url):\n", "def any_download(url, **kwargs):\n", "def any_download_playlist(url, **kwargs):\n", "def main(**kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "get_content", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.", "docstring_tokens": ["Gets", "the", "content", "of", "a", "URL", "via", "sending", "a", "HTTP", "GET", "request", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454", "partition": "test", "up_fun_num": 19, "context": "#!/usr/bin/env python\n\nimport io\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport socket\nimport locale\nimport logging\nimport argparse\nimport ssl\nfrom http import cookiejar\nfrom importlib import import_module\nfrom urllib import request, parse, error\n\nfrom .version import __version__\nfrom .util import log, term\nfrom .util.git import get_version\nfrom .util.strings import get_filename, unescape_html\nfrom . import json_output as json_output_\n\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf8\")\n\nSITES = {\n    \"163\": \"netease\",\n    \"56\": \"w56\",\n    \"365yg\": \"toutiao\",\n    \"acfun\": \"acfun\",\n    \"archive\": \"archive\",\n    \"baidu\": \"baidu\",\n    \"bandcamp\": \"bandcamp\",\n    \"baomihua\": \"baomihua\",\n    \"bigthink\": \"bigthink\",\n    \"bilibili\": \"bilibili\",\n    \"cctv\": \"cntv\",\n    \"cntv\": \"cntv\",\n    \"cbs\": \"cbs\",\n    \"coub\": \"coub\",\n    \"dailymotion\": \"dailymotion\",\n    \"douban\": \"douban\",\n    \"douyin\": \"douyin\",\n    \"douyu\": \"douyutv\",\n    \"ehow\": \"ehow\",\n    \"facebook\": \"facebook\",\n    \"fc2\": \"fc2video\",\n    \"flickr\": \"flickr\",\n    \"freesound\": \"freesound\",\n    \"fun\": \"funshion\",\n    \"google\": \"google\",\n    \"giphy\": \"giphy\",\n    \"heavy-music\": \"heavymusic\",\n    \"huomao\": \"huomaotv\",\n    \"iask\": \"sina\",\n    \"icourses\": \"icourses\",\n    \"ifeng\": \"ifeng\",\n    \"imgur\": \"imgur\",\n    \"in\": \"alive\",\n    \"infoq\": \"infoq\",\n    \"instagram\": \"instagram\",\n    \"interest\": \"interest\",\n    \"iqilu\": \"iqilu\",\n    \"iqiyi\": \"iqiyi\",\n    \"ixigua\": \"ixigua\",\n    \"isuntv\": \"suntv\",\n    \"iwara\": \"iwara\",\n    \"joy\": \"joy\",\n    \"kankanews\": \"bilibili\",\n    \"khanacademy\": \"khan\",\n    \"ku6\": \"ku6\",\n    \"kuaishou\": \"kuaishou\",\n    \"kugou\": \"kugou\",\n    \"kuwo\": \"kuwo\",\n    \"le\": \"le\",\n    \"letv\": \"le\",\n    \"lizhi\": \"lizhi\",\n    \"longzhu\": \"longzhu\",\n    \"magisto\": \"magisto\",\n    \"metacafe\": \"metacafe\",\n    \"mgtv\": \"mgtv\",\n    \"miomio\": \"miomio\",\n    \"mixcloud\": \"mixcloud\",\n    \"mtv81\": \"mtv81\",\n    \"musicplayon\": \"musicplayon\",\n    \"miaopai\": \"yixia\",\n    \"naver\": \"naver\",\n    \"7gogo\": \"nanagogo\",\n    \"nicovideo\": \"nicovideo\",\n    \"panda\": \"panda\",\n    \"pinterest\": \"pinterest\",\n    \"pixnet\": \"pixnet\",\n    \"pptv\": \"pptv\",\n    \"qingting\": \"qingting\",\n    \"qq\": \"qq\",\n    \"showroom-live\": \"showroom\",\n    \"sina\": \"sina\",\n    \"smgbb\": \"bilibili\",\n    \"sohu\": \"sohu\",\n    \"soundcloud\": \"soundcloud\",\n    \"ted\": \"ted\",\n    \"theplatform\": \"theplatform\",\n    \"tiktok\": \"tiktok\",\n    \"tucao\": \"tucao\",\n    \"tudou\": \"tudou\",\n    \"tumblr\": \"tumblr\",\n    \"twimg\": \"twitter\",\n    \"twitter\": \"twitter\",\n    \"ucas\": \"ucas\",\n    \"videomega\": \"videomega\",\n    \"vidto\": \"vidto\",\n    \"vimeo\": \"vimeo\",\n    \"wanmen\": \"wanmen\",\n    \"weibo\": \"miaopai\",\n    \"veoh\": \"veoh\",\n    \"vine\": \"vine\",\n    \"vk\": \"vk\",\n    \"xiami\": \"xiami\",\n    \"xiaokaxiu\": \"yixia\",\n    \"xiaojiadianvideo\": \"fc2video\",\n    \"ximalaya\": \"ximalaya\",\n    \"yinyuetai\": \"yinyuetai\",\n    \"yizhibo\": \"yizhibo\",\n    \"youku\": \"youku\",\n    \"youtu\": \"youtube\",\n    \"youtube\": \"youtube\",\n    \"zhanqi\": \"zhanqi\",\n    \"zhibo\": \"zhibo\",\n    \"zhihu\": \"zhihu\",\n}\n\ndry_run = False\njson_output = False\nforce = False\nplayer = None\nextractor_proxy = None\ncookies = None\noutput_filename = None\nauto_rename = False\ninsecure = False\n\nfake_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",  # noqa\n    \"Accept-Charset\": \"UTF-8,*;q=0.5\",\n    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0\",  # noqa\n}\n\nif sys.stdout.isatty():\n    default_encoding = sys.stdout.encoding.lower()\nelse:\n    default_encoding = locale.getpreferredencoding().lower()\n\n\ndef rc4(key, data):\n    # all encryption algo should work on bytes\n    assert type(key) == type(data) and type(key) == type(b\"\")\n    state = list(range(256))\n    j = 0\n    for i in range(256):\n        j += state[i] + key[i % len(key)]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n\n    i = 0\n    j = 0\n    out_list = []\n    for char in data:\n        i += 1\n        i &= 0xFF\n        j += state[i]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n        prn = state[(state[i] + state[j]) & 0xFF]\n        out_list.append(char ^ prn)\n\n    return bytes(out_list)\n\n\ndef general_m3u8_extractor(url, headers={}):\n    m3u8_list = get_content(url, headers=headers).split(\"\\n\")\n    urls = []\n    for line in m3u8_list:\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            if line.startswith(\"http\"):\n                urls.append(line)\n            else:\n                seg_url = parse.urljoin(url, line)\n                urls.append(seg_url)\n    return urls\n\n\ndef maybe_print(*s):\n    try:\n        print(*s)\n    except:\n        pass\n\n\ndef tr(s):\n    if default_encoding == \"utf-8\":\n        return s\n    else:\n        return s\n        # return str(s.encode('utf-8'))[2:-1]\n\n\n# DEPRECATED in favor of match1()\ndef r1(pattern, text):\n    m = re.search(pattern, text)\n    if m:\n        return m.group(1)\n\n\n# DEPRECATED in favor of match1()\ndef r1_of(patterns, text):\n    for p in patterns:\n        x = r1(p, text)\n        if x:\n            return x\n\n\ndef match1(text, *patterns):\n    \"\"\"Scans through a string for substrings matched some patterns (first-subgroups only).\n\n    Args:\n        text: A string to be scanned.\n        patterns: Arbitrary number of regex patterns.\n\n    Returns:\n        When only one pattern is given, returns a string (None if no match found).\n        When more than one pattern are given, returns a list of strings ([] if no match found).\n    \"\"\"\n\n    if len(patterns) == 1:\n        pattern = patterns[0]\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    else:\n        ret = []\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                ret.append(match.group(1))\n        return ret\n\n\ndef matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret\n\n\ndef launch_player(player, urls):\n    import subprocess\n    import shlex\n\n    if sys.version_info >= (3, 3):\n        import shutil\n\n        exefile = shlex.split(player)[0]\n        if shutil.which(exefile) is not None:\n            subprocess.call(shlex.split(player) + list(urls))\n        else:\n            log.wtf('[Failed] Cannot find player \"%s\"' % exefile)\n    else:\n        subprocess.call(shlex.split(player) + list(urls))\n\n\ndef parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None\n\n\ndef unicodize(text):\n    return re.sub(\n        r\"\\\\u([0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f])\",\n        lambda x: chr(int(x.group(0)[2:], 16)),\n        text,\n    )\n\n\n# DEPRECATED in favor of util.legitimize()\ndef escape_file_path(path):\n    path = path.replace(\"/\", \"-\")\n    path = path.replace(\"\\\\\", \"-\")\n    path = path.replace(\"*\", \"-\")\n    path = path.replace(\"?\", \"-\")\n    return path\n\n\ndef ungzip(data):\n    \"\"\"Decompresses data for Content-Encoding: gzip.\"\"\"\n    from io import BytesIO\n    import gzip\n\n    buffer = BytesIO(data)\n    f = gzip.GzipFile(fileobj=buffer)\n    return f.read()\n\n\ndef undeflate(data):\n    \"\"\"Decompresses data for Content-Encoding: deflate.\n    (the zlib compression is used.)\n    \"\"\"\n    import zlib\n\n    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)\n    return decompressobj.decompress(data) + decompressobj.flush()\n\n\n# DEPRECATED in favor of get_content()\ndef get_response(url, faker=False):\n    logging.debug(\"get_response: %s\" % url)\n\n    # install cookies\n    if cookies:\n        opener = request.build_opener(request.HTTPCookieProcessor(cookies))\n        request.install_opener(opener)\n\n    if faker:\n        response = request.urlopen(request.Request(url, headers=fake_headers), None)\n    else:\n        response = request.urlopen(url)\n\n    data = response.read()\n    if response.info().get(\"Content-Encoding\") == \"gzip\":\n        data = ungzip(data)\n    elif response.info().get(\"Content-Encoding\") == \"deflate\":\n        data = undeflate(data)\n    response.data = data\n    return response\n\n\n# DEPRECATED in favor of get_content()\ndef get_html(url, encoding=None, faker=False):\n    content = get_response(url, faker).data\n    return str(content, \"utf-8\", \"ignore\")\n\n\n# DEPRECATED in favor of get_content()\ndef get_decoded_html(url, faker=False):\n    response = get_response(url, faker)\n    data = response.data\n    charset = r1(r\"charset=([\\w-]+)\", response.headers[\"content-type\"])\n    if charset:\n        return data.decode(charset, \"ignore\")\n    else:\n        return data\n\n\ndef get_location(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_location: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.geturl()\n\n\ndef urlopen_with_retry(*args, **kwargs):\n    retry_time = 3\n    for i in range(retry_time):\n        try:\n            if insecure:\n                # ignore ssl errors\n                ctx = ssl.create_default_context()\n                ctx.check_hostname = False\n                ctx.verify_mode = ssl.CERT_NONE\n                return request.urlopen(*args, context=ctx, **kwargs)\n            else:\n                return request.urlopen(*args, **kwargs)\n        except socket.timeout as e:\n            logging.debug(\"request attempt %s timeout\" % str(i + 1))\n            if i + 1 == retry_time:\n                raise e\n        # try to tackle youku CDN fails\n        except error.HTTPError as http_error:\n            logging.debug(\"HTTP Error with code{}\".format(http_error.code))\n            if i + 1 == retry_time:\n                raise http_error\n\n\ndef post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get(\"post_data_raw\"):\n        logging.debug(\n            \"post_content: %s\\npost_data_raw: %s\" % (url, kwargs[\"post_data_raw\"])\n        )\n    else:\n        logging.debug(\"post_content: %s\\npost_data: %s\" % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get(\"post_data_raw\"):\n        post_data_enc = bytes(kwargs[\"post_data_raw\"], \"utf-8\")\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), \"utf-8\")\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode(\"utf-8\")\n\n    return data\n\n\ndef url_size(url, faker=False, headers={}):\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(url)\n\n    size = response.headers[\"content-length\"]\n    return int(size) if size is not None else float(\"inf\")\n\n\ndef urls_size(urls, faker=False, headers={}):\n    return sum([url_size(url, faker=faker, headers=headers) for url in urls])\n\n\ndef get_head(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_head: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.headers\n\n\ndef url_info(url, faker=False, headers={}):\n    logging.debug(\"url_info: %s\" % url)\n\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(request.Request(url))\n\n    headers = response.headers\n\n    type = headers[\"content-type\"]\n    if type == \"image/jpg; charset=UTF-8\" or type == \"image/jpg\":\n        type = \"audio/mpeg\"  # fix for netease\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/f4v\": \"flv\",\n        \"video/mp4\": \"mp4\",\n        \"video/MP2T\": \"ts\",\n        \"video/quicktime\": \"mov\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n        \"video/x-ms-asf\": \"asf\",\n        \"audio/mp4\": \"mp4\",\n        \"audio/mpeg\": \"mp3\",\n        \"audio/wav\": \"wav\",\n        \"audio/x-wav\": \"wav\",\n        \"audio/wave\": \"wav\",\n        \"image/jpeg\": \"jpg\",\n        \"image/png\": \"png\",\n        \"image/gif\": \"gif\",\n        \"application/pdf\": \"pdf\",\n    }\n    if type in mapping:\n        ext = mapping[type]\n    else:\n        type = None\n        if headers[\"content-disposition\"]:\n            try:\n                filename = parse.unquote(\n                    r1(r'filename=\"?([^\"]+)\"?', headers[\"content-disposition\"])\n                )\n                if len(filename.split(\".\")) > 1:\n                    ext = filename.split(\".\")[-1]\n                else:\n                    ext = None\n            except:\n                ext = None\n        else:\n            ext = None\n\n    if headers[\"transfer-encoding\"] != \"chunked\":\n        size = headers[\"content-length\"] and int(headers[\"content-length\"])\n    else:\n        size = None\n\n    return type, ext, size\n\n\ndef url_locations(urls, faker=False, headers={}):\n    locations = []\n    for url in urls:\n        logging.debug(\"url_locations: %s\" % url)\n\n        if faker:\n            response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n        elif headers:\n            response = urlopen_with_retry(request.Request(url, headers=headers))\n        else:\n            response = urlopen_with_retry(request.Request(url))\n\n        locations.append(response.url)\n    return locations\n\n\ndef url_save(\n    url,\n    filepath,\n    bar,\n    refer=None,\n    is_part=False,\n    faker=False,\n    headers=None,\n    timeout=None,\n    **kwargs\n):\n    tmp_headers = headers.copy() if headers is not None else {}\n    # When a referer specified with param refer,\n    # the key must be 'Referer' for the hack here\n    if refer is not None:\n        tmp_headers[\"Referer\"] = refer\n    if type(url) is list:\n        file_size = urls_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = True, url\n    else:\n        file_size = url_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = False, [url]\n\n    continue_renameing = True\n    while continue_renameing:\n        continue_renameing = False\n        if os.path.exists(filepath):\n            if not force and file_size == os.path.getsize(filepath):\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    log.w(\n                        \"Skipping {}: file already exists\".format(\n                            tr(os.path.basename(filepath))\n                        )\n                    )\n                else:\n                    if bar:\n                        bar.update_received(file_size)\n                return\n            else:\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    if not force and auto_rename:\n                        path, ext = os.path.basename(filepath).rsplit(\".\", 1)\n                        finder = re.compile(\" \\([1-9]\\d*?\\)$\")\n                        if finder.search(path) is None:\n                            thisfile = path + \" (1).\" + ext\n                        else:\n\n                            def numreturn(a):\n                                return \" (\" + str(int(a.group()[2:-1]) + 1) + \").\"\n\n                            thisfile = finder.sub(numreturn, path) + ext\n                        filepath = os.path.join(os.path.dirname(filepath), thisfile)\n                        print(\n                            \"Changing name to %s\" % tr(os.path.basename(filepath)),\n                            \"...\",\n                        )\n                        continue_renameing = True\n                        continue\n                    if log.yes_or_no(\"File with this name already exists. Overwrite?\"):\n                        log.w(\"Overwriting %s ...\" % tr(os.path.basename(filepath)))\n                    else:\n                        return\n        elif not os.path.exists(os.path.dirname(filepath)):\n            os.mkdir(os.path.dirname(filepath))\n\n    temp_filepath = filepath + \".download\" if file_size != float(\"inf\") else filepath\n    received = 0\n    if not force:\n        open_mode = \"ab\"\n\n        if os.path.exists(temp_filepath):\n            received += os.path.getsize(temp_filepath)\n            if bar:\n                bar.update_received(os.path.getsize(temp_filepath))\n    else:\n        open_mode = \"wb\"\n\n    for url in urls:\n        received_chunk = 0\n        if received < file_size:\n            if faker:\n                tmp_headers = fake_headers\n            \"\"\"\n            if parameter headers passed in, we have it copied as tmp_header\n            elif headers:\n                headers = headers\n            else:\n                headers = {}\n            \"\"\"\n            if received and not is_chunked:  # only request a range when not chunked\n                tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n            if refer:\n                tmp_headers[\"Referer\"] = refer\n\n            if timeout:\n                response = urlopen_with_retry(\n                    request.Request(url, headers=tmp_headers), timeout=timeout\n                )\n            else:\n                response = urlopen_with_retry(request.Request(url, headers=tmp_headers))\n            try:\n                range_start = int(\n                    response.headers[\"content-range\"][6:].split(\"/\")[0].split(\"-\")[0]\n                )\n                end_length = int(response.headers[\"content-range\"][6:].split(\"/\")[1])\n                range_length = end_length - range_start\n            except:\n                content_length = response.headers[\"content-length\"]\n                range_length = (\n                    int(content_length) if content_length is not None else float(\"inf\")\n                )\n\n            if is_chunked:  # always append if chunked\n                open_mode = \"ab\"\n            elif file_size != received + range_length:  # is it ever necessary?\n                received = 0\n                if bar:\n                    bar.received = 0\n                open_mode = \"wb\"\n\n            with open(temp_filepath, open_mode) as output:\n                while True:\n                    buffer = None\n                    try:\n                        buffer = response.read(1024 * 256)\n                    except socket.timeout:\n                        pass\n                    if not buffer:\n                        if is_chunked and received_chunk == range_length:\n                            break\n                        elif (\n                            not is_chunked and received == file_size\n                        ):  # Download finished\n                            break\n                        # Unexpected termination. Retry request\n                        if not is_chunked:  # when\n                            tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n                        response = urlopen_with_retry(\n                            request.Request(url, headers=tmp_headers)\n                        )\n                        continue\n                    output.write(buffer)\n                    received += len(buffer)\n                    received_chunk += len(buffer)\n                    if bar:\n                        bar.update_received(len(buffer))\n\n    assert received == os.path.getsize(temp_filepath), \"%s == %s == %s\" % (\n        received,\n        os.path.getsize(temp_filepath),\n        temp_filepath,\n    )\n\n    if os.access(filepath, os.W_OK):\n        # on Windows rename could fail if destination filepath exists\n        os.remove(filepath)\n    os.rename(temp_filepath, filepath)\n\n\nclass SimpleProgressBar:\n    term_size = term.get_terminal_size()[1]\n\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n        self.speed = \"\"\n        self.last_updated = time.time()\n\n        total_pieces_len = len(str(total_pieces))\n        # 38 is the size of all statically known size in self.bar\n        total_str = \"%5s\" % round(self.total_size / 1048576, 1)\n        total_str_width = max(len(total_str), 5)\n        self.bar_size = self.term_size - 28 - 2 * total_pieces_len - 2 * total_str_width\n        self.bar = \"{:>4}%% ({:>%s}/%sMB) \u251c{:\u2500<%s}\u2524[{:>%s}/{:>%s}] {}\" % (\n            total_str_width,\n            total_str,\n            self.bar_size,\n            total_pieces_len,\n            total_pieces_len,\n        )\n\n    def update(self):\n        self.displayed = True\n        bar_size = self.bar_size\n        percent = round(self.received * 100 / self.total_size, 1)\n        if percent >= 100:\n            percent = 100\n        dots = bar_size * int(percent) // 100\n        plus = int(percent) - dots // bar_size * 100\n        if plus > 0.8:\n            plus = \"\u2588\"\n        elif plus > 0.4:\n            plus = \">\"\n        else:\n            plus = \"\"\n        bar = \"\u2588\" * dots + plus\n        bar = self.bar.format(\n            percent,\n            round(self.received / 1048576, 1),\n            bar,\n            self.current_piece,\n            self.total_pieces,\n            self.speed,\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        time_diff = time.time() - self.last_updated\n        bytes_ps = n / time_diff if time_diff else 0\n        if bytes_ps >= 1024 ** 3:\n            self.speed = \"{:4.0f} GB/s\".format(bytes_ps / 1024 ** 3)\n        elif bytes_ps >= 1024 ** 2:\n            self.speed = \"{:4.0f} MB/s\".format(bytes_ps / 1024 ** 2)\n        elif bytes_ps >= 1024:\n            self.speed = \"{:4.0f} kB/s\".format(bytes_ps / 1024)\n        else:\n            self.speed = \"{:4.0f}  B/s\".format(bytes_ps)\n        self.last_updated = time.time()\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass PiecesProgressBar:\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n\n    def update(self):\n        self.displayed = True\n        bar = \"{0:>5}%[{1:<40}] {2}/{3}\".format(\n            \"\", \"=\" * 40, self.current_piece, self.total_pieces\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass DummyProgressBar:\n    def __init__(self, *args):\n        pass\n\n    def update_received(self, n):\n        pass\n\n    def update_piece(self, n):\n        pass\n\n    def done(self):\n        pass\n\n\ndef get_output_filename(urls, title, ext, output_dir, merge):\n    # lame hack for the --output-filename option\n    global output_filename\n    if output_filename:\n        if ext:\n            return output_filename + \".\" + ext\n        return output_filename\n\n    merged_ext = ext\n    if (len(urls) > 1) and merge:\n        from .processor.ffmpeg import has_ffmpeg_installed\n\n        if ext in [\"flv\", \"f4v\"]:\n            if has_ffmpeg_installed():\n                merged_ext = \"mp4\"\n            else:\n                merged_ext = \"flv\"\n        elif ext == \"mp4\":\n            merged_ext = \"mp4\"\n        elif ext == \"ts\":\n            if has_ffmpeg_installed():\n                merged_ext = \"mkv\"\n            else:\n                merged_ext = \"ts\"\n    return \"%s.%s\" % (title, merged_ext)\n\n\ndef print_user_agent(faker=False):\n    urllib_default_user_agent = \"Python-urllib/%d.%d\" % sys.version_info[:2]\n    user_agent = fake_headers[\"User-Agent\"] if faker else urllib_default_user_agent\n    print(\"User Agent: %s\" % user_agent)\n\n\ndef download_urls(\n    urls,\n    title,\n    ext,\n    total_size,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    headers={},\n    **kwargs\n):\n    assert urls\n    if json_output:\n        json_output_.download_urls(\n            urls=urls, title=title, ext=ext, total_size=total_size, refer=refer\n        )\n        return\n    if dry_run:\n        print_user_agent(faker=faker)\n        try:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join(urls))\n        except:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join([j for i in urls for j in i]))\n        return\n\n    if player:\n        launch_player(player, urls)\n        return\n\n    if not total_size:\n        try:\n            total_size = urls_size(urls, faker=faker, headers=headers)\n        except:\n            import traceback\n\n            traceback.print_exc(file=sys.stdout)\n            pass\n\n    title = tr(get_filename(title))\n    output_filename = get_output_filename(urls, title, ext, output_dir, merge)\n    output_filepath = os.path.join(output_dir, output_filename)\n\n    if total_size:\n        if (\n            not force\n            and os.path.exists(output_filepath)\n            and not auto_rename\n            and os.path.getsize(output_filepath) >= total_size * 0.9\n        ):\n            log.w(\"Skipping %s: file already exists\" % output_filepath)\n            print()\n            return\n        bar = SimpleProgressBar(total_size, len(urls))\n    else:\n        bar = PiecesProgressBar(total_size, len(urls))\n\n    if len(urls) == 1:\n        url = urls[0]\n        print(\"Downloading %s ...\" % tr(output_filename))\n        bar.update()\n        url_save(\n            url,\n            output_filepath,\n            bar,\n            refer=refer,\n            faker=faker,\n            headers=headers,\n            **kwargs\n        )\n        bar.done()\n    else:\n        parts = []\n        print(\"Downloading %s.%s ...\" % (tr(title), ext))\n        bar.update()\n        for i, url in enumerate(urls):\n            filename = \"%s[%02d].%s\" % (title, i, ext)\n            filepath = os.path.join(output_dir, filename)\n            parts.append(filepath)\n            # print 'Downloading %s [%s/%s]...' % (tr(filename), i + 1, len(urls))\n            bar.update_piece(i + 1)\n            url_save(\n                url,\n                filepath,\n                bar,\n                refer=refer,\n                is_part=True,\n                faker=faker,\n                headers=headers,\n                **kwargs\n            )\n        bar.done()\n\n        if not merge:\n            print()\n            return\n\n        if \"av\" in kwargs and kwargs[\"av\"]:\n            from .processor.ffmpeg import has_ffmpeg_installed\n\n            if has_ffmpeg_installed():\n                from .processor.ffmpeg import ffmpeg_concat_av\n\n                ret = ffmpeg_concat_av(parts, output_filepath, ext)\n                print(\"Merged into %s\" % output_filename)\n                if ret == 0:\n                    for part in parts:\n                        os.remove(part)\n\n        elif ext in [\"flv\", \"f4v\"]:\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_flv_to_mp4\n\n                    ffmpeg_concat_flv_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_flv import concat_flv\n\n                    concat_flv(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"mp4\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_mp4_to_mp4\n\n                    ffmpeg_concat_mp4_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_mp4 import concat_mp4\n\n                    concat_mp4(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"ts\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_ts_to_mkv\n\n                    ffmpeg_concat_ts_to_mkv(parts, output_filepath)\n                else:\n                    from .processor.join_ts import concat_ts\n\n                    concat_ts(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        else:\n            print(\"Can't merge %s files\" % ext)\n\n    print()\n\n\ndef download_rtmp_url(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset -> False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        from .processor.rtmpdump import play_rtmpdump_stream\n\n        play_rtmpdump_stream(player, url, params)\n        return\n\n    from .processor.rtmpdump import has_rtmpdump_installed, download_rtmpdump_stream\n\n    assert has_rtmpdump_installed(), \"RTMPDump not installed.\"\n    download_rtmpdump_stream(url, title, ext, params, output_dir)\n\n\ndef download_url_ffmpeg(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    stream=True,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset ->False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        launch_player(player, [url])\n        return\n\n    from .processor.ffmpeg import has_ffmpeg_installed, ffmpeg_download_stream\n\n    assert has_ffmpeg_installed(), \"FFmpeg not installed.\"\n\n    global output_filename\n    if output_filename:\n        dotPos = output_filename.rfind(\".\")\n        if dotPos > 0:\n            title = output_filename[:dotPos]\n            ext = output_filename[dotPos + 1 :]\n        else:\n            title = output_filename\n\n    title = tr(get_filename(title))\n\n    ffmpeg_download_stream(url, title, ext, params, output_dir, stream=stream)\n\n\ndef playlist_not_supported(name):\n    def f(*args, **kwargs):\n        raise NotImplementedError(\"Playlist is not supported for \" + name)\n\n    return f\n\n\ndef print_info(site_info, title, type, size, **kwargs):\n    if json_output:\n        json_output_.print_info(site_info=site_info, title=title, type=type, size=size)\n        return\n    if type:\n        type = type.lower()\n    if type in [\"3gp\"]:\n        type = \"video/3gpp\"\n    elif type in [\"asf\", \"wmv\"]:\n        type = \"video/x-ms-asf\"\n    elif type in [\"flv\", \"f4v\"]:\n        type = \"video/x-flv\"\n    elif type in [\"mkv\"]:\n        type = \"video/x-matroska\"\n    elif type in [\"mp3\"]:\n        type = \"audio/mpeg\"\n    elif type in [\"mp4\"]:\n        type = \"video/mp4\"\n    elif type in [\"mov\"]:\n        type = \"video/quicktime\"\n    elif type in [\"ts\"]:\n        type = \"video/MP2T\"\n    elif type in [\"webm\"]:\n        type = \"video/webm\"\n\n    elif type in [\"jpg\"]:\n        type = \"image/jpeg\"\n    elif type in [\"png\"]:\n        type = \"image/png\"\n    elif type in [\"gif\"]:\n        type = \"image/gif\"\n\n    if type in [\"video/3gpp\"]:\n        type_info = \"3GPP multimedia file (%s)\" % type\n    elif type in [\"video/x-flv\", \"video/f4v\"]:\n        type_info = \"Flash video (%s)\" % type\n    elif type in [\"video/mp4\", \"video/x-m4v\"]:\n        type_info = \"MPEG-4 video (%s)\" % type\n    elif type in [\"video/MP2T\"]:\n        type_info = \"MPEG-2 transport stream (%s)\" % type\n    elif type in [\"video/webm\"]:\n        type_info = \"WebM video (%s)\" % type\n    # elif type in ['video/ogg']:\n    #    type_info = 'Ogg video (%s)' % type\n    elif type in [\"video/quicktime\"]:\n        type_info = \"QuickTime video (%s)\" % type\n    elif type in [\"video/x-matroska\"]:\n        type_info = \"Matroska video (%s)\" % type\n    # elif type in ['video/x-ms-wmv']:\n    #    type_info = 'Windows Media video (%s)' % type\n    elif type in [\"video/x-ms-asf\"]:\n        type_info = \"Advanced Systems Format (%s)\" % type\n    # elif type in ['video/mpeg']:\n    #    type_info = 'MPEG video (%s)' % type\n    elif type in [\"audio/mp4\", \"audio/m4a\"]:\n        type_info = \"MPEG-4 audio (%s)\" % type\n    elif type in [\"audio/mpeg\"]:\n        type_info = \"MP3 (%s)\" % type\n    elif type in [\"audio/wav\", \"audio/wave\", \"audio/x-wav\"]:\n        type_info = \"Waveform Audio File Format ({})\".format(type)\n\n    elif type in [\"image/jpeg\"]:\n        type_info = \"JPEG Image (%s)\" % type\n    elif type in [\"image/png\"]:\n        type_info = \"Portable Network Graphics (%s)\" % type\n    elif type in [\"image/gif\"]:\n        type_info = \"Graphics Interchange Format (%s)\" % type\n    elif type in [\"m3u8\"]:\n        if \"m3u8_type\" in kwargs:\n            if kwargs[\"m3u8_type\"] == \"master\":\n                type_info = \"M3U8 Master {}\".format(type)\n        else:\n            type_info = \"M3U8 Playlist {}\".format(type)\n    else:\n        type_info = \"Unknown type (%s)\" % type\n\n    maybe_print(\"Site:      \", site_info)\n    maybe_print(\"Title:     \", unescape_html(tr(title)))\n    print(\"Type:      \", type_info)\n    if type != \"m3u8\":\n        print(\"Size:      \", round(size / 1048576, 2), \"MiB (\" + str(size) + \" Bytes)\")\n    if type == \"m3u8\" and \"m3u8_url\" in kwargs:\n        print(\"M3U8 Url:   {}\".format(kwargs[\"m3u8_url\"]))\n    print()\n\n\ndef mime_to_container(mime):\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/mp4\": \"mp4\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n    }\n    if mime in mapping:\n        return mapping[mime]\n    else:\n        return mime.split(\"/\")[1]\n\n\ndef parse_host(host):\n    \"\"\"Parses host name and port number from a string.\"\"\"\n    if re.match(r\"^(\\d+)$\", host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r\"^(\\w+)://\", host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)\n\n\ndef set_proxy(proxy):\n    proxy_handler = request.ProxyHandler(\n        {\n            \"http\": \"%s:%s\" % proxy,\n            \"https\": \"%s:%s\" % proxy,\n        }\n    )\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\ndef unset_proxy():\n    proxy_handler = request.ProxyHandler({})\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\n# DEPRECATED in favor of set_proxy() and unset_proxy()\ndef set_http_proxy(proxy):\n    if proxy is None:  # Use system default setting\n        proxy_support = request.ProxyHandler()\n    elif proxy == \"\":  # Don't use any proxy\n        proxy_support = request.ProxyHandler({})\n    else:  # Use proxy\n        proxy_support = request.ProxyHandler(\n            {\"http\": \"%s\" % proxy, \"https\": \"%s\" % proxy}\n        )\n    opener = request.build_opener(proxy_support)\n    request.install_opener(opener)\n\n\ndef print_more_compatible(*args, **kwargs):\n    import builtins as __builtin__\n\n    \"\"\"Overload default print function as py (<3.3) does not support 'flush' keyword.\n    Although the function name can be same as print to get itself overloaded automatically,\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n    \"\"\"\n    # nothing happens on py3.3 and later\n    if sys.version_info[:2] >= (3, 3):\n        return __builtin__.print(*args, **kwargs)\n\n    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested\n    doFlush = kwargs.pop(\"flush\", False)\n    ret = __builtin__.print(*args, **kwargs)\n    if doFlush:\n        kwargs.get(\"file\", sys.stdout).flush()\n    return ret\n\n\ndef download_main(download, download_playlist, urls, playlist, **kwargs):\n    for url in urls:\n        if re.match(r\"https?://\", url) is None:\n            url = \"http://\" + url\n\n        if playlist:\n            download_playlist(url, **kwargs)\n        else:\n            download(url, **kwargs)\n\n\ndef load_cookies(cookiefile):\n    global cookies\n    if cookiefile.endswith(\".txt\"):\n        # MozillaCookieJar treats prefix '#HttpOnly_' as comments incorrectly!\n        # do not use its load()\n        # see also:\n        #   - https://docs.python.org/3/library/http.cookiejar.html#http.cookiejar.MozillaCookieJar\n        #   - https://github.com/python/cpython/blob/4b219ce/Lib/http/cookiejar.py#L2014\n        #   - https://curl.haxx.se/libcurl/c/CURLOPT_COOKIELIST.html#EXAMPLE\n        # cookies = cookiejar.MozillaCookieJar(cookiefile)\n        # cookies.load()\n        from http.cookiejar import Cookie\n\n        cookies = cookiejar.MozillaCookieJar()\n        now = time.time()\n        ignore_discard, ignore_expires = False, False\n        with open(cookiefile, \"r\") as f:\n            for line in f:\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"):\n                    line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if line.strip().startswith((\"#\", \"$\")) or line.strip() == \"\":\n                    if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                        continue\n\n                (\n                    domain,\n                    domain_specified,\n                    path,\n                    secure,\n                    expires,\n                    name,\n                    value,\n                ) = line.split(\"\\t\")\n                secure = secure == \"TRUE\"\n                domain_specified = domain_specified == \"TRUE\"\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas http.cookiejar regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                    assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(\n                    0,\n                    name,\n                    value,\n                    None,\n                    False,\n                    domain,\n                    domain_specified,\n                    initial_dot,\n                    path,\n                    False,\n                    secure,\n                    expires,\n                    discard,\n                    None,\n                    None,\n                    {},\n                )\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                cookies.set_cookie(c)\n\n    elif cookiefile.endswith((\".sqlite\", \".sqlite3\")):\n        import sqlite3, shutil, tempfile\n\n        temp_dir = tempfile.gettempdir()\n        temp_cookiefile = os.path.join(temp_dir, \"temp_cookiefile.sqlite\")\n        shutil.copy2(cookiefile, temp_cookiefile)\n\n        cookies = cookiejar.MozillaCookieJar()\n        con = sqlite3.connect(temp_cookiefile)\n        cur = con.cursor()\n        cur.execute(\n            \"\"\"SELECT host, path, isSecure, expiry, name, value\n        FROM moz_cookies\"\"\"\n        )\n        for item in cur.fetchall():\n            c = cookiejar.Cookie(\n                0,\n                item[4],\n                item[5],\n                None,\n                False,\n                item[0],\n                item[0].startswith(\".\"),\n                item[0].startswith(\".\"),\n                item[1],\n                False,\n                item[2],\n                item[3],\n                item[3] == \"\",\n                None,\n                None,\n                {},\n            )\n            cookies.set_cookie(c)\n\n    else:\n        log.e(\"[error] unsupported cookies format\")\n        # TODO: Chromium Cookies\n        # SELECT host_key, path, secure, expires_utc, name, encrypted_value\n        # FROM cookies\n        # http://n8henrie.com/2013/11/use-chromes-cookies-for-easier-downloading-with-python-requests/\n\n\ndef set_socks_proxy(proxy):\n    try:\n        import socks\n\n        socks_proxy_addrs = proxy.split(\":\")\n        socks.set_default_proxy(\n            socks.SOCKS5, socks_proxy_addrs[0], int(socks_proxy_addrs[1])\n        )\n        socket.socket = socks.socksocket\n\n        def getaddrinfo(*args):\n            return [(socket.AF_INET, socket.SOCK_STREAM, 6, \"\", (args[0], args[1]))]\n\n        socket.getaddrinfo = getaddrinfo\n    except ImportError:\n        log.w(\n            \"Error importing PySocks library, socks proxy ignored.\"\n            \"In order to use use socks proxy, please install PySocks.\"\n        )\n\n\ndef script_main(download, download_playlist, **kwargs):\n    logging.basicConfig(format=\"[%(levelname)s] %(message)s\")\n\n    def print_version():\n        version = get_version(\n            kwargs[\"repo_path\"] if \"repo_path\" in kwargs else __version__\n        )\n        log.i(\"version {}, a tiny downloader that scrapes the web.\".format(version))\n\n    parser = argparse.ArgumentParser(\n        prog=\"you-get\",\n        usage=\"you-get [OPTION]... URL...\",\n        description=\"A tiny downloader that scrapes the web\",\n        add_help=False,\n    )\n    parser.add_argument(\n        \"-V\", \"--version\", action=\"store_true\", help=\"Print version and exit\"\n    )\n    parser.add_argument(\n        \"-h\", \"--help\", action=\"store_true\", help=\"Print this help message and exit\"\n    )\n\n    dry_run_grp = parser.add_argument_group(\n        \"Dry-run options\", \"(no actual downloading)\"\n    )\n    dry_run_grp = dry_run_grp.add_mutually_exclusive_group()\n    dry_run_grp.add_argument(\n        \"-i\", \"--info\", action=\"store_true\", help=\"Print extracted information\"\n    )\n    dry_run_grp.add_argument(\n        \"-u\", \"--url\", action=\"store_true\", help=\"Print extracted information with URLs\"\n    )\n    dry_run_grp.add_argument(\n        \"--json\", action=\"store_true\", help=\"Print extracted URLs in JSON format\"\n    )\n\n    download_grp = parser.add_argument_group(\"Download options\")\n    download_grp.add_argument(\n        \"-n\",\n        \"--no-merge\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not merge video parts\",\n    )\n    download_grp.add_argument(\n        \"--no-caption\",\n        action=\"store_true\",\n        help=\"Do not download captions (subtitles, lyrics, danmaku, ...)\",\n    )\n    download_grp.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force overwriting existing files\",\n    )\n    download_grp.add_argument(\n        \"-F\", \"--format\", metavar=\"STREAM_ID\", help=\"Set video format to STREAM_ID\"\n    )\n    download_grp.add_argument(\n        \"-O\", \"--output-filename\", metavar=\"FILE\", help=\"Set output filename\"\n    )\n    download_grp.add_argument(\n        \"-o\", \"--output-dir\", metavar=\"DIR\", default=\".\", help=\"Set output directory\"\n    )\n    download_grp.add_argument(\n        \"-p\", \"--player\", metavar=\"PLAYER\", help=\"Stream extracted URL to a PLAYER\"\n    )\n    download_grp.add_argument(\n        \"-c\",\n        \"--cookies\",\n        metavar=\"COOKIES_FILE\",\n        help=\"Load cookies.txt or cookies.sqlite\",\n    )\n    download_grp.add_argument(\n        \"-t\",\n        \"--timeout\",\n        metavar=\"SECONDS\",\n        type=int,\n        default=600,\n        help=\"Set socket timeout\",\n    )\n    download_grp.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show traceback and other debug info\"\n    )\n    download_grp.add_argument(\n        \"-I\",\n        \"--input-file\",\n        metavar=\"FILE\",\n        type=argparse.FileType(\"r\"),\n        help=\"Read non-playlist URLs from FILE\",\n    )\n    download_grp.add_argument(\n        \"-P\", \"--password\", help=\"Set video visit password to PASSWORD\"\n    )\n    download_grp.add_argument(\n        \"-l\", \"--playlist\", action=\"store_true\", help=\"Prefer to download a playlist\"\n    )\n    download_grp.add_argument(\n        \"-a\",\n        \"--auto-rename\",\n        action=\"store_true\",\n        default=False,\n        help=\"Auto rename same name different files\",\n    )\n\n    download_grp.add_argument(\n        \"-k\", \"--insecure\", action=\"store_true\", default=False, help=\"ignore ssl errors\"\n    )\n\n    proxy_grp = parser.add_argument_group(\"Proxy options\")\n    proxy_grp = proxy_grp.add_mutually_exclusive_group()\n    proxy_grp.add_argument(\n        \"-x\",\n        \"--http-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for downloading\",\n    )\n    proxy_grp.add_argument(\n        \"-y\",\n        \"--extractor-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for extracting only\",\n    )\n    proxy_grp.add_argument(\"--no-proxy\", action=\"store_true\", help=\"Never use a proxy\")\n    proxy_grp.add_argument(\n        \"-s\",\n        \"--socks-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an SOCKS5 proxy for downloading\",\n    )\n\n    download_grp.add_argument(\"--stream\", help=argparse.SUPPRESS)\n    download_grp.add_argument(\"--itag\", help=argparse.SUPPRESS)\n\n    parser.add_argument(\"URL\", nargs=\"*\", help=argparse.SUPPRESS)\n\n    args = parser.parse_args()\n\n    if args.help:\n        print_version()\n        parser.print_help()\n        sys.exit()\n    if args.version:\n        print_version()\n        sys.exit()\n\n    if args.debug:\n        # Set level of root logger to DEBUG\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    global force\n    global dry_run\n    global json_output\n    global player\n    global extractor_proxy\n    global output_filename\n    global auto_rename\n    global insecure\n    output_filename = args.output_filename\n    extractor_proxy = args.extractor_proxy\n\n    info_only = args.info\n    if args.force:\n        force = True\n    if args.auto_rename:\n        auto_rename = True\n    if args.url:\n        dry_run = True\n    if args.json:\n        json_output = True\n        # to fix extractors not use VideoExtractor\n        dry_run = True\n        info_only = False\n\n    if args.cookies:\n        load_cookies(args.cookies)\n\n    caption = True\n    stream_id = args.format or args.stream or args.itag\n    if args.no_caption:\n        caption = False\n    if args.player:\n        player = args.player\n        caption = False\n\n    if args.insecure:\n        # ignore ssl\n        insecure = True\n\n    if args.no_proxy:\n        set_http_proxy(\"\")\n    else:\n        set_http_proxy(args.http_proxy)\n    if args.socks_proxy:\n        set_socks_proxy(args.socks_proxy)\n\n    URLs = []\n    if args.input_file:\n        logging.debug(\"you are trying to load urls from %s\", args.input_file)\n        if args.playlist:\n            log.e(\n                \"reading playlist from a file is unsupported \"\n                \"and won't make your life easier\"\n            )\n            sys.exit(2)\n        URLs.extend(args.input_file.read().splitlines())\n        args.input_file.close()\n    URLs.extend(args.URL)\n\n    if not URLs:\n        parser.print_help()\n        sys.exit()\n\n    socket.setdefaulttimeout(args.timeout)\n\n    try:\n        extra = {}\n        if extractor_proxy:\n            extra[\"extractor_proxy\"] = extractor_proxy\n        if stream_id:\n            extra[\"stream_id\"] = stream_id\n        download_main(\n            download,\n            download_playlist,\n            URLs,\n            args.playlist,\n            output_dir=args.output_dir,\n            merge=not args.no_merge,\n            info_only=info_only,\n            json_output=json_output,\n            caption=caption,\n            password=args.password,\n            **extra\n        )\n    except KeyboardInterrupt:\n        if args.debug:\n            raise\n        else:\n            sys.exit(1)\n    except UnicodeEncodeError:\n        if args.debug:\n            raise\n        log.e(\n            \"[error] oops, the current environment does not seem to support \" \"Unicode.\"\n        )\n        log.e(\"please set it to a UTF-8-aware locale first,\")\n        log.e(\"so as to save the video (with some Unicode characters) correctly.\")\n        log.e(\"you can do it like this:\")\n        log.e(\"    (Windows)    % chcp 65001 \")\n        log.e(\"    (Linux)      $ LC_CTYPE=en_US.UTF-8\")\n        sys.exit(1)\n    except Exception:\n        if not args.debug:\n            log.e(\"[error] oops, something went wrong.\")\n            log.e(\"don't panic, c'est la vie. please try the following steps:\")\n            log.e(\"  (1) Rule out any network problem.\")\n            log.e(\"  (2) Make sure you-get is up-to-date.\")\n            log.e(\"  (3) Check if the issue is already known, on\")\n            log.e(\"        https://github.com/soimort/you-get/wiki/Known-Bugs\")\n            log.e(\"        https://github.com/soimort/you-get/issues\")\n            log.e(\"  (4) Run the command with '--debug' option,\")\n            log.e(\"      and report this issue with the full output.\")\n        else:\n            print_version()\n            log.i(args)\n            raise\n        sys.exit(1)\n\n\ndef google_search(url):\n    keywords = r1(r\"https?://(.*)\", url)\n    url = \"https://www.google.com/search?tbm=vid&q=%s\" % parse.quote(keywords)\n    page = get_content(url, headers=fake_headers)\n    videos = re.findall(\n        r'<a href=\"(https?://[^\"]+)\" onmousedown=\"[^\"]+\"><h3 class=\"[^\"]*\">([^<]+)<',\n        page,\n    )\n    vdurs = re.findall(r'<span class=\"vdur[^\"]*\">([^<]+)<', page)\n    durs = [r1(r\"(\\d+:\\d+)\", unescape_html(dur)) for dur in vdurs]\n    print(\"Google Videos search:\")\n    for v in zip(videos, durs):\n        print(\"- video:  {} [{}]\".format(unescape_html(v[0][1]), v[1] if v[1] else \"?\"))\n        print(\"# you-get %s\" % log.sprint(v[0][0], log.UNDERLINE))\n        print()\n    print(\"Best matched result:\")\n    return videos[0][0]\n\n\ndef url_to_module(url):\n    try:\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n        assert video_host and video_url\n    except AssertionError:\n        url = google_search(url)\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    if video_host.endswith(\".com.cn\") or video_host.endswith(\".ac.cn\"):\n        video_host = video_host[:-3]\n    domain = r1(r\"(\\.[^.]+\\.[^.]+)$\", video_host) or video_host\n    assert domain, \"unsupported url: \" + url\n\n    # all non-ASCII code points must be quoted (percent-encoded UTF-8)\n    url = \"\".join([ch if ord(ch) in range(128) else parse.quote(ch) for ch in url])\n    video_host = r1(r\"https?://([^/]+)/\", url)\n    video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    k = r1(r\"([^.]+)\", domain)\n    if k in SITES:\n        return (import_module(\".\".join([\"you_get\", \"extractors\", SITES[k]])), url)\n    else:\n        try:\n            location = get_location(url)  # t.co isn't happy with fake_headers\n        except:\n            location = get_location(url, headers=fake_headers)\n\n        if location and location != url and not location.startswith(\"/\"):\n            return url_to_module(location)\n        else:\n            return import_module(\"you_get.extractors.universal\"), url\n\n\ndef any_download(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download(url, **kwargs)\n\n\ndef any_download_playlist(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download_playlist(url, **kwargs)\n\n\ndef main(**kwargs):\n    script_main(any_download, any_download_playlist, **kwargs)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "package": ["import io", "import os", "import re", "import sys", "import time", "import json", "import socket", "import locale", "import logging", "import argparse", "import ssl", "from http import cookiejar", "from importlib import import_module", "from urllib import request, parse, error", "from .version import __version__", "from .util import log, term", "from .util.git import get_version", "from .util.strings import get_filename, unescape_html", "from . import json_output as json_output_", "import subprocess", "import shlex", "import shutil", "from io import BytesIO", "import gzip", "import zlib"], "function": ["def rc4(key, data):\n", "def general_m3u8_extractor(url, headers={}):\n", "def maybe_print(*s):\n", "def tr(s):\n", "def r1(pattern, text):\n", "def r1_of(patterns, text):\n", "def match1(text, *patterns):\n", "def matchall(text, patterns):\n", "def launch_player(player, urls):\n", "def parse_query_param(url, param):\n", "def unicodize(text):\n", "def escape_file_path(path):\n", "def ungzip(data):\n", "def undeflate(data):\n", "def get_response(url, faker=False):\n", "def get_html(url, encoding=None, faker=False):\n", "def get_decoded_html(url, faker=False):\n", "def get_location(url, headers=None, get_method=\"HEAD\"):\n", "def urlopen_with_retry(*args, **kwargs):\n", "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n", "def url_size(url, faker=False, headers={}):\n", "def urls_size(urls, faker=False, headers={}):\n", "def get_head(url, headers=None, get_method=\"HEAD\"):\n", "def url_info(url, faker=False, headers={}):\n", "def url_locations(urls, faker=False, headers={}):\n", "class SimpleProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class PiecesProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class DummyProgressBar:\n", "    def __init__(self, *args):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "def get_output_filename(urls, title, ext, output_dir, merge):\n", "def print_user_agent(faker=False):\n", "def playlist_not_supported(name):\n", "    def f(*args, **kwargs):\n", "def print_info(site_info, title, type, size, **kwargs):\n", "def mime_to_container(mime):\n", "def parse_host(host):\n", "def set_proxy(proxy):\n", "def unset_proxy():\n", "def set_http_proxy(proxy):\n", "def print_more_compatible(*args, **kwargs):\n", "def download_main(download, download_playlist, urls, playlist, **kwargs):\n", "def load_cookies(cookiefile):\n", "def set_socks_proxy(proxy):\n", "def script_main(download, download_playlist, **kwargs):\n", "    def print_version():\n", "def google_search(url):\n", "def url_to_module(url):\n", "def any_download(url, **kwargs):\n", "def any_download_playlist(url, **kwargs):\n", "def main(**kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "post_content", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.", "docstring_tokens": ["Post", "the", "content", "of", "a", "URL", "via", "sending", "a", "HTTP", "POST", "request", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501", "partition": "test", "up_fun_num": 20, "context": "#!/usr/bin/env python\n\nimport io\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport socket\nimport locale\nimport logging\nimport argparse\nimport ssl\nfrom http import cookiejar\nfrom importlib import import_module\nfrom urllib import request, parse, error\n\nfrom .version import __version__\nfrom .util import log, term\nfrom .util.git import get_version\nfrom .util.strings import get_filename, unescape_html\nfrom . import json_output as json_output_\n\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf8\")\n\nSITES = {\n    \"163\": \"netease\",\n    \"56\": \"w56\",\n    \"365yg\": \"toutiao\",\n    \"acfun\": \"acfun\",\n    \"archive\": \"archive\",\n    \"baidu\": \"baidu\",\n    \"bandcamp\": \"bandcamp\",\n    \"baomihua\": \"baomihua\",\n    \"bigthink\": \"bigthink\",\n    \"bilibili\": \"bilibili\",\n    \"cctv\": \"cntv\",\n    \"cntv\": \"cntv\",\n    \"cbs\": \"cbs\",\n    \"coub\": \"coub\",\n    \"dailymotion\": \"dailymotion\",\n    \"douban\": \"douban\",\n    \"douyin\": \"douyin\",\n    \"douyu\": \"douyutv\",\n    \"ehow\": \"ehow\",\n    \"facebook\": \"facebook\",\n    \"fc2\": \"fc2video\",\n    \"flickr\": \"flickr\",\n    \"freesound\": \"freesound\",\n    \"fun\": \"funshion\",\n    \"google\": \"google\",\n    \"giphy\": \"giphy\",\n    \"heavy-music\": \"heavymusic\",\n    \"huomao\": \"huomaotv\",\n    \"iask\": \"sina\",\n    \"icourses\": \"icourses\",\n    \"ifeng\": \"ifeng\",\n    \"imgur\": \"imgur\",\n    \"in\": \"alive\",\n    \"infoq\": \"infoq\",\n    \"instagram\": \"instagram\",\n    \"interest\": \"interest\",\n    \"iqilu\": \"iqilu\",\n    \"iqiyi\": \"iqiyi\",\n    \"ixigua\": \"ixigua\",\n    \"isuntv\": \"suntv\",\n    \"iwara\": \"iwara\",\n    \"joy\": \"joy\",\n    \"kankanews\": \"bilibili\",\n    \"khanacademy\": \"khan\",\n    \"ku6\": \"ku6\",\n    \"kuaishou\": \"kuaishou\",\n    \"kugou\": \"kugou\",\n    \"kuwo\": \"kuwo\",\n    \"le\": \"le\",\n    \"letv\": \"le\",\n    \"lizhi\": \"lizhi\",\n    \"longzhu\": \"longzhu\",\n    \"magisto\": \"magisto\",\n    \"metacafe\": \"metacafe\",\n    \"mgtv\": \"mgtv\",\n    \"miomio\": \"miomio\",\n    \"mixcloud\": \"mixcloud\",\n    \"mtv81\": \"mtv81\",\n    \"musicplayon\": \"musicplayon\",\n    \"miaopai\": \"yixia\",\n    \"naver\": \"naver\",\n    \"7gogo\": \"nanagogo\",\n    \"nicovideo\": \"nicovideo\",\n    \"panda\": \"panda\",\n    \"pinterest\": \"pinterest\",\n    \"pixnet\": \"pixnet\",\n    \"pptv\": \"pptv\",\n    \"qingting\": \"qingting\",\n    \"qq\": \"qq\",\n    \"showroom-live\": \"showroom\",\n    \"sina\": \"sina\",\n    \"smgbb\": \"bilibili\",\n    \"sohu\": \"sohu\",\n    \"soundcloud\": \"soundcloud\",\n    \"ted\": \"ted\",\n    \"theplatform\": \"theplatform\",\n    \"tiktok\": \"tiktok\",\n    \"tucao\": \"tucao\",\n    \"tudou\": \"tudou\",\n    \"tumblr\": \"tumblr\",\n    \"twimg\": \"twitter\",\n    \"twitter\": \"twitter\",\n    \"ucas\": \"ucas\",\n    \"videomega\": \"videomega\",\n    \"vidto\": \"vidto\",\n    \"vimeo\": \"vimeo\",\n    \"wanmen\": \"wanmen\",\n    \"weibo\": \"miaopai\",\n    \"veoh\": \"veoh\",\n    \"vine\": \"vine\",\n    \"vk\": \"vk\",\n    \"xiami\": \"xiami\",\n    \"xiaokaxiu\": \"yixia\",\n    \"xiaojiadianvideo\": \"fc2video\",\n    \"ximalaya\": \"ximalaya\",\n    \"yinyuetai\": \"yinyuetai\",\n    \"yizhibo\": \"yizhibo\",\n    \"youku\": \"youku\",\n    \"youtu\": \"youtube\",\n    \"youtube\": \"youtube\",\n    \"zhanqi\": \"zhanqi\",\n    \"zhibo\": \"zhibo\",\n    \"zhihu\": \"zhihu\",\n}\n\ndry_run = False\njson_output = False\nforce = False\nplayer = None\nextractor_proxy = None\ncookies = None\noutput_filename = None\nauto_rename = False\ninsecure = False\n\nfake_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",  # noqa\n    \"Accept-Charset\": \"UTF-8,*;q=0.5\",\n    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0\",  # noqa\n}\n\nif sys.stdout.isatty():\n    default_encoding = sys.stdout.encoding.lower()\nelse:\n    default_encoding = locale.getpreferredencoding().lower()\n\n\ndef rc4(key, data):\n    # all encryption algo should work on bytes\n    assert type(key) == type(data) and type(key) == type(b\"\")\n    state = list(range(256))\n    j = 0\n    for i in range(256):\n        j += state[i] + key[i % len(key)]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n\n    i = 0\n    j = 0\n    out_list = []\n    for char in data:\n        i += 1\n        i &= 0xFF\n        j += state[i]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n        prn = state[(state[i] + state[j]) & 0xFF]\n        out_list.append(char ^ prn)\n\n    return bytes(out_list)\n\n\ndef general_m3u8_extractor(url, headers={}):\n    m3u8_list = get_content(url, headers=headers).split(\"\\n\")\n    urls = []\n    for line in m3u8_list:\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            if line.startswith(\"http\"):\n                urls.append(line)\n            else:\n                seg_url = parse.urljoin(url, line)\n                urls.append(seg_url)\n    return urls\n\n\ndef maybe_print(*s):\n    try:\n        print(*s)\n    except:\n        pass\n\n\ndef tr(s):\n    if default_encoding == \"utf-8\":\n        return s\n    else:\n        return s\n        # return str(s.encode('utf-8'))[2:-1]\n\n\n# DEPRECATED in favor of match1()\ndef r1(pattern, text):\n    m = re.search(pattern, text)\n    if m:\n        return m.group(1)\n\n\n# DEPRECATED in favor of match1()\ndef r1_of(patterns, text):\n    for p in patterns:\n        x = r1(p, text)\n        if x:\n            return x\n\n\ndef match1(text, *patterns):\n    \"\"\"Scans through a string for substrings matched some patterns (first-subgroups only).\n\n    Args:\n        text: A string to be scanned.\n        patterns: Arbitrary number of regex patterns.\n\n    Returns:\n        When only one pattern is given, returns a string (None if no match found).\n        When more than one pattern are given, returns a list of strings ([] if no match found).\n    \"\"\"\n\n    if len(patterns) == 1:\n        pattern = patterns[0]\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    else:\n        ret = []\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                ret.append(match.group(1))\n        return ret\n\n\ndef matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret\n\n\ndef launch_player(player, urls):\n    import subprocess\n    import shlex\n\n    if sys.version_info >= (3, 3):\n        import shutil\n\n        exefile = shlex.split(player)[0]\n        if shutil.which(exefile) is not None:\n            subprocess.call(shlex.split(player) + list(urls))\n        else:\n            log.wtf('[Failed] Cannot find player \"%s\"' % exefile)\n    else:\n        subprocess.call(shlex.split(player) + list(urls))\n\n\ndef parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None\n\n\ndef unicodize(text):\n    return re.sub(\n        r\"\\\\u([0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f])\",\n        lambda x: chr(int(x.group(0)[2:], 16)),\n        text,\n    )\n\n\n# DEPRECATED in favor of util.legitimize()\ndef escape_file_path(path):\n    path = path.replace(\"/\", \"-\")\n    path = path.replace(\"\\\\\", \"-\")\n    path = path.replace(\"*\", \"-\")\n    path = path.replace(\"?\", \"-\")\n    return path\n\n\ndef ungzip(data):\n    \"\"\"Decompresses data for Content-Encoding: gzip.\"\"\"\n    from io import BytesIO\n    import gzip\n\n    buffer = BytesIO(data)\n    f = gzip.GzipFile(fileobj=buffer)\n    return f.read()\n\n\ndef undeflate(data):\n    \"\"\"Decompresses data for Content-Encoding: deflate.\n    (the zlib compression is used.)\n    \"\"\"\n    import zlib\n\n    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)\n    return decompressobj.decompress(data) + decompressobj.flush()\n\n\n# DEPRECATED in favor of get_content()\ndef get_response(url, faker=False):\n    logging.debug(\"get_response: %s\" % url)\n\n    # install cookies\n    if cookies:\n        opener = request.build_opener(request.HTTPCookieProcessor(cookies))\n        request.install_opener(opener)\n\n    if faker:\n        response = request.urlopen(request.Request(url, headers=fake_headers), None)\n    else:\n        response = request.urlopen(url)\n\n    data = response.read()\n    if response.info().get(\"Content-Encoding\") == \"gzip\":\n        data = ungzip(data)\n    elif response.info().get(\"Content-Encoding\") == \"deflate\":\n        data = undeflate(data)\n    response.data = data\n    return response\n\n\n# DEPRECATED in favor of get_content()\ndef get_html(url, encoding=None, faker=False):\n    content = get_response(url, faker).data\n    return str(content, \"utf-8\", \"ignore\")\n\n\n# DEPRECATED in favor of get_content()\ndef get_decoded_html(url, faker=False):\n    response = get_response(url, faker)\n    data = response.data\n    charset = r1(r\"charset=([\\w-]+)\", response.headers[\"content-type\"])\n    if charset:\n        return data.decode(charset, \"ignore\")\n    else:\n        return data\n\n\ndef get_location(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_location: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.geturl()\n\n\ndef urlopen_with_retry(*args, **kwargs):\n    retry_time = 3\n    for i in range(retry_time):\n        try:\n            if insecure:\n                # ignore ssl errors\n                ctx = ssl.create_default_context()\n                ctx.check_hostname = False\n                ctx.verify_mode = ssl.CERT_NONE\n                return request.urlopen(*args, context=ctx, **kwargs)\n            else:\n                return request.urlopen(*args, **kwargs)\n        except socket.timeout as e:\n            logging.debug(\"request attempt %s timeout\" % str(i + 1))\n            if i + 1 == retry_time:\n                raise e\n        # try to tackle youku CDN fails\n        except error.HTTPError as http_error:\n            logging.debug(\"HTTP Error with code{}\".format(http_error.code))\n            if i + 1 == retry_time:\n                raise http_error\n\n\ndef get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug(\"get_content: %s\" % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\", \"\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset, \"ignore\")\n        else:\n            data = data.decode(\"utf-8\", \"ignore\")\n\n    return data\n\n\ndef url_size(url, faker=False, headers={}):\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(url)\n\n    size = response.headers[\"content-length\"]\n    return int(size) if size is not None else float(\"inf\")\n\n\ndef urls_size(urls, faker=False, headers={}):\n    return sum([url_size(url, faker=faker, headers=headers) for url in urls])\n\n\ndef get_head(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_head: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.headers\n\n\ndef url_info(url, faker=False, headers={}):\n    logging.debug(\"url_info: %s\" % url)\n\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(request.Request(url))\n\n    headers = response.headers\n\n    type = headers[\"content-type\"]\n    if type == \"image/jpg; charset=UTF-8\" or type == \"image/jpg\":\n        type = \"audio/mpeg\"  # fix for netease\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/f4v\": \"flv\",\n        \"video/mp4\": \"mp4\",\n        \"video/MP2T\": \"ts\",\n        \"video/quicktime\": \"mov\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n        \"video/x-ms-asf\": \"asf\",\n        \"audio/mp4\": \"mp4\",\n        \"audio/mpeg\": \"mp3\",\n        \"audio/wav\": \"wav\",\n        \"audio/x-wav\": \"wav\",\n        \"audio/wave\": \"wav\",\n        \"image/jpeg\": \"jpg\",\n        \"image/png\": \"png\",\n        \"image/gif\": \"gif\",\n        \"application/pdf\": \"pdf\",\n    }\n    if type in mapping:\n        ext = mapping[type]\n    else:\n        type = None\n        if headers[\"content-disposition\"]:\n            try:\n                filename = parse.unquote(\n                    r1(r'filename=\"?([^\"]+)\"?', headers[\"content-disposition\"])\n                )\n                if len(filename.split(\".\")) > 1:\n                    ext = filename.split(\".\")[-1]\n                else:\n                    ext = None\n            except:\n                ext = None\n        else:\n            ext = None\n\n    if headers[\"transfer-encoding\"] != \"chunked\":\n        size = headers[\"content-length\"] and int(headers[\"content-length\"])\n    else:\n        size = None\n\n    return type, ext, size\n\n\ndef url_locations(urls, faker=False, headers={}):\n    locations = []\n    for url in urls:\n        logging.debug(\"url_locations: %s\" % url)\n\n        if faker:\n            response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n        elif headers:\n            response = urlopen_with_retry(request.Request(url, headers=headers))\n        else:\n            response = urlopen_with_retry(request.Request(url))\n\n        locations.append(response.url)\n    return locations\n\n\ndef url_save(\n    url,\n    filepath,\n    bar,\n    refer=None,\n    is_part=False,\n    faker=False,\n    headers=None,\n    timeout=None,\n    **kwargs\n):\n    tmp_headers = headers.copy() if headers is not None else {}\n    # When a referer specified with param refer,\n    # the key must be 'Referer' for the hack here\n    if refer is not None:\n        tmp_headers[\"Referer\"] = refer\n    if type(url) is list:\n        file_size = urls_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = True, url\n    else:\n        file_size = url_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = False, [url]\n\n    continue_renameing = True\n    while continue_renameing:\n        continue_renameing = False\n        if os.path.exists(filepath):\n            if not force and file_size == os.path.getsize(filepath):\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    log.w(\n                        \"Skipping {}: file already exists\".format(\n                            tr(os.path.basename(filepath))\n                        )\n                    )\n                else:\n                    if bar:\n                        bar.update_received(file_size)\n                return\n            else:\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    if not force and auto_rename:\n                        path, ext = os.path.basename(filepath).rsplit(\".\", 1)\n                        finder = re.compile(\" \\([1-9]\\d*?\\)$\")\n                        if finder.search(path) is None:\n                            thisfile = path + \" (1).\" + ext\n                        else:\n\n                            def numreturn(a):\n                                return \" (\" + str(int(a.group()[2:-1]) + 1) + \").\"\n\n                            thisfile = finder.sub(numreturn, path) + ext\n                        filepath = os.path.join(os.path.dirname(filepath), thisfile)\n                        print(\n                            \"Changing name to %s\" % tr(os.path.basename(filepath)),\n                            \"...\",\n                        )\n                        continue_renameing = True\n                        continue\n                    if log.yes_or_no(\"File with this name already exists. Overwrite?\"):\n                        log.w(\"Overwriting %s ...\" % tr(os.path.basename(filepath)))\n                    else:\n                        return\n        elif not os.path.exists(os.path.dirname(filepath)):\n            os.mkdir(os.path.dirname(filepath))\n\n    temp_filepath = filepath + \".download\" if file_size != float(\"inf\") else filepath\n    received = 0\n    if not force:\n        open_mode = \"ab\"\n\n        if os.path.exists(temp_filepath):\n            received += os.path.getsize(temp_filepath)\n            if bar:\n                bar.update_received(os.path.getsize(temp_filepath))\n    else:\n        open_mode = \"wb\"\n\n    for url in urls:\n        received_chunk = 0\n        if received < file_size:\n            if faker:\n                tmp_headers = fake_headers\n            \"\"\"\n            if parameter headers passed in, we have it copied as tmp_header\n            elif headers:\n                headers = headers\n            else:\n                headers = {}\n            \"\"\"\n            if received and not is_chunked:  # only request a range when not chunked\n                tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n            if refer:\n                tmp_headers[\"Referer\"] = refer\n\n            if timeout:\n                response = urlopen_with_retry(\n                    request.Request(url, headers=tmp_headers), timeout=timeout\n                )\n            else:\n                response = urlopen_with_retry(request.Request(url, headers=tmp_headers))\n            try:\n                range_start = int(\n                    response.headers[\"content-range\"][6:].split(\"/\")[0].split(\"-\")[0]\n                )\n                end_length = int(response.headers[\"content-range\"][6:].split(\"/\")[1])\n                range_length = end_length - range_start\n            except:\n                content_length = response.headers[\"content-length\"]\n                range_length = (\n                    int(content_length) if content_length is not None else float(\"inf\")\n                )\n\n            if is_chunked:  # always append if chunked\n                open_mode = \"ab\"\n            elif file_size != received + range_length:  # is it ever necessary?\n                received = 0\n                if bar:\n                    bar.received = 0\n                open_mode = \"wb\"\n\n            with open(temp_filepath, open_mode) as output:\n                while True:\n                    buffer = None\n                    try:\n                        buffer = response.read(1024 * 256)\n                    except socket.timeout:\n                        pass\n                    if not buffer:\n                        if is_chunked and received_chunk == range_length:\n                            break\n                        elif (\n                            not is_chunked and received == file_size\n                        ):  # Download finished\n                            break\n                        # Unexpected termination. Retry request\n                        if not is_chunked:  # when\n                            tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n                        response = urlopen_with_retry(\n                            request.Request(url, headers=tmp_headers)\n                        )\n                        continue\n                    output.write(buffer)\n                    received += len(buffer)\n                    received_chunk += len(buffer)\n                    if bar:\n                        bar.update_received(len(buffer))\n\n    assert received == os.path.getsize(temp_filepath), \"%s == %s == %s\" % (\n        received,\n        os.path.getsize(temp_filepath),\n        temp_filepath,\n    )\n\n    if os.access(filepath, os.W_OK):\n        # on Windows rename could fail if destination filepath exists\n        os.remove(filepath)\n    os.rename(temp_filepath, filepath)\n\n\nclass SimpleProgressBar:\n    term_size = term.get_terminal_size()[1]\n\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n        self.speed = \"\"\n        self.last_updated = time.time()\n\n        total_pieces_len = len(str(total_pieces))\n        # 38 is the size of all statically known size in self.bar\n        total_str = \"%5s\" % round(self.total_size / 1048576, 1)\n        total_str_width = max(len(total_str), 5)\n        self.bar_size = self.term_size - 28 - 2 * total_pieces_len - 2 * total_str_width\n        self.bar = \"{:>4}%% ({:>%s}/%sMB) \u251c{:\u2500<%s}\u2524[{:>%s}/{:>%s}] {}\" % (\n            total_str_width,\n            total_str,\n            self.bar_size,\n            total_pieces_len,\n            total_pieces_len,\n        )\n\n    def update(self):\n        self.displayed = True\n        bar_size = self.bar_size\n        percent = round(self.received * 100 / self.total_size, 1)\n        if percent >= 100:\n            percent = 100\n        dots = bar_size * int(percent) // 100\n        plus = int(percent) - dots // bar_size * 100\n        if plus > 0.8:\n            plus = \"\u2588\"\n        elif plus > 0.4:\n            plus = \">\"\n        else:\n            plus = \"\"\n        bar = \"\u2588\" * dots + plus\n        bar = self.bar.format(\n            percent,\n            round(self.received / 1048576, 1),\n            bar,\n            self.current_piece,\n            self.total_pieces,\n            self.speed,\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        time_diff = time.time() - self.last_updated\n        bytes_ps = n / time_diff if time_diff else 0\n        if bytes_ps >= 1024 ** 3:\n            self.speed = \"{:4.0f} GB/s\".format(bytes_ps / 1024 ** 3)\n        elif bytes_ps >= 1024 ** 2:\n            self.speed = \"{:4.0f} MB/s\".format(bytes_ps / 1024 ** 2)\n        elif bytes_ps >= 1024:\n            self.speed = \"{:4.0f} kB/s\".format(bytes_ps / 1024)\n        else:\n            self.speed = \"{:4.0f}  B/s\".format(bytes_ps)\n        self.last_updated = time.time()\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass PiecesProgressBar:\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n\n    def update(self):\n        self.displayed = True\n        bar = \"{0:>5}%[{1:<40}] {2}/{3}\".format(\n            \"\", \"=\" * 40, self.current_piece, self.total_pieces\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass DummyProgressBar:\n    def __init__(self, *args):\n        pass\n\n    def update_received(self, n):\n        pass\n\n    def update_piece(self, n):\n        pass\n\n    def done(self):\n        pass\n\n\ndef get_output_filename(urls, title, ext, output_dir, merge):\n    # lame hack for the --output-filename option\n    global output_filename\n    if output_filename:\n        if ext:\n            return output_filename + \".\" + ext\n        return output_filename\n\n    merged_ext = ext\n    if (len(urls) > 1) and merge:\n        from .processor.ffmpeg import has_ffmpeg_installed\n\n        if ext in [\"flv\", \"f4v\"]:\n            if has_ffmpeg_installed():\n                merged_ext = \"mp4\"\n            else:\n                merged_ext = \"flv\"\n        elif ext == \"mp4\":\n            merged_ext = \"mp4\"\n        elif ext == \"ts\":\n            if has_ffmpeg_installed():\n                merged_ext = \"mkv\"\n            else:\n                merged_ext = \"ts\"\n    return \"%s.%s\" % (title, merged_ext)\n\n\ndef print_user_agent(faker=False):\n    urllib_default_user_agent = \"Python-urllib/%d.%d\" % sys.version_info[:2]\n    user_agent = fake_headers[\"User-Agent\"] if faker else urllib_default_user_agent\n    print(\"User Agent: %s\" % user_agent)\n\n\ndef download_urls(\n    urls,\n    title,\n    ext,\n    total_size,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    headers={},\n    **kwargs\n):\n    assert urls\n    if json_output:\n        json_output_.download_urls(\n            urls=urls, title=title, ext=ext, total_size=total_size, refer=refer\n        )\n        return\n    if dry_run:\n        print_user_agent(faker=faker)\n        try:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join(urls))\n        except:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join([j for i in urls for j in i]))\n        return\n\n    if player:\n        launch_player(player, urls)\n        return\n\n    if not total_size:\n        try:\n            total_size = urls_size(urls, faker=faker, headers=headers)\n        except:\n            import traceback\n\n            traceback.print_exc(file=sys.stdout)\n            pass\n\n    title = tr(get_filename(title))\n    output_filename = get_output_filename(urls, title, ext, output_dir, merge)\n    output_filepath = os.path.join(output_dir, output_filename)\n\n    if total_size:\n        if (\n            not force\n            and os.path.exists(output_filepath)\n            and not auto_rename\n            and os.path.getsize(output_filepath) >= total_size * 0.9\n        ):\n            log.w(\"Skipping %s: file already exists\" % output_filepath)\n            print()\n            return\n        bar = SimpleProgressBar(total_size, len(urls))\n    else:\n        bar = PiecesProgressBar(total_size, len(urls))\n\n    if len(urls) == 1:\n        url = urls[0]\n        print(\"Downloading %s ...\" % tr(output_filename))\n        bar.update()\n        url_save(\n            url,\n            output_filepath,\n            bar,\n            refer=refer,\n            faker=faker,\n            headers=headers,\n            **kwargs\n        )\n        bar.done()\n    else:\n        parts = []\n        print(\"Downloading %s.%s ...\" % (tr(title), ext))\n        bar.update()\n        for i, url in enumerate(urls):\n            filename = \"%s[%02d].%s\" % (title, i, ext)\n            filepath = os.path.join(output_dir, filename)\n            parts.append(filepath)\n            # print 'Downloading %s [%s/%s]...' % (tr(filename), i + 1, len(urls))\n            bar.update_piece(i + 1)\n            url_save(\n                url,\n                filepath,\n                bar,\n                refer=refer,\n                is_part=True,\n                faker=faker,\n                headers=headers,\n                **kwargs\n            )\n        bar.done()\n\n        if not merge:\n            print()\n            return\n\n        if \"av\" in kwargs and kwargs[\"av\"]:\n            from .processor.ffmpeg import has_ffmpeg_installed\n\n            if has_ffmpeg_installed():\n                from .processor.ffmpeg import ffmpeg_concat_av\n\n                ret = ffmpeg_concat_av(parts, output_filepath, ext)\n                print(\"Merged into %s\" % output_filename)\n                if ret == 0:\n                    for part in parts:\n                        os.remove(part)\n\n        elif ext in [\"flv\", \"f4v\"]:\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_flv_to_mp4\n\n                    ffmpeg_concat_flv_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_flv import concat_flv\n\n                    concat_flv(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"mp4\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_mp4_to_mp4\n\n                    ffmpeg_concat_mp4_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_mp4 import concat_mp4\n\n                    concat_mp4(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"ts\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_ts_to_mkv\n\n                    ffmpeg_concat_ts_to_mkv(parts, output_filepath)\n                else:\n                    from .processor.join_ts import concat_ts\n\n                    concat_ts(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        else:\n            print(\"Can't merge %s files\" % ext)\n\n    print()\n\n\ndef download_rtmp_url(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset -> False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        from .processor.rtmpdump import play_rtmpdump_stream\n\n        play_rtmpdump_stream(player, url, params)\n        return\n\n    from .processor.rtmpdump import has_rtmpdump_installed, download_rtmpdump_stream\n\n    assert has_rtmpdump_installed(), \"RTMPDump not installed.\"\n    download_rtmpdump_stream(url, title, ext, params, output_dir)\n\n\ndef download_url_ffmpeg(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    stream=True,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset ->False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        launch_player(player, [url])\n        return\n\n    from .processor.ffmpeg import has_ffmpeg_installed, ffmpeg_download_stream\n\n    assert has_ffmpeg_installed(), \"FFmpeg not installed.\"\n\n    global output_filename\n    if output_filename:\n        dotPos = output_filename.rfind(\".\")\n        if dotPos > 0:\n            title = output_filename[:dotPos]\n            ext = output_filename[dotPos + 1 :]\n        else:\n            title = output_filename\n\n    title = tr(get_filename(title))\n\n    ffmpeg_download_stream(url, title, ext, params, output_dir, stream=stream)\n\n\ndef playlist_not_supported(name):\n    def f(*args, **kwargs):\n        raise NotImplementedError(\"Playlist is not supported for \" + name)\n\n    return f\n\n\ndef print_info(site_info, title, type, size, **kwargs):\n    if json_output:\n        json_output_.print_info(site_info=site_info, title=title, type=type, size=size)\n        return\n    if type:\n        type = type.lower()\n    if type in [\"3gp\"]:\n        type = \"video/3gpp\"\n    elif type in [\"asf\", \"wmv\"]:\n        type = \"video/x-ms-asf\"\n    elif type in [\"flv\", \"f4v\"]:\n        type = \"video/x-flv\"\n    elif type in [\"mkv\"]:\n        type = \"video/x-matroska\"\n    elif type in [\"mp3\"]:\n        type = \"audio/mpeg\"\n    elif type in [\"mp4\"]:\n        type = \"video/mp4\"\n    elif type in [\"mov\"]:\n        type = \"video/quicktime\"\n    elif type in [\"ts\"]:\n        type = \"video/MP2T\"\n    elif type in [\"webm\"]:\n        type = \"video/webm\"\n\n    elif type in [\"jpg\"]:\n        type = \"image/jpeg\"\n    elif type in [\"png\"]:\n        type = \"image/png\"\n    elif type in [\"gif\"]:\n        type = \"image/gif\"\n\n    if type in [\"video/3gpp\"]:\n        type_info = \"3GPP multimedia file (%s)\" % type\n    elif type in [\"video/x-flv\", \"video/f4v\"]:\n        type_info = \"Flash video (%s)\" % type\n    elif type in [\"video/mp4\", \"video/x-m4v\"]:\n        type_info = \"MPEG-4 video (%s)\" % type\n    elif type in [\"video/MP2T\"]:\n        type_info = \"MPEG-2 transport stream (%s)\" % type\n    elif type in [\"video/webm\"]:\n        type_info = \"WebM video (%s)\" % type\n    # elif type in ['video/ogg']:\n    #    type_info = 'Ogg video (%s)' % type\n    elif type in [\"video/quicktime\"]:\n        type_info = \"QuickTime video (%s)\" % type\n    elif type in [\"video/x-matroska\"]:\n        type_info = \"Matroska video (%s)\" % type\n    # elif type in ['video/x-ms-wmv']:\n    #    type_info = 'Windows Media video (%s)' % type\n    elif type in [\"video/x-ms-asf\"]:\n        type_info = \"Advanced Systems Format (%s)\" % type\n    # elif type in ['video/mpeg']:\n    #    type_info = 'MPEG video (%s)' % type\n    elif type in [\"audio/mp4\", \"audio/m4a\"]:\n        type_info = \"MPEG-4 audio (%s)\" % type\n    elif type in [\"audio/mpeg\"]:\n        type_info = \"MP3 (%s)\" % type\n    elif type in [\"audio/wav\", \"audio/wave\", \"audio/x-wav\"]:\n        type_info = \"Waveform Audio File Format ({})\".format(type)\n\n    elif type in [\"image/jpeg\"]:\n        type_info = \"JPEG Image (%s)\" % type\n    elif type in [\"image/png\"]:\n        type_info = \"Portable Network Graphics (%s)\" % type\n    elif type in [\"image/gif\"]:\n        type_info = \"Graphics Interchange Format (%s)\" % type\n    elif type in [\"m3u8\"]:\n        if \"m3u8_type\" in kwargs:\n            if kwargs[\"m3u8_type\"] == \"master\":\n                type_info = \"M3U8 Master {}\".format(type)\n        else:\n            type_info = \"M3U8 Playlist {}\".format(type)\n    else:\n        type_info = \"Unknown type (%s)\" % type\n\n    maybe_print(\"Site:      \", site_info)\n    maybe_print(\"Title:     \", unescape_html(tr(title)))\n    print(\"Type:      \", type_info)\n    if type != \"m3u8\":\n        print(\"Size:      \", round(size / 1048576, 2), \"MiB (\" + str(size) + \" Bytes)\")\n    if type == \"m3u8\" and \"m3u8_url\" in kwargs:\n        print(\"M3U8 Url:   {}\".format(kwargs[\"m3u8_url\"]))\n    print()\n\n\ndef mime_to_container(mime):\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/mp4\": \"mp4\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n    }\n    if mime in mapping:\n        return mapping[mime]\n    else:\n        return mime.split(\"/\")[1]\n\n\ndef parse_host(host):\n    \"\"\"Parses host name and port number from a string.\"\"\"\n    if re.match(r\"^(\\d+)$\", host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r\"^(\\w+)://\", host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)\n\n\ndef set_proxy(proxy):\n    proxy_handler = request.ProxyHandler(\n        {\n            \"http\": \"%s:%s\" % proxy,\n            \"https\": \"%s:%s\" % proxy,\n        }\n    )\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\ndef unset_proxy():\n    proxy_handler = request.ProxyHandler({})\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\n# DEPRECATED in favor of set_proxy() and unset_proxy()\ndef set_http_proxy(proxy):\n    if proxy is None:  # Use system default setting\n        proxy_support = request.ProxyHandler()\n    elif proxy == \"\":  # Don't use any proxy\n        proxy_support = request.ProxyHandler({})\n    else:  # Use proxy\n        proxy_support = request.ProxyHandler(\n            {\"http\": \"%s\" % proxy, \"https\": \"%s\" % proxy}\n        )\n    opener = request.build_opener(proxy_support)\n    request.install_opener(opener)\n\n\ndef print_more_compatible(*args, **kwargs):\n    import builtins as __builtin__\n\n    \"\"\"Overload default print function as py (<3.3) does not support 'flush' keyword.\n    Although the function name can be same as print to get itself overloaded automatically,\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n    \"\"\"\n    # nothing happens on py3.3 and later\n    if sys.version_info[:2] >= (3, 3):\n        return __builtin__.print(*args, **kwargs)\n\n    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested\n    doFlush = kwargs.pop(\"flush\", False)\n    ret = __builtin__.print(*args, **kwargs)\n    if doFlush:\n        kwargs.get(\"file\", sys.stdout).flush()\n    return ret\n\n\ndef download_main(download, download_playlist, urls, playlist, **kwargs):\n    for url in urls:\n        if re.match(r\"https?://\", url) is None:\n            url = \"http://\" + url\n\n        if playlist:\n            download_playlist(url, **kwargs)\n        else:\n            download(url, **kwargs)\n\n\ndef load_cookies(cookiefile):\n    global cookies\n    if cookiefile.endswith(\".txt\"):\n        # MozillaCookieJar treats prefix '#HttpOnly_' as comments incorrectly!\n        # do not use its load()\n        # see also:\n        #   - https://docs.python.org/3/library/http.cookiejar.html#http.cookiejar.MozillaCookieJar\n        #   - https://github.com/python/cpython/blob/4b219ce/Lib/http/cookiejar.py#L2014\n        #   - https://curl.haxx.se/libcurl/c/CURLOPT_COOKIELIST.html#EXAMPLE\n        # cookies = cookiejar.MozillaCookieJar(cookiefile)\n        # cookies.load()\n        from http.cookiejar import Cookie\n\n        cookies = cookiejar.MozillaCookieJar()\n        now = time.time()\n        ignore_discard, ignore_expires = False, False\n        with open(cookiefile, \"r\") as f:\n            for line in f:\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"):\n                    line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if line.strip().startswith((\"#\", \"$\")) or line.strip() == \"\":\n                    if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                        continue\n\n                (\n                    domain,\n                    domain_specified,\n                    path,\n                    secure,\n                    expires,\n                    name,\n                    value,\n                ) = line.split(\"\\t\")\n                secure = secure == \"TRUE\"\n                domain_specified = domain_specified == \"TRUE\"\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas http.cookiejar regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                    assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(\n                    0,\n                    name,\n                    value,\n                    None,\n                    False,\n                    domain,\n                    domain_specified,\n                    initial_dot,\n                    path,\n                    False,\n                    secure,\n                    expires,\n                    discard,\n                    None,\n                    None,\n                    {},\n                )\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                cookies.set_cookie(c)\n\n    elif cookiefile.endswith((\".sqlite\", \".sqlite3\")):\n        import sqlite3, shutil, tempfile\n\n        temp_dir = tempfile.gettempdir()\n        temp_cookiefile = os.path.join(temp_dir, \"temp_cookiefile.sqlite\")\n        shutil.copy2(cookiefile, temp_cookiefile)\n\n        cookies = cookiejar.MozillaCookieJar()\n        con = sqlite3.connect(temp_cookiefile)\n        cur = con.cursor()\n        cur.execute(\n            \"\"\"SELECT host, path, isSecure, expiry, name, value\n        FROM moz_cookies\"\"\"\n        )\n        for item in cur.fetchall():\n            c = cookiejar.Cookie(\n                0,\n                item[4],\n                item[5],\n                None,\n                False,\n                item[0],\n                item[0].startswith(\".\"),\n                item[0].startswith(\".\"),\n                item[1],\n                False,\n                item[2],\n                item[3],\n                item[3] == \"\",\n                None,\n                None,\n                {},\n            )\n            cookies.set_cookie(c)\n\n    else:\n        log.e(\"[error] unsupported cookies format\")\n        # TODO: Chromium Cookies\n        # SELECT host_key, path, secure, expires_utc, name, encrypted_value\n        # FROM cookies\n        # http://n8henrie.com/2013/11/use-chromes-cookies-for-easier-downloading-with-python-requests/\n\n\ndef set_socks_proxy(proxy):\n    try:\n        import socks\n\n        socks_proxy_addrs = proxy.split(\":\")\n        socks.set_default_proxy(\n            socks.SOCKS5, socks_proxy_addrs[0], int(socks_proxy_addrs[1])\n        )\n        socket.socket = socks.socksocket\n\n        def getaddrinfo(*args):\n            return [(socket.AF_INET, socket.SOCK_STREAM, 6, \"\", (args[0], args[1]))]\n\n        socket.getaddrinfo = getaddrinfo\n    except ImportError:\n        log.w(\n            \"Error importing PySocks library, socks proxy ignored.\"\n            \"In order to use use socks proxy, please install PySocks.\"\n        )\n\n\ndef script_main(download, download_playlist, **kwargs):\n    logging.basicConfig(format=\"[%(levelname)s] %(message)s\")\n\n    def print_version():\n        version = get_version(\n            kwargs[\"repo_path\"] if \"repo_path\" in kwargs else __version__\n        )\n        log.i(\"version {}, a tiny downloader that scrapes the web.\".format(version))\n\n    parser = argparse.ArgumentParser(\n        prog=\"you-get\",\n        usage=\"you-get [OPTION]... URL...\",\n        description=\"A tiny downloader that scrapes the web\",\n        add_help=False,\n    )\n    parser.add_argument(\n        \"-V\", \"--version\", action=\"store_true\", help=\"Print version and exit\"\n    )\n    parser.add_argument(\n        \"-h\", \"--help\", action=\"store_true\", help=\"Print this help message and exit\"\n    )\n\n    dry_run_grp = parser.add_argument_group(\n        \"Dry-run options\", \"(no actual downloading)\"\n    )\n    dry_run_grp = dry_run_grp.add_mutually_exclusive_group()\n    dry_run_grp.add_argument(\n        \"-i\", \"--info\", action=\"store_true\", help=\"Print extracted information\"\n    )\n    dry_run_grp.add_argument(\n        \"-u\", \"--url\", action=\"store_true\", help=\"Print extracted information with URLs\"\n    )\n    dry_run_grp.add_argument(\n        \"--json\", action=\"store_true\", help=\"Print extracted URLs in JSON format\"\n    )\n\n    download_grp = parser.add_argument_group(\"Download options\")\n    download_grp.add_argument(\n        \"-n\",\n        \"--no-merge\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not merge video parts\",\n    )\n    download_grp.add_argument(\n        \"--no-caption\",\n        action=\"store_true\",\n        help=\"Do not download captions (subtitles, lyrics, danmaku, ...)\",\n    )\n    download_grp.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force overwriting existing files\",\n    )\n    download_grp.add_argument(\n        \"-F\", \"--format\", metavar=\"STREAM_ID\", help=\"Set video format to STREAM_ID\"\n    )\n    download_grp.add_argument(\n        \"-O\", \"--output-filename\", metavar=\"FILE\", help=\"Set output filename\"\n    )\n    download_grp.add_argument(\n        \"-o\", \"--output-dir\", metavar=\"DIR\", default=\".\", help=\"Set output directory\"\n    )\n    download_grp.add_argument(\n        \"-p\", \"--player\", metavar=\"PLAYER\", help=\"Stream extracted URL to a PLAYER\"\n    )\n    download_grp.add_argument(\n        \"-c\",\n        \"--cookies\",\n        metavar=\"COOKIES_FILE\",\n        help=\"Load cookies.txt or cookies.sqlite\",\n    )\n    download_grp.add_argument(\n        \"-t\",\n        \"--timeout\",\n        metavar=\"SECONDS\",\n        type=int,\n        default=600,\n        help=\"Set socket timeout\",\n    )\n    download_grp.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show traceback and other debug info\"\n    )\n    download_grp.add_argument(\n        \"-I\",\n        \"--input-file\",\n        metavar=\"FILE\",\n        type=argparse.FileType(\"r\"),\n        help=\"Read non-playlist URLs from FILE\",\n    )\n    download_grp.add_argument(\n        \"-P\", \"--password\", help=\"Set video visit password to PASSWORD\"\n    )\n    download_grp.add_argument(\n        \"-l\", \"--playlist\", action=\"store_true\", help=\"Prefer to download a playlist\"\n    )\n    download_grp.add_argument(\n        \"-a\",\n        \"--auto-rename\",\n        action=\"store_true\",\n        default=False,\n        help=\"Auto rename same name different files\",\n    )\n\n    download_grp.add_argument(\n        \"-k\", \"--insecure\", action=\"store_true\", default=False, help=\"ignore ssl errors\"\n    )\n\n    proxy_grp = parser.add_argument_group(\"Proxy options\")\n    proxy_grp = proxy_grp.add_mutually_exclusive_group()\n    proxy_grp.add_argument(\n        \"-x\",\n        \"--http-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for downloading\",\n    )\n    proxy_grp.add_argument(\n        \"-y\",\n        \"--extractor-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for extracting only\",\n    )\n    proxy_grp.add_argument(\"--no-proxy\", action=\"store_true\", help=\"Never use a proxy\")\n    proxy_grp.add_argument(\n        \"-s\",\n        \"--socks-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an SOCKS5 proxy for downloading\",\n    )\n\n    download_grp.add_argument(\"--stream\", help=argparse.SUPPRESS)\n    download_grp.add_argument(\"--itag\", help=argparse.SUPPRESS)\n\n    parser.add_argument(\"URL\", nargs=\"*\", help=argparse.SUPPRESS)\n\n    args = parser.parse_args()\n\n    if args.help:\n        print_version()\n        parser.print_help()\n        sys.exit()\n    if args.version:\n        print_version()\n        sys.exit()\n\n    if args.debug:\n        # Set level of root logger to DEBUG\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    global force\n    global dry_run\n    global json_output\n    global player\n    global extractor_proxy\n    global output_filename\n    global auto_rename\n    global insecure\n    output_filename = args.output_filename\n    extractor_proxy = args.extractor_proxy\n\n    info_only = args.info\n    if args.force:\n        force = True\n    if args.auto_rename:\n        auto_rename = True\n    if args.url:\n        dry_run = True\n    if args.json:\n        json_output = True\n        # to fix extractors not use VideoExtractor\n        dry_run = True\n        info_only = False\n\n    if args.cookies:\n        load_cookies(args.cookies)\n\n    caption = True\n    stream_id = args.format or args.stream or args.itag\n    if args.no_caption:\n        caption = False\n    if args.player:\n        player = args.player\n        caption = False\n\n    if args.insecure:\n        # ignore ssl\n        insecure = True\n\n    if args.no_proxy:\n        set_http_proxy(\"\")\n    else:\n        set_http_proxy(args.http_proxy)\n    if args.socks_proxy:\n        set_socks_proxy(args.socks_proxy)\n\n    URLs = []\n    if args.input_file:\n        logging.debug(\"you are trying to load urls from %s\", args.input_file)\n        if args.playlist:\n            log.e(\n                \"reading playlist from a file is unsupported \"\n                \"and won't make your life easier\"\n            )\n            sys.exit(2)\n        URLs.extend(args.input_file.read().splitlines())\n        args.input_file.close()\n    URLs.extend(args.URL)\n\n    if not URLs:\n        parser.print_help()\n        sys.exit()\n\n    socket.setdefaulttimeout(args.timeout)\n\n    try:\n        extra = {}\n        if extractor_proxy:\n            extra[\"extractor_proxy\"] = extractor_proxy\n        if stream_id:\n            extra[\"stream_id\"] = stream_id\n        download_main(\n            download,\n            download_playlist,\n            URLs,\n            args.playlist,\n            output_dir=args.output_dir,\n            merge=not args.no_merge,\n            info_only=info_only,\n            json_output=json_output,\n            caption=caption,\n            password=args.password,\n            **extra\n        )\n    except KeyboardInterrupt:\n        if args.debug:\n            raise\n        else:\n            sys.exit(1)\n    except UnicodeEncodeError:\n        if args.debug:\n            raise\n        log.e(\n            \"[error] oops, the current environment does not seem to support \" \"Unicode.\"\n        )\n        log.e(\"please set it to a UTF-8-aware locale first,\")\n        log.e(\"so as to save the video (with some Unicode characters) correctly.\")\n        log.e(\"you can do it like this:\")\n        log.e(\"    (Windows)    % chcp 65001 \")\n        log.e(\"    (Linux)      $ LC_CTYPE=en_US.UTF-8\")\n        sys.exit(1)\n    except Exception:\n        if not args.debug:\n            log.e(\"[error] oops, something went wrong.\")\n            log.e(\"don't panic, c'est la vie. please try the following steps:\")\n            log.e(\"  (1) Rule out any network problem.\")\n            log.e(\"  (2) Make sure you-get is up-to-date.\")\n            log.e(\"  (3) Check if the issue is already known, on\")\n            log.e(\"        https://github.com/soimort/you-get/wiki/Known-Bugs\")\n            log.e(\"        https://github.com/soimort/you-get/issues\")\n            log.e(\"  (4) Run the command with '--debug' option,\")\n            log.e(\"      and report this issue with the full output.\")\n        else:\n            print_version()\n            log.i(args)\n            raise\n        sys.exit(1)\n\n\ndef google_search(url):\n    keywords = r1(r\"https?://(.*)\", url)\n    url = \"https://www.google.com/search?tbm=vid&q=%s\" % parse.quote(keywords)\n    page = get_content(url, headers=fake_headers)\n    videos = re.findall(\n        r'<a href=\"(https?://[^\"]+)\" onmousedown=\"[^\"]+\"><h3 class=\"[^\"]*\">([^<]+)<',\n        page,\n    )\n    vdurs = re.findall(r'<span class=\"vdur[^\"]*\">([^<]+)<', page)\n    durs = [r1(r\"(\\d+:\\d+)\", unescape_html(dur)) for dur in vdurs]\n    print(\"Google Videos search:\")\n    for v in zip(videos, durs):\n        print(\"- video:  {} [{}]\".format(unescape_html(v[0][1]), v[1] if v[1] else \"?\"))\n        print(\"# you-get %s\" % log.sprint(v[0][0], log.UNDERLINE))\n        print()\n    print(\"Best matched result:\")\n    return videos[0][0]\n\n\ndef url_to_module(url):\n    try:\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n        assert video_host and video_url\n    except AssertionError:\n        url = google_search(url)\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    if video_host.endswith(\".com.cn\") or video_host.endswith(\".ac.cn\"):\n        video_host = video_host[:-3]\n    domain = r1(r\"(\\.[^.]+\\.[^.]+)$\", video_host) or video_host\n    assert domain, \"unsupported url: \" + url\n\n    # all non-ASCII code points must be quoted (percent-encoded UTF-8)\n    url = \"\".join([ch if ord(ch) in range(128) else parse.quote(ch) for ch in url])\n    video_host = r1(r\"https?://([^/]+)/\", url)\n    video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    k = r1(r\"([^.]+)\", domain)\n    if k in SITES:\n        return (import_module(\".\".join([\"you_get\", \"extractors\", SITES[k]])), url)\n    else:\n        try:\n            location = get_location(url)  # t.co isn't happy with fake_headers\n        except:\n            location = get_location(url, headers=fake_headers)\n\n        if location and location != url and not location.startswith(\"/\"):\n            return url_to_module(location)\n        else:\n            return import_module(\"you_get.extractors.universal\"), url\n\n\ndef any_download(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download(url, **kwargs)\n\n\ndef any_download_playlist(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download_playlist(url, **kwargs)\n\n\ndef main(**kwargs):\n    script_main(any_download, any_download_playlist, **kwargs)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "package": ["import io", "import os", "import re", "import sys", "import time", "import json", "import socket", "import locale", "import logging", "import argparse", "import ssl", "from http import cookiejar", "from importlib import import_module", "from urllib import request, parse, error", "from .version import __version__", "from .util import log, term", "from .util.git import get_version", "from .util.strings import get_filename, unescape_html", "from . import json_output as json_output_", "import subprocess", "import shlex", "import shutil", "from io import BytesIO", "import gzip", "import zlib"], "function": ["def rc4(key, data):\n", "def general_m3u8_extractor(url, headers={}):\n", "def maybe_print(*s):\n", "def tr(s):\n", "def r1(pattern, text):\n", "def r1_of(patterns, text):\n", "def match1(text, *patterns):\n", "def matchall(text, patterns):\n", "def launch_player(player, urls):\n", "def parse_query_param(url, param):\n", "def unicodize(text):\n", "def escape_file_path(path):\n", "def ungzip(data):\n", "def undeflate(data):\n", "def get_response(url, faker=False):\n", "def get_html(url, encoding=None, faker=False):\n", "def get_decoded_html(url, faker=False):\n", "def get_location(url, headers=None, get_method=\"HEAD\"):\n", "def urlopen_with_retry(*args, **kwargs):\n", "def get_content(url, headers={}, decoded=True):\n", "def url_size(url, faker=False, headers={}):\n", "def urls_size(urls, faker=False, headers={}):\n", "def get_head(url, headers=None, get_method=\"HEAD\"):\n", "def url_info(url, faker=False, headers={}):\n", "def url_locations(urls, faker=False, headers={}):\n", "class SimpleProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class PiecesProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class DummyProgressBar:\n", "    def __init__(self, *args):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "def get_output_filename(urls, title, ext, output_dir, merge):\n", "def print_user_agent(faker=False):\n", "def playlist_not_supported(name):\n", "    def f(*args, **kwargs):\n", "def print_info(site_info, title, type, size, **kwargs):\n", "def mime_to_container(mime):\n", "def parse_host(host):\n", "def set_proxy(proxy):\n", "def unset_proxy():\n", "def set_http_proxy(proxy):\n", "def print_more_compatible(*args, **kwargs):\n", "def download_main(download, download_playlist, urls, playlist, **kwargs):\n", "def load_cookies(cookiefile):\n", "def set_socks_proxy(proxy):\n", "def script_main(download, download_playlist, **kwargs):\n", "    def print_version():\n", "def google_search(url):\n", "def url_to_module(url):\n", "def any_download(url, **kwargs):\n", "def any_download_playlist(url, **kwargs):\n", "def main(**kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "parse_host", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Parses host name and port number from a string.", "docstring_tokens": ["Parses", "host", "name", "and", "port", "number", "from", "a", "string", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1216-L1226", "partition": "test", "up_fun_num": 50, "context": "#!/usr/bin/env python\n\nimport io\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport socket\nimport locale\nimport logging\nimport argparse\nimport ssl\nfrom http import cookiejar\nfrom importlib import import_module\nfrom urllib import request, parse, error\n\nfrom .version import __version__\nfrom .util import log, term\nfrom .util.git import get_version\nfrom .util.strings import get_filename, unescape_html\nfrom . import json_output as json_output_\n\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding=\"utf8\")\n\nSITES = {\n    \"163\": \"netease\",\n    \"56\": \"w56\",\n    \"365yg\": \"toutiao\",\n    \"acfun\": \"acfun\",\n    \"archive\": \"archive\",\n    \"baidu\": \"baidu\",\n    \"bandcamp\": \"bandcamp\",\n    \"baomihua\": \"baomihua\",\n    \"bigthink\": \"bigthink\",\n    \"bilibili\": \"bilibili\",\n    \"cctv\": \"cntv\",\n    \"cntv\": \"cntv\",\n    \"cbs\": \"cbs\",\n    \"coub\": \"coub\",\n    \"dailymotion\": \"dailymotion\",\n    \"douban\": \"douban\",\n    \"douyin\": \"douyin\",\n    \"douyu\": \"douyutv\",\n    \"ehow\": \"ehow\",\n    \"facebook\": \"facebook\",\n    \"fc2\": \"fc2video\",\n    \"flickr\": \"flickr\",\n    \"freesound\": \"freesound\",\n    \"fun\": \"funshion\",\n    \"google\": \"google\",\n    \"giphy\": \"giphy\",\n    \"heavy-music\": \"heavymusic\",\n    \"huomao\": \"huomaotv\",\n    \"iask\": \"sina\",\n    \"icourses\": \"icourses\",\n    \"ifeng\": \"ifeng\",\n    \"imgur\": \"imgur\",\n    \"in\": \"alive\",\n    \"infoq\": \"infoq\",\n    \"instagram\": \"instagram\",\n    \"interest\": \"interest\",\n    \"iqilu\": \"iqilu\",\n    \"iqiyi\": \"iqiyi\",\n    \"ixigua\": \"ixigua\",\n    \"isuntv\": \"suntv\",\n    \"iwara\": \"iwara\",\n    \"joy\": \"joy\",\n    \"kankanews\": \"bilibili\",\n    \"khanacademy\": \"khan\",\n    \"ku6\": \"ku6\",\n    \"kuaishou\": \"kuaishou\",\n    \"kugou\": \"kugou\",\n    \"kuwo\": \"kuwo\",\n    \"le\": \"le\",\n    \"letv\": \"le\",\n    \"lizhi\": \"lizhi\",\n    \"longzhu\": \"longzhu\",\n    \"magisto\": \"magisto\",\n    \"metacafe\": \"metacafe\",\n    \"mgtv\": \"mgtv\",\n    \"miomio\": \"miomio\",\n    \"mixcloud\": \"mixcloud\",\n    \"mtv81\": \"mtv81\",\n    \"musicplayon\": \"musicplayon\",\n    \"miaopai\": \"yixia\",\n    \"naver\": \"naver\",\n    \"7gogo\": \"nanagogo\",\n    \"nicovideo\": \"nicovideo\",\n    \"panda\": \"panda\",\n    \"pinterest\": \"pinterest\",\n    \"pixnet\": \"pixnet\",\n    \"pptv\": \"pptv\",\n    \"qingting\": \"qingting\",\n    \"qq\": \"qq\",\n    \"showroom-live\": \"showroom\",\n    \"sina\": \"sina\",\n    \"smgbb\": \"bilibili\",\n    \"sohu\": \"sohu\",\n    \"soundcloud\": \"soundcloud\",\n    \"ted\": \"ted\",\n    \"theplatform\": \"theplatform\",\n    \"tiktok\": \"tiktok\",\n    \"tucao\": \"tucao\",\n    \"tudou\": \"tudou\",\n    \"tumblr\": \"tumblr\",\n    \"twimg\": \"twitter\",\n    \"twitter\": \"twitter\",\n    \"ucas\": \"ucas\",\n    \"videomega\": \"videomega\",\n    \"vidto\": \"vidto\",\n    \"vimeo\": \"vimeo\",\n    \"wanmen\": \"wanmen\",\n    \"weibo\": \"miaopai\",\n    \"veoh\": \"veoh\",\n    \"vine\": \"vine\",\n    \"vk\": \"vk\",\n    \"xiami\": \"xiami\",\n    \"xiaokaxiu\": \"yixia\",\n    \"xiaojiadianvideo\": \"fc2video\",\n    \"ximalaya\": \"ximalaya\",\n    \"yinyuetai\": \"yinyuetai\",\n    \"yizhibo\": \"yizhibo\",\n    \"youku\": \"youku\",\n    \"youtu\": \"youtube\",\n    \"youtube\": \"youtube\",\n    \"zhanqi\": \"zhanqi\",\n    \"zhibo\": \"zhibo\",\n    \"zhihu\": \"zhihu\",\n}\n\ndry_run = False\njson_output = False\nforce = False\nplayer = None\nextractor_proxy = None\ncookies = None\noutput_filename = None\nauto_rename = False\ninsecure = False\n\nfake_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",  # noqa\n    \"Accept-Charset\": \"UTF-8,*;q=0.5\",\n    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n    \"Accept-Language\": \"en-US,en;q=0.8\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0\",  # noqa\n}\n\nif sys.stdout.isatty():\n    default_encoding = sys.stdout.encoding.lower()\nelse:\n    default_encoding = locale.getpreferredencoding().lower()\n\n\ndef rc4(key, data):\n    # all encryption algo should work on bytes\n    assert type(key) == type(data) and type(key) == type(b\"\")\n    state = list(range(256))\n    j = 0\n    for i in range(256):\n        j += state[i] + key[i % len(key)]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n\n    i = 0\n    j = 0\n    out_list = []\n    for char in data:\n        i += 1\n        i &= 0xFF\n        j += state[i]\n        j &= 0xFF\n        state[i], state[j] = state[j], state[i]\n        prn = state[(state[i] + state[j]) & 0xFF]\n        out_list.append(char ^ prn)\n\n    return bytes(out_list)\n\n\ndef general_m3u8_extractor(url, headers={}):\n    m3u8_list = get_content(url, headers=headers).split(\"\\n\")\n    urls = []\n    for line in m3u8_list:\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            if line.startswith(\"http\"):\n                urls.append(line)\n            else:\n                seg_url = parse.urljoin(url, line)\n                urls.append(seg_url)\n    return urls\n\n\ndef maybe_print(*s):\n    try:\n        print(*s)\n    except:\n        pass\n\n\ndef tr(s):\n    if default_encoding == \"utf-8\":\n        return s\n    else:\n        return s\n        # return str(s.encode('utf-8'))[2:-1]\n\n\n# DEPRECATED in favor of match1()\ndef r1(pattern, text):\n    m = re.search(pattern, text)\n    if m:\n        return m.group(1)\n\n\n# DEPRECATED in favor of match1()\ndef r1_of(patterns, text):\n    for p in patterns:\n        x = r1(p, text)\n        if x:\n            return x\n\n\ndef match1(text, *patterns):\n    \"\"\"Scans through a string for substrings matched some patterns (first-subgroups only).\n\n    Args:\n        text: A string to be scanned.\n        patterns: Arbitrary number of regex patterns.\n\n    Returns:\n        When only one pattern is given, returns a string (None if no match found).\n        When more than one pattern are given, returns a list of strings ([] if no match found).\n    \"\"\"\n\n    if len(patterns) == 1:\n        pattern = patterns[0]\n        match = re.search(pattern, text)\n        if match:\n            return match.group(1)\n        else:\n            return None\n    else:\n        ret = []\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                ret.append(match.group(1))\n        return ret\n\n\ndef matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret\n\n\ndef launch_player(player, urls):\n    import subprocess\n    import shlex\n\n    if sys.version_info >= (3, 3):\n        import shutil\n\n        exefile = shlex.split(player)[0]\n        if shutil.which(exefile) is not None:\n            subprocess.call(shlex.split(player) + list(urls))\n        else:\n            log.wtf('[Failed] Cannot find player \"%s\"' % exefile)\n    else:\n        subprocess.call(shlex.split(player) + list(urls))\n\n\ndef parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None\n\n\ndef unicodize(text):\n    return re.sub(\n        r\"\\\\u([0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f][0-9A-Fa-f])\",\n        lambda x: chr(int(x.group(0)[2:], 16)),\n        text,\n    )\n\n\n# DEPRECATED in favor of util.legitimize()\ndef escape_file_path(path):\n    path = path.replace(\"/\", \"-\")\n    path = path.replace(\"\\\\\", \"-\")\n    path = path.replace(\"*\", \"-\")\n    path = path.replace(\"?\", \"-\")\n    return path\n\n\ndef ungzip(data):\n    \"\"\"Decompresses data for Content-Encoding: gzip.\"\"\"\n    from io import BytesIO\n    import gzip\n\n    buffer = BytesIO(data)\n    f = gzip.GzipFile(fileobj=buffer)\n    return f.read()\n\n\ndef undeflate(data):\n    \"\"\"Decompresses data for Content-Encoding: deflate.\n    (the zlib compression is used.)\n    \"\"\"\n    import zlib\n\n    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)\n    return decompressobj.decompress(data) + decompressobj.flush()\n\n\n# DEPRECATED in favor of get_content()\ndef get_response(url, faker=False):\n    logging.debug(\"get_response: %s\" % url)\n\n    # install cookies\n    if cookies:\n        opener = request.build_opener(request.HTTPCookieProcessor(cookies))\n        request.install_opener(opener)\n\n    if faker:\n        response = request.urlopen(request.Request(url, headers=fake_headers), None)\n    else:\n        response = request.urlopen(url)\n\n    data = response.read()\n    if response.info().get(\"Content-Encoding\") == \"gzip\":\n        data = ungzip(data)\n    elif response.info().get(\"Content-Encoding\") == \"deflate\":\n        data = undeflate(data)\n    response.data = data\n    return response\n\n\n# DEPRECATED in favor of get_content()\ndef get_html(url, encoding=None, faker=False):\n    content = get_response(url, faker).data\n    return str(content, \"utf-8\", \"ignore\")\n\n\n# DEPRECATED in favor of get_content()\ndef get_decoded_html(url, faker=False):\n    response = get_response(url, faker)\n    data = response.data\n    charset = r1(r\"charset=([\\w-]+)\", response.headers[\"content-type\"])\n    if charset:\n        return data.decode(charset, \"ignore\")\n    else:\n        return data\n\n\ndef get_location(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_location: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.geturl()\n\n\ndef urlopen_with_retry(*args, **kwargs):\n    retry_time = 3\n    for i in range(retry_time):\n        try:\n            if insecure:\n                # ignore ssl errors\n                ctx = ssl.create_default_context()\n                ctx.check_hostname = False\n                ctx.verify_mode = ssl.CERT_NONE\n                return request.urlopen(*args, context=ctx, **kwargs)\n            else:\n                return request.urlopen(*args, **kwargs)\n        except socket.timeout as e:\n            logging.debug(\"request attempt %s timeout\" % str(i + 1))\n            if i + 1 == retry_time:\n                raise e\n        # try to tackle youku CDN fails\n        except error.HTTPError as http_error:\n            logging.debug(\"HTTP Error with code{}\".format(http_error.code))\n            if i + 1 == retry_time:\n                raise http_error\n\n\ndef get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug(\"get_content: %s\" % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\", \"\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset, \"ignore\")\n        else:\n            data = data.decode(\"utf-8\", \"ignore\")\n\n    return data\n\n\ndef post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get(\"post_data_raw\"):\n        logging.debug(\n            \"post_content: %s\\npost_data_raw: %s\" % (url, kwargs[\"post_data_raw\"])\n        )\n    else:\n        logging.debug(\"post_content: %s\\npost_data: %s\" % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get(\"post_data_raw\"):\n        post_data_enc = bytes(kwargs[\"post_data_raw\"], \"utf-8\")\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), \"utf-8\")\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader(\"Content-Encoding\")\n    if content_encoding == \"gzip\":\n        data = ungzip(data)\n    elif content_encoding == \"deflate\":\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(response.getheader(\"Content-Type\"), r\"charset=([\\w-]+)\")\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode(\"utf-8\")\n\n    return data\n\n\ndef url_size(url, faker=False, headers={}):\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(url)\n\n    size = response.headers[\"content-length\"]\n    return int(size) if size is not None else float(\"inf\")\n\n\ndef urls_size(urls, faker=False, headers={}):\n    return sum([url_size(url, faker=faker, headers=headers) for url in urls])\n\n\ndef get_head(url, headers=None, get_method=\"HEAD\"):\n    logging.debug(\"get_head: %s\" % url)\n\n    if headers:\n        req = request.Request(url, headers=headers)\n    else:\n        req = request.Request(url)\n    req.get_method = lambda: get_method\n    res = urlopen_with_retry(req)\n    return res.headers\n\n\ndef url_info(url, faker=False, headers={}):\n    logging.debug(\"url_info: %s\" % url)\n\n    if faker:\n        response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n    elif headers:\n        response = urlopen_with_retry(request.Request(url, headers=headers))\n    else:\n        response = urlopen_with_retry(request.Request(url))\n\n    headers = response.headers\n\n    type = headers[\"content-type\"]\n    if type == \"image/jpg; charset=UTF-8\" or type == \"image/jpg\":\n        type = \"audio/mpeg\"  # fix for netease\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/f4v\": \"flv\",\n        \"video/mp4\": \"mp4\",\n        \"video/MP2T\": \"ts\",\n        \"video/quicktime\": \"mov\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n        \"video/x-ms-asf\": \"asf\",\n        \"audio/mp4\": \"mp4\",\n        \"audio/mpeg\": \"mp3\",\n        \"audio/wav\": \"wav\",\n        \"audio/x-wav\": \"wav\",\n        \"audio/wave\": \"wav\",\n        \"image/jpeg\": \"jpg\",\n        \"image/png\": \"png\",\n        \"image/gif\": \"gif\",\n        \"application/pdf\": \"pdf\",\n    }\n    if type in mapping:\n        ext = mapping[type]\n    else:\n        type = None\n        if headers[\"content-disposition\"]:\n            try:\n                filename = parse.unquote(\n                    r1(r'filename=\"?([^\"]+)\"?', headers[\"content-disposition\"])\n                )\n                if len(filename.split(\".\")) > 1:\n                    ext = filename.split(\".\")[-1]\n                else:\n                    ext = None\n            except:\n                ext = None\n        else:\n            ext = None\n\n    if headers[\"transfer-encoding\"] != \"chunked\":\n        size = headers[\"content-length\"] and int(headers[\"content-length\"])\n    else:\n        size = None\n\n    return type, ext, size\n\n\ndef url_locations(urls, faker=False, headers={}):\n    locations = []\n    for url in urls:\n        logging.debug(\"url_locations: %s\" % url)\n\n        if faker:\n            response = urlopen_with_retry(request.Request(url, headers=fake_headers))\n        elif headers:\n            response = urlopen_with_retry(request.Request(url, headers=headers))\n        else:\n            response = urlopen_with_retry(request.Request(url))\n\n        locations.append(response.url)\n    return locations\n\n\ndef url_save(\n    url,\n    filepath,\n    bar,\n    refer=None,\n    is_part=False,\n    faker=False,\n    headers=None,\n    timeout=None,\n    **kwargs\n):\n    tmp_headers = headers.copy() if headers is not None else {}\n    # When a referer specified with param refer,\n    # the key must be 'Referer' for the hack here\n    if refer is not None:\n        tmp_headers[\"Referer\"] = refer\n    if type(url) is list:\n        file_size = urls_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = True, url\n    else:\n        file_size = url_size(url, faker=faker, headers=tmp_headers)\n        is_chunked, urls = False, [url]\n\n    continue_renameing = True\n    while continue_renameing:\n        continue_renameing = False\n        if os.path.exists(filepath):\n            if not force and file_size == os.path.getsize(filepath):\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    log.w(\n                        \"Skipping {}: file already exists\".format(\n                            tr(os.path.basename(filepath))\n                        )\n                    )\n                else:\n                    if bar:\n                        bar.update_received(file_size)\n                return\n            else:\n                if not is_part:\n                    if bar:\n                        bar.done()\n                    if not force and auto_rename:\n                        path, ext = os.path.basename(filepath).rsplit(\".\", 1)\n                        finder = re.compile(\" \\([1-9]\\d*?\\)$\")\n                        if finder.search(path) is None:\n                            thisfile = path + \" (1).\" + ext\n                        else:\n\n                            def numreturn(a):\n                                return \" (\" + str(int(a.group()[2:-1]) + 1) + \").\"\n\n                            thisfile = finder.sub(numreturn, path) + ext\n                        filepath = os.path.join(os.path.dirname(filepath), thisfile)\n                        print(\n                            \"Changing name to %s\" % tr(os.path.basename(filepath)),\n                            \"...\",\n                        )\n                        continue_renameing = True\n                        continue\n                    if log.yes_or_no(\"File with this name already exists. Overwrite?\"):\n                        log.w(\"Overwriting %s ...\" % tr(os.path.basename(filepath)))\n                    else:\n                        return\n        elif not os.path.exists(os.path.dirname(filepath)):\n            os.mkdir(os.path.dirname(filepath))\n\n    temp_filepath = filepath + \".download\" if file_size != float(\"inf\") else filepath\n    received = 0\n    if not force:\n        open_mode = \"ab\"\n\n        if os.path.exists(temp_filepath):\n            received += os.path.getsize(temp_filepath)\n            if bar:\n                bar.update_received(os.path.getsize(temp_filepath))\n    else:\n        open_mode = \"wb\"\n\n    for url in urls:\n        received_chunk = 0\n        if received < file_size:\n            if faker:\n                tmp_headers = fake_headers\n            \"\"\"\n            if parameter headers passed in, we have it copied as tmp_header\n            elif headers:\n                headers = headers\n            else:\n                headers = {}\n            \"\"\"\n            if received and not is_chunked:  # only request a range when not chunked\n                tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n            if refer:\n                tmp_headers[\"Referer\"] = refer\n\n            if timeout:\n                response = urlopen_with_retry(\n                    request.Request(url, headers=tmp_headers), timeout=timeout\n                )\n            else:\n                response = urlopen_with_retry(request.Request(url, headers=tmp_headers))\n            try:\n                range_start = int(\n                    response.headers[\"content-range\"][6:].split(\"/\")[0].split(\"-\")[0]\n                )\n                end_length = int(response.headers[\"content-range\"][6:].split(\"/\")[1])\n                range_length = end_length - range_start\n            except:\n                content_length = response.headers[\"content-length\"]\n                range_length = (\n                    int(content_length) if content_length is not None else float(\"inf\")\n                )\n\n            if is_chunked:  # always append if chunked\n                open_mode = \"ab\"\n            elif file_size != received + range_length:  # is it ever necessary?\n                received = 0\n                if bar:\n                    bar.received = 0\n                open_mode = \"wb\"\n\n            with open(temp_filepath, open_mode) as output:\n                while True:\n                    buffer = None\n                    try:\n                        buffer = response.read(1024 * 256)\n                    except socket.timeout:\n                        pass\n                    if not buffer:\n                        if is_chunked and received_chunk == range_length:\n                            break\n                        elif (\n                            not is_chunked and received == file_size\n                        ):  # Download finished\n                            break\n                        # Unexpected termination. Retry request\n                        if not is_chunked:  # when\n                            tmp_headers[\"Range\"] = \"bytes=\" + str(received) + \"-\"\n                        response = urlopen_with_retry(\n                            request.Request(url, headers=tmp_headers)\n                        )\n                        continue\n                    output.write(buffer)\n                    received += len(buffer)\n                    received_chunk += len(buffer)\n                    if bar:\n                        bar.update_received(len(buffer))\n\n    assert received == os.path.getsize(temp_filepath), \"%s == %s == %s\" % (\n        received,\n        os.path.getsize(temp_filepath),\n        temp_filepath,\n    )\n\n    if os.access(filepath, os.W_OK):\n        # on Windows rename could fail if destination filepath exists\n        os.remove(filepath)\n    os.rename(temp_filepath, filepath)\n\n\nclass SimpleProgressBar:\n    term_size = term.get_terminal_size()[1]\n\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n        self.speed = \"\"\n        self.last_updated = time.time()\n\n        total_pieces_len = len(str(total_pieces))\n        # 38 is the size of all statically known size in self.bar\n        total_str = \"%5s\" % round(self.total_size / 1048576, 1)\n        total_str_width = max(len(total_str), 5)\n        self.bar_size = self.term_size - 28 - 2 * total_pieces_len - 2 * total_str_width\n        self.bar = \"{:>4}%% ({:>%s}/%sMB) \u251c{:\u2500<%s}\u2524[{:>%s}/{:>%s}] {}\" % (\n            total_str_width,\n            total_str,\n            self.bar_size,\n            total_pieces_len,\n            total_pieces_len,\n        )\n\n    def update(self):\n        self.displayed = True\n        bar_size = self.bar_size\n        percent = round(self.received * 100 / self.total_size, 1)\n        if percent >= 100:\n            percent = 100\n        dots = bar_size * int(percent) // 100\n        plus = int(percent) - dots // bar_size * 100\n        if plus > 0.8:\n            plus = \"\u2588\"\n        elif plus > 0.4:\n            plus = \">\"\n        else:\n            plus = \"\"\n        bar = \"\u2588\" * dots + plus\n        bar = self.bar.format(\n            percent,\n            round(self.received / 1048576, 1),\n            bar,\n            self.current_piece,\n            self.total_pieces,\n            self.speed,\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        time_diff = time.time() - self.last_updated\n        bytes_ps = n / time_diff if time_diff else 0\n        if bytes_ps >= 1024 ** 3:\n            self.speed = \"{:4.0f} GB/s\".format(bytes_ps / 1024 ** 3)\n        elif bytes_ps >= 1024 ** 2:\n            self.speed = \"{:4.0f} MB/s\".format(bytes_ps / 1024 ** 2)\n        elif bytes_ps >= 1024:\n            self.speed = \"{:4.0f} kB/s\".format(bytes_ps / 1024)\n        else:\n            self.speed = \"{:4.0f}  B/s\".format(bytes_ps)\n        self.last_updated = time.time()\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass PiecesProgressBar:\n    def __init__(self, total_size, total_pieces=1):\n        self.displayed = False\n        self.total_size = total_size\n        self.total_pieces = total_pieces\n        self.current_piece = 1\n        self.received = 0\n\n    def update(self):\n        self.displayed = True\n        bar = \"{0:>5}%[{1:<40}] {2}/{3}\".format(\n            \"\", \"=\" * 40, self.current_piece, self.total_pieces\n        )\n        sys.stdout.write(\"\\r\" + bar)\n        sys.stdout.flush()\n\n    def update_received(self, n):\n        self.received += n\n        self.update()\n\n    def update_piece(self, n):\n        self.current_piece = n\n\n    def done(self):\n        if self.displayed:\n            print()\n            self.displayed = False\n\n\nclass DummyProgressBar:\n    def __init__(self, *args):\n        pass\n\n    def update_received(self, n):\n        pass\n\n    def update_piece(self, n):\n        pass\n\n    def done(self):\n        pass\n\n\ndef get_output_filename(urls, title, ext, output_dir, merge):\n    # lame hack for the --output-filename option\n    global output_filename\n    if output_filename:\n        if ext:\n            return output_filename + \".\" + ext\n        return output_filename\n\n    merged_ext = ext\n    if (len(urls) > 1) and merge:\n        from .processor.ffmpeg import has_ffmpeg_installed\n\n        if ext in [\"flv\", \"f4v\"]:\n            if has_ffmpeg_installed():\n                merged_ext = \"mp4\"\n            else:\n                merged_ext = \"flv\"\n        elif ext == \"mp4\":\n            merged_ext = \"mp4\"\n        elif ext == \"ts\":\n            if has_ffmpeg_installed():\n                merged_ext = \"mkv\"\n            else:\n                merged_ext = \"ts\"\n    return \"%s.%s\" % (title, merged_ext)\n\n\ndef print_user_agent(faker=False):\n    urllib_default_user_agent = \"Python-urllib/%d.%d\" % sys.version_info[:2]\n    user_agent = fake_headers[\"User-Agent\"] if faker else urllib_default_user_agent\n    print(\"User Agent: %s\" % user_agent)\n\n\ndef download_urls(\n    urls,\n    title,\n    ext,\n    total_size,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    headers={},\n    **kwargs\n):\n    assert urls\n    if json_output:\n        json_output_.download_urls(\n            urls=urls, title=title, ext=ext, total_size=total_size, refer=refer\n        )\n        return\n    if dry_run:\n        print_user_agent(faker=faker)\n        try:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join(urls))\n        except:\n            print(\"Real URLs:\\n%s\" % \"\\n\".join([j for i in urls for j in i]))\n        return\n\n    if player:\n        launch_player(player, urls)\n        return\n\n    if not total_size:\n        try:\n            total_size = urls_size(urls, faker=faker, headers=headers)\n        except:\n            import traceback\n\n            traceback.print_exc(file=sys.stdout)\n            pass\n\n    title = tr(get_filename(title))\n    output_filename = get_output_filename(urls, title, ext, output_dir, merge)\n    output_filepath = os.path.join(output_dir, output_filename)\n\n    if total_size:\n        if (\n            not force\n            and os.path.exists(output_filepath)\n            and not auto_rename\n            and os.path.getsize(output_filepath) >= total_size * 0.9\n        ):\n            log.w(\"Skipping %s: file already exists\" % output_filepath)\n            print()\n            return\n        bar = SimpleProgressBar(total_size, len(urls))\n    else:\n        bar = PiecesProgressBar(total_size, len(urls))\n\n    if len(urls) == 1:\n        url = urls[0]\n        print(\"Downloading %s ...\" % tr(output_filename))\n        bar.update()\n        url_save(\n            url,\n            output_filepath,\n            bar,\n            refer=refer,\n            faker=faker,\n            headers=headers,\n            **kwargs\n        )\n        bar.done()\n    else:\n        parts = []\n        print(\"Downloading %s.%s ...\" % (tr(title), ext))\n        bar.update()\n        for i, url in enumerate(urls):\n            filename = \"%s[%02d].%s\" % (title, i, ext)\n            filepath = os.path.join(output_dir, filename)\n            parts.append(filepath)\n            # print 'Downloading %s [%s/%s]...' % (tr(filename), i + 1, len(urls))\n            bar.update_piece(i + 1)\n            url_save(\n                url,\n                filepath,\n                bar,\n                refer=refer,\n                is_part=True,\n                faker=faker,\n                headers=headers,\n                **kwargs\n            )\n        bar.done()\n\n        if not merge:\n            print()\n            return\n\n        if \"av\" in kwargs and kwargs[\"av\"]:\n            from .processor.ffmpeg import has_ffmpeg_installed\n\n            if has_ffmpeg_installed():\n                from .processor.ffmpeg import ffmpeg_concat_av\n\n                ret = ffmpeg_concat_av(parts, output_filepath, ext)\n                print(\"Merged into %s\" % output_filename)\n                if ret == 0:\n                    for part in parts:\n                        os.remove(part)\n\n        elif ext in [\"flv\", \"f4v\"]:\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_flv_to_mp4\n\n                    ffmpeg_concat_flv_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_flv import concat_flv\n\n                    concat_flv(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"mp4\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_mp4_to_mp4\n\n                    ffmpeg_concat_mp4_to_mp4(parts, output_filepath)\n                else:\n                    from .processor.join_mp4 import concat_mp4\n\n                    concat_mp4(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        elif ext == \"ts\":\n            try:\n                from .processor.ffmpeg import has_ffmpeg_installed\n\n                if has_ffmpeg_installed():\n                    from .processor.ffmpeg import ffmpeg_concat_ts_to_mkv\n\n                    ffmpeg_concat_ts_to_mkv(parts, output_filepath)\n                else:\n                    from .processor.join_ts import concat_ts\n\n                    concat_ts(parts, output_filepath)\n                print(\"Merged into %s\" % output_filename)\n            except:\n                raise\n            else:\n                for part in parts:\n                    os.remove(part)\n\n        else:\n            print(\"Can't merge %s files\" % ext)\n\n    print()\n\n\ndef download_rtmp_url(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset -> False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        from .processor.rtmpdump import play_rtmpdump_stream\n\n        play_rtmpdump_stream(player, url, params)\n        return\n\n    from .processor.rtmpdump import has_rtmpdump_installed, download_rtmpdump_stream\n\n    assert has_rtmpdump_installed(), \"RTMPDump not installed.\"\n    download_rtmpdump_stream(url, title, ext, params, output_dir)\n\n\ndef download_url_ffmpeg(\n    url,\n    title,\n    ext,\n    params={},\n    total_size=0,\n    output_dir=\".\",\n    refer=None,\n    merge=True,\n    faker=False,\n    stream=True,\n):\n    assert url\n    if dry_run:\n        print_user_agent(faker=faker)\n        print(\"Real URL:\\n%s\\n\" % [url])\n        if params.get(\"-y\", False):  # None or unset ->False\n            print(\"Real Playpath:\\n%s\\n\" % [params.get(\"-y\")])\n        return\n\n    if player:\n        launch_player(player, [url])\n        return\n\n    from .processor.ffmpeg import has_ffmpeg_installed, ffmpeg_download_stream\n\n    assert has_ffmpeg_installed(), \"FFmpeg not installed.\"\n\n    global output_filename\n    if output_filename:\n        dotPos = output_filename.rfind(\".\")\n        if dotPos > 0:\n            title = output_filename[:dotPos]\n            ext = output_filename[dotPos + 1 :]\n        else:\n            title = output_filename\n\n    title = tr(get_filename(title))\n\n    ffmpeg_download_stream(url, title, ext, params, output_dir, stream=stream)\n\n\ndef playlist_not_supported(name):\n    def f(*args, **kwargs):\n        raise NotImplementedError(\"Playlist is not supported for \" + name)\n\n    return f\n\n\ndef print_info(site_info, title, type, size, **kwargs):\n    if json_output:\n        json_output_.print_info(site_info=site_info, title=title, type=type, size=size)\n        return\n    if type:\n        type = type.lower()\n    if type in [\"3gp\"]:\n        type = \"video/3gpp\"\n    elif type in [\"asf\", \"wmv\"]:\n        type = \"video/x-ms-asf\"\n    elif type in [\"flv\", \"f4v\"]:\n        type = \"video/x-flv\"\n    elif type in [\"mkv\"]:\n        type = \"video/x-matroska\"\n    elif type in [\"mp3\"]:\n        type = \"audio/mpeg\"\n    elif type in [\"mp4\"]:\n        type = \"video/mp4\"\n    elif type in [\"mov\"]:\n        type = \"video/quicktime\"\n    elif type in [\"ts\"]:\n        type = \"video/MP2T\"\n    elif type in [\"webm\"]:\n        type = \"video/webm\"\n\n    elif type in [\"jpg\"]:\n        type = \"image/jpeg\"\n    elif type in [\"png\"]:\n        type = \"image/png\"\n    elif type in [\"gif\"]:\n        type = \"image/gif\"\n\n    if type in [\"video/3gpp\"]:\n        type_info = \"3GPP multimedia file (%s)\" % type\n    elif type in [\"video/x-flv\", \"video/f4v\"]:\n        type_info = \"Flash video (%s)\" % type\n    elif type in [\"video/mp4\", \"video/x-m4v\"]:\n        type_info = \"MPEG-4 video (%s)\" % type\n    elif type in [\"video/MP2T\"]:\n        type_info = \"MPEG-2 transport stream (%s)\" % type\n    elif type in [\"video/webm\"]:\n        type_info = \"WebM video (%s)\" % type\n    # elif type in ['video/ogg']:\n    #    type_info = 'Ogg video (%s)' % type\n    elif type in [\"video/quicktime\"]:\n        type_info = \"QuickTime video (%s)\" % type\n    elif type in [\"video/x-matroska\"]:\n        type_info = \"Matroska video (%s)\" % type\n    # elif type in ['video/x-ms-wmv']:\n    #    type_info = 'Windows Media video (%s)' % type\n    elif type in [\"video/x-ms-asf\"]:\n        type_info = \"Advanced Systems Format (%s)\" % type\n    # elif type in ['video/mpeg']:\n    #    type_info = 'MPEG video (%s)' % type\n    elif type in [\"audio/mp4\", \"audio/m4a\"]:\n        type_info = \"MPEG-4 audio (%s)\" % type\n    elif type in [\"audio/mpeg\"]:\n        type_info = \"MP3 (%s)\" % type\n    elif type in [\"audio/wav\", \"audio/wave\", \"audio/x-wav\"]:\n        type_info = \"Waveform Audio File Format ({})\".format(type)\n\n    elif type in [\"image/jpeg\"]:\n        type_info = \"JPEG Image (%s)\" % type\n    elif type in [\"image/png\"]:\n        type_info = \"Portable Network Graphics (%s)\" % type\n    elif type in [\"image/gif\"]:\n        type_info = \"Graphics Interchange Format (%s)\" % type\n    elif type in [\"m3u8\"]:\n        if \"m3u8_type\" in kwargs:\n            if kwargs[\"m3u8_type\"] == \"master\":\n                type_info = \"M3U8 Master {}\".format(type)\n        else:\n            type_info = \"M3U8 Playlist {}\".format(type)\n    else:\n        type_info = \"Unknown type (%s)\" % type\n\n    maybe_print(\"Site:      \", site_info)\n    maybe_print(\"Title:     \", unescape_html(tr(title)))\n    print(\"Type:      \", type_info)\n    if type != \"m3u8\":\n        print(\"Size:      \", round(size / 1048576, 2), \"MiB (\" + str(size) + \" Bytes)\")\n    if type == \"m3u8\" and \"m3u8_url\" in kwargs:\n        print(\"M3U8 Url:   {}\".format(kwargs[\"m3u8_url\"]))\n    print()\n\n\ndef mime_to_container(mime):\n    mapping = {\n        \"video/3gpp\": \"3gp\",\n        \"video/mp4\": \"mp4\",\n        \"video/webm\": \"webm\",\n        \"video/x-flv\": \"flv\",\n    }\n    if mime in mapping:\n        return mapping[mime]\n    else:\n        return mime.split(\"/\")[1]\n\n\ndef set_proxy(proxy):\n    proxy_handler = request.ProxyHandler(\n        {\n            \"http\": \"%s:%s\" % proxy,\n            \"https\": \"%s:%s\" % proxy,\n        }\n    )\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\ndef unset_proxy():\n    proxy_handler = request.ProxyHandler({})\n    opener = request.build_opener(proxy_handler)\n    request.install_opener(opener)\n\n\n# DEPRECATED in favor of set_proxy() and unset_proxy()\ndef set_http_proxy(proxy):\n    if proxy is None:  # Use system default setting\n        proxy_support = request.ProxyHandler()\n    elif proxy == \"\":  # Don't use any proxy\n        proxy_support = request.ProxyHandler({})\n    else:  # Use proxy\n        proxy_support = request.ProxyHandler(\n            {\"http\": \"%s\" % proxy, \"https\": \"%s\" % proxy}\n        )\n    opener = request.build_opener(proxy_support)\n    request.install_opener(opener)\n\n\ndef print_more_compatible(*args, **kwargs):\n    import builtins as __builtin__\n\n    \"\"\"Overload default print function as py (<3.3) does not support 'flush' keyword.\n    Although the function name can be same as print to get itself overloaded automatically,\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n    \"\"\"\n    # nothing happens on py3.3 and later\n    if sys.version_info[:2] >= (3, 3):\n        return __builtin__.print(*args, **kwargs)\n\n    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested\n    doFlush = kwargs.pop(\"flush\", False)\n    ret = __builtin__.print(*args, **kwargs)\n    if doFlush:\n        kwargs.get(\"file\", sys.stdout).flush()\n    return ret\n\n\ndef download_main(download, download_playlist, urls, playlist, **kwargs):\n    for url in urls:\n        if re.match(r\"https?://\", url) is None:\n            url = \"http://\" + url\n\n        if playlist:\n            download_playlist(url, **kwargs)\n        else:\n            download(url, **kwargs)\n\n\ndef load_cookies(cookiefile):\n    global cookies\n    if cookiefile.endswith(\".txt\"):\n        # MozillaCookieJar treats prefix '#HttpOnly_' as comments incorrectly!\n        # do not use its load()\n        # see also:\n        #   - https://docs.python.org/3/library/http.cookiejar.html#http.cookiejar.MozillaCookieJar\n        #   - https://github.com/python/cpython/blob/4b219ce/Lib/http/cookiejar.py#L2014\n        #   - https://curl.haxx.se/libcurl/c/CURLOPT_COOKIELIST.html#EXAMPLE\n        # cookies = cookiejar.MozillaCookieJar(cookiefile)\n        # cookies.load()\n        from http.cookiejar import Cookie\n\n        cookies = cookiejar.MozillaCookieJar()\n        now = time.time()\n        ignore_discard, ignore_expires = False, False\n        with open(cookiefile, \"r\") as f:\n            for line in f:\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"):\n                    line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if line.strip().startswith((\"#\", \"$\")) or line.strip() == \"\":\n                    if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                        continue\n\n                (\n                    domain,\n                    domain_specified,\n                    path,\n                    secure,\n                    expires,\n                    name,\n                    value,\n                ) = line.split(\"\\t\")\n                secure = secure == \"TRUE\"\n                domain_specified = domain_specified == \"TRUE\"\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas http.cookiejar regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                if not line.strip().startswith(\"#HttpOnly_\"):  # skip for #HttpOnly_\n                    assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(\n                    0,\n                    name,\n                    value,\n                    None,\n                    False,\n                    domain,\n                    domain_specified,\n                    initial_dot,\n                    path,\n                    False,\n                    secure,\n                    expires,\n                    discard,\n                    None,\n                    None,\n                    {},\n                )\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                cookies.set_cookie(c)\n\n    elif cookiefile.endswith((\".sqlite\", \".sqlite3\")):\n        import sqlite3, shutil, tempfile\n\n        temp_dir = tempfile.gettempdir()\n        temp_cookiefile = os.path.join(temp_dir, \"temp_cookiefile.sqlite\")\n        shutil.copy2(cookiefile, temp_cookiefile)\n\n        cookies = cookiejar.MozillaCookieJar()\n        con = sqlite3.connect(temp_cookiefile)\n        cur = con.cursor()\n        cur.execute(\n            \"\"\"SELECT host, path, isSecure, expiry, name, value\n        FROM moz_cookies\"\"\"\n        )\n        for item in cur.fetchall():\n            c = cookiejar.Cookie(\n                0,\n                item[4],\n                item[5],\n                None,\n                False,\n                item[0],\n                item[0].startswith(\".\"),\n                item[0].startswith(\".\"),\n                item[1],\n                False,\n                item[2],\n                item[3],\n                item[3] == \"\",\n                None,\n                None,\n                {},\n            )\n            cookies.set_cookie(c)\n\n    else:\n        log.e(\"[error] unsupported cookies format\")\n        # TODO: Chromium Cookies\n        # SELECT host_key, path, secure, expires_utc, name, encrypted_value\n        # FROM cookies\n        # http://n8henrie.com/2013/11/use-chromes-cookies-for-easier-downloading-with-python-requests/\n\n\ndef set_socks_proxy(proxy):\n    try:\n        import socks\n\n        socks_proxy_addrs = proxy.split(\":\")\n        socks.set_default_proxy(\n            socks.SOCKS5, socks_proxy_addrs[0], int(socks_proxy_addrs[1])\n        )\n        socket.socket = socks.socksocket\n\n        def getaddrinfo(*args):\n            return [(socket.AF_INET, socket.SOCK_STREAM, 6, \"\", (args[0], args[1]))]\n\n        socket.getaddrinfo = getaddrinfo\n    except ImportError:\n        log.w(\n            \"Error importing PySocks library, socks proxy ignored.\"\n            \"In order to use use socks proxy, please install PySocks.\"\n        )\n\n\ndef script_main(download, download_playlist, **kwargs):\n    logging.basicConfig(format=\"[%(levelname)s] %(message)s\")\n\n    def print_version():\n        version = get_version(\n            kwargs[\"repo_path\"] if \"repo_path\" in kwargs else __version__\n        )\n        log.i(\"version {}, a tiny downloader that scrapes the web.\".format(version))\n\n    parser = argparse.ArgumentParser(\n        prog=\"you-get\",\n        usage=\"you-get [OPTION]... URL...\",\n        description=\"A tiny downloader that scrapes the web\",\n        add_help=False,\n    )\n    parser.add_argument(\n        \"-V\", \"--version\", action=\"store_true\", help=\"Print version and exit\"\n    )\n    parser.add_argument(\n        \"-h\", \"--help\", action=\"store_true\", help=\"Print this help message and exit\"\n    )\n\n    dry_run_grp = parser.add_argument_group(\n        \"Dry-run options\", \"(no actual downloading)\"\n    )\n    dry_run_grp = dry_run_grp.add_mutually_exclusive_group()\n    dry_run_grp.add_argument(\n        \"-i\", \"--info\", action=\"store_true\", help=\"Print extracted information\"\n    )\n    dry_run_grp.add_argument(\n        \"-u\", \"--url\", action=\"store_true\", help=\"Print extracted information with URLs\"\n    )\n    dry_run_grp.add_argument(\n        \"--json\", action=\"store_true\", help=\"Print extracted URLs in JSON format\"\n    )\n\n    download_grp = parser.add_argument_group(\"Download options\")\n    download_grp.add_argument(\n        \"-n\",\n        \"--no-merge\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not merge video parts\",\n    )\n    download_grp.add_argument(\n        \"--no-caption\",\n        action=\"store_true\",\n        help=\"Do not download captions (subtitles, lyrics, danmaku, ...)\",\n    )\n    download_grp.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force overwriting existing files\",\n    )\n    download_grp.add_argument(\n        \"-F\", \"--format\", metavar=\"STREAM_ID\", help=\"Set video format to STREAM_ID\"\n    )\n    download_grp.add_argument(\n        \"-O\", \"--output-filename\", metavar=\"FILE\", help=\"Set output filename\"\n    )\n    download_grp.add_argument(\n        \"-o\", \"--output-dir\", metavar=\"DIR\", default=\".\", help=\"Set output directory\"\n    )\n    download_grp.add_argument(\n        \"-p\", \"--player\", metavar=\"PLAYER\", help=\"Stream extracted URL to a PLAYER\"\n    )\n    download_grp.add_argument(\n        \"-c\",\n        \"--cookies\",\n        metavar=\"COOKIES_FILE\",\n        help=\"Load cookies.txt or cookies.sqlite\",\n    )\n    download_grp.add_argument(\n        \"-t\",\n        \"--timeout\",\n        metavar=\"SECONDS\",\n        type=int,\n        default=600,\n        help=\"Set socket timeout\",\n    )\n    download_grp.add_argument(\n        \"-d\", \"--debug\", action=\"store_true\", help=\"Show traceback and other debug info\"\n    )\n    download_grp.add_argument(\n        \"-I\",\n        \"--input-file\",\n        metavar=\"FILE\",\n        type=argparse.FileType(\"r\"),\n        help=\"Read non-playlist URLs from FILE\",\n    )\n    download_grp.add_argument(\n        \"-P\", \"--password\", help=\"Set video visit password to PASSWORD\"\n    )\n    download_grp.add_argument(\n        \"-l\", \"--playlist\", action=\"store_true\", help=\"Prefer to download a playlist\"\n    )\n    download_grp.add_argument(\n        \"-a\",\n        \"--auto-rename\",\n        action=\"store_true\",\n        default=False,\n        help=\"Auto rename same name different files\",\n    )\n\n    download_grp.add_argument(\n        \"-k\", \"--insecure\", action=\"store_true\", default=False, help=\"ignore ssl errors\"\n    )\n\n    proxy_grp = parser.add_argument_group(\"Proxy options\")\n    proxy_grp = proxy_grp.add_mutually_exclusive_group()\n    proxy_grp.add_argument(\n        \"-x\",\n        \"--http-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for downloading\",\n    )\n    proxy_grp.add_argument(\n        \"-y\",\n        \"--extractor-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an HTTP proxy for extracting only\",\n    )\n    proxy_grp.add_argument(\"--no-proxy\", action=\"store_true\", help=\"Never use a proxy\")\n    proxy_grp.add_argument(\n        \"-s\",\n        \"--socks-proxy\",\n        metavar=\"HOST:PORT\",\n        help=\"Use an SOCKS5 proxy for downloading\",\n    )\n\n    download_grp.add_argument(\"--stream\", help=argparse.SUPPRESS)\n    download_grp.add_argument(\"--itag\", help=argparse.SUPPRESS)\n\n    parser.add_argument(\"URL\", nargs=\"*\", help=argparse.SUPPRESS)\n\n    args = parser.parse_args()\n\n    if args.help:\n        print_version()\n        parser.print_help()\n        sys.exit()\n    if args.version:\n        print_version()\n        sys.exit()\n\n    if args.debug:\n        # Set level of root logger to DEBUG\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    global force\n    global dry_run\n    global json_output\n    global player\n    global extractor_proxy\n    global output_filename\n    global auto_rename\n    global insecure\n    output_filename = args.output_filename\n    extractor_proxy = args.extractor_proxy\n\n    info_only = args.info\n    if args.force:\n        force = True\n    if args.auto_rename:\n        auto_rename = True\n    if args.url:\n        dry_run = True\n    if args.json:\n        json_output = True\n        # to fix extractors not use VideoExtractor\n        dry_run = True\n        info_only = False\n\n    if args.cookies:\n        load_cookies(args.cookies)\n\n    caption = True\n    stream_id = args.format or args.stream or args.itag\n    if args.no_caption:\n        caption = False\n    if args.player:\n        player = args.player\n        caption = False\n\n    if args.insecure:\n        # ignore ssl\n        insecure = True\n\n    if args.no_proxy:\n        set_http_proxy(\"\")\n    else:\n        set_http_proxy(args.http_proxy)\n    if args.socks_proxy:\n        set_socks_proxy(args.socks_proxy)\n\n    URLs = []\n    if args.input_file:\n        logging.debug(\"you are trying to load urls from %s\", args.input_file)\n        if args.playlist:\n            log.e(\n                \"reading playlist from a file is unsupported \"\n                \"and won't make your life easier\"\n            )\n            sys.exit(2)\n        URLs.extend(args.input_file.read().splitlines())\n        args.input_file.close()\n    URLs.extend(args.URL)\n\n    if not URLs:\n        parser.print_help()\n        sys.exit()\n\n    socket.setdefaulttimeout(args.timeout)\n\n    try:\n        extra = {}\n        if extractor_proxy:\n            extra[\"extractor_proxy\"] = extractor_proxy\n        if stream_id:\n            extra[\"stream_id\"] = stream_id\n        download_main(\n            download,\n            download_playlist,\n            URLs,\n            args.playlist,\n            output_dir=args.output_dir,\n            merge=not args.no_merge,\n            info_only=info_only,\n            json_output=json_output,\n            caption=caption,\n            password=args.password,\n            **extra\n        )\n    except KeyboardInterrupt:\n        if args.debug:\n            raise\n        else:\n            sys.exit(1)\n    except UnicodeEncodeError:\n        if args.debug:\n            raise\n        log.e(\n            \"[error] oops, the current environment does not seem to support \" \"Unicode.\"\n        )\n        log.e(\"please set it to a UTF-8-aware locale first,\")\n        log.e(\"so as to save the video (with some Unicode characters) correctly.\")\n        log.e(\"you can do it like this:\")\n        log.e(\"    (Windows)    % chcp 65001 \")\n        log.e(\"    (Linux)      $ LC_CTYPE=en_US.UTF-8\")\n        sys.exit(1)\n    except Exception:\n        if not args.debug:\n            log.e(\"[error] oops, something went wrong.\")\n            log.e(\"don't panic, c'est la vie. please try the following steps:\")\n            log.e(\"  (1) Rule out any network problem.\")\n            log.e(\"  (2) Make sure you-get is up-to-date.\")\n            log.e(\"  (3) Check if the issue is already known, on\")\n            log.e(\"        https://github.com/soimort/you-get/wiki/Known-Bugs\")\n            log.e(\"        https://github.com/soimort/you-get/issues\")\n            log.e(\"  (4) Run the command with '--debug' option,\")\n            log.e(\"      and report this issue with the full output.\")\n        else:\n            print_version()\n            log.i(args)\n            raise\n        sys.exit(1)\n\n\ndef google_search(url):\n    keywords = r1(r\"https?://(.*)\", url)\n    url = \"https://www.google.com/search?tbm=vid&q=%s\" % parse.quote(keywords)\n    page = get_content(url, headers=fake_headers)\n    videos = re.findall(\n        r'<a href=\"(https?://[^\"]+)\" onmousedown=\"[^\"]+\"><h3 class=\"[^\"]*\">([^<]+)<',\n        page,\n    )\n    vdurs = re.findall(r'<span class=\"vdur[^\"]*\">([^<]+)<', page)\n    durs = [r1(r\"(\\d+:\\d+)\", unescape_html(dur)) for dur in vdurs]\n    print(\"Google Videos search:\")\n    for v in zip(videos, durs):\n        print(\"- video:  {} [{}]\".format(unescape_html(v[0][1]), v[1] if v[1] else \"?\"))\n        print(\"# you-get %s\" % log.sprint(v[0][0], log.UNDERLINE))\n        print()\n    print(\"Best matched result:\")\n    return videos[0][0]\n\n\ndef url_to_module(url):\n    try:\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n        assert video_host and video_url\n    except AssertionError:\n        url = google_search(url)\n        video_host = r1(r\"https?://([^/]+)/\", url)\n        video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    if video_host.endswith(\".com.cn\") or video_host.endswith(\".ac.cn\"):\n        video_host = video_host[:-3]\n    domain = r1(r\"(\\.[^.]+\\.[^.]+)$\", video_host) or video_host\n    assert domain, \"unsupported url: \" + url\n\n    # all non-ASCII code points must be quoted (percent-encoded UTF-8)\n    url = \"\".join([ch if ord(ch) in range(128) else parse.quote(ch) for ch in url])\n    video_host = r1(r\"https?://([^/]+)/\", url)\n    video_url = r1(r\"https?://[^/]+(.*)\", url)\n\n    k = r1(r\"([^.]+)\", domain)\n    if k in SITES:\n        return (import_module(\".\".join([\"you_get\", \"extractors\", SITES[k]])), url)\n    else:\n        try:\n            location = get_location(url)  # t.co isn't happy with fake_headers\n        except:\n            location = get_location(url, headers=fake_headers)\n\n        if location and location != url and not location.startswith(\"/\"):\n            return url_to_module(location)\n        else:\n            return import_module(\"you_get.extractors.universal\"), url\n\n\ndef any_download(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download(url, **kwargs)\n\n\ndef any_download_playlist(url, **kwargs):\n    m, url = url_to_module(url)\n    m.download_playlist(url, **kwargs)\n\n\ndef main(**kwargs):\n    script_main(any_download, any_download_playlist, **kwargs)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "package": ["import io", "import os", "import re", "import sys", "import time", "import json", "import socket", "import locale", "import logging", "import argparse", "import ssl", "from http import cookiejar", "from importlib import import_module", "from urllib import request, parse, error", "from .version import __version__", "from .util import log, term", "from .util.git import get_version", "from .util.strings import get_filename, unescape_html", "from . import json_output as json_output_", "import subprocess", "import shlex", "import shutil", "from io import BytesIO", "import gzip", "import zlib"], "function": ["def rc4(key, data):\n", "def general_m3u8_extractor(url, headers={}):\n", "def maybe_print(*s):\n", "def tr(s):\n", "def r1(pattern, text):\n", "def r1_of(patterns, text):\n", "def match1(text, *patterns):\n", "def matchall(text, patterns):\n", "def launch_player(player, urls):\n", "def parse_query_param(url, param):\n", "def unicodize(text):\n", "def escape_file_path(path):\n", "def ungzip(data):\n", "def undeflate(data):\n", "def get_response(url, faker=False):\n", "def get_html(url, encoding=None, faker=False):\n", "def get_decoded_html(url, faker=False):\n", "def get_location(url, headers=None, get_method=\"HEAD\"):\n", "def urlopen_with_retry(*args, **kwargs):\n", "def get_content(url, headers={}, decoded=True):\n", "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n", "def url_size(url, faker=False, headers={}):\n", "def urls_size(urls, faker=False, headers={}):\n", "def get_head(url, headers=None, get_method=\"HEAD\"):\n", "def url_info(url, faker=False, headers={}):\n", "def url_locations(urls, faker=False, headers={}):\n", "class SimpleProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class PiecesProgressBar:\n", "    def __init__(self, total_size, total_pieces=1):\n", "    def update(self):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "class DummyProgressBar:\n", "    def __init__(self, *args):\n", "    def update_received(self, n):\n", "    def update_piece(self, n):\n", "    def done(self):\n", "def get_output_filename(urls, title, ext, output_dir, merge):\n", "def print_user_agent(faker=False):\n", "def playlist_not_supported(name):\n", "    def f(*args, **kwargs):\n", "def print_info(site_info, title, type, size, **kwargs):\n", "def mime_to_container(mime):\n", "def set_proxy(proxy):\n", "def unset_proxy():\n", "def set_http_proxy(proxy):\n", "def print_more_compatible(*args, **kwargs):\n", "def download_main(download, download_playlist, urls, playlist, **kwargs):\n", "def load_cookies(cookiefile):\n", "def set_socks_proxy(proxy):\n", "def script_main(download, download_playlist, **kwargs):\n", "    def print_version():\n", "def google_search(url):\n", "def url_to_module(url):\n", "def any_download(url, **kwargs):\n", "def any_download_playlist(url, **kwargs):\n", "def main(**kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/showroom.py", "func_name": "showroom_get_roomid_by_room_url_key", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "str->str", "docstring_tokens": ["str", "-", ">", "str"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L11-L24", "partition": "test", "up_fun_num": 0, "context": "#!/usr/bin/env python\n\n__all__ = [\"showroom_download\"]\n\nfrom ..common import *\nimport urllib.error\nfrom json import loads\nfrom time import time, sleep\n\n# ----------------------------------------------------------------------\n\n\ndef showroom_download_by_room_id(\n    room_id, output_dir=\".\", merge=False, info_only=False, **kwargs\n):\n    \"\"\"Source: Android mobile\"\"\"\n    while True:\n        timestamp = str(int(time() * 1000))\n        api_endpoint = \"https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}\".format(\n            room_id=room_id, timestamp=timestamp\n        )\n        html = get_content(api_endpoint)\n        html = json.loads(html)\n        # {'streaming_url_list': [{'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 1, 'label': 'original spec(low latency)', 'is_default': True, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed/playlist.m3u8', 'is_default': True, 'id': 2, 'type': 'hls', 'label': 'original spec'}, {'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 3, 'label': 'low spec(low latency)', 'is_default': False, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low/playlist.m3u8', 'is_default': False, 'id': 4, 'type': 'hls', 'label': 'low spec'}]}\n        if len(html) >= 1:\n            break\n        log.w(\"The live show is currently offline.\")\n        sleep(1)\n\n    # This is mainly for testing the M3U FFmpeg parser so I would ignore any non-m3u ones\n    stream_url = [\n        i[\"url\"]\n        for i in html[\"streaming_url_list\"]\n        if i[\"is_default\"] and i[\"type\"] == \"hls\"\n    ][0]\n\n    assert stream_url\n\n    # title\n    title = \"\"\n    profile_api = (\n        \"https://www.showroom-live.com/api/room/profile?room_id={room_id}\".format(\n            room_id=room_id\n        )\n    )\n    html = loads(get_content(profile_api))\n    try:\n        title = html[\"main_name\"]\n    except KeyError:\n        title = \"Showroom_{room_id}\".format(room_id=room_id)\n\n    type_, ext, size = url_info(stream_url)\n    print_info(site_info, title, type_, size)\n    if not info_only:\n        download_url_ffmpeg(\n            url=stream_url, title=title, ext=\"mp4\", output_dir=output_dir\n        )\n\n\n# ----------------------------------------------------------------------\ndef showroom_download(url, output_dir=\".\", merge=False, info_only=False, **kwargs):\n    \"\"\"\"\"\"\n    if re.match(r\"(\\w+)://www.showroom-live.com/([-\\w]+)\", url):\n        room_url_key = match1(url, r\"\\w+://www.showroom-live.com/([-\\w]+)\")\n        room_id = showroom_get_roomid_by_room_url_key(room_url_key)\n        showroom_download_by_room_id(room_id, output_dir, merge, info_only)\n\n\nsite_info = \"Showroom\"\ndownload = showroom_download\ndownload_playlist = playlist_not_supported(\"showroom\")\n", "levels": [0], "package": ["from ..common import *", "import urllib.error", "from json import loads", "from time import time, sleep"], "function": ["def showroom_download(url, output_dir=\".\", merge=False, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/wanmen.py", "func_name": "_wanmen_get_title_by_json_topic_part", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.", "docstring_tokens": ["JSON", "int", "int", "int", "-", ">", "str", "Get", "a", "proper", "title", "with", "courseid", "+", "topicID", "+", "partID", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L18-L25", "partition": "test", "up_fun_num": 1, "context": "#!/usr/bin/env python\n\n__all__ = [\n    \"wanmen_download\",\n    \"wanmen_download_by_course\",\n    \"wanmen_download_by_course_topic\",\n    \"wanmen_download_by_course_topic_part\",\n]\n\nfrom ..common import *\nfrom .bokecc import bokecc_download_by_id\nfrom json import loads\n\n\n##Helper functions\ndef _wanmen_get_json_api_content_by_courseID(courseID):\n    \"\"\"int->JSON\n\n    Return a parsed JSON tree of WanMen's API.\"\"\"\n\n    return loads(\n        get_content(\n            \"http://api.wanmen.org/course/getCourseNested/{courseID}\".format(\n                courseID=courseID\n            )\n        )\n    )\n\n\ndef _wanmen_get_boke_id_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n\n    Get one BokeCC video ID with courseid+topicID+partID.\"\"\"\n\n    return json_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"ccVideoLink\"]\n\n\n##Parsers\ndef wanmen_download_by_course(\n    json_api_content, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    \"\"\"int->None\n\n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0][\"Topics\"])):\n        for pIndex in range(len(json_api_content[0][\"Topics\"][tIndex][\"Parts\"])):\n            wanmen_download_by_course_topic_part(\n                json_api_content,\n                tIndex,\n                pIndex,\n                output_dir=output_dir,\n                merge=merge,\n                info_only=info_only,\n                **kwargs\n            )\n\n\ndef wanmen_download_by_course_topic(\n    json_api_content, tIndex, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    \"\"\"int, int->None\n\n    Download a TOPIC of a course.\n    Reuse the API call to save time.\"\"\"\n\n    for pIndex in range(len(json_api_content[0][\"Topics\"][tIndex][\"Parts\"])):\n        wanmen_download_by_course_topic_part(\n            json_api_content,\n            tIndex,\n            pIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n            **kwargs\n        )\n\n\ndef wanmen_download_by_course_topic_part(\n    json_api_content,\n    tIndex,\n    pIndex,\n    output_dir=\".\",\n    merge=True,\n    info_only=False,\n    **kwargs\n):\n    \"\"\"int, int, int->None\n\n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, tIndex, pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html, tIndex, pIndex)\n\n    bokecc_download_by_id(\n        vid=bokeccID,\n        title=title,\n        output_dir=output_dir,\n        merge=merge,\n        info_only=info_only,\n        **kwargs\n    )\n\n\n##Main entrance\ndef wanmen_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n\n    if not \"wanmen.org\" in url:\n        log.wtf(\"You are at the wrong place dude. This is for WanMen University!\")\n        raise\n\n    courseID = int(match1(url, r\"course\\/(\\d+)\"))\n    assert courseID > 0  # without courseID we cannot do anything\n\n    tIndex = int(match1(url, r\"tIndex=(\\d+)\"))\n\n    pIndex = int(match1(url, r\"pIndex=(\\d+)\"))\n\n    json_api_content = _wanmen_get_json_api_content_by_courseID(courseID)\n\n    if pIndex:  # only download ONE single part\n        assert tIndex >= 0\n        wanmen_download_by_course_topic_part(\n            json_api_content,\n            tIndex,\n            pIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n        )\n    elif tIndex:  # download a topic\n        wanmen_download_by_course_topic(\n            json_api_content,\n            tIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n        )\n    else:  # download the whole course\n        wanmen_download_by_course(\n            json_api_content, output_dir=output_dir, merge=merge, info_only=info_only\n        )\n\n\nsite_info = \"WanMen University\"\ndownload = wanmen_download\ndownload_playlist = wanmen_download_by_course\n", "levels": [0, 0, 0], "package": ["from ..common import *", "from .bokecc import bokecc_download_by_id", "from json import loads"], "function": ["def _wanmen_get_json_api_content_by_courseID(courseID):\n", "def _wanmen_get_boke_id_by_json_topic_part(json_content, tIndex, pIndex):\n", "def wanmen_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/wanmen.py", "func_name": "wanmen_download_by_course", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.", "docstring_tokens": ["int", "-", ">", "None", "Download", "a", "WHOLE", "course", ".", "Reuse", "the", "API", "call", "to", "save", "time", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L37-L51", "partition": "test", "up_fun_num": 3, "context": "#!/usr/bin/env python\n\n__all__ = [\n    \"wanmen_download\",\n    \"wanmen_download_by_course\",\n    \"wanmen_download_by_course_topic\",\n    \"wanmen_download_by_course_topic_part\",\n]\n\nfrom ..common import *\nfrom .bokecc import bokecc_download_by_id\nfrom json import loads\n\n\n##Helper functions\ndef _wanmen_get_json_api_content_by_courseID(courseID):\n    \"\"\"int->JSON\n\n    Return a parsed JSON tree of WanMen's API.\"\"\"\n\n    return loads(\n        get_content(\n            \"http://api.wanmen.org/course/getCourseNested/{courseID}\".format(\n                courseID=courseID\n            )\n        )\n    )\n\n\ndef _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n\n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return \"_\".join(\n        [\n            json_content[0][\"name\"],\n            json_content[0][\"Topics\"][tIndex][\"name\"],\n            json_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"name\"],\n        ]\n    )\n\n\ndef _wanmen_get_boke_id_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n\n    Get one BokeCC video ID with courseid+topicID+partID.\"\"\"\n\n    return json_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"ccVideoLink\"]\n\n\n##Parsers\n\n\ndef wanmen_download_by_course_topic(\n    json_api_content, tIndex, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    \"\"\"int, int->None\n\n    Download a TOPIC of a course.\n    Reuse the API call to save time.\"\"\"\n\n    for pIndex in range(len(json_api_content[0][\"Topics\"][tIndex][\"Parts\"])):\n        wanmen_download_by_course_topic_part(\n            json_api_content,\n            tIndex,\n            pIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n            **kwargs\n        )\n\n\ndef wanmen_download_by_course_topic_part(\n    json_api_content,\n    tIndex,\n    pIndex,\n    output_dir=\".\",\n    merge=True,\n    info_only=False,\n    **kwargs\n):\n    \"\"\"int, int, int->None\n\n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, tIndex, pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html, tIndex, pIndex)\n\n    bokecc_download_by_id(\n        vid=bokeccID,\n        title=title,\n        output_dir=output_dir,\n        merge=merge,\n        info_only=info_only,\n        **kwargs\n    )\n\n\n##Main entrance\ndef wanmen_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n\n    if not \"wanmen.org\" in url:\n        log.wtf(\"You are at the wrong place dude. This is for WanMen University!\")\n        raise\n\n    courseID = int(match1(url, r\"course\\/(\\d+)\"))\n    assert courseID > 0  # without courseID we cannot do anything\n\n    tIndex = int(match1(url, r\"tIndex=(\\d+)\"))\n\n    pIndex = int(match1(url, r\"pIndex=(\\d+)\"))\n\n    json_api_content = _wanmen_get_json_api_content_by_courseID(courseID)\n\n    if pIndex:  # only download ONE single part\n        assert tIndex >= 0\n        wanmen_download_by_course_topic_part(\n            json_api_content,\n            tIndex,\n            pIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n        )\n    elif tIndex:  # download a topic\n        wanmen_download_by_course_topic(\n            json_api_content,\n            tIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n        )\n    else:  # download the whole course\n        wanmen_download_by_course(\n            json_api_content, output_dir=output_dir, merge=merge, info_only=info_only\n        )\n\n\nsite_info = \"WanMen University\"\ndownload = wanmen_download\ndownload_playlist = wanmen_download_by_course\n", "levels": [0, 0, 0, 0], "package": ["from ..common import *", "from .bokecc import bokecc_download_by_id", "from json import loads"], "function": ["def _wanmen_get_json_api_content_by_courseID(courseID):\n", "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n", "def _wanmen_get_boke_id_by_json_topic_part(json_content, tIndex, pIndex):\n", "def wanmen_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "soimort/you-get", "path": "src/you_get/extractors/wanmen.py", "func_name": "wanmen_download_by_course_topic_part", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "int, int, int->None\n    \n    Download ONE PART of the course.", "docstring_tokens": ["int", "int", "int", "-", ">", "None", "Download", "ONE", "PART", "of", "the", "course", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L69-L84", "partition": "test", "up_fun_num": 5, "context": "#!/usr/bin/env python\n\n__all__ = [\n    \"wanmen_download\",\n    \"wanmen_download_by_course\",\n    \"wanmen_download_by_course_topic\",\n    \"wanmen_download_by_course_topic_part\",\n]\n\nfrom ..common import *\nfrom .bokecc import bokecc_download_by_id\nfrom json import loads\n\n\n##Helper functions\ndef _wanmen_get_json_api_content_by_courseID(courseID):\n    \"\"\"int->JSON\n\n    Return a parsed JSON tree of WanMen's API.\"\"\"\n\n    return loads(\n        get_content(\n            \"http://api.wanmen.org/course/getCourseNested/{courseID}\".format(\n                courseID=courseID\n            )\n        )\n    )\n\n\ndef _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n\n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return \"_\".join(\n        [\n            json_content[0][\"name\"],\n            json_content[0][\"Topics\"][tIndex][\"name\"],\n            json_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"name\"],\n        ]\n    )\n\n\ndef _wanmen_get_boke_id_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n\n    Get one BokeCC video ID with courseid+topicID+partID.\"\"\"\n\n    return json_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"ccVideoLink\"]\n\n\n##Parsers\ndef wanmen_download_by_course(\n    json_api_content, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    \"\"\"int->None\n\n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0][\"Topics\"])):\n        for pIndex in range(len(json_api_content[0][\"Topics\"][tIndex][\"Parts\"])):\n            wanmen_download_by_course_topic_part(\n                json_api_content,\n                tIndex,\n                pIndex,\n                output_dir=output_dir,\n                merge=merge,\n                info_only=info_only,\n                **kwargs\n            )\n\n\ndef wanmen_download_by_course_topic(\n    json_api_content, tIndex, output_dir=\".\", merge=True, info_only=False, **kwargs\n):\n    \"\"\"int, int->None\n\n    Download a TOPIC of a course.\n    Reuse the API call to save time.\"\"\"\n\n    for pIndex in range(len(json_api_content[0][\"Topics\"][tIndex][\"Parts\"])):\n        wanmen_download_by_course_topic_part(\n            json_api_content,\n            tIndex,\n            pIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n            **kwargs\n        )\n\n\n##Main entrance\ndef wanmen_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n\n    if not \"wanmen.org\" in url:\n        log.wtf(\"You are at the wrong place dude. This is for WanMen University!\")\n        raise\n\n    courseID = int(match1(url, r\"course\\/(\\d+)\"))\n    assert courseID > 0  # without courseID we cannot do anything\n\n    tIndex = int(match1(url, r\"tIndex=(\\d+)\"))\n\n    pIndex = int(match1(url, r\"pIndex=(\\d+)\"))\n\n    json_api_content = _wanmen_get_json_api_content_by_courseID(courseID)\n\n    if pIndex:  # only download ONE single part\n        assert tIndex >= 0\n        wanmen_download_by_course_topic_part(\n            json_api_content,\n            tIndex,\n            pIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n        )\n    elif tIndex:  # download a topic\n        wanmen_download_by_course_topic(\n            json_api_content,\n            tIndex,\n            output_dir=output_dir,\n            merge=merge,\n            info_only=info_only,\n        )\n    else:  # download the whole course\n        wanmen_download_by_course(\n            json_api_content, output_dir=output_dir, merge=merge, info_only=info_only\n        )\n\n\nsite_info = \"WanMen University\"\ndownload = wanmen_download\ndownload_playlist = wanmen_download_by_course\n", "levels": [0, 0, 0, 0], "package": ["from ..common import *", "from .bokecc import bokecc_download_by_id", "from json import loads"], "function": ["def _wanmen_get_json_api_content_by_courseID(courseID):\n", "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n", "def _wanmen_get_boke_id_by_json_topic_part(json_content, tIndex, pIndex):\n", "def wanmen_download(url, output_dir=\".\", merge=True, info_only=False, **kwargs):\n"]}
{"repo": "apache/airflow", "path": "airflow/executors/base_executor.py", "func_name": "BaseExecutor.has_task", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor", "docstring_tokens": ["Checks", "if", "a", "task", "is", "either", "queued", "or", "running", "in", "this", "executor"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L97-L105", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import range\nfrom collections import OrderedDict\n\n# To avoid circular imports\nimport airflow.utils.dag_processing\nfrom airflow import configuration\nfrom airflow.stats import Stats\nfrom airflow.utils.log.logging_mixin import LoggingMixin\nfrom airflow.utils.state import State\n\nPARALLELISM = configuration.conf.getint(\"core\", \"PARALLELISM\")\n\n\nclass BaseExecutor(LoggingMixin):\n    def __init__(self, parallelism=PARALLELISM):\n        \"\"\"\n        Class to derive in order to interface with executor-type systems\n        like Celery, Yarn and the likes.\n\n        :param parallelism: how many jobs should run at one time. Set to\n            ``0`` for infinity\n        :type parallelism: int\n        \"\"\"\n        self.parallelism = parallelism\n        self.queued_tasks = OrderedDict()\n        self.running = {}\n        self.event_buffer = {}\n\n    def start(self):  # pragma: no cover\n        \"\"\"\n        Executors may need to get things started. For example LocalExecutor\n        starts N workers.\n        \"\"\"\n        pass\n\n    def queue_command(self, simple_task_instance, command, priority=1, queue=None):\n        key = simple_task_instance.key\n        if key not in self.queued_tasks and key not in self.running:\n            self.log.info(\"Adding to queue: %s\", command)\n            self.queued_tasks[key] = (command, priority, queue, simple_task_instance)\n        else:\n            self.log.info(\"could not queue task %s\", key)\n\n    def queue_task_instance(\n        self,\n        task_instance,\n        mark_success=False,\n        pickle_id=None,\n        ignore_all_deps=False,\n        ignore_depends_on_past=False,\n        ignore_task_deps=False,\n        ignore_ti_state=False,\n        pool=None,\n        cfg_path=None,\n    ):\n        pool = pool or task_instance.pool\n\n        # TODO (edgarRd): AIRFLOW-1985:\n        # cfg_path is needed to propagate the config values if using impersonation\n        # (run_as_user), given that there are different code paths running tasks.\n        # For a long term solution we need to address AIRFLOW-1986\n        command = task_instance.command_as_list(\n            local=True,\n            mark_success=mark_success,\n            ignore_all_deps=ignore_all_deps,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state,\n            pool=pool,\n            pickle_id=pickle_id,\n            cfg_path=cfg_path,\n        )\n        self.queue_command(\n            airflow.utils.dag_processing.SimpleTaskInstance(task_instance),\n            command,\n            priority=task_instance.task.priority_weight_total,\n            queue=task_instance.task.queue,\n        )\n\n    def sync(self):\n        \"\"\"\n        Sync will get called periodically by the heartbeat method.\n        Executors should override this to perform gather statuses.\n        \"\"\"\n        pass\n\n    def heartbeat(self):\n        # Triggering new jobs\n        if not self.parallelism:\n            open_slots = len(self.queued_tasks)\n        else:\n            open_slots = self.parallelism - len(self.running)\n\n        num_running_tasks = len(self.running)\n        num_queued_tasks = len(self.queued_tasks)\n\n        self.log.debug(\"%s running task instances\", num_running_tasks)\n        self.log.debug(\"%s in queue\", num_queued_tasks)\n        self.log.debug(\"%s open slots\", open_slots)\n\n        Stats.gauge(\"executor.open_slots\", open_slots)\n        Stats.gauge(\"executor.queued_tasks\", num_queued_tasks)\n        Stats.gauge(\"executor.running_tasks\", num_running_tasks)\n\n        sorted_queue = sorted(\n            [(k, v) for k, v in self.queued_tasks.items()],\n            key=lambda x: x[1][1],\n            reverse=True,\n        )\n        for i in range(min((open_slots, len(self.queued_tasks)))):\n            key, (command, _, queue, simple_ti) = sorted_queue.pop(0)\n            self.queued_tasks.pop(key)\n            self.running[key] = command\n            self.execute_async(\n                key=key,\n                command=command,\n                queue=queue,\n                executor_config=simple_ti.executor_config,\n            )\n\n        # Calling child class sync method\n        self.log.debug(\"Calling the %s sync method\", self.__class__)\n        self.sync()\n\n    def change_state(self, key, state):\n        self.log.debug(\"Changing state: %s\", key)\n        self.running.pop(key, None)\n        self.event_buffer[key] = state\n\n    def fail(self, key):\n        self.change_state(key, State.FAILED)\n\n    def success(self, key):\n        self.change_state(key, State.SUCCESS)\n\n    def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events\n\n    def execute_async(\n        self, key, command, queue=None, executor_config=None\n    ):  # pragma: no cover\n        \"\"\"\n        This method will execute the command asynchronously.\n        \"\"\"\n        raise NotImplementedError()\n\n    def end(self):  # pragma: no cover\n        \"\"\"\n        This method is called when the caller is done submitting job and\n        wants to wait synchronously for the job submitted previously to be\n        all done.\n        \"\"\"\n        raise NotImplementedError()\n\n    def terminate(self):\n        \"\"\"\n        This method is called when the daemon receives a SIGTERM\n        \"\"\"\n        raise NotImplementedError()\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import range", "from collections import OrderedDict", "import airflow.utils.dag_processing", "from airflow import configuration", "from airflow.stats import Stats", "from airflow.utils.log.logging_mixin import LoggingMixin", "from airflow.utils.state import State"], "function": ["class BaseExecutor(LoggingMixin):\n", "    def __init__(self, parallelism=PARALLELISM):\n", "    def queue_command(self, simple_task_instance, command, priority=1, queue=None):\n", "    def sync(self):\n", "    def heartbeat(self):\n", "    def change_state(self, key, state):\n", "    def fail(self, key):\n", "    def success(self, key):\n", "    def get_event_buffer(self, dag_ids=None):\n", "    def terminate(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/executors/base_executor.py", "func_name": "BaseExecutor.get_event_buffer", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events", "docstring_tokens": ["Returns", "and", "flush", "the", "event", "buffer", ".", "In", "case", "dag_ids", "is", "specified", "it", "will", "only", "return", "and", "flush", "events", "for", "the", "given", "dag_ids", ".", "Otherwise", "it", "returns", "and", "flushes", "all"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L160-L179", "partition": "test", "up_fun_num": 10, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import range\nfrom collections import OrderedDict\n\n# To avoid circular imports\nimport airflow.utils.dag_processing\nfrom airflow import configuration\nfrom airflow.stats import Stats\nfrom airflow.utils.log.logging_mixin import LoggingMixin\nfrom airflow.utils.state import State\n\nPARALLELISM = configuration.conf.getint(\"core\", \"PARALLELISM\")\n\n\nclass BaseExecutor(LoggingMixin):\n    def __init__(self, parallelism=PARALLELISM):\n        \"\"\"\n        Class to derive in order to interface with executor-type systems\n        like Celery, Yarn and the likes.\n\n        :param parallelism: how many jobs should run at one time. Set to\n            ``0`` for infinity\n        :type parallelism: int\n        \"\"\"\n        self.parallelism = parallelism\n        self.queued_tasks = OrderedDict()\n        self.running = {}\n        self.event_buffer = {}\n\n    def start(self):  # pragma: no cover\n        \"\"\"\n        Executors may need to get things started. For example LocalExecutor\n        starts N workers.\n        \"\"\"\n        pass\n\n    def queue_command(self, simple_task_instance, command, priority=1, queue=None):\n        key = simple_task_instance.key\n        if key not in self.queued_tasks and key not in self.running:\n            self.log.info(\"Adding to queue: %s\", command)\n            self.queued_tasks[key] = (command, priority, queue, simple_task_instance)\n        else:\n            self.log.info(\"could not queue task %s\", key)\n\n    def queue_task_instance(\n        self,\n        task_instance,\n        mark_success=False,\n        pickle_id=None,\n        ignore_all_deps=False,\n        ignore_depends_on_past=False,\n        ignore_task_deps=False,\n        ignore_ti_state=False,\n        pool=None,\n        cfg_path=None,\n    ):\n        pool = pool or task_instance.pool\n\n        # TODO (edgarRd): AIRFLOW-1985:\n        # cfg_path is needed to propagate the config values if using impersonation\n        # (run_as_user), given that there are different code paths running tasks.\n        # For a long term solution we need to address AIRFLOW-1986\n        command = task_instance.command_as_list(\n            local=True,\n            mark_success=mark_success,\n            ignore_all_deps=ignore_all_deps,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state,\n            pool=pool,\n            pickle_id=pickle_id,\n            cfg_path=cfg_path,\n        )\n        self.queue_command(\n            airflow.utils.dag_processing.SimpleTaskInstance(task_instance),\n            command,\n            priority=task_instance.task.priority_weight_total,\n            queue=task_instance.task.queue,\n        )\n\n    def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True\n\n    def sync(self):\n        \"\"\"\n        Sync will get called periodically by the heartbeat method.\n        Executors should override this to perform gather statuses.\n        \"\"\"\n        pass\n\n    def heartbeat(self):\n        # Triggering new jobs\n        if not self.parallelism:\n            open_slots = len(self.queued_tasks)\n        else:\n            open_slots = self.parallelism - len(self.running)\n\n        num_running_tasks = len(self.running)\n        num_queued_tasks = len(self.queued_tasks)\n\n        self.log.debug(\"%s running task instances\", num_running_tasks)\n        self.log.debug(\"%s in queue\", num_queued_tasks)\n        self.log.debug(\"%s open slots\", open_slots)\n\n        Stats.gauge(\"executor.open_slots\", open_slots)\n        Stats.gauge(\"executor.queued_tasks\", num_queued_tasks)\n        Stats.gauge(\"executor.running_tasks\", num_running_tasks)\n\n        sorted_queue = sorted(\n            [(k, v) for k, v in self.queued_tasks.items()],\n            key=lambda x: x[1][1],\n            reverse=True,\n        )\n        for i in range(min((open_slots, len(self.queued_tasks)))):\n            key, (command, _, queue, simple_ti) = sorted_queue.pop(0)\n            self.queued_tasks.pop(key)\n            self.running[key] = command\n            self.execute_async(\n                key=key,\n                command=command,\n                queue=queue,\n                executor_config=simple_ti.executor_config,\n            )\n\n        # Calling child class sync method\n        self.log.debug(\"Calling the %s sync method\", self.__class__)\n        self.sync()\n\n    def change_state(self, key, state):\n        self.log.debug(\"Changing state: %s\", key)\n        self.running.pop(key, None)\n        self.event_buffer[key] = state\n\n    def fail(self, key):\n        self.change_state(key, State.FAILED)\n\n    def success(self, key):\n        self.change_state(key, State.SUCCESS)\n\n    def execute_async(\n        self, key, command, queue=None, executor_config=None\n    ):  # pragma: no cover\n        \"\"\"\n        This method will execute the command asynchronously.\n        \"\"\"\n        raise NotImplementedError()\n\n    def end(self):  # pragma: no cover\n        \"\"\"\n        This method is called when the caller is done submitting job and\n        wants to wait synchronously for the job submitted previously to be\n        all done.\n        \"\"\"\n        raise NotImplementedError()\n\n    def terminate(self):\n        \"\"\"\n        This method is called when the daemon receives a SIGTERM\n        \"\"\"\n        raise NotImplementedError()\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import range", "from collections import OrderedDict", "import airflow.utils.dag_processing", "from airflow import configuration", "from airflow.stats import Stats", "from airflow.utils.log.logging_mixin import LoggingMixin", "from airflow.utils.state import State"], "function": ["class BaseExecutor(LoggingMixin):\n", "    def __init__(self, parallelism=PARALLELISM):\n", "    def queue_command(self, simple_task_instance, command, priority=1, queue=None):\n", "    def has_task(self, task_instance):\n", "    def sync(self):\n", "    def heartbeat(self):\n", "    def change_state(self, key, state):\n", "    def fail(self, key):\n", "    def success(self, key):\n", "    def terminate(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/snowflake_hook.py", "func_name": "SnowflakeHook.get_conn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns a snowflake.connection object", "docstring_tokens": ["Returns", "a", "snowflake", ".", "connection", "object"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L107-L113", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport snowflake.connector\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass SnowflakeHook(DbApiHook):\n    \"\"\"\n    Interact with Snowflake.\n\n    get_sqlalchemy_engine() depends on snowflake-sqlalchemy\n\n    \"\"\"\n\n    conn_name_attr = \"snowflake_conn_id\"\n    default_conn_name = \"snowflake_default\"\n    supports_autocommit = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.account = kwargs.pop(\"account\", None)\n        self.warehouse = kwargs.pop(\"warehouse\", None)\n        self.database = kwargs.pop(\"database\", None)\n        self.region = kwargs.pop(\"region\", None)\n        self.role = kwargs.pop(\"role\", None)\n\n    def _get_conn_params(self):\n        \"\"\"\n        one method to fetch connection params as a dict\n        used in get_uri() and get_connection()\n        \"\"\"\n        conn = self.get_connection(self.snowflake_conn_id)\n        account = conn.extra_dejson.get(\"account\", None)\n        warehouse = conn.extra_dejson.get(\"warehouse\", None)\n        database = conn.extra_dejson.get(\"database\", None)\n        region = conn.extra_dejson.get(\"region\", None)\n        role = conn.extra_dejson.get(\"role\", None)\n\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or \"\",\n            \"schema\": conn.schema or \"\",\n            \"database\": self.database or database or \"\",\n            \"account\": self.account or account or \"\",\n            \"warehouse\": self.warehouse or warehouse or \"\",\n            \"region\": self.region or region or \"\",\n            \"role\": self.role or role or \"\",\n        }\n\n        \"\"\"\n        If private_key_file is specified in the extra json, load the contents of the file as a private\n        key and specify that in the connection configuration. The connection password then becomes the\n        passphrase for the private key. If your private key file is not encrypted (not recommended), then\n        leave the password empty.\n        \"\"\"\n        private_key_file = conn.extra_dejson.get(\"private_key_file\", None)\n        if private_key_file:\n            with open(private_key_file, \"rb\") as key:\n                passphrase = None\n                if conn.password:\n                    passphrase = conn.password.strip().encode()\n\n                p_key = serialization.load_pem_private_key(\n                    key.read(), password=passphrase, backend=default_backend()\n                )\n\n            pkb = p_key.private_bytes(\n                encoding=serialization.Encoding.DER,\n                format=serialization.PrivateFormat.PKCS8,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n\n            conn_config[\"private_key\"] = pkb\n            conn_config.pop(\"password\", None)\n\n        return conn_config\n\n    def get_uri(self):\n        \"\"\"\n        override DbApiHook get_uri method for get_sqlalchemy_engine()\n        \"\"\"\n        conn_config = self._get_conn_params()\n        uri = \"snowflake://{user}:{password}@{account}/{database}/\"\n        uri += \"{schema}?warehouse={warehouse}&role={role}\"\n        return uri.format(**conn_config)\n\n    def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if \"aws_secret_access_key\" in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    \"aws_access_key_id\"\n                )\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    \"aws_secret_access_key\"\n                )\n        return aws_access_key_id, aws_secret_access_key\n\n    def set_autocommit(self, conn, autocommit):\n        conn.autocommit(autocommit)\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["import snowflake.connector", "from cryptography.hazmat.backends import default_backend", "from cryptography.hazmat.primitives import serialization", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class SnowflakeHook(DbApiHook):\n", "    def __init__(self, *args, **kwargs):\n", "    def _get_conn_params(self):\n", "    def get_uri(self):\n", "    def _get_aws_credentials(self):\n", "    def set_autocommit(self, conn, autocommit):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/snowflake_hook.py", "func_name": "SnowflakeHook._get_aws_credentials", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements", "docstring_tokens": ["returns", "aws_access_key_id", "aws_secret_access_key", "from", "extra"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L115-L129", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport snowflake.connector\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass SnowflakeHook(DbApiHook):\n    \"\"\"\n    Interact with Snowflake.\n\n    get_sqlalchemy_engine() depends on snowflake-sqlalchemy\n\n    \"\"\"\n\n    conn_name_attr = \"snowflake_conn_id\"\n    default_conn_name = \"snowflake_default\"\n    supports_autocommit = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.account = kwargs.pop(\"account\", None)\n        self.warehouse = kwargs.pop(\"warehouse\", None)\n        self.database = kwargs.pop(\"database\", None)\n        self.region = kwargs.pop(\"region\", None)\n        self.role = kwargs.pop(\"role\", None)\n\n    def _get_conn_params(self):\n        \"\"\"\n        one method to fetch connection params as a dict\n        used in get_uri() and get_connection()\n        \"\"\"\n        conn = self.get_connection(self.snowflake_conn_id)\n        account = conn.extra_dejson.get(\"account\", None)\n        warehouse = conn.extra_dejson.get(\"warehouse\", None)\n        database = conn.extra_dejson.get(\"database\", None)\n        region = conn.extra_dejson.get(\"region\", None)\n        role = conn.extra_dejson.get(\"role\", None)\n\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or \"\",\n            \"schema\": conn.schema or \"\",\n            \"database\": self.database or database or \"\",\n            \"account\": self.account or account or \"\",\n            \"warehouse\": self.warehouse or warehouse or \"\",\n            \"region\": self.region or region or \"\",\n            \"role\": self.role or role or \"\",\n        }\n\n        \"\"\"\n        If private_key_file is specified in the extra json, load the contents of the file as a private\n        key and specify that in the connection configuration. The connection password then becomes the\n        passphrase for the private key. If your private key file is not encrypted (not recommended), then\n        leave the password empty.\n        \"\"\"\n        private_key_file = conn.extra_dejson.get(\"private_key_file\", None)\n        if private_key_file:\n            with open(private_key_file, \"rb\") as key:\n                passphrase = None\n                if conn.password:\n                    passphrase = conn.password.strip().encode()\n\n                p_key = serialization.load_pem_private_key(\n                    key.read(), password=passphrase, backend=default_backend()\n                )\n\n            pkb = p_key.private_bytes(\n                encoding=serialization.Encoding.DER,\n                format=serialization.PrivateFormat.PKCS8,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n\n            conn_config[\"private_key\"] = pkb\n            conn_config.pop(\"password\", None)\n\n        return conn_config\n\n    def get_uri(self):\n        \"\"\"\n        override DbApiHook get_uri method for get_sqlalchemy_engine()\n        \"\"\"\n        conn_config = self._get_conn_params()\n        uri = \"snowflake://{user}:{password}@{account}/{database}/\"\n        uri += \"{schema}?warehouse={warehouse}&role={role}\"\n        return uri.format(**conn_config)\n\n    def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn\n\n    def set_autocommit(self, conn, autocommit):\n        conn.autocommit(autocommit)\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["import snowflake.connector", "from cryptography.hazmat.backends import default_backend", "from cryptography.hazmat.primitives import serialization", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class SnowflakeHook(DbApiHook):\n", "    def __init__(self, *args, **kwargs):\n", "    def _get_conn_params(self):\n", "    def get_uri(self):\n", "    def get_conn(self):\n", "    def set_autocommit(self, conn, autocommit):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/grpc_hook.py", "func_name": "GrpcHook._get_field", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.", "docstring_tokens": ["Fetches", "a", "field", "from", "extras", "and", "returns", "it", ".", "This", "is", "some", "Airflow", "magic", ".", "The", "grpc", "hook", "type", "adds", "custom", "UI", "elements", "to", "the", "hook", "page", "which", "allow", "admins", "to", "specify", "scopes", "credential", "pem", "files", "etc", ".", "They", "get", "formatted", "as", "shown", "below", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/grpc_hook.py#L112-L123", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport grpc\nfrom google import auth as google_auth\nfrom google.auth import jwt as google_auth_jwt\nfrom google.auth.transport import grpc as google_auth_transport_grpc\nfrom google.auth.transport import requests as google_auth_transport_requests\n\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.exceptions import AirflowConfigException\n\n\nclass GrpcHook(BaseHook):\n    \"\"\"\n    General interaction with gRPC servers.\n    \"\"\"\n\n    def __init__(self, grpc_conn_id, interceptors=None, custom_connection_func=None):\n        \"\"\"\n        :param grpc_conn_id: The connection ID to use when fetching connection info.\n        :type grpc_conn_id: str\n        :param interceptors: a list of gRPC interceptor objects which would be applied\n            to the connected gRPC channel. None by default.\n        :type interceptors: a list of gRPC interceptors based on or extends the four\n            official gRPC interceptors, eg, UnaryUnaryClientInterceptor,\n            UnaryStreamClientInterceptor, StreamUnaryClientInterceptor,\n            StreamStreamClientInterceptor.\n        ::param custom_connection_func: The customized connection function to return gRPC channel.\n        :type custom_connection_func: python callable objects that accept the connection as\n            its only arg. Could be partial or lambda.\n        \"\"\"\n        self.grpc_conn_id = grpc_conn_id\n        self.conn = self.get_connection(self.grpc_conn_id)\n        self.extras = self.conn.extra_dejson\n        self.interceptors = interceptors if interceptors else []\n        self.custom_connection_func = custom_connection_func\n\n    def get_conn(self):\n        base_url = self.conn.host\n\n        if self.conn.port:\n            base_url = base_url + \":\" + str(self.conn.port)\n\n        auth_type = self._get_field(\"auth_type\")\n\n        if auth_type == \"NO_AUTH\":\n            channel = grpc.insecure_channel(base_url)\n        elif auth_type == \"SSL\" or auth_type == \"TLS\":\n            credential_file_name = self._get_field(\"credential_pem_file\")\n            creds = grpc.ssl_channel_credentials(open(credential_file_name).read())\n            channel = grpc.secure_channel(base_url, creds)\n        elif auth_type == \"JWT_GOOGLE\":\n            credentials, _ = google_auth.default()\n            jwt_creds = google_auth_jwt.OnDemandCredentials.from_signing_credentials(\n                credentials\n            )\n            channel = google_auth_transport_grpc.secure_authorized_channel(\n                jwt_creds, None, base_url\n            )\n        elif auth_type == \"OATH_GOOGLE\":\n            scopes = self._get_field(\"scopes\").split(\",\")\n            credentials, _ = google_auth.default(scopes=scopes)\n            request = google_auth_transport_requests.Request()\n            channel = google_auth_transport_grpc.secure_authorized_channel(\n                credentials, request, base_url\n            )\n        elif auth_type == \"CUSTOM\":\n            if not self.custom_connection_func:\n                raise AirflowConfigException(\n                    \"Customized connection function not set, not able to establish a channel\"\n                )\n            channel = self.custom_connection_func(self.conn)\n        else:\n            raise AirflowConfigException(\n                \"auth_type not supported or not provided, channel cannot be established,\\\n                given value: %s\"\n                % str(auth_type)\n            )\n\n        if self.interceptors:\n            for interceptor in self.interceptors:\n                channel = grpc.intercept_channel(channel, interceptor)\n\n        return channel\n\n    def run(self, stub_class, call_func, streaming=False, data={}):\n        with self.get_conn() as channel:\n            stub = stub_class(channel)\n            try:\n                rpc_func = getattr(stub, call_func)\n                response = rpc_func(**data)\n                if not streaming:\n                    yield response\n                else:\n                    for single_response in response:\n                        yield single_response\n            except grpc.RpcError as ex:\n                self.log.exception(\n                    \"Error occured when calling the grpc service: {0}, method: {1} \\\n                    status code: {2}, error details: {3}\".format(\n                        stub.__class__.__name__, call_func, ex.code(), ex.details()\n                    )\n                )\n                raise ex\n", "levels": [0, 1, 1, 1], "package": ["import grpc", "from google import auth as google_auth", "from google.auth import jwt as google_auth_jwt", "from google.auth.transport import grpc as google_auth_transport_grpc", "from google.auth.transport import requests as google_auth_transport_requests", "from airflow.hooks.base_hook import BaseHook", "from airflow.exceptions import AirflowConfigException"], "function": ["class GrpcHook(BaseHook):\n", "    def __init__(self, grpc_conn_id, interceptors=None, custom_connection_func=None):\n", "    def get_conn(self):\n", "    def run(self, stub_class, call_func, streaming=False, data={}):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/postgres_hook.py", "func_name": "PostgresHook.copy_expert", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.", "docstring_tokens": ["Executes", "SQL", "using", "psycopg2", "copy_expert", "method", ".", "Necessary", "to", "execute", "COPY", "command", "without", "access", "to", "a", "superuser", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L63-L83", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport psycopg2\nimport psycopg2.extensions\nfrom contextlib import closing\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PostgresHook(DbApiHook):\n    \"\"\"\n    Interact with Postgres.\n    You can specify ssl parameters in the extra field of your connection\n    as ``{\"sslmode\": \"require\", \"sslcert\": \"/path/to/cert.pem\", etc}``.\n\n    Note: For Redshift, use keepalives_idle in the extra connection parameters\n    and set it to less than 300 seconds.\n    \"\"\"\n\n    conn_name_attr = \"postgres_conn_id\"\n    default_conn_name = \"postgres_default\"\n    supports_autocommit = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.schema = kwargs.pop(\"schema\", None)\n\n    def get_conn(self):\n        conn = self.get_connection(self.postgres_conn_id)\n        conn_args = dict(\n            host=conn.host,\n            user=conn.login,\n            password=conn.password,\n            dbname=self.schema or conn.schema,\n            port=conn.port,\n        )\n        # check for ssl parameters in conn.extra\n        for arg_name, arg_val in conn.extra_dejson.items():\n            if arg_name in [\n                \"sslmode\",\n                \"sslcert\",\n                \"sslkey\",\n                \"sslrootcert\",\n                \"sslcrl\",\n                \"application_name\",\n                \"keepalives_idle\",\n            ]:\n                conn_args[arg_name] = arg_val\n\n        self.conn = psycopg2.connect(**conn_args)\n        return self.conn\n\n    def bulk_load(self, table, tmp_file):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        self.copy_expert(\"COPY {table} FROM STDIN\".format(table=table), tmp_file)\n\n    def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)\n\n    @staticmethod\n    def _serialize_cell(cell, conn):\n        \"\"\"\n        Postgresql will adapt all arguments to the execute() method internally,\n        hence we return cell without any conversion.\n\n        See http://initd.org/psycopg/docs/advanced.html#adapting-new-types for\n        more information.\n\n        :param cell: The cell to insert into the table\n        :type cell: object\n        :param conn: The database connection\n        :type conn: connection object\n        :return: The cell\n        :rtype: object\n        \"\"\"\n        return cell\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["import os", "import psycopg2", "import psycopg2.extensions", "from contextlib import closing", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PostgresHook(DbApiHook):\n", "    def __init__(self, *args, **kwargs):\n", "    def get_conn(self):\n", "    def bulk_load(self, table, tmp_file):\n", "    def bulk_dump(self, table, tmp_file):\n", "    def _serialize_cell(cell, conn):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/postgres_hook.py", "func_name": "PostgresHook.bulk_dump", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Dumps a database table into a tab-delimited file", "docstring_tokens": ["Dumps", "a", "database", "table", "into", "a", "tab", "-", "delimited", "file"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L91-L95", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport psycopg2\nimport psycopg2.extensions\nfrom contextlib import closing\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PostgresHook(DbApiHook):\n    \"\"\"\n    Interact with Postgres.\n    You can specify ssl parameters in the extra field of your connection\n    as ``{\"sslmode\": \"require\", \"sslcert\": \"/path/to/cert.pem\", etc}``.\n\n    Note: For Redshift, use keepalives_idle in the extra connection parameters\n    and set it to less than 300 seconds.\n    \"\"\"\n\n    conn_name_attr = \"postgres_conn_id\"\n    default_conn_name = \"postgres_default\"\n    supports_autocommit = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.schema = kwargs.pop(\"schema\", None)\n\n    def get_conn(self):\n        conn = self.get_connection(self.postgres_conn_id)\n        conn_args = dict(\n            host=conn.host,\n            user=conn.login,\n            password=conn.password,\n            dbname=self.schema or conn.schema,\n            port=conn.port,\n        )\n        # check for ssl parameters in conn.extra\n        for arg_name, arg_val in conn.extra_dejson.items():\n            if arg_name in [\n                \"sslmode\",\n                \"sslcert\",\n                \"sslkey\",\n                \"sslrootcert\",\n                \"sslcrl\",\n                \"application_name\",\n                \"keepalives_idle\",\n            ]:\n                conn_args[arg_name] = arg_val\n\n        self.conn = psycopg2.connect(**conn_args)\n        return self.conn\n\n    def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, \"w\"):\n                pass\n\n        with open(filename, \"r+\") as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()\n\n    def bulk_load(self, table, tmp_file):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        self.copy_expert(\"COPY {table} FROM STDIN\".format(table=table), tmp_file)\n\n    @staticmethod\n    def _serialize_cell(cell, conn):\n        \"\"\"\n        Postgresql will adapt all arguments to the execute() method internally,\n        hence we return cell without any conversion.\n\n        See http://initd.org/psycopg/docs/advanced.html#adapting-new-types for\n        more information.\n\n        :param cell: The cell to insert into the table\n        :type cell: object\n        :param conn: The database connection\n        :type conn: connection object\n        :return: The cell\n        :rtype: object\n        \"\"\"\n        return cell\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["import os", "import psycopg2", "import psycopg2.extensions", "from contextlib import closing", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PostgresHook(DbApiHook):\n", "    def __init__(self, *args, **kwargs):\n", "    def get_conn(self):\n", "    def copy_expert(self, sql, filename, open=open):\n", "    def bulk_load(self, table, tmp_file):\n", "    def _serialize_cell(cell, conn):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/operators/file_to_gcs.py", "func_name": "FileToGoogleCloudStorageOperator.execute", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Uploads the file to Google cloud storage", "docstring_tokens": ["Uploads", "the", "file", "to", "Google", "cloud", "storage"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_gcs.py#L68-L82", "partition": "test", "up_fun_num": 1, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\nfrom airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass FileToGoogleCloudStorageOperator(BaseOperator):\n    \"\"\"\n    Uploads a file to Google Cloud Storage.\n    Optionally can compress the file for upload.\n\n    :param src: Path to the local file. (templated)\n    :type src: str\n    :param dst: Destination path within the specified bucket. (templated)\n    :type dst: str\n    :param bucket: The bucket to upload to. (templated)\n    :type bucket: str\n    :param google_cloud_storage_conn_id: The Airflow connection ID to upload with\n    :type google_cloud_storage_conn_id: str\n    :param mime_type: The mime-type string\n    :type mime_type: str\n    :param delegate_to: The account to impersonate, if any\n    :type delegate_to: str\n    :param gzip: Allows for file to be compressed and uploaded as gzip\n    :type gzip: bool\n    \"\"\"\n\n    template_fields = (\"src\", \"dst\", \"bucket\")\n\n    @apply_defaults\n    def __init__(\n        self,\n        src,\n        dst,\n        bucket,\n        google_cloud_storage_conn_id=\"google_cloud_default\",\n        mime_type=\"application/octet-stream\",\n        delegate_to=None,\n        gzip=False,\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.src = src\n        self.dst = dst\n        self.bucket = bucket\n        self.google_cloud_storage_conn_id = google_cloud_storage_conn_id\n        self.mime_type = mime_type\n        self.delegate_to = delegate_to\n        self.gzip = gzip\n", "levels": [0], "package": ["from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook", "from airflow.models import BaseOperator", "from airflow.utils.decorators import apply_defaults"], "function": ["class FileToGoogleCloudStorageOperator(BaseOperator):\n"]}
{"repo": "apache/airflow", "path": "airflow/macros/hive.py", "func_name": "max_partition", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'", "docstring_tokens": ["Gets", "the", "max", "partition", "for", "a", "table", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L23-L55", "partition": "test", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport datetime\n\n\ndef _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()\n\n\ndef closest_ds_partition(\n    table, ds, before=True, schema=\"default\", metastore_conn_id=\"metastore_default\"\n):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param table: A hive table name\n    :type table: str\n    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``\n    :type ds: list[datetime.date]\n    :param before: closest before (True), after (False) or either side of ds\n    :type before: bool or None\n    :returns: The closest date\n    :rtype: str or None\n\n    >>> tbl = 'airflow.static_babynames_partitioned'\n    >>> closest_ds_partition(tbl, '2015-01-02')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n\n    if \".\" in table:\n        schema, table = table.split(\".\")\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    partitions = hh.get_partitions(schema=schema, table_name=table)\n    if not partitions:\n        return None\n    part_vals = [list(p.values())[0] for p in partitions]\n    if ds in part_vals:\n        return ds\n    else:\n        parts = [datetime.datetime.strptime(pv, \"%Y-%m-%d\") for pv in part_vals]\n        target_dt = datetime.datetime.strptime(ds, \"%Y-%m-%d\")\n        closest_ds = _closest_date(target_dt, parts, before_target=before)\n        return closest_ds.isoformat()\n", "levels": [0], "package": ["import datetime", "from airflow.hooks.hive_hooks import HiveMetastoreHook"], "function": ["def _closest_date(target_dt, date_list, before_target=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/mysql_hook.py", "func_name": "MySqlHook.get_conn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns a mysql connection object", "docstring_tokens": ["Returns", "a", "mysql", "connection", "object"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L62-L105", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport MySQLdb\nimport MySQLdb.cursors\nimport json\nimport six\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass MySqlHook(DbApiHook):\n    \"\"\"\n    Interact with MySQL.\n\n    You can specify charset in the extra field of your connection\n    as ``{\"charset\": \"utf8\"}``. Also you can choose cursor as\n    ``{\"cursor\": \"SSCursor\"}``. Refer to the MySQLdb.cursors for more details.\n    \"\"\"\n\n    conn_name_attr = \"mysql_conn_id\"\n    default_conn_name = \"mysql_default\"\n    supports_autocommit = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.schema = kwargs.pop(\"schema\", None)\n\n    def set_autocommit(self, conn, autocommit):\n        \"\"\"\n        MySql connection sets autocommit in a different way.\n        \"\"\"\n        conn.autocommit(autocommit)\n\n    def get_autocommit(self, conn):\n        \"\"\"\n        MySql connection gets autocommit in a different way.\n\n        :param conn: connection to get autocommit setting from.\n        :type conn: connection object.\n        :return: connection autocommit setting\n        :rtype: bool\n        \"\"\"\n        return conn.get_autocommit()\n\n    def bulk_load(self, table, tmp_file):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        conn = self.get_conn()\n        cur = conn.cursor()\n        cur.execute(\n            \"\"\"\n            LOAD DATA LOCAL INFILE '{tmp_file}'\n            INTO TABLE {table}\n            \"\"\".format(\n                tmp_file=tmp_file, table=table\n            )\n        )\n        conn.commit()\n\n    def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        conn = self.get_conn()\n        cur = conn.cursor()\n        cur.execute(\n            \"\"\"\n            SELECT * INTO OUTFILE '{tmp_file}'\n            FROM {table}\n            \"\"\".format(\n                tmp_file=tmp_file, table=table\n            )\n        )\n        conn.commit()\n\n    @staticmethod\n    def _serialize_cell(cell, conn):\n        \"\"\"\n        MySQLdb converts an argument to a literal\n        when passing those separately to execute. Hence, this method does nothing.\n\n        :param cell: The cell to insert into the table\n        :type cell: object\n        :param conn: The database connection\n        :type conn: connection object\n        :return: The same cell\n        :rtype: object\n        \"\"\"\n\n        return cell\n", "levels": [0, 1, 1, 1, 1, 1, 1], "package": ["import MySQLdb", "import MySQLdb.cursors", "import json", "import six", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class MySqlHook(DbApiHook):\n", "    def __init__(self, *args, **kwargs):\n", "    def set_autocommit(self, conn, autocommit):\n", "    def get_autocommit(self, conn):\n", "    def bulk_load(self, table, tmp_file):\n", "    def bulk_dump(self, table, tmp_file):\n", "    def _serialize_cell(cell, conn):\n"]}
{"repo": "apache/airflow", "path": "airflow/bin/cli.py", "func_name": "task_state", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success", "docstring_tokens": ["Returns", "the", "state", "of", "a", "TaskInstance", "at", "the", "command", "line", ".", ">>>", "airflow", "task_state", "tutorial", "sleep", "2015", "-", "01", "-", "01", "success"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L554-L563", "partition": "test", "up_fun_num": 23, "context": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport importlib\nimport logging\n\nimport os\nimport subprocess\nimport textwrap\nimport random\nimport string\nfrom importlib import import_module\n\nimport getpass\nimport reprlib\nimport argparse\nfrom argparse import RawTextHelpFormatter\nfrom builtins import input\n\nfrom airflow.utils.timezone import parse as parsedate\nimport json\nfrom tabulate import tabulate\n\nimport daemon\nfrom daemon.pidfile import TimeoutPIDLockFile\nimport signal\nimport sys\nimport threading\nimport traceback\nimport time\nimport psutil\nimport re\nfrom urllib.parse import urlunparse\nfrom typing import Any\n\nimport airflow\nfrom airflow import api\nfrom airflow import jobs, settings\nfrom airflow import configuration as conf\nfrom airflow.exceptions import AirflowException, AirflowWebServerTimeout\nfrom airflow.executors import get_default_executor\nfrom airflow.models import (\n    Connection,\n    DagModel,\n    DagBag,\n    DagPickle,\n    TaskInstance,\n    DagRun,\n    Variable,\n    DAG,\n)\nfrom airflow.ti_deps.dep_context import DepContext, SCHEDULER_DEPS\nfrom airflow.utils import cli as cli_utils, db\nfrom airflow.utils.net import get_hostname\nfrom airflow.utils.log.logging_mixin import (\n    LoggingMixin,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom airflow.www.app import cached_app, create_app, cached_appbuilder\n\nfrom sqlalchemy.orm import exc\n\napi.load_auth()\napi_module = import_module(conf.get(\"cli\", \"api_client\"))  # type: Any\napi_client = api_module.Client(\n    api_base_url=conf.get(\"cli\", \"endpoint_url\"), auth=api.api_auth.client_auth\n)\n\nlog = LoggingMixin().log\n\nDAGS_FOLDER = settings.DAGS_FOLDER\n\nif \"BUILDING_AIRFLOW_DOCS\" in os.environ:\n    DAGS_FOLDER = \"[AIRFLOW_HOME]/dags\"\n\n\ndef sigint_handler(sig, frame):\n    sys.exit(0)\n\n\ndef sigquit_handler(sig, frame):\n    \"\"\"Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n    e.g. kill -s QUIT <PID> or CTRL+\\\n    \"\"\"\n    print(\"Dumping stack traces for all threads in PID {}\".format(os.getpid()))\n    id_to_name = dict([(th.ident, th.name) for th in threading.enumerate()])\n    code = []\n    for thread_id, stack in sys._current_frames().items():\n        code.append(\n            \"\\n# Thread: {}({})\".format(id_to_name.get(thread_id, \"\"), thread_id)\n        )\n        for filename, line_number, name, line in traceback.extract_stack(stack):\n            code.append(\n                'File: \"{}\", line {}, in {}'.format(filename, line_number, name)\n            )\n            if line:\n                code.append(\"  {}\".format(line.strip()))\n    print(\"\\n\".join(code))\n\n\ndef setup_logging(filename):\n    root = logging.getLogger()\n    handler = logging.FileHandler(filename)\n    formatter = logging.Formatter(settings.SIMPLE_LOG_FORMAT)\n    handler.setFormatter(formatter)\n    root.addHandler(handler)\n    root.setLevel(settings.LOGGING_LEVEL)\n\n    return handler.stream\n\n\ndef setup_locations(process, pid=None, stdout=None, stderr=None, log=None):\n    if not stderr:\n        stderr = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.err\".format(process))\n    if not stdout:\n        stdout = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.out\".format(process))\n    if not log:\n        log = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.log\".format(process))\n    if not pid:\n        pid = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.pid\".format(process))\n\n    return pid, stdout, stderr, log\n\n\ndef process_subdir(subdir):\n    if subdir:\n        subdir = subdir.replace(\"DAGS_FOLDER\", DAGS_FOLDER)\n        subdir = os.path.abspath(os.path.expanduser(subdir))\n        return subdir\n\n\ndef get_dag(args):\n    dagbag = DagBag(process_subdir(args.subdir))\n    if args.dag_id not in dagbag.dags:\n        raise AirflowException(\n            \"dag_id could not be found: {}. Either the dag did not exist or it failed to \"\n            \"parse.\".format(args.dag_id)\n        )\n    return dagbag.dags[args.dag_id]\n\n\ndef get_dags(args):\n    if not args.dag_regex:\n        return [get_dag(args)]\n    dagbag = DagBag(process_subdir(args.subdir))\n    matched_dags = [\n        dag for dag in dagbag.dags.values() if re.search(args.dag_id, dag.dag_id)\n    ]\n    if not matched_dags:\n        raise AirflowException(\n            \"dag_id could not be found with regex: {}. Either the dag did not exist \"\n            \"or it failed to parse.\".format(args.dag_id)\n        )\n    return matched_dags\n\n\n@cli_utils.action_logging\ndef backfill(args, dag=None):\n    logging.basicConfig(level=settings.LOGGING_LEVEL, format=settings.SIMPLE_LOG_FORMAT)\n\n    signal.signal(signal.SIGTERM, sigint_handler)\n\n    dag = dag or get_dag(args)\n\n    if not args.start_date and not args.end_date:\n        raise AirflowException(\"Provide a start_date and/or end_date\")\n\n    # If only one date is passed, using same as start and end\n    args.end_date = args.end_date or args.start_date\n    args.start_date = args.start_date or args.end_date\n\n    if args.task_regex:\n        dag = dag.sub_dag(\n            task_regex=args.task_regex, include_upstream=not args.ignore_dependencies\n        )\n\n    run_conf = None\n    if args.conf:\n        run_conf = json.loads(args.conf)\n\n    if args.dry_run:\n        print(\"Dry run of DAG {0} on {1}\".format(args.dag_id, args.start_date))\n        for task in dag.tasks:\n            print(\"Task {0}\".format(task.task_id))\n            ti = TaskInstance(task, args.start_date)\n            ti.dry_run()\n    else:\n        if args.reset_dagruns:\n            DAG.clear_dags(\n                [dag],\n                start_date=args.start_date,\n                end_date=args.end_date,\n                confirm_prompt=True,\n                include_subdags=True,\n            )\n\n        dag.run(\n            start_date=args.start_date,\n            end_date=args.end_date,\n            mark_success=args.mark_success,\n            local=args.local,\n            donot_pickle=(args.donot_pickle or conf.getboolean(\"core\", \"donot_pickle\")),\n            ignore_first_depends_on_past=args.ignore_first_depends_on_past,\n            ignore_task_deps=args.ignore_dependencies,\n            pool=args.pool,\n            delay_on_limit_secs=args.delay_on_limit,\n            verbose=args.verbose,\n            conf=run_conf,\n            rerun_failed_tasks=args.rerun_failed_tasks,\n            run_backwards=args.run_backwards,\n        )\n\n\n@cli_utils.action_logging\ndef trigger_dag(args):\n    \"\"\"\n    Creates a dag run for the specified dag\n    :param args:\n    :return:\n    \"\"\"\n    log = LoggingMixin().log\n    try:\n        message = api_client.trigger_dag(\n            dag_id=args.dag_id,\n            run_id=args.run_id,\n            conf=args.conf,\n            execution_date=args.exec_date,\n        )\n    except IOError as err:\n        log.error(err)\n        raise AirflowException(err)\n    log.info(message)\n\n\n@cli_utils.action_logging\ndef delete_dag(args):\n    \"\"\"\n    Deletes all DB records related to the specified dag\n    :param args:\n    :return:\n    \"\"\"\n    log = LoggingMixin().log\n    if (\n        args.yes\n        or input(\n            \"This will drop all existing records related to the specified DAG. \"\n            \"Proceed? (y/n)\"\n        ).upper()\n        == \"Y\"\n    ):\n        try:\n            message = api_client.delete_dag(dag_id=args.dag_id)\n        except IOError as err:\n            log.error(err)\n            raise AirflowException(err)\n        log.info(message)\n    else:\n        print(\"Bail.\")\n\n\n@cli_utils.action_logging\ndef pool(args):\n    log = LoggingMixin().log\n\n    def _tabulate(pools):\n        return \"\\n%s\" % tabulate(\n            pools, [\"Pool\", \"Slots\", \"Description\"], tablefmt=\"fancy_grid\"\n        )\n\n    try:\n        imp = getattr(args, \"import\")\n        if args.get is not None:\n            pools = [api_client.get_pool(name=args.get)]\n        elif args.set:\n            pools = [\n                api_client.create_pool(\n                    name=args.set[0], slots=args.set[1], description=args.set[2]\n                )\n            ]\n        elif args.delete:\n            pools = [api_client.delete_pool(name=args.delete)]\n        elif imp:\n            if os.path.exists(imp):\n                pools = pool_import_helper(imp)\n            else:\n                print(\"Missing pools file.\")\n                pools = api_client.get_pools()\n        elif args.export:\n            pools = pool_export_helper(args.export)\n        else:\n            pools = api_client.get_pools()\n    except (AirflowException, IOError) as err:\n        log.error(err)\n    else:\n        log.info(_tabulate(pools=pools))\n\n\ndef pool_import_helper(filepath):\n    with open(filepath, \"r\") as poolfile:\n        pl = poolfile.read()\n    try:\n        d = json.loads(pl)\n    except Exception as e:\n        print(\"Please check the validity of the json file: \" + str(e))\n    else:\n        try:\n            pools = []\n            n = 0\n            for k, v in d.items():\n                if isinstance(v, dict) and len(v) == 2:\n                    pools.append(\n                        api_client.create_pool(\n                            name=k, slots=v[\"slots\"], description=v[\"description\"]\n                        )\n                    )\n                    n += 1\n                else:\n                    pass\n        except Exception:\n            pass\n        finally:\n            print(\"{} of {} pool(s) successfully updated.\".format(n, len(d)))\n            return pools\n\n\ndef pool_export_helper(filepath):\n    pool_dict = {}\n    pools = api_client.get_pools()\n    for pool in pools:\n        pool_dict[pool[0]] = {\"slots\": pool[1], \"description\": pool[2]}\n    with open(filepath, \"w\") as poolfile:\n        poolfile.write(json.dumps(pool_dict, sort_keys=True, indent=4))\n    print(\"{} pools successfully exported to {}\".format(len(pool_dict), filepath))\n    return pools\n\n\n@cli_utils.action_logging\ndef variables(args):\n    if args.get:\n        try:\n            var = Variable.get(\n                args.get, deserialize_json=args.json, default_var=args.default\n            )\n            print(var)\n        except ValueError as e:\n            print(e)\n    if args.delete:\n        Variable.delete(args.delete)\n    if args.set:\n        Variable.set(args.set[0], args.set[1])\n    # Work around 'import' as a reserved keyword\n    imp = getattr(args, \"import\")\n    if imp:\n        if os.path.exists(imp):\n            import_helper(imp)\n        else:\n            print(\"Missing variables file.\")\n    if args.export:\n        export_helper(args.export)\n    if not (args.set or args.get or imp or args.export or args.delete):\n        # list all variables\n        with db.create_session() as session:\n            vars = session.query(Variable)\n            msg = \"\\n\".join(var.key for var in vars)\n            print(msg)\n\n\ndef import_helper(filepath):\n    with open(filepath, \"r\") as varfile:\n        var = varfile.read()\n\n    try:\n        d = json.loads(var)\n    except Exception:\n        print(\"Invalid variables file.\")\n    else:\n        try:\n            n = 0\n            for k, v in d.items():\n                if isinstance(v, dict):\n                    Variable.set(k, v, serialize_json=True)\n                else:\n                    Variable.set(k, v)\n                n += 1\n        except Exception:\n            pass\n        finally:\n            print(\"{} of {} variables successfully updated.\".format(n, len(d)))\n\n\ndef export_helper(filepath):\n    var_dict = {}\n    with db.create_session() as session:\n        qry = session.query(Variable).all()\n\n        d = json.JSONDecoder()\n        for var in qry:\n            try:\n                val = d.decode(var.val)\n            except Exception:\n                val = var.val\n            var_dict[var.key] = val\n\n    with open(filepath, \"w\") as varfile:\n        varfile.write(json.dumps(var_dict, sort_keys=True, indent=4))\n    print(\"{} variables successfully exported to {}\".format(len(var_dict), filepath))\n\n\n@cli_utils.action_logging\ndef pause(args, dag=None):\n    set_is_paused(True, args, dag)\n\n\n@cli_utils.action_logging\ndef unpause(args, dag=None):\n    set_is_paused(False, args, dag)\n\n\ndef set_is_paused(is_paused, args, dag=None):\n    dag = dag or get_dag(args)\n\n    with db.create_session() as session:\n        dm = session.query(DagModel).filter(DagModel.dag_id == dag.dag_id).first()\n        dm.is_paused = is_paused\n        session.commit()\n\n    print(\"Dag: {}, paused: {}\".format(dag, str(dag.is_paused)))\n\n\ndef _run(args, dag, ti):\n    if args.local:\n        run_job = jobs.LocalTaskJob(\n            task_instance=ti,\n            mark_success=args.mark_success,\n            pickle_id=args.pickle,\n            ignore_all_deps=args.ignore_all_dependencies,\n            ignore_depends_on_past=args.ignore_depends_on_past,\n            ignore_task_deps=args.ignore_dependencies,\n            ignore_ti_state=args.force,\n            pool=args.pool,\n        )\n        run_job.run()\n    elif args.raw:\n        ti._run_raw_task(\n            mark_success=args.mark_success,\n            job_id=args.job_id,\n            pool=args.pool,\n        )\n    else:\n        pickle_id = None\n        if args.ship_dag:\n            try:\n                # Running remotely, so pickling the DAG\n                with db.create_session() as session:\n                    pickle = DagPickle(dag)\n                    session.add(pickle)\n                    pickle_id = pickle.id\n                    # TODO: This should be written to a log\n                    print(\n                        \"Pickled dag {dag} as pickle_id: {pickle_id}\".format(\n                            dag=dag, pickle_id=pickle_id\n                        )\n                    )\n            except Exception as e:\n                print(\"Could not pickle the DAG\")\n                print(e)\n                raise e\n\n        executor = get_default_executor()\n        executor.start()\n        print(\"Sending to executor.\")\n        executor.queue_task_instance(\n            ti,\n            mark_success=args.mark_success,\n            pickle_id=pickle_id,\n            ignore_all_deps=args.ignore_all_dependencies,\n            ignore_depends_on_past=args.ignore_depends_on_past,\n            ignore_task_deps=args.ignore_dependencies,\n            ignore_ti_state=args.force,\n            pool=args.pool,\n        )\n        executor.heartbeat()\n        executor.end()\n\n\n@cli_utils.action_logging\ndef run(args, dag=None):\n    if dag:\n        args.dag_id = dag.dag_id\n\n    log = LoggingMixin().log\n\n    # Load custom airflow config\n    if args.cfg_path:\n        with open(args.cfg_path, \"r\") as conf_file:\n            conf_dict = json.load(conf_file)\n\n        if os.path.exists(args.cfg_path):\n            os.remove(args.cfg_path)\n\n        conf.conf.read_dict(conf_dict, source=args.cfg_path)\n        settings.configure_vars()\n\n    # IMPORTANT, have to use the NullPool, otherwise, each \"run\" command may leave\n    # behind multiple open sleeping connections while heartbeating, which could\n    # easily exceed the database connection limit when\n    # processing hundreds of simultaneous tasks.\n    settings.configure_orm(disable_connection_pool=True)\n\n    if not args.pickle and not dag:\n        dag = get_dag(args)\n    elif not dag:\n        with db.create_session() as session:\n            log.info(\"Loading pickle id %s\", args.pickle)\n            dag_pickle = (\n                session.query(DagPickle).filter(DagPickle.id == args.pickle).first()\n            )\n            if not dag_pickle:\n                raise AirflowException(\"Who hid the pickle!? [missing pickle]\")\n            dag = dag_pickle.pickle\n\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    ti.refresh_from_db()\n\n    ti.init_run_context(raw=args.raw)\n\n    hostname = get_hostname()\n    log.info(\"Running %s on host %s\", ti, hostname)\n\n    if args.interactive:\n        _run(args, dag, ti)\n    else:\n        with redirect_stdout(ti.log, logging.INFO), redirect_stderr(\n            ti.log, logging.WARN\n        ):\n            _run(args, dag, ti)\n    logging.shutdown()\n\n\n@cli_utils.action_logging\ndef task_failed_deps(args):\n    \"\"\"\n    Returns the unmet dependencies for a task instance from the perspective of the\n    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n    scheduler, and then run by an executor).\n    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n    Task instance dependencies not met:\n    Dagrun Running: Task instance's dagrun did not exist: Unknown reason\n    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks\n    to have succeeded, but found 1 non-success(es).\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n\n    dep_context = DepContext(deps=SCHEDULER_DEPS)\n    failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n    # TODO, Do we want to print or log this\n    if failed_deps:\n        print(\"Task instance dependencies not met:\")\n        for dep in failed_deps:\n            print(\"{}: {}\".format(dep.dep_name, dep.reason))\n    else:\n        print(\"Task instance dependencies are all met.\")\n\n\n@cli_utils.action_logging\n@cli_utils.action_logging\ndef dag_state(args):\n    \"\"\"\n    Returns the state of a DagRun at the command line.\n    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000\n    running\n    \"\"\"\n    dag = get_dag(args)\n    dr = DagRun.find(dag.dag_id, execution_date=args.execution_date)\n    print(dr[0].state if len(dr) > 0 else None)\n\n\n@cli_utils.action_logging\ndef next_execution(args):\n    \"\"\"\n    Returns the next execution datetime of a DAG at the command line.\n    >>> airflow next_execution tutorial\n    2018-08-31 10:38:00\n    \"\"\"\n    dag = get_dag(args)\n\n    if dag.is_paused:\n        print(\"[INFO] Please be reminded this DAG is PAUSED now.\")\n\n    if dag.latest_execution_date:\n        next_execution_dttm = dag.following_schedule(dag.latest_execution_date)\n\n        if next_execution_dttm is None:\n            print(\n                \"[WARN] No following schedule can be found. \"\n                + \"This DAG may have schedule interval '@once' or `None`.\"\n            )\n\n        print(next_execution_dttm)\n    else:\n        print(\n            \"[WARN] Only applicable when there is execution record found for the DAG.\"\n        )\n        print(None)\n\n\n@cli_utils.action_logging\ndef rotate_fernet_key(args):\n    with db.create_session() as session:\n        for conn in session.query(Connection).filter(\n            Connection.is_encrypted | Connection.is_extra_encrypted\n        ):\n            conn.rotate_fernet_key()\n        for var in session.query(Variable).filter(Variable.is_encrypted):\n            var.rotate_fernet_key()\n\n\n@cli_utils.action_logging\ndef list_dags(args):\n    dagbag = DagBag(process_subdir(args.subdir))\n    s = textwrap.dedent(\n        \"\"\"\\n\n    -------------------------------------------------------------------\n    DAGS\n    -------------------------------------------------------------------\n    {dag_list}\n    \"\"\"\n    )\n    dag_list = \"\\n\".join(sorted(dagbag.dags))\n    print(s.format(dag_list=dag_list))\n    if args.report:\n        print(dagbag.dagbag_report())\n\n\n@cli_utils.action_logging\ndef list_tasks(args, dag=None):\n    dag = dag or get_dag(args)\n    if args.tree:\n        dag.tree_view()\n    else:\n        tasks = sorted([t.task_id for t in dag.tasks])\n        print(\"\\n\".join(sorted(tasks)))\n\n\n@cli_utils.action_logging\ndef list_jobs(args, dag=None):\n    queries = []\n    if dag:\n        args.dag_id = dag.dag_id\n    if args.dag_id:\n        dagbag = DagBag()\n\n        if args.dag_id not in dagbag.dags:\n            error_message = \"Dag id {} not found\".format(args.dag_id)\n            raise AirflowException(error_message)\n        queries.append(jobs.BaseJob.dag_id == args.dag_id)\n\n    if args.state:\n        queries.append(jobs.BaseJob.state == args.state)\n\n    with db.create_session() as session:\n        all_jobs = (\n            session.query(jobs.BaseJob)\n            .filter(*queries)\n            .order_by(jobs.BaseJob.start_date.desc())\n            .limit(args.limit)\n            .all()\n        )\n        fields = [\"dag_id\", \"state\", \"job_type\", \"start_date\", \"end_date\"]\n        all_jobs = [\n            [job.__getattribute__(field) for field in fields] for job in all_jobs\n        ]\n        msg = tabulate(\n            all_jobs,\n            [field.capitalize().replace(\"_\", \" \") for field in fields],\n            tablefmt=\"fancy_grid\",\n        )\n        print(msg)\n\n\n@cli_utils.action_logging\ndef test(args, dag=None):\n    # We want log outout from operators etc to show up here. Normally\n    # airflow.task would redirect to a file, but here we want it to propagate\n    # up to the normal airflow handler.\n    logging.getLogger(\"airflow.task\").propagate = True\n\n    dag = dag or get_dag(args)\n\n    task = dag.get_task(task_id=args.task_id)\n    # Add CLI provided task_params to task.params\n    if args.task_params:\n        passed_in_params = json.loads(args.task_params)\n        task.params.update(passed_in_params)\n    ti = TaskInstance(task, args.execution_date)\n\n    try:\n        if args.dry_run:\n            ti.dry_run()\n        else:\n            ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)\n    except Exception:\n        if args.post_mortem:\n            try:\n                debugger = importlib.import_module(\"ipdb\")\n            except ImportError:\n                debugger = importlib.import_module(\"pdb\")\n            debugger.post_mortem()\n        else:\n            raise\n\n\n@cli_utils.action_logging\ndef render(args):\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    ti.render_templates()\n    for attr in task.__class__.template_fields:\n        print(\n            textwrap.dedent(\n                \"\"\"\\\n        # ----------------------------------------------------------\n        # property: {}\n        # ----------------------------------------------------------\n        {}\n        \"\"\".format(\n                    attr, getattr(task, attr)\n                )\n            )\n        )\n\n\n@cli_utils.action_logging\ndef clear(args):\n    logging.basicConfig(level=settings.LOGGING_LEVEL, format=settings.SIMPLE_LOG_FORMAT)\n    dags = get_dags(args)\n\n    if args.task_regex:\n        for idx, dag in enumerate(dags):\n            dags[idx] = dag.sub_dag(\n                task_regex=args.task_regex,\n                include_downstream=args.downstream,\n                include_upstream=args.upstream,\n            )\n\n    DAG.clear_dags(\n        dags,\n        start_date=args.start_date,\n        end_date=args.end_date,\n        only_failed=args.only_failed,\n        only_running=args.only_running,\n        confirm_prompt=not args.no_confirm,\n        include_subdags=not args.exclude_subdags,\n        include_parentdag=not args.exclude_parentdag,\n    )\n\n\ndef get_num_ready_workers_running(gunicorn_master_proc):\n    workers = psutil.Process(gunicorn_master_proc.pid).children()\n\n    def ready_prefix_on_cmdline(proc):\n        try:\n            cmdline = proc.cmdline()\n            if len(cmdline) > 0:\n                return settings.GUNICORN_WORKER_READY_PREFIX in cmdline[0]\n        except psutil.NoSuchProcess:\n            pass\n        return False\n\n    ready_workers = [proc for proc in workers if ready_prefix_on_cmdline(proc)]\n    return len(ready_workers)\n\n\ndef get_num_workers_running(gunicorn_master_proc):\n    workers = psutil.Process(gunicorn_master_proc.pid).children()\n    return len(workers)\n\n\ndef restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\".format(\n                        timeout\n                    )\n                )\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint(\"webserver\", \"worker_refresh_batch_size\")\n        log.debug(\"%s doing a refresh of %s workers\", state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(\n                lambda: num_workers_expected + excess\n                == get_num_workers_running(gunicorn_master_proc),\n                master_timeout,\n            )\n\n    try:\n        wait_until_true(\n            lambda: num_workers_expected\n            == get_num_workers_running(gunicorn_master_proc),\n            master_timeout,\n        )\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = get_num_ready_workers_running(\n                gunicorn_master_proc\n            )\n\n            state = \"[{0} / {1}]\".format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug(\"%s some workers are starting up, waiting...\", state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug(\"%s killing %s workers\", state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(\n                        lambda: num_workers_expected + excess\n                        == get_num_workers_running(gunicorn_master_proc),\n                        master_timeout,\n                    )\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint(\"webserver\", \"worker_refresh_interval\")\n                log.debug(\n                    \"%s sleeping for %ss starting doing a refresh...\",\n                    state,\n                    refresh_interval,\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error(\n                    (\n                        \"%s some workers seem to have died and gunicorn\"\n                        \"did not restart them as expected\"\n                    ),\n                    state,\n                )\n                time.sleep(10)\n                if (\n                    len(psutil.Process(gunicorn_master_proc.pid).children())\n                    < num_workers_expected\n                ):\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)\n\n\n@cli_utils.action_logging\ndef webserver(args):\n    print(settings.HEADER)\n\n    access_logfile = args.access_logfile or conf.get(\"webserver\", \"access_logfile\")\n    error_logfile = args.error_logfile or conf.get(\"webserver\", \"error_logfile\")\n    num_workers = args.workers or conf.get(\"webserver\", \"workers\")\n    worker_timeout = args.worker_timeout or conf.get(\n        \"webserver\", \"web_server_worker_timeout\"\n    )\n    ssl_cert = args.ssl_cert or conf.get(\"webserver\", \"web_server_ssl_cert\")\n    ssl_key = args.ssl_key or conf.get(\"webserver\", \"web_server_ssl_key\")\n    if not ssl_cert and ssl_key:\n        raise AirflowException(\n            \"An SSL certificate must also be provided for use with \" + ssl_key\n        )\n    if ssl_cert and not ssl_key:\n        raise AirflowException(\n            \"An SSL key must also be provided for use with \" + ssl_cert\n        )\n\n    if args.debug:\n        print(\n            \"Starting the web server on port {0} and host {1}.\".format(\n                args.port, args.hostname\n            )\n        )\n        app, _ = create_app(None, testing=conf.get(\"core\", \"unit_test_mode\"))\n        app.run(\n            debug=True,\n            use_reloader=False if app.config[\"TESTING\"] else True,\n            port=args.port,\n            host=args.hostname,\n            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,\n        )\n    else:\n        os.environ[\"SKIP_DAGS_PARSING\"] = \"True\"\n        app = cached_app(None)\n        pid, stdout, stderr, log_file = setup_locations(\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        os.environ.pop(\"SKIP_DAGS_PARSING\")\n        if args.daemon:\n            handle = setup_logging(log_file)\n            stdout = open(stdout, \"w+\")\n            stderr = open(stderr, \"w+\")\n\n        print(\n            textwrap.dedent(\n                \"\"\"\\\n                Running the Gunicorn Server with:\n                Workers: {num_workers} {workerclass}\n                Host: {hostname}:{port}\n                Timeout: {worker_timeout}\n                Logfiles: {access_logfile} {error_logfile}\n                =================================================================\\\n            \"\"\".format(\n                    num_workers=num_workers,\n                    workerclass=args.workerclass,\n                    hostname=args.hostname,\n                    port=args.port,\n                    worker_timeout=worker_timeout,\n                    access_logfile=access_logfile,\n                    error_logfile=error_logfile,\n                )\n            )\n        )\n\n        run_args = [\n            \"gunicorn\",\n            \"-w\",\n            str(num_workers),\n            \"-k\",\n            str(args.workerclass),\n            \"-t\",\n            str(worker_timeout),\n            \"-b\",\n            args.hostname + \":\" + str(args.port),\n            \"-n\",\n            \"airflow-webserver\",\n            \"-p\",\n            str(pid),\n            \"-c\",\n            \"python:airflow.www.gunicorn_config\",\n        ]\n\n        if args.access_logfile:\n            run_args += [\"--access-logfile\", str(args.access_logfile)]\n\n        if args.error_logfile:\n            run_args += [\"--error-logfile\", str(args.error_logfile)]\n\n        if args.daemon:\n            run_args += [\"-D\"]\n\n        if ssl_cert:\n            run_args += [\"--certfile\", ssl_cert, \"--keyfile\", ssl_key]\n\n        webserver_module = \"www\"\n        run_args += [\"airflow.\" + webserver_module + \".app:cached_app()\"]\n\n        gunicorn_master_proc = None\n\n        def kill_proc(dummy_signum, dummy_frame):\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n            sys.exit(0)\n\n        def monitor_gunicorn(gunicorn_master_proc):\n            # These run forever until SIG{INT, TERM, KILL, ...} signal is sent\n            if conf.getint(\"webserver\", \"worker_refresh_interval\") > 0:\n                master_timeout = conf.getint(\"webserver\", \"web_server_master_timeout\")\n                restart_workers(gunicorn_master_proc, num_workers, master_timeout)\n            else:\n                while gunicorn_master_proc.poll() is None:\n                    time.sleep(1)\n\n                sys.exit(gunicorn_master_proc.returncode)\n\n        if args.daemon:\n            base, ext = os.path.splitext(pid)\n            ctx = daemon.DaemonContext(\n                pidfile=TimeoutPIDLockFile(base + \"-monitor\" + ext, -1),\n                files_preserve=[handle],\n                stdout=stdout,\n                stderr=stderr,\n                signal_map={signal.SIGINT: kill_proc, signal.SIGTERM: kill_proc},\n            )\n            with ctx:\n                subprocess.Popen(run_args, close_fds=True)\n\n                # Reading pid file directly, since Popen#pid doesn't\n                # seem to return the right value with DaemonContext.\n                while True:\n                    try:\n                        with open(pid) as f:\n                            gunicorn_master_proc_pid = int(f.read())\n                            break\n                    except IOError:\n                        log.debug(\"Waiting for gunicorn's pid file to be created.\")\n                        time.sleep(0.1)\n\n                gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)\n                monitor_gunicorn(gunicorn_master_proc)\n\n            stdout.close()\n            stderr.close()\n        else:\n            gunicorn_master_proc = subprocess.Popen(run_args, close_fds=True)\n\n            signal.signal(signal.SIGINT, kill_proc)\n            signal.signal(signal.SIGTERM, kill_proc)\n\n            monitor_gunicorn(gunicorn_master_proc)\n\n\n@cli_utils.action_logging\ndef scheduler(args):\n    print(settings.HEADER)\n    job = jobs.SchedulerJob(\n        dag_id=args.dag_id,\n        subdir=process_subdir(args.subdir),\n        num_runs=args.num_runs,\n        do_pickle=args.do_pickle,\n    )\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"scheduler\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        handle = setup_logging(log_file)\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            files_preserve=[handle],\n            stdout=stdout,\n            stderr=stderr,\n        )\n        with ctx:\n            job.run()\n\n        stdout.close()\n        stderr.close()\n    else:\n        signal.signal(signal.SIGINT, sigint_handler)\n        signal.signal(signal.SIGTERM, sigint_handler)\n        signal.signal(signal.SIGQUIT, sigquit_handler)\n        job.run()\n\n\n@cli_utils.action_logging\ndef serve_logs(args):\n    print(\"Starting flask\")\n    import flask\n\n    flask_app = flask.Flask(__name__)\n\n    @flask_app.route(\"/log/<path:filename>\")\n    def serve_logs(filename):\n        log = os.path.expanduser(conf.get(\"core\", \"BASE_LOG_FOLDER\"))\n        return flask.send_from_directory(\n            log, filename, mimetype=\"application/json\", as_attachment=False\n        )\n\n    worker_log_server_port = int(conf.get(\"celery\", \"WORKER_LOG_SERVER_PORT\"))\n    flask_app.run(host=\"0.0.0.0\", port=worker_log_server_port)\n\n\n@cli_utils.action_logging\ndef worker(args):\n    env = os.environ.copy()\n    env[\"AIRFLOW_HOME\"] = settings.AIRFLOW_HOME\n\n    if not settings.validate_session():\n        log = LoggingMixin().log\n        log.error(\"Worker exiting... database connection precheck failed! \")\n        sys.exit(1)\n\n    # Celery worker\n    from airflow.executors.celery_executor import app as celery_app\n    from celery.bin import worker\n\n    autoscale = args.autoscale\n    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):\n        autoscale = conf.get(\"celery\", \"worker_autoscale\")\n    worker = worker.worker(app=celery_app)\n    options = {\n        \"optimization\": \"fair\",\n        \"O\": \"fair\",\n        \"queues\": args.queues,\n        \"concurrency\": args.concurrency,\n        \"autoscale\": autoscale,\n        \"hostname\": args.celery_hostname,\n        \"loglevel\": conf.get(\"core\", \"LOGGING_LEVEL\"),\n    }\n\n    if conf.has_option(\"celery\", \"pool\"):\n        options[\"pool\"] = conf.get(\"celery\", \"pool\")\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"worker\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        handle = setup_logging(log_file)\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            files_preserve=[handle],\n            stdout=stdout,\n            stderr=stderr,\n        )\n        with ctx:\n            sp = subprocess.Popen([\"airflow\", \"serve_logs\"], env=env, close_fds=True)\n            worker.run(**options)\n            sp.kill()\n\n        stdout.close()\n        stderr.close()\n    else:\n        signal.signal(signal.SIGINT, sigint_handler)\n        signal.signal(signal.SIGTERM, sigint_handler)\n\n        sp = subprocess.Popen([\"airflow\", \"serve_logs\"], env=env, close_fds=True)\n\n        worker.run(**options)\n        sp.kill()\n\n\ndef initdb(args):\n    print(\"DB: \" + repr(settings.engine.url))\n    db.initdb()\n    print(\"Done.\")\n\n\ndef resetdb(args):\n    print(\"DB: \" + repr(settings.engine.url))\n    if (\n        args.yes\n        or input(\n            \"This will drop existing tables \" \"if they exist. Proceed? \" \"(y/n)\"\n        ).upper()\n        == \"Y\"\n    ):\n        db.resetdb()\n    else:\n        print(\"Bail.\")\n\n\n@cli_utils.action_logging\ndef upgradedb(args):\n    print(\"DB: \" + repr(settings.engine.url))\n    db.upgradedb()\n\n\n@cli_utils.action_logging\ndef version(args):\n    print(settings.HEADER + \"  v\" + airflow.__version__)\n\n\nalternative_conn_specs = [\n    \"conn_type\",\n    \"conn_host\",\n    \"conn_login\",\n    \"conn_password\",\n    \"conn_schema\",\n    \"conn_port\",\n]\n\n\n@cli_utils.action_logging\ndef connections(args):\n    if args.list:\n        # Check that no other flags were passed to the command\n        invalid_args = list()\n        for arg in [\"conn_id\", \"conn_uri\", \"conn_extra\"] + alternative_conn_specs:\n            if getattr(args, arg) is not None:\n                invalid_args.append(arg)\n        if invalid_args:\n            msg = (\n                \"\\n\\tThe following args are not compatible with the \"\n                + \"--list flag: {invalid!r}\\n\"\n            )\n            msg = msg.format(invalid=invalid_args)\n            print(msg)\n            return\n\n        with db.create_session() as session:\n            conns = session.query(\n                Connection.conn_id,\n                Connection.conn_type,\n                Connection.host,\n                Connection.port,\n                Connection.is_encrypted,\n                Connection.is_extra_encrypted,\n                Connection.extra,\n            ).all()\n            conns = [map(reprlib.repr, conn) for conn in conns]\n            msg = tabulate(\n                conns,\n                [\n                    \"Conn Id\",\n                    \"Conn Type\",\n                    \"Host\",\n                    \"Port\",\n                    \"Is Encrypted\",\n                    \"Is Extra Encrypted\",\n                    \"Extra\",\n                ],\n                tablefmt=\"fancy_grid\",\n            )\n            print(msg)\n            return\n\n    if args.delete:\n        # Check that only the `conn_id` arg was passed to the command\n        invalid_args = list()\n        for arg in [\"conn_uri\", \"conn_extra\"] + alternative_conn_specs:\n            if getattr(args, arg) is not None:\n                invalid_args.append(arg)\n        if invalid_args:\n            msg = (\n                \"\\n\\tThe following args are not compatible with the \"\n                + \"--delete flag: {invalid!r}\\n\"\n            )\n            msg = msg.format(invalid=invalid_args)\n            print(msg)\n            return\n\n        if args.conn_id is None:\n            print(\n                \"\\n\\tTo delete a connection, you Must provide a value for \"\n                + \"the --conn_id flag.\\n\"\n            )\n            return\n\n        with db.create_session() as session:\n            try:\n                to_delete = (\n                    session.query(Connection)\n                    .filter(Connection.conn_id == args.conn_id)\n                    .one()\n                )\n            except exc.NoResultFound:\n                msg = \"\\n\\tDid not find a connection with `conn_id`={conn_id}\\n\"\n                msg = msg.format(conn_id=args.conn_id)\n                print(msg)\n                return\n            except exc.MultipleResultsFound:\n                msg = (\n                    \"\\n\\tFound more than one connection with \" + \"`conn_id`={conn_id}\\n\"\n                )\n                msg = msg.format(conn_id=args.conn_id)\n                print(msg)\n                return\n            else:\n                deleted_conn_id = to_delete.conn_id\n                session.delete(to_delete)\n                msg = \"\\n\\tSuccessfully deleted `conn_id`={conn_id}\\n\"\n                msg = msg.format(conn_id=deleted_conn_id)\n                print(msg)\n            return\n\n    if args.add:\n        # Check that the conn_id and conn_uri args were passed to the command:\n        missing_args = list()\n        invalid_args = list()\n        if not args.conn_id:\n            missing_args.append(\"conn_id\")\n        if args.conn_uri:\n            for arg in alternative_conn_specs:\n                if getattr(args, arg) is not None:\n                    invalid_args.append(arg)\n        elif not args.conn_type:\n            missing_args.append(\"conn_uri or conn_type\")\n        if missing_args:\n            msg = (\n                \"\\n\\tThe following args are required to add a connection:\"\n                + \" {missing!r}\\n\".format(missing=missing_args)\n            )\n            print(msg)\n        if invalid_args:\n            msg = (\n                \"\\n\\tThe following args are not compatible with the \"\n                + \"--add flag and --conn_uri flag: {invalid!r}\\n\"\n            )\n            msg = msg.format(invalid=invalid_args)\n            print(msg)\n        if missing_args or invalid_args:\n            return\n\n        if args.conn_uri:\n            new_conn = Connection(conn_id=args.conn_id, uri=args.conn_uri)\n        else:\n            new_conn = Connection(\n                conn_id=args.conn_id,\n                conn_type=args.conn_type,\n                host=args.conn_host,\n                login=args.conn_login,\n                password=args.conn_password,\n                schema=args.conn_schema,\n                port=args.conn_port,\n            )\n        if args.conn_extra is not None:\n            new_conn.set_extra(args.conn_extra)\n\n        with db.create_session() as session:\n            if not (\n                session.query(Connection)\n                .filter(Connection.conn_id == new_conn.conn_id)\n                .first()\n            ):\n                session.add(new_conn)\n                msg = \"\\n\\tSuccessfully added `conn_id`={conn_id} : {uri}\\n\"\n                msg = msg.format(\n                    conn_id=new_conn.conn_id,\n                    uri=args.conn_uri\n                    or urlunparse(\n                        (\n                            args.conn_type,\n                            \"{login}:{password}@{host}:{port}\".format(\n                                login=args.conn_login or \"\",\n                                password=args.conn_password or \"\",\n                                host=args.conn_host or \"\",\n                                port=args.conn_port or \"\",\n                            ),\n                            args.conn_schema or \"\",\n                            \"\",\n                            \"\",\n                            \"\",\n                        )\n                    ),\n                )\n                print(msg)\n            else:\n                msg = \"\\n\\tA connection with `conn_id`={conn_id} already exists\\n\"\n                msg = msg.format(conn_id=new_conn.conn_id)\n                print(msg)\n\n        return\n\n\n@cli_utils.action_logging\ndef flower(args):\n    broka = conf.get(\"celery\", \"BROKER_URL\")\n    address = \"--address={}\".format(args.hostname)\n    port = \"--port={}\".format(args.port)\n    api = \"\"\n    if args.broker_api:\n        api = \"--broker_api=\" + args.broker_api\n\n    url_prefix = \"\"\n    if args.url_prefix:\n        url_prefix = \"--url-prefix=\" + args.url_prefix\n\n    basic_auth = \"\"\n    if args.basic_auth:\n        basic_auth = \"--basic_auth=\" + args.basic_auth\n\n    flower_conf = \"\"\n    if args.flower_conf:\n        flower_conf = \"--conf=\" + args.flower_conf\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"flower\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n        with ctx:\n            os.execvp(\n                \"flower\",\n                [\n                    \"flower\",\n                    \"-b\",\n                    broka,\n                    address,\n                    port,\n                    api,\n                    flower_conf,\n                    url_prefix,\n                    basic_auth,\n                ],\n            )\n\n        stdout.close()\n        stderr.close()\n    else:\n        signal.signal(signal.SIGINT, sigint_handler)\n        signal.signal(signal.SIGTERM, sigint_handler)\n\n        os.execvp(\n            \"flower\",\n            [\n                \"flower\",\n                \"-b\",\n                broka,\n                address,\n                port,\n                api,\n                flower_conf,\n                url_prefix,\n                basic_auth,\n            ],\n        )\n\n\n@cli_utils.action_logging\ndef kerberos(args):\n    print(settings.HEADER)\n    import airflow.security.kerberos\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"kerberos\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n        with ctx:\n            airflow.security.kerberos.run(principal=args.principal, keytab=args.keytab)\n\n        stdout.close()\n        stderr.close()\n    else:\n        airflow.security.kerberos.run(principal=args.principal, keytab=args.keytab)\n\n\n@cli_utils.action_logging\ndef users(args):\n    if args.list:\n\n        appbuilder = cached_appbuilder()\n        users = appbuilder.sm.get_all_users()\n        fields = [\"id\", \"username\", \"email\", \"first_name\", \"last_name\", \"roles\"]\n        users = [[user.__getattribute__(field) for field in fields] for user in users]\n        msg = tabulate(\n            users,\n            [field.capitalize().replace(\"_\", \" \") for field in fields],\n            tablefmt=\"fancy_grid\",\n        )\n        print(msg)\n\n        return\n\n    elif args.create:\n        fields = {\n            \"role\": args.role,\n            \"username\": args.username,\n            \"email\": args.email,\n            \"firstname\": args.firstname,\n            \"lastname\": args.lastname,\n        }\n        empty_fields = [k for k, v in fields.items() if not v]\n        if empty_fields:\n            raise SystemExit(\n                \"Required arguments are missing: {}.\".format(\", \".join(empty_fields))\n            )\n\n        appbuilder = cached_appbuilder()\n        role = appbuilder.sm.find_role(args.role)\n        if not role:\n            raise SystemExit(\"{} is not a valid role.\".format(args.role))\n\n        if args.use_random_password:\n            password = \"\".join(random.choice(string.printable) for _ in range(16))\n        elif args.password:\n            password = args.password\n        else:\n            password = getpass.getpass(\"Password:\")\n            password_confirmation = getpass.getpass(\"Repeat for confirmation:\")\n            if password != password_confirmation:\n                raise SystemExit(\"Passwords did not match!\")\n\n        if appbuilder.sm.find_user(args.username):\n            print(\"{} already exist in the db\".format(args.username))\n            return\n        user = appbuilder.sm.add_user(\n            args.username, args.firstname, args.lastname, args.email, role, password\n        )\n        if user:\n            print(\"{} user {} created.\".format(args.role, args.username))\n        else:\n            raise SystemExit(\"Failed to create user.\")\n\n    elif args.delete:\n        if not args.username:\n            raise SystemExit(\"Required arguments are missing: username\")\n\n        appbuilder = cached_appbuilder()\n\n        try:\n            u = next(\n                u for u in appbuilder.sm.get_all_users() if u.username == args.username\n            )\n        except StopIteration:\n            raise SystemExit(\"{} is not a valid user.\".format(args.username))\n\n        if appbuilder.sm.del_register_user(u):\n            print(\"User {} deleted.\".format(args.username))\n        else:\n            raise SystemExit(\"Failed to delete user.\")\n\n    elif args.add_role or args.remove_role:\n        if args.add_role and args.remove_role:\n            raise SystemExit(\n                \"Conflicting args: --add-role and --remove-role\"\n                \" are mutually exclusive\"\n            )\n\n        if not args.username and not args.email:\n            raise SystemExit(\"Missing args: must supply one of --username or --email\")\n\n        if args.username and args.email:\n            raise SystemExit(\n                \"Conflicting args: must supply either --username\"\n                \" or --email, but not both\"\n            )\n        if not args.role:\n            raise SystemExit(\"Required args are missing: role\")\n\n        appbuilder = cached_appbuilder()\n        user = appbuilder.sm.find_user(\n            username=args.username\n        ) or appbuilder.sm.find_user(email=args.email)\n        if not user:\n            raise SystemExit(\n                'User \"{}\" does not exist'.format(args.username or args.email)\n            )\n\n        role = appbuilder.sm.find_role(args.role)\n        if not role:\n            raise SystemExit('\"{}\" is not a valid role.'.format(args.role))\n\n        if args.remove_role:\n            if role in user.roles:\n                user.roles = [r for r in user.roles if r != role]\n                appbuilder.sm.update_user(user)\n                print('User \"{}\" removed from role \"{}\".'.format(user, args.role))\n            else:\n                raise SystemExit(\n                    'User \"{}\" is not a member of role \"{}\".'.format(user, args.role)\n                )\n        elif args.add_role:\n            if role in user.roles:\n                raise SystemExit(\n                    'User \"{}\" is already a member of role \"{}\".'.format(\n                        user, args.role\n                    )\n                )\n            else:\n                user.roles.append(role)\n                appbuilder.sm.update_user(user)\n                print('User \"{}\" added to role \"{}\".'.format(user, args.role))\n    elif args.export:\n        appbuilder = cached_appbuilder()\n        users = appbuilder.sm.get_all_users()\n        fields = [\"id\", \"username\", \"email\", \"first_name\", \"last_name\", \"roles\"]\n\n        # In the User model the first and last name fields have underscores,\n        # but the corresponding parameters in the CLI don't\n        def remove_underscores(s):\n            return re.sub(\"_\", \"\", s)\n\n        users = [\n            {\n                remove_underscores(field): user.__getattribute__(field)\n                if field != \"roles\"\n                else [r.name for r in user.roles]\n                for field in fields\n            }\n            for user in users\n        ]\n\n        with open(args.export, \"w\") as f:\n            f.write(json.dumps(users, sort_keys=True, indent=4))\n            print(\"{} users successfully exported to {}\".format(len(users), f.name))\n\n    elif getattr(args, \"import\"):  # \"import\" is a reserved word\n        json_file = getattr(args, \"import\")\n        if not os.path.exists(json_file):\n            print(\"File '{}' does not exist\")\n            exit(1)\n\n        users_list = None\n        try:\n            with open(json_file, \"r\") as f:\n                users_list = json.loads(f.read())\n        except ValueError as e:\n            print(\"File '{}' is not valid JSON. Error: {}\".format(json_file, e))\n            exit(1)\n\n        users_created, users_updated = _import_users(users_list)\n        if users_created:\n            print(\n                \"Created the following users:\\n\\t{}\".format(\"\\n\\t\".join(users_created))\n            )\n\n        if users_updated:\n            print(\n                \"Updated the following users:\\n\\t{}\".format(\"\\n\\t\".join(users_updated))\n            )\n\n\ndef _import_users(users_list):\n    appbuilder = cached_appbuilder()\n    users_created = []\n    users_updated = []\n\n    for user in users_list:\n        roles = []\n        for rolename in user[\"roles\"]:\n            role = appbuilder.sm.find_role(rolename)\n            if not role:\n                print(\"Error: '{}' is not a valid role\".format(rolename))\n                exit(1)\n            else:\n                roles.append(role)\n\n        required_fields = [\"username\", \"firstname\", \"lastname\", \"email\", \"roles\"]\n        for field in required_fields:\n            if not user.get(field):\n                print(\n                    \"Error: '{}' is a required field, but was not \"\n                    \"specified\".format(field)\n                )\n                exit(1)\n\n        existing_user = appbuilder.sm.find_user(email=user[\"email\"])\n        if existing_user:\n            print(\"Found existing user with email '{}'\".format(user[\"email\"]))\n            existing_user.roles = roles\n            existing_user.first_name = user[\"firstname\"]\n            existing_user.last_name = user[\"lastname\"]\n\n            if existing_user.username != user[\"username\"]:\n                print(\n                    \"Error: Changing ther username is not allowed - \"\n                    \"please delete and recreate the user with \"\n                    \"email '{}'\".format(user[\"email\"])\n                )\n                exit(1)\n\n            appbuilder.sm.update_user(existing_user)\n            users_updated.append(user[\"email\"])\n        else:\n            print(\"Creating new user with email '{}'\".format(user[\"email\"]))\n            appbuilder.sm.add_user(\n                username=user[\"username\"],\n                first_name=user[\"firstname\"],\n                last_name=user[\"lastname\"],\n                email=user[\"email\"],\n                role=roles[0],  # add_user() requires exactly 1 role\n            )\n\n            if len(roles) > 1:\n                new_user = appbuilder.sm.find_user(email=user[\"email\"])\n                new_user.roles = roles\n                appbuilder.sm.update_user(new_user)\n\n            users_created.append(user[\"email\"])\n\n    return users_created, users_updated\n\n\n@cli_utils.action_logging\ndef roles(args):\n    if args.create and args.list:\n        raise AirflowException(\n            \"Please specify either --create or --list, \" \"but not both\"\n        )\n\n    appbuilder = cached_appbuilder()\n    if args.create:\n        for role_name in args.role:\n            appbuilder.sm.add_role(role_name)\n    elif args.list:\n        roles = appbuilder.sm.get_all_roles()\n        print(\"Existing roles:\\n\")\n        role_names = sorted([[r.name] for r in roles])\n        msg = tabulate(role_names, headers=[\"Role\"], tablefmt=\"fancy_grid\")\n        print(msg)\n\n\n@cli_utils.action_logging\ndef list_dag_runs(args, dag=None):\n    if dag:\n        args.dag_id = dag.dag_id\n\n    dagbag = DagBag()\n\n    if args.dag_id not in dagbag.dags:\n        error_message = \"Dag id {} not found\".format(args.dag_id)\n        raise AirflowException(error_message)\n\n    dag_runs = list()\n    state = args.state.lower() if args.state else None\n    for run in DagRun.find(\n        dag_id=args.dag_id, state=state, no_backfills=args.no_backfill\n    ):\n        dag_runs.append(\n            {\n                \"id\": run.id,\n                \"run_id\": run.run_id,\n                \"state\": run.state,\n                \"dag_id\": run.dag_id,\n                \"execution_date\": run.execution_date.isoformat(),\n                \"start_date\": ((run.start_date or \"\") and run.start_date.isoformat()),\n            }\n        )\n    if not dag_runs:\n        print(\"No dag runs for {dag_id}\".format(dag_id=args.dag_id))\n\n    s = textwrap.dedent(\n        \"\"\"\\n\n    {line}\n    DAG RUNS\n    {line}\n    {dag_run_header}\n    \"\"\"\n    )\n\n    dag_runs.sort(key=lambda x: x[\"execution_date\"], reverse=True)\n    dag_run_header = \"%-3s | %-20s | %-10s | %-20s | %-20s |\" % (\n        \"id\",\n        \"run_id\",\n        \"state\",\n        \"execution_date\",\n        \"state_date\",\n    )\n    print(s.format(dag_run_header=dag_run_header, line=\"-\" * 120))\n    for dag_run in dag_runs:\n        record = \"%-3s | %-20s | %-10s | %-20s | %-20s |\" % (\n            dag_run[\"id\"],\n            dag_run[\"run_id\"],\n            dag_run[\"state\"],\n            dag_run[\"execution_date\"],\n            dag_run[\"start_date\"],\n        )\n        print(record)\n\n\n@cli_utils.action_logging\ndef sync_perm(args):\n    appbuilder = cached_appbuilder()\n    print(\"Updating permission, view-menu for all existing roles\")\n    appbuilder.sm.sync_roles()\n    print(\"Updating permission on all DAG views\")\n    dags = DagBag().dags.values()\n    for dag in dags:\n        appbuilder.sm.sync_perm_for_dag(dag.dag_id, dag.access_control)\n\n\nclass Arg(object):\n    def __init__(\n        self,\n        flags=None,\n        help=None,\n        action=None,\n        default=None,\n        nargs=None,\n        type=None,\n        choices=None,\n        metavar=None,\n    ):\n        self.flags = flags\n        self.help = help\n        self.action = action\n        self.default = default\n        self.nargs = nargs\n        self.type = type\n        self.choices = choices\n        self.metavar = metavar\n\n\nclass CLIFactory(object):\n    args = {\n        # Shared\n        \"dag_id\": Arg((\"dag_id\",), \"The id of the dag\"),\n        \"task_id\": Arg((\"task_id\",), \"The id of the task\"),\n        \"execution_date\": Arg(\n            (\"execution_date\",), help=\"The execution date of the DAG\", type=parsedate\n        ),\n        \"task_regex\": Arg(\n            (\"-t\", \"--task_regex\"),\n            \"The regex to filter specific task_ids to backfill (optional)\",\n        ),\n        \"subdir\": Arg(\n            (\"-sd\", \"--subdir\"),\n            \"File location or directory from which to look for the dag. \"\n            \"Defaults to '[AIRFLOW_HOME]/dags' where [AIRFLOW_HOME] is the \"\n            \"value you set for 'AIRFLOW_HOME' config you set in 'airflow.cfg' \",\n            default=DAGS_FOLDER,\n        ),\n        \"start_date\": Arg(\n            (\"-s\", \"--start_date\"), \"Override start_date YYYY-MM-DD\", type=parsedate\n        ),\n        \"end_date\": Arg(\n            (\"-e\", \"--end_date\"), \"Override end_date YYYY-MM-DD\", type=parsedate\n        ),\n        \"dry_run\": Arg((\"-dr\", \"--dry_run\"), \"Perform a dry run\", \"store_true\"),\n        \"pid\": Arg((\"--pid\",), \"PID file location\", nargs=\"?\"),\n        \"daemon\": Arg(\n            (\"-D\", \"--daemon\"),\n            \"Daemonize instead of running \" \"in the foreground\",\n            \"store_true\",\n        ),\n        \"stderr\": Arg((\"--stderr\",), \"Redirect stderr to this file\"),\n        \"stdout\": Arg((\"--stdout\",), \"Redirect stdout to this file\"),\n        \"log_file\": Arg((\"-l\", \"--log-file\"), \"Location of the log file\"),\n        \"yes\": Arg(\n            (\"-y\", \"--yes\"),\n            \"Do not prompt to confirm reset. Use with care!\",\n            \"store_true\",\n            default=False,\n        ),\n        # list_dag_runs\n        \"no_backfill\": Arg(\n            (\"--no_backfill\",),\n            \"filter all the backfill dagruns given the dag id\",\n            \"store_true\",\n        ),\n        \"state\": Arg((\"--state\",), \"Only list the dag runs corresponding to the state\"),\n        # list_jobs\n        \"limit\": Arg((\"--limit\",), \"Return a limited number of records\"),\n        # backfill\n        \"mark_success\": Arg(\n            (\"-m\", \"--mark_success\"),\n            \"Mark jobs as succeeded without running them\",\n            \"store_true\",\n        ),\n        \"verbose\": Arg(\n            (\"-v\", \"--verbose\"), \"Make logging output more verbose\", \"store_true\"\n        ),\n        \"local\": Arg(\n            (\"-l\", \"--local\"), \"Run the task using the LocalExecutor\", \"store_true\"\n        ),\n        \"donot_pickle\": Arg(\n            (\"-x\", \"--donot_pickle\"),\n            (\n                \"Do not attempt to pickle the DAG object to send over \"\n                \"to the workers, just tell the workers to run their version \"\n                \"of the code.\"\n            ),\n            \"store_true\",\n        ),\n        \"bf_ignore_dependencies\": Arg(\n            (\"-i\", \"--ignore_dependencies\"),\n            (\n                \"Skip upstream tasks, run only the tasks \"\n                \"matching the regexp. Only works in conjunction \"\n                \"with task_regex\"\n            ),\n            \"store_true\",\n        ),\n        \"bf_ignore_first_depends_on_past\": Arg(\n            (\"-I\", \"--ignore_first_depends_on_past\"),\n            (\n                \"Ignores depends_on_past dependencies for the first \"\n                \"set of tasks only (subsequent executions in the backfill \"\n                \"DO respect depends_on_past).\"\n            ),\n            \"store_true\",\n        ),\n        \"pool\": Arg((\"--pool\",), \"Resource pool to use\"),\n        \"delay_on_limit\": Arg(\n            (\"--delay_on_limit\",),\n            help=(\n                \"Amount of time in seconds to wait when the limit \"\n                \"on maximum active dag runs (max_active_runs) has \"\n                \"been reached before trying to execute a dag run \"\n                \"again.\"\n            ),\n            type=float,\n            default=1.0,\n        ),\n        \"reset_dag_run\": Arg(\n            (\"--reset_dagruns\",),\n            (\n                \"if set, the backfill will delete existing \"\n                \"backfill-related DAG runs and start \"\n                \"anew with fresh, running DAG runs\"\n            ),\n            \"store_true\",\n        ),\n        \"rerun_failed_tasks\": Arg(\n            (\"--rerun_failed_tasks\",),\n            (\n                \"if set, the backfill will auto-rerun \"\n                \"all the failed tasks for the backfill date range \"\n                \"instead of throwing exceptions\"\n            ),\n            \"store_true\",\n        ),\n        \"run_backwards\": Arg(\n            (\n                \"-B\",\n                \"--run_backwards\",\n            ),\n            (\n                \"if set, the backfill will run tasks from the most \"\n                \"recent day first.  if there are tasks that depend_on_past \"\n                \"this option will throw an exception\"\n            ),\n            \"store_true\",\n        ),\n        # list_tasks\n        \"tree\": Arg((\"-t\", \"--tree\"), \"Tree view\", \"store_true\"),\n        # list_dags\n        \"report\": Arg((\"-r\", \"--report\"), \"Show DagBag loading report\", \"store_true\"),\n        # clear\n        \"upstream\": Arg((\"-u\", \"--upstream\"), \"Include upstream tasks\", \"store_true\"),\n        \"only_failed\": Arg((\"-f\", \"--only_failed\"), \"Only failed jobs\", \"store_true\"),\n        \"only_running\": Arg(\n            (\"-r\", \"--only_running\"), \"Only running jobs\", \"store_true\"\n        ),\n        \"downstream\": Arg(\n            (\"-d\", \"--downstream\"), \"Include downstream tasks\", \"store_true\"\n        ),\n        \"no_confirm\": Arg(\n            (\"-c\", \"--no_confirm\"), \"Do not request confirmation\", \"store_true\"\n        ),\n        \"exclude_subdags\": Arg(\n            (\"-x\", \"--exclude_subdags\"), \"Exclude subdags\", \"store_true\"\n        ),\n        \"exclude_parentdag\": Arg(\n            (\"-xp\", \"--exclude_parentdag\"),\n            \"Exclude ParentDAGS if the task cleared is a part of a SubDAG\",\n            \"store_true\",\n        ),\n        \"dag_regex\": Arg(\n            (\"-dx\", \"--dag_regex\"),\n            \"Search dag_id as regex instead of exact string\",\n            \"store_true\",\n        ),\n        # trigger_dag\n        \"run_id\": Arg((\"-r\", \"--run_id\"), \"Helps to identify this run\"),\n        \"conf\": Arg(\n            (\"-c\", \"--conf\"),\n            \"JSON string that gets pickled into the DagRun's conf attribute\",\n        ),\n        \"exec_date\": Arg(\n            (\"-e\", \"--exec_date\"), help=\"The execution date of the DAG\", type=parsedate\n        ),\n        # pool\n        \"pool_set\": Arg(\n            (\"-s\", \"--set\"),\n            nargs=3,\n            metavar=(\"NAME\", \"SLOT_COUNT\", \"POOL_DESCRIPTION\"),\n            help=\"Set pool slot count and description, respectively\",\n        ),\n        \"pool_get\": Arg((\"-g\", \"--get\"), metavar=\"NAME\", help=\"Get pool info\"),\n        \"pool_delete\": Arg((\"-x\", \"--delete\"), metavar=\"NAME\", help=\"Delete a pool\"),\n        \"pool_import\": Arg(\n            (\"-i\", \"--import\"), metavar=\"FILEPATH\", help=\"Import pool from JSON file\"\n        ),\n        \"pool_export\": Arg(\n            (\"-e\", \"--export\"), metavar=\"FILEPATH\", help=\"Export pool to JSON file\"\n        ),\n        # variables\n        \"set\": Arg(\n            (\"-s\", \"--set\"), nargs=2, metavar=(\"KEY\", \"VAL\"), help=\"Set a variable\"\n        ),\n        \"get\": Arg((\"-g\", \"--get\"), metavar=\"KEY\", help=\"Get value of a variable\"),\n        \"default\": Arg(\n            (\"-d\", \"--default\"),\n            metavar=\"VAL\",\n            default=None,\n            help=\"Default value returned if variable does not exist\",\n        ),\n        \"json\": Arg(\n            (\"-j\", \"--json\"), help=\"Deserialize JSON variable\", action=\"store_true\"\n        ),\n        \"var_import\": Arg(\n            (\"-i\", \"--import\"),\n            metavar=\"FILEPATH\",\n            help=\"Import variables from JSON file\",\n        ),\n        \"var_export\": Arg(\n            (\"-e\", \"--export\"), metavar=\"FILEPATH\", help=\"Export variables to JSON file\"\n        ),\n        \"var_delete\": Arg((\"-x\", \"--delete\"), metavar=\"KEY\", help=\"Delete a variable\"),\n        # kerberos\n        \"principal\": Arg((\"principal\",), \"kerberos principal\", nargs=\"?\"),\n        \"keytab\": Arg(\n            (\"-kt\", \"--keytab\"),\n            \"keytab\",\n            nargs=\"?\",\n            default=conf.get(\"kerberos\", \"keytab\"),\n        ),\n        # run\n        # TODO(aoen): \"force\" is a poor choice of name here since it implies it overrides\n        # all dependencies (not just past success), e.g. the ignore_depends_on_past\n        # dependency. This flag should be deprecated and renamed to 'ignore_ti_state' and\n        # the \"ignore_all_dependencies\" command should be called the\"force\" command\n        # instead.\n        \"interactive\": Arg(\n            (\"-int\", \"--interactive\"),\n            help=\"Do not capture standard output and error streams \"\n            \"(useful for interactive debugging)\",\n            action=\"store_true\",\n        ),\n        \"force\": Arg(\n            (\"-f\", \"--force\"),\n            \"Ignore previous task instance state, rerun regardless if task already \"\n            \"succeeded/failed\",\n            \"store_true\",\n        ),\n        \"raw\": Arg((\"-r\", \"--raw\"), argparse.SUPPRESS, \"store_true\"),\n        \"ignore_all_dependencies\": Arg(\n            (\"-A\", \"--ignore_all_dependencies\"),\n            \"Ignores all non-critical dependencies, including ignore_ti_state and \"\n            \"ignore_task_deps\",\n            \"store_true\",\n        ),\n        # TODO(aoen): ignore_dependencies is a poor choice of name here because it is too\n        # vague (e.g. a task being in the appropriate state to be run is also a dependency\n        # but is not ignored by this flag), the name 'ignore_task_dependencies' is\n        # slightly better (as it ignores all dependencies that are specific to the task),\n        # so deprecate the old command name and use this instead.\n        \"ignore_dependencies\": Arg(\n            (\"-i\", \"--ignore_dependencies\"),\n            \"Ignore task-specific dependencies, e.g. upstream, depends_on_past, and \"\n            \"retry delay dependencies\",\n            \"store_true\",\n        ),\n        \"ignore_depends_on_past\": Arg(\n            (\"-I\", \"--ignore_depends_on_past\"),\n            \"Ignore depends_on_past dependencies (but respect \"\n            \"upstream dependencies)\",\n            \"store_true\",\n        ),\n        \"ship_dag\": Arg(\n            (\"--ship_dag\",),\n            \"Pickles (serializes) the DAG and ships it to the worker\",\n            \"store_true\",\n        ),\n        \"pickle\": Arg(\n            (\"-p\", \"--pickle\"),\n            \"Serialized pickle object of the entire dag (used internally)\",\n        ),\n        \"job_id\": Arg((\"-j\", \"--job_id\"), argparse.SUPPRESS),\n        \"cfg_path\": Arg(\n            (\"--cfg_path\",), \"Path to config file to use instead of airflow.cfg\"\n        ),\n        # webserver\n        \"port\": Arg(\n            (\"-p\", \"--port\"),\n            default=conf.get(\"webserver\", \"WEB_SERVER_PORT\"),\n            type=int,\n            help=\"The port on which to run the server\",\n        ),\n        \"ssl_cert\": Arg(\n            (\"--ssl_cert\",),\n            default=conf.get(\"webserver\", \"WEB_SERVER_SSL_CERT\"),\n            help=\"Path to the SSL certificate for the webserver\",\n        ),\n        \"ssl_key\": Arg(\n            (\"--ssl_key\",),\n            default=conf.get(\"webserver\", \"WEB_SERVER_SSL_KEY\"),\n            help=\"Path to the key to use with the SSL certificate\",\n        ),\n        \"workers\": Arg(\n            (\"-w\", \"--workers\"),\n            default=conf.get(\"webserver\", \"WORKERS\"),\n            type=int,\n            help=\"Number of workers to run the webserver on\",\n        ),\n        \"workerclass\": Arg(\n            (\"-k\", \"--workerclass\"),\n            default=conf.get(\"webserver\", \"WORKER_CLASS\"),\n            choices=[\"sync\", \"eventlet\", \"gevent\", \"tornado\"],\n            help=\"The worker class to use for Gunicorn\",\n        ),\n        \"worker_timeout\": Arg(\n            (\"-t\", \"--worker_timeout\"),\n            default=conf.get(\"webserver\", \"WEB_SERVER_WORKER_TIMEOUT\"),\n            type=int,\n            help=\"The timeout for waiting on webserver workers\",\n        ),\n        \"hostname\": Arg(\n            (\"-hn\", \"--hostname\"),\n            default=conf.get(\"webserver\", \"WEB_SERVER_HOST\"),\n            help=\"Set the hostname on which to run the web server\",\n        ),\n        \"debug\": Arg(\n            (\"-d\", \"--debug\"),\n            \"Use the server that ships with Flask in debug mode\",\n            \"store_true\",\n        ),\n        \"access_logfile\": Arg(\n            (\"-A\", \"--access_logfile\"),\n            default=conf.get(\"webserver\", \"ACCESS_LOGFILE\"),\n            help=\"The logfile to store the webserver access log. Use '-' to print to \"\n            \"stderr.\",\n        ),\n        \"error_logfile\": Arg(\n            (\"-E\", \"--error_logfile\"),\n            default=conf.get(\"webserver\", \"ERROR_LOGFILE\"),\n            help=\"The logfile to store the webserver error log. Use '-' to print to \"\n            \"stderr.\",\n        ),\n        # scheduler\n        \"dag_id_opt\": Arg((\"-d\", \"--dag_id\"), help=\"The id of the dag to run\"),\n        \"num_runs\": Arg(\n            (\"-n\", \"--num_runs\"),\n            default=conf.getint(\"scheduler\", \"num_runs\"),\n            type=int,\n            help=\"Set the number of runs to execute before exiting\",\n        ),\n        # worker\n        \"do_pickle\": Arg(\n            (\"-p\", \"--do_pickle\"),\n            default=False,\n            help=(\n                \"Attempt to pickle the DAG object to send over \"\n                \"to the workers, instead of letting workers run their version \"\n                \"of the code.\"\n            ),\n            action=\"store_true\",\n        ),\n        \"queues\": Arg(\n            (\"-q\", \"--queues\"),\n            help=\"Comma delimited list of queues to serve\",\n            default=conf.get(\"celery\", \"DEFAULT_QUEUE\"),\n        ),\n        \"concurrency\": Arg(\n            (\"-c\", \"--concurrency\"),\n            type=int,\n            help=\"The number of worker processes\",\n            default=conf.get(\"celery\", \"worker_concurrency\"),\n        ),\n        \"celery_hostname\": Arg(\n            (\"-cn\", \"--celery_hostname\"),\n            help=(\n                \"Set the hostname of celery worker \"\n                \"if you have multiple workers on a single machine.\"\n            ),\n        ),\n        # flower\n        \"broker_api\": Arg((\"-a\", \"--broker_api\"), help=\"Broker api\"),\n        \"flower_hostname\": Arg(\n            (\"-hn\", \"--hostname\"),\n            default=conf.get(\"celery\", \"FLOWER_HOST\"),\n            help=\"Set the hostname on which to run the server\",\n        ),\n        \"flower_port\": Arg(\n            (\"-p\", \"--port\"),\n            default=conf.get(\"celery\", \"FLOWER_PORT\"),\n            type=int,\n            help=\"The port on which to run the server\",\n        ),\n        \"flower_conf\": Arg(\n            (\"-fc\", \"--flower_conf\"), help=\"Configuration file for flower\"\n        ),\n        \"flower_url_prefix\": Arg(\n            (\"-u\", \"--url_prefix\"),\n            default=conf.get(\"celery\", \"FLOWER_URL_PREFIX\"),\n            help=\"URL prefix for Flower\",\n        ),\n        \"flower_basic_auth\": Arg(\n            (\"-ba\", \"--basic_auth\"),\n            default=conf.get(\"celery\", \"FLOWER_BASIC_AUTH\"),\n            help=(\n                \"Securing Flower with Basic Authentication. \"\n                \"Accepts user:password pairs separated by a comma. \"\n                \"Example: flower_basic_auth = user1:password1,user2:password2\"\n            ),\n        ),\n        \"task_params\": Arg(\n            (\"-tp\", \"--task_params\"), help=\"Sends a JSON params dict to the task\"\n        ),\n        \"post_mortem\": Arg(\n            (\"-pm\", \"--post_mortem\"),\n            action=\"store_true\",\n            help=\"Open debugger on uncaught exception\",\n        ),\n        # connections\n        \"list_connections\": Arg(\n            (\"-l\", \"--list\"), help=\"List all connections\", action=\"store_true\"\n        ),\n        \"add_connection\": Arg(\n            (\"-a\", \"--add\"), help=\"Add a connection\", action=\"store_true\"\n        ),\n        \"delete_connection\": Arg(\n            (\"-d\", \"--delete\"), help=\"Delete a connection\", action=\"store_true\"\n        ),\n        \"conn_id\": Arg(\n            (\"--conn_id\",),\n            help=\"Connection id, required to add/delete a connection\",\n            type=str,\n        ),\n        \"conn_uri\": Arg(\n            (\"--conn_uri\",),\n            help=\"Connection URI, required to add a connection without conn_type\",\n            type=str,\n        ),\n        \"conn_type\": Arg(\n            (\"--conn_type\",),\n            help=\"Connection type, required to add a connection without conn_uri\",\n            type=str,\n        ),\n        \"conn_host\": Arg(\n            (\"--conn_host\",),\n            help=\"Connection host, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_login\": Arg(\n            (\"--conn_login\",),\n            help=\"Connection login, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_password\": Arg(\n            (\"--conn_password\",),\n            help=\"Connection password, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_schema\": Arg(\n            (\"--conn_schema\",),\n            help=\"Connection schema, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_port\": Arg(\n            (\"--conn_port\",),\n            help=\"Connection port, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_extra\": Arg(\n            (\"--conn_extra\",),\n            help=\"Connection `Extra` field, optional when adding a connection\",\n            type=str,\n        ),\n        # users\n        \"username\": Arg(\n            (\"--username\",),\n            help=\"Username of the user, required to create/delete a user\",\n            type=str,\n        ),\n        \"firstname\": Arg(\n            (\"--firstname\",),\n            help=\"First name of the user, required to create a user\",\n            type=str,\n        ),\n        \"lastname\": Arg(\n            (\"--lastname\",),\n            help=\"Last name of the user, required to create a user\",\n            type=str,\n        ),\n        \"role\": Arg(\n            (\"--role\",),\n            help=\"Role of the user. Existing roles include Admin, \"\n            \"User, Op, Viewer, and Public. Required to create a user\",\n            type=str,\n        ),\n        \"email\": Arg(\n            (\"--email\",), help=\"Email of the user, required to create a user\", type=str\n        ),\n        \"password\": Arg(\n            (\"--password\",),\n            help=\"Password of the user, required to create a user \"\n            \"without --use_random_password\",\n            type=str,\n        ),\n        \"use_random_password\": Arg(\n            (\"--use_random_password\",),\n            help=\"Do not prompt for password. Use random string instead.\"\n            \" Required to create a user without --password \",\n            default=False,\n            action=\"store_true\",\n        ),\n        \"list_users\": Arg((\"-l\", \"--list\"), help=\"List all users\", action=\"store_true\"),\n        \"create_user\": Arg(\n            (\"-c\", \"--create\"), help=\"Create a user\", action=\"store_true\"\n        ),\n        \"delete_user\": Arg(\n            (\"-d\", \"--delete\"), help=\"Delete a user\", action=\"store_true\"\n        ),\n        \"add_role\": Arg(\n            (\"--add-role\",), help=\"Add user to a role\", action=\"store_true\"\n        ),\n        \"remove_role\": Arg(\n            (\"--remove-role\",), help=\"Remove user from a role\", action=\"store_true\"\n        ),\n        \"user_import\": Arg(\n            (\"-i\", \"--import\"),\n            metavar=\"FILEPATH\",\n            help=\"Import users from JSON file. Example format:\"\n            + textwrap.dedent(\n                \"\"\"\n                    [\n                        {\n                            \"email\": \"foo@bar.org\",\n                            \"firstname\": \"Jon\",\n                            \"lastname\": \"Doe\",\n                            \"roles\": [\"Public\"],\n                            \"username\": \"jondoe\"\n                        }\n                    ]\"\"\"\n            ),\n        ),\n        \"user_export\": Arg(\n            (\"-e\", \"--export\"), metavar=\"FILEPATH\", help=\"Export users to JSON file\"\n        ),\n        # roles\n        \"create_role\": Arg(\n            (\"-c\", \"--create\"), help=\"Create a new role\", action=\"store_true\"\n        ),\n        \"list_roles\": Arg((\"-l\", \"--list\"), help=\"List roles\", action=\"store_true\"),\n        \"roles\": Arg((\"role\",), help=\"The name of a role\", nargs=\"*\"),\n        \"autoscale\": Arg(\n            (\"-a\", \"--autoscale\"),\n            help=\"Minimum and Maximum number of worker to autoscale\",\n        ),\n    }\n    subparsers = (\n        {\n            \"func\": backfill,\n            \"help\": \"Run subsections of a DAG for a specified date range. \"\n            \"If reset_dag_run option is used,\"\n            \" backfill will first prompt users whether airflow \"\n            \"should clear all the previous dag_run and task_instances \"\n            \"within the backfill date range. \"\n            \"If rerun_failed_tasks is used, backfill \"\n            \"will auto re-run the previous failed task instances\"\n            \" within the backfill date range.\",\n            \"args\": (\n                \"dag_id\",\n                \"task_regex\",\n                \"start_date\",\n                \"end_date\",\n                \"mark_success\",\n                \"local\",\n                \"donot_pickle\",\n                \"bf_ignore_dependencies\",\n                \"bf_ignore_first_depends_on_past\",\n                \"subdir\",\n                \"pool\",\n                \"delay_on_limit\",\n                \"dry_run\",\n                \"verbose\",\n                \"conf\",\n                \"reset_dag_run\",\n                \"rerun_failed_tasks\",\n                \"run_backwards\",\n            ),\n        },\n        {\n            \"func\": list_dag_runs,\n            \"help\": \"List dag runs given a DAG id. If state option is given, it will only\"\n            \"search for all the dagruns with the given state. \"\n            \"If no_backfill option is given, it will filter out\"\n            \"all backfill dagruns for given dag id.\",\n            \"args\": (\"dag_id\", \"no_backfill\", \"state\"),\n        },\n        {\n            \"func\": list_tasks,\n            \"help\": \"List the tasks within a DAG\",\n            \"args\": (\"dag_id\", \"tree\", \"subdir\"),\n        },\n        {\n            \"func\": list_jobs,\n            \"help\": \"List the jobs\",\n            \"args\": (\"dag_id_opt\", \"state\", \"limit\"),\n        },\n        {\n            \"func\": clear,\n            \"help\": \"Clear a set of task instance, as if they never ran\",\n            \"args\": (\n                \"dag_id\",\n                \"task_regex\",\n                \"start_date\",\n                \"end_date\",\n                \"subdir\",\n                \"upstream\",\n                \"downstream\",\n                \"no_confirm\",\n                \"only_failed\",\n                \"only_running\",\n                \"exclude_subdags\",\n                \"exclude_parentdag\",\n                \"dag_regex\",\n            ),\n        },\n        {\n            \"func\": pause,\n            \"help\": \"Pause a DAG\",\n            \"args\": (\"dag_id\", \"subdir\"),\n        },\n        {\n            \"func\": unpause,\n            \"help\": \"Resume a paused DAG\",\n            \"args\": (\"dag_id\", \"subdir\"),\n        },\n        {\n            \"func\": trigger_dag,\n            \"help\": \"Trigger a DAG run\",\n            \"args\": (\"dag_id\", \"subdir\", \"run_id\", \"conf\", \"exec_date\"),\n        },\n        {\n            \"func\": delete_dag,\n            \"help\": \"Delete all DB records related to the specified DAG\",\n            \"args\": (\n                \"dag_id\",\n                \"yes\",\n            ),\n        },\n        {\n            \"func\": pool,\n            \"help\": \"CRUD operations on pools\",\n            \"args\": (\n                \"pool_set\",\n                \"pool_get\",\n                \"pool_delete\",\n                \"pool_import\",\n                \"pool_export\",\n            ),\n        },\n        {\n            \"func\": variables,\n            \"help\": \"CRUD operations on variables\",\n            \"args\": (\n                \"set\",\n                \"get\",\n                \"json\",\n                \"default\",\n                \"var_import\",\n                \"var_export\",\n                \"var_delete\",\n            ),\n        },\n        {\n            \"func\": kerberos,\n            \"help\": \"Start a kerberos ticket renewer\",\n            \"args\": (\n                \"principal\",\n                \"keytab\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n            ),\n        },\n        {\n            \"func\": render,\n            \"help\": \"Render a task instance's template(s)\",\n            \"args\": (\"dag_id\", \"task_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": run,\n            \"help\": \"Run a single task instance\",\n            \"args\": (\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"subdir\",\n                \"mark_success\",\n                \"force\",\n                \"pool\",\n                \"cfg_path\",\n                \"local\",\n                \"raw\",\n                \"ignore_all_dependencies\",\n                \"ignore_dependencies\",\n                \"ignore_depends_on_past\",\n                \"ship_dag\",\n                \"pickle\",\n                \"job_id\",\n                \"interactive\",\n            ),\n        },\n        {\n            \"func\": initdb,\n            \"help\": \"Initialize the metadata database\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": list_dags,\n            \"help\": \"List all the DAGs\",\n            \"args\": (\"subdir\", \"report\"),\n        },\n        {\n            \"func\": dag_state,\n            \"help\": \"Get the status of a dag run\",\n            \"args\": (\"dag_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": task_failed_deps,\n            \"help\": (\n                \"Returns the unmet dependencies for a task instance from the perspective \"\n                \"of the scheduler. In other words, why a task instance doesn't get \"\n                \"scheduled and then queued by the scheduler, and then run by an \"\n                \"executor).\"\n            ),\n            \"args\": (\"dag_id\", \"task_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": task_state,\n            \"help\": \"Get the status of a task instance\",\n            \"args\": (\"dag_id\", \"task_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": serve_logs,\n            \"help\": \"Serve logs generate by worker\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": test,\n            \"help\": (\n                \"Test a task instance. This will run a task without checking for \"\n                \"dependencies or recording its state in the database.\"\n            ),\n            \"args\": (\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"subdir\",\n                \"dry_run\",\n                \"task_params\",\n                \"post_mortem\",\n            ),\n        },\n        {\n            \"func\": webserver,\n            \"help\": \"Start a Airflow webserver instance\",\n            \"args\": (\n                \"port\",\n                \"workers\",\n                \"workerclass\",\n                \"worker_timeout\",\n                \"hostname\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"access_logfile\",\n                \"error_logfile\",\n                \"log_file\",\n                \"ssl_cert\",\n                \"ssl_key\",\n                \"debug\",\n            ),\n        },\n        {\n            \"func\": resetdb,\n            \"help\": \"Burn down and rebuild the metadata database\",\n            \"args\": (\"yes\",),\n        },\n        {\n            \"func\": upgradedb,\n            \"help\": \"Upgrade the metadata database to latest version\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": scheduler,\n            \"help\": \"Start a scheduler instance\",\n            \"args\": (\n                \"dag_id_opt\",\n                \"subdir\",\n                \"num_runs\",\n                \"do_pickle\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n            ),\n        },\n        {\n            \"func\": worker,\n            \"help\": \"Start a Celery worker node\",\n            \"args\": (\n                \"do_pickle\",\n                \"queues\",\n                \"concurrency\",\n                \"celery_hostname\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n                \"autoscale\",\n            ),\n        },\n        {\n            \"func\": flower,\n            \"help\": \"Start a Celery Flower\",\n            \"args\": (\n                \"flower_hostname\",\n                \"flower_port\",\n                \"flower_conf\",\n                \"flower_url_prefix\",\n                \"flower_basic_auth\",\n                \"broker_api\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n            ),\n        },\n        {\n            \"func\": version,\n            \"help\": \"Show the version\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": connections,\n            \"help\": \"List/Add/Delete connections\",\n            \"args\": (\n                \"list_connections\",\n                \"add_connection\",\n                \"delete_connection\",\n                \"conn_id\",\n                \"conn_uri\",\n                \"conn_extra\",\n            )\n            + tuple(alternative_conn_specs),\n        },\n        {\n            \"func\": users,\n            \"help\": \"List/Create/Delete/Update users\",\n            \"args\": (\n                \"list_users\",\n                \"create_user\",\n                \"delete_user\",\n                \"add_role\",\n                \"remove_role\",\n                \"user_import\",\n                \"user_export\",\n                \"username\",\n                \"email\",\n                \"firstname\",\n                \"lastname\",\n                \"role\",\n                \"password\",\n                \"use_random_password\",\n            ),\n        },\n        {\n            \"func\": roles,\n            \"help\": \"Create/List roles\",\n            \"args\": (\"create_role\", \"list_roles\", \"roles\"),\n        },\n        {\n            \"func\": sync_perm,\n            \"help\": \"Update permissions for existing roles and DAGs.\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": next_execution,\n            \"help\": \"Get the next execution datetime of a DAG.\",\n            \"args\": (\"dag_id\", \"subdir\"),\n        },\n        {\n            \"func\": rotate_fernet_key,\n            \"help\": \"Rotate all encrypted connection credentials and variables; see \"\n            \"https://airflow.readthedocs.io/en/stable/howto/secure-connections.html\"\n            \"#rotating-encryption-keys.\",\n            \"args\": (),\n        },\n    )\n    subparsers_dict = {sp[\"func\"].__name__: sp for sp in subparsers}\n    dag_subparsers = (\n        \"list_tasks\",\n        \"backfill\",\n        \"test\",\n        \"run\",\n        \"pause\",\n        \"unpause\",\n        \"list_dag_runs\",\n    )\n\n    @classmethod\n    def get_parser(cls, dag_parser=False):\n        parser = argparse.ArgumentParser()\n        subparsers = parser.add_subparsers(help=\"sub-command help\", dest=\"subcommand\")\n        subparsers.required = True\n\n        subparser_list = (\n            cls.dag_subparsers if dag_parser else cls.subparsers_dict.keys()\n        )\n        for sub in sorted(subparser_list):\n            sub = cls.subparsers_dict[sub]\n            sp = subparsers.add_parser(sub[\"func\"].__name__, help=sub[\"help\"])\n            sp.formatter_class = RawTextHelpFormatter\n            for arg in sub[\"args\"]:\n                if \"dag_id\" in arg and dag_parser:\n                    continue\n                arg = cls.args[arg]\n                kwargs = {f: v for f, v in vars(arg).items() if f != \"flags\" and v}\n                sp.add_argument(*arg.flags, **kwargs)\n            sp.set_defaults(func=sub[\"func\"])\n        return parser\n\n\ndef get_parser():\n    return CLIFactory.get_parser()\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "package": ["import importlib", "import logging", "import os", "import subprocess", "import textwrap", "import random", "import string", "from importlib import import_module", "import getpass", "import reprlib", "import argparse", "from argparse import RawTextHelpFormatter", "from builtins import input", "from airflow.utils.timezone import parse as parsedate", "import json", "from tabulate import tabulate", "import daemon", "from daemon.pidfile import TimeoutPIDLockFile", "import signal", "import sys", "import threading", "import traceback", "import time", "import psutil", "import re", "from urllib.parse import urlunparse", "from typing import Any", "import airflow", "from airflow import api", "from airflow import jobs, settings", "from airflow import configuration as conf", "from airflow.exceptions import AirflowException, AirflowWebServerTimeout", "from airflow.executors import get_default_executor", "from airflow.models import (", "from airflow.ti_deps.dep_context import DepContext, SCHEDULER_DEPS", "from airflow.utils import cli as cli_utils, db", "from airflow.utils.net import get_hostname", "from airflow.utils.log.logging_mixin import (", "from airflow.www.app import cached_app, create_app, cached_appbuilder", "from sqlalchemy.orm import exc", "import flask", "from airflow.executors.celery_executor import app as celery_app", "from celery.bin import worker", "import airflow.security.kerberos"], "function": ["def sigint_handler(sig, frame):\n", "def sigquit_handler(sig, frame):\n", "def setup_logging(filename):\n", "def setup_locations(process, pid=None, stdout=None, stderr=None, log=None):\n", "def process_subdir(subdir):\n", "def get_dag(args):\n", "def get_dags(args):\n", "def backfill(args, dag=None):\n", "def trigger_dag(args):\n", "def delete_dag(args):\n", "def pool(args):\n", "    def _tabulate(pools):\n", "def pool_import_helper(filepath):\n", "def pool_export_helper(filepath):\n", "def variables(args):\n", "def import_helper(filepath):\n", "def export_helper(filepath):\n", "def pause(args, dag=None):\n", "def unpause(args, dag=None):\n", "def set_is_paused(is_paused, args, dag=None):\n", "def _run(args, dag, ti):\n", "def run(args, dag=None):\n", "def task_failed_deps(args):\n", "def dag_state(args):\n", "def next_execution(args):\n", "def rotate_fernet_key(args):\n", "def list_dags(args):\n", "def list_tasks(args, dag=None):\n", "def list_jobs(args, dag=None):\n", "def test(args, dag=None):\n", "def render(args):\n", "def clear(args):\n", "def get_num_ready_workers_running(gunicorn_master_proc):\n", "    def ready_prefix_on_cmdline(proc):\n", "def get_num_workers_running(gunicorn_master_proc):\n", "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n", "    def wait_until_true(fn, timeout=0):\n", "    def start_refresh(gunicorn_master_proc):\n", "def webserver(args):\n", "def scheduler(args):\n", "def serve_logs(args):\n", "    def serve_logs(filename):\n", "def worker(args):\n", "def initdb(args):\n", "def resetdb(args):\n", "def upgradedb(args):\n", "def version(args):\n", "def connections(args):\n", "def flower(args):\n", "def kerberos(args):\n", "def users(args):\n", "def _import_users(users_list):\n", "def roles(args):\n", "def list_dag_runs(args, dag=None):\n", "def sync_perm(args):\n", "class Arg(object):\n", "class CLIFactory(object):\n", "    def get_parser(cls, dag_parser=False):\n", "def get_parser():\n"]}
{"repo": "apache/airflow", "path": "airflow/bin/cli.py", "func_name": "restart_workers", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.", "docstring_tokens": ["Runs", "forever", "monitoring", "the", "child", "processes", "of"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L763-L868", "partition": "test", "up_fun_num": 36, "context": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport importlib\nimport logging\n\nimport os\nimport subprocess\nimport textwrap\nimport random\nimport string\nfrom importlib import import_module\n\nimport getpass\nimport reprlib\nimport argparse\nfrom argparse import RawTextHelpFormatter\nfrom builtins import input\n\nfrom airflow.utils.timezone import parse as parsedate\nimport json\nfrom tabulate import tabulate\n\nimport daemon\nfrom daemon.pidfile import TimeoutPIDLockFile\nimport signal\nimport sys\nimport threading\nimport traceback\nimport time\nimport psutil\nimport re\nfrom urllib.parse import urlunparse\nfrom typing import Any\n\nimport airflow\nfrom airflow import api\nfrom airflow import jobs, settings\nfrom airflow import configuration as conf\nfrom airflow.exceptions import AirflowException, AirflowWebServerTimeout\nfrom airflow.executors import get_default_executor\nfrom airflow.models import (\n    Connection,\n    DagModel,\n    DagBag,\n    DagPickle,\n    TaskInstance,\n    DagRun,\n    Variable,\n    DAG,\n)\nfrom airflow.ti_deps.dep_context import DepContext, SCHEDULER_DEPS\nfrom airflow.utils import cli as cli_utils, db\nfrom airflow.utils.net import get_hostname\nfrom airflow.utils.log.logging_mixin import (\n    LoggingMixin,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom airflow.www.app import cached_app, create_app, cached_appbuilder\n\nfrom sqlalchemy.orm import exc\n\napi.load_auth()\napi_module = import_module(conf.get(\"cli\", \"api_client\"))  # type: Any\napi_client = api_module.Client(\n    api_base_url=conf.get(\"cli\", \"endpoint_url\"), auth=api.api_auth.client_auth\n)\n\nlog = LoggingMixin().log\n\nDAGS_FOLDER = settings.DAGS_FOLDER\n\nif \"BUILDING_AIRFLOW_DOCS\" in os.environ:\n    DAGS_FOLDER = \"[AIRFLOW_HOME]/dags\"\n\n\ndef sigint_handler(sig, frame):\n    sys.exit(0)\n\n\ndef sigquit_handler(sig, frame):\n    \"\"\"Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n    e.g. kill -s QUIT <PID> or CTRL+\\\n    \"\"\"\n    print(\"Dumping stack traces for all threads in PID {}\".format(os.getpid()))\n    id_to_name = dict([(th.ident, th.name) for th in threading.enumerate()])\n    code = []\n    for thread_id, stack in sys._current_frames().items():\n        code.append(\n            \"\\n# Thread: {}({})\".format(id_to_name.get(thread_id, \"\"), thread_id)\n        )\n        for filename, line_number, name, line in traceback.extract_stack(stack):\n            code.append(\n                'File: \"{}\", line {}, in {}'.format(filename, line_number, name)\n            )\n            if line:\n                code.append(\"  {}\".format(line.strip()))\n    print(\"\\n\".join(code))\n\n\ndef setup_logging(filename):\n    root = logging.getLogger()\n    handler = logging.FileHandler(filename)\n    formatter = logging.Formatter(settings.SIMPLE_LOG_FORMAT)\n    handler.setFormatter(formatter)\n    root.addHandler(handler)\n    root.setLevel(settings.LOGGING_LEVEL)\n\n    return handler.stream\n\n\ndef setup_locations(process, pid=None, stdout=None, stderr=None, log=None):\n    if not stderr:\n        stderr = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.err\".format(process))\n    if not stdout:\n        stdout = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.out\".format(process))\n    if not log:\n        log = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.log\".format(process))\n    if not pid:\n        pid = os.path.join(settings.AIRFLOW_HOME, \"airflow-{}.pid\".format(process))\n\n    return pid, stdout, stderr, log\n\n\ndef process_subdir(subdir):\n    if subdir:\n        subdir = subdir.replace(\"DAGS_FOLDER\", DAGS_FOLDER)\n        subdir = os.path.abspath(os.path.expanduser(subdir))\n        return subdir\n\n\ndef get_dag(args):\n    dagbag = DagBag(process_subdir(args.subdir))\n    if args.dag_id not in dagbag.dags:\n        raise AirflowException(\n            \"dag_id could not be found: {}. Either the dag did not exist or it failed to \"\n            \"parse.\".format(args.dag_id)\n        )\n    return dagbag.dags[args.dag_id]\n\n\ndef get_dags(args):\n    if not args.dag_regex:\n        return [get_dag(args)]\n    dagbag = DagBag(process_subdir(args.subdir))\n    matched_dags = [\n        dag for dag in dagbag.dags.values() if re.search(args.dag_id, dag.dag_id)\n    ]\n    if not matched_dags:\n        raise AirflowException(\n            \"dag_id could not be found with regex: {}. Either the dag did not exist \"\n            \"or it failed to parse.\".format(args.dag_id)\n        )\n    return matched_dags\n\n\n@cli_utils.action_logging\ndef backfill(args, dag=None):\n    logging.basicConfig(level=settings.LOGGING_LEVEL, format=settings.SIMPLE_LOG_FORMAT)\n\n    signal.signal(signal.SIGTERM, sigint_handler)\n\n    dag = dag or get_dag(args)\n\n    if not args.start_date and not args.end_date:\n        raise AirflowException(\"Provide a start_date and/or end_date\")\n\n    # If only one date is passed, using same as start and end\n    args.end_date = args.end_date or args.start_date\n    args.start_date = args.start_date or args.end_date\n\n    if args.task_regex:\n        dag = dag.sub_dag(\n            task_regex=args.task_regex, include_upstream=not args.ignore_dependencies\n        )\n\n    run_conf = None\n    if args.conf:\n        run_conf = json.loads(args.conf)\n\n    if args.dry_run:\n        print(\"Dry run of DAG {0} on {1}\".format(args.dag_id, args.start_date))\n        for task in dag.tasks:\n            print(\"Task {0}\".format(task.task_id))\n            ti = TaskInstance(task, args.start_date)\n            ti.dry_run()\n    else:\n        if args.reset_dagruns:\n            DAG.clear_dags(\n                [dag],\n                start_date=args.start_date,\n                end_date=args.end_date,\n                confirm_prompt=True,\n                include_subdags=True,\n            )\n\n        dag.run(\n            start_date=args.start_date,\n            end_date=args.end_date,\n            mark_success=args.mark_success,\n            local=args.local,\n            donot_pickle=(args.donot_pickle or conf.getboolean(\"core\", \"donot_pickle\")),\n            ignore_first_depends_on_past=args.ignore_first_depends_on_past,\n            ignore_task_deps=args.ignore_dependencies,\n            pool=args.pool,\n            delay_on_limit_secs=args.delay_on_limit,\n            verbose=args.verbose,\n            conf=run_conf,\n            rerun_failed_tasks=args.rerun_failed_tasks,\n            run_backwards=args.run_backwards,\n        )\n\n\n@cli_utils.action_logging\ndef trigger_dag(args):\n    \"\"\"\n    Creates a dag run for the specified dag\n    :param args:\n    :return:\n    \"\"\"\n    log = LoggingMixin().log\n    try:\n        message = api_client.trigger_dag(\n            dag_id=args.dag_id,\n            run_id=args.run_id,\n            conf=args.conf,\n            execution_date=args.exec_date,\n        )\n    except IOError as err:\n        log.error(err)\n        raise AirflowException(err)\n    log.info(message)\n\n\n@cli_utils.action_logging\ndef delete_dag(args):\n    \"\"\"\n    Deletes all DB records related to the specified dag\n    :param args:\n    :return:\n    \"\"\"\n    log = LoggingMixin().log\n    if (\n        args.yes\n        or input(\n            \"This will drop all existing records related to the specified DAG. \"\n            \"Proceed? (y/n)\"\n        ).upper()\n        == \"Y\"\n    ):\n        try:\n            message = api_client.delete_dag(dag_id=args.dag_id)\n        except IOError as err:\n            log.error(err)\n            raise AirflowException(err)\n        log.info(message)\n    else:\n        print(\"Bail.\")\n\n\n@cli_utils.action_logging\ndef pool(args):\n    log = LoggingMixin().log\n\n    def _tabulate(pools):\n        return \"\\n%s\" % tabulate(\n            pools, [\"Pool\", \"Slots\", \"Description\"], tablefmt=\"fancy_grid\"\n        )\n\n    try:\n        imp = getattr(args, \"import\")\n        if args.get is not None:\n            pools = [api_client.get_pool(name=args.get)]\n        elif args.set:\n            pools = [\n                api_client.create_pool(\n                    name=args.set[0], slots=args.set[1], description=args.set[2]\n                )\n            ]\n        elif args.delete:\n            pools = [api_client.delete_pool(name=args.delete)]\n        elif imp:\n            if os.path.exists(imp):\n                pools = pool_import_helper(imp)\n            else:\n                print(\"Missing pools file.\")\n                pools = api_client.get_pools()\n        elif args.export:\n            pools = pool_export_helper(args.export)\n        else:\n            pools = api_client.get_pools()\n    except (AirflowException, IOError) as err:\n        log.error(err)\n    else:\n        log.info(_tabulate(pools=pools))\n\n\ndef pool_import_helper(filepath):\n    with open(filepath, \"r\") as poolfile:\n        pl = poolfile.read()\n    try:\n        d = json.loads(pl)\n    except Exception as e:\n        print(\"Please check the validity of the json file: \" + str(e))\n    else:\n        try:\n            pools = []\n            n = 0\n            for k, v in d.items():\n                if isinstance(v, dict) and len(v) == 2:\n                    pools.append(\n                        api_client.create_pool(\n                            name=k, slots=v[\"slots\"], description=v[\"description\"]\n                        )\n                    )\n                    n += 1\n                else:\n                    pass\n        except Exception:\n            pass\n        finally:\n            print(\"{} of {} pool(s) successfully updated.\".format(n, len(d)))\n            return pools\n\n\ndef pool_export_helper(filepath):\n    pool_dict = {}\n    pools = api_client.get_pools()\n    for pool in pools:\n        pool_dict[pool[0]] = {\"slots\": pool[1], \"description\": pool[2]}\n    with open(filepath, \"w\") as poolfile:\n        poolfile.write(json.dumps(pool_dict, sort_keys=True, indent=4))\n    print(\"{} pools successfully exported to {}\".format(len(pool_dict), filepath))\n    return pools\n\n\n@cli_utils.action_logging\ndef variables(args):\n    if args.get:\n        try:\n            var = Variable.get(\n                args.get, deserialize_json=args.json, default_var=args.default\n            )\n            print(var)\n        except ValueError as e:\n            print(e)\n    if args.delete:\n        Variable.delete(args.delete)\n    if args.set:\n        Variable.set(args.set[0], args.set[1])\n    # Work around 'import' as a reserved keyword\n    imp = getattr(args, \"import\")\n    if imp:\n        if os.path.exists(imp):\n            import_helper(imp)\n        else:\n            print(\"Missing variables file.\")\n    if args.export:\n        export_helper(args.export)\n    if not (args.set or args.get or imp or args.export or args.delete):\n        # list all variables\n        with db.create_session() as session:\n            vars = session.query(Variable)\n            msg = \"\\n\".join(var.key for var in vars)\n            print(msg)\n\n\ndef import_helper(filepath):\n    with open(filepath, \"r\") as varfile:\n        var = varfile.read()\n\n    try:\n        d = json.loads(var)\n    except Exception:\n        print(\"Invalid variables file.\")\n    else:\n        try:\n            n = 0\n            for k, v in d.items():\n                if isinstance(v, dict):\n                    Variable.set(k, v, serialize_json=True)\n                else:\n                    Variable.set(k, v)\n                n += 1\n        except Exception:\n            pass\n        finally:\n            print(\"{} of {} variables successfully updated.\".format(n, len(d)))\n\n\ndef export_helper(filepath):\n    var_dict = {}\n    with db.create_session() as session:\n        qry = session.query(Variable).all()\n\n        d = json.JSONDecoder()\n        for var in qry:\n            try:\n                val = d.decode(var.val)\n            except Exception:\n                val = var.val\n            var_dict[var.key] = val\n\n    with open(filepath, \"w\") as varfile:\n        varfile.write(json.dumps(var_dict, sort_keys=True, indent=4))\n    print(\"{} variables successfully exported to {}\".format(len(var_dict), filepath))\n\n\n@cli_utils.action_logging\ndef pause(args, dag=None):\n    set_is_paused(True, args, dag)\n\n\n@cli_utils.action_logging\ndef unpause(args, dag=None):\n    set_is_paused(False, args, dag)\n\n\ndef set_is_paused(is_paused, args, dag=None):\n    dag = dag or get_dag(args)\n\n    with db.create_session() as session:\n        dm = session.query(DagModel).filter(DagModel.dag_id == dag.dag_id).first()\n        dm.is_paused = is_paused\n        session.commit()\n\n    print(\"Dag: {}, paused: {}\".format(dag, str(dag.is_paused)))\n\n\ndef _run(args, dag, ti):\n    if args.local:\n        run_job = jobs.LocalTaskJob(\n            task_instance=ti,\n            mark_success=args.mark_success,\n            pickle_id=args.pickle,\n            ignore_all_deps=args.ignore_all_dependencies,\n            ignore_depends_on_past=args.ignore_depends_on_past,\n            ignore_task_deps=args.ignore_dependencies,\n            ignore_ti_state=args.force,\n            pool=args.pool,\n        )\n        run_job.run()\n    elif args.raw:\n        ti._run_raw_task(\n            mark_success=args.mark_success,\n            job_id=args.job_id,\n            pool=args.pool,\n        )\n    else:\n        pickle_id = None\n        if args.ship_dag:\n            try:\n                # Running remotely, so pickling the DAG\n                with db.create_session() as session:\n                    pickle = DagPickle(dag)\n                    session.add(pickle)\n                    pickle_id = pickle.id\n                    # TODO: This should be written to a log\n                    print(\n                        \"Pickled dag {dag} as pickle_id: {pickle_id}\".format(\n                            dag=dag, pickle_id=pickle_id\n                        )\n                    )\n            except Exception as e:\n                print(\"Could not pickle the DAG\")\n                print(e)\n                raise e\n\n        executor = get_default_executor()\n        executor.start()\n        print(\"Sending to executor.\")\n        executor.queue_task_instance(\n            ti,\n            mark_success=args.mark_success,\n            pickle_id=pickle_id,\n            ignore_all_deps=args.ignore_all_dependencies,\n            ignore_depends_on_past=args.ignore_depends_on_past,\n            ignore_task_deps=args.ignore_dependencies,\n            ignore_ti_state=args.force,\n            pool=args.pool,\n        )\n        executor.heartbeat()\n        executor.end()\n\n\n@cli_utils.action_logging\ndef run(args, dag=None):\n    if dag:\n        args.dag_id = dag.dag_id\n\n    log = LoggingMixin().log\n\n    # Load custom airflow config\n    if args.cfg_path:\n        with open(args.cfg_path, \"r\") as conf_file:\n            conf_dict = json.load(conf_file)\n\n        if os.path.exists(args.cfg_path):\n            os.remove(args.cfg_path)\n\n        conf.conf.read_dict(conf_dict, source=args.cfg_path)\n        settings.configure_vars()\n\n    # IMPORTANT, have to use the NullPool, otherwise, each \"run\" command may leave\n    # behind multiple open sleeping connections while heartbeating, which could\n    # easily exceed the database connection limit when\n    # processing hundreds of simultaneous tasks.\n    settings.configure_orm(disable_connection_pool=True)\n\n    if not args.pickle and not dag:\n        dag = get_dag(args)\n    elif not dag:\n        with db.create_session() as session:\n            log.info(\"Loading pickle id %s\", args.pickle)\n            dag_pickle = (\n                session.query(DagPickle).filter(DagPickle.id == args.pickle).first()\n            )\n            if not dag_pickle:\n                raise AirflowException(\"Who hid the pickle!? [missing pickle]\")\n            dag = dag_pickle.pickle\n\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    ti.refresh_from_db()\n\n    ti.init_run_context(raw=args.raw)\n\n    hostname = get_hostname()\n    log.info(\"Running %s on host %s\", ti, hostname)\n\n    if args.interactive:\n        _run(args, dag, ti)\n    else:\n        with redirect_stdout(ti.log, logging.INFO), redirect_stderr(\n            ti.log, logging.WARN\n        ):\n            _run(args, dag, ti)\n    logging.shutdown()\n\n\n@cli_utils.action_logging\ndef task_failed_deps(args):\n    \"\"\"\n    Returns the unmet dependencies for a task instance from the perspective of the\n    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n    scheduler, and then run by an executor).\n    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n    Task instance dependencies not met:\n    Dagrun Running: Task instance's dagrun did not exist: Unknown reason\n    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks\n    to have succeeded, but found 1 non-success(es).\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n\n    dep_context = DepContext(deps=SCHEDULER_DEPS)\n    failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))\n    # TODO, Do we want to print or log this\n    if failed_deps:\n        print(\"Task instance dependencies not met:\")\n        for dep in failed_deps:\n            print(\"{}: {}\".format(dep.dep_name, dep.reason))\n    else:\n        print(\"Task instance dependencies are all met.\")\n\n\n@cli_utils.action_logging\ndef task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())\n\n\n@cli_utils.action_logging\ndef dag_state(args):\n    \"\"\"\n    Returns the state of a DagRun at the command line.\n    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000\n    running\n    \"\"\"\n    dag = get_dag(args)\n    dr = DagRun.find(dag.dag_id, execution_date=args.execution_date)\n    print(dr[0].state if len(dr) > 0 else None)\n\n\n@cli_utils.action_logging\ndef next_execution(args):\n    \"\"\"\n    Returns the next execution datetime of a DAG at the command line.\n    >>> airflow next_execution tutorial\n    2018-08-31 10:38:00\n    \"\"\"\n    dag = get_dag(args)\n\n    if dag.is_paused:\n        print(\"[INFO] Please be reminded this DAG is PAUSED now.\")\n\n    if dag.latest_execution_date:\n        next_execution_dttm = dag.following_schedule(dag.latest_execution_date)\n\n        if next_execution_dttm is None:\n            print(\n                \"[WARN] No following schedule can be found. \"\n                + \"This DAG may have schedule interval '@once' or `None`.\"\n            )\n\n        print(next_execution_dttm)\n    else:\n        print(\n            \"[WARN] Only applicable when there is execution record found for the DAG.\"\n        )\n        print(None)\n\n\n@cli_utils.action_logging\ndef rotate_fernet_key(args):\n    with db.create_session() as session:\n        for conn in session.query(Connection).filter(\n            Connection.is_encrypted | Connection.is_extra_encrypted\n        ):\n            conn.rotate_fernet_key()\n        for var in session.query(Variable).filter(Variable.is_encrypted):\n            var.rotate_fernet_key()\n\n\n@cli_utils.action_logging\ndef list_dags(args):\n    dagbag = DagBag(process_subdir(args.subdir))\n    s = textwrap.dedent(\n        \"\"\"\\n\n    -------------------------------------------------------------------\n    DAGS\n    -------------------------------------------------------------------\n    {dag_list}\n    \"\"\"\n    )\n    dag_list = \"\\n\".join(sorted(dagbag.dags))\n    print(s.format(dag_list=dag_list))\n    if args.report:\n        print(dagbag.dagbag_report())\n\n\n@cli_utils.action_logging\ndef list_tasks(args, dag=None):\n    dag = dag or get_dag(args)\n    if args.tree:\n        dag.tree_view()\n    else:\n        tasks = sorted([t.task_id for t in dag.tasks])\n        print(\"\\n\".join(sorted(tasks)))\n\n\n@cli_utils.action_logging\ndef list_jobs(args, dag=None):\n    queries = []\n    if dag:\n        args.dag_id = dag.dag_id\n    if args.dag_id:\n        dagbag = DagBag()\n\n        if args.dag_id not in dagbag.dags:\n            error_message = \"Dag id {} not found\".format(args.dag_id)\n            raise AirflowException(error_message)\n        queries.append(jobs.BaseJob.dag_id == args.dag_id)\n\n    if args.state:\n        queries.append(jobs.BaseJob.state == args.state)\n\n    with db.create_session() as session:\n        all_jobs = (\n            session.query(jobs.BaseJob)\n            .filter(*queries)\n            .order_by(jobs.BaseJob.start_date.desc())\n            .limit(args.limit)\n            .all()\n        )\n        fields = [\"dag_id\", \"state\", \"job_type\", \"start_date\", \"end_date\"]\n        all_jobs = [\n            [job.__getattribute__(field) for field in fields] for job in all_jobs\n        ]\n        msg = tabulate(\n            all_jobs,\n            [field.capitalize().replace(\"_\", \" \") for field in fields],\n            tablefmt=\"fancy_grid\",\n        )\n        print(msg)\n\n\n@cli_utils.action_logging\ndef test(args, dag=None):\n    # We want log outout from operators etc to show up here. Normally\n    # airflow.task would redirect to a file, but here we want it to propagate\n    # up to the normal airflow handler.\n    logging.getLogger(\"airflow.task\").propagate = True\n\n    dag = dag or get_dag(args)\n\n    task = dag.get_task(task_id=args.task_id)\n    # Add CLI provided task_params to task.params\n    if args.task_params:\n        passed_in_params = json.loads(args.task_params)\n        task.params.update(passed_in_params)\n    ti = TaskInstance(task, args.execution_date)\n\n    try:\n        if args.dry_run:\n            ti.dry_run()\n        else:\n            ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)\n    except Exception:\n        if args.post_mortem:\n            try:\n                debugger = importlib.import_module(\"ipdb\")\n            except ImportError:\n                debugger = importlib.import_module(\"pdb\")\n            debugger.post_mortem()\n        else:\n            raise\n\n\n@cli_utils.action_logging\ndef render(args):\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    ti.render_templates()\n    for attr in task.__class__.template_fields:\n        print(\n            textwrap.dedent(\n                \"\"\"\\\n        # ----------------------------------------------------------\n        # property: {}\n        # ----------------------------------------------------------\n        {}\n        \"\"\".format(\n                    attr, getattr(task, attr)\n                )\n            )\n        )\n\n\n@cli_utils.action_logging\ndef clear(args):\n    logging.basicConfig(level=settings.LOGGING_LEVEL, format=settings.SIMPLE_LOG_FORMAT)\n    dags = get_dags(args)\n\n    if args.task_regex:\n        for idx, dag in enumerate(dags):\n            dags[idx] = dag.sub_dag(\n                task_regex=args.task_regex,\n                include_downstream=args.downstream,\n                include_upstream=args.upstream,\n            )\n\n    DAG.clear_dags(\n        dags,\n        start_date=args.start_date,\n        end_date=args.end_date,\n        only_failed=args.only_failed,\n        only_running=args.only_running,\n        confirm_prompt=not args.no_confirm,\n        include_subdags=not args.exclude_subdags,\n        include_parentdag=not args.exclude_parentdag,\n    )\n\n\ndef get_num_ready_workers_running(gunicorn_master_proc):\n    workers = psutil.Process(gunicorn_master_proc.pid).children()\n\n    def ready_prefix_on_cmdline(proc):\n        try:\n            cmdline = proc.cmdline()\n            if len(cmdline) > 0:\n                return settings.GUNICORN_WORKER_READY_PREFIX in cmdline[0]\n        except psutil.NoSuchProcess:\n            pass\n        return False\n\n    ready_workers = [proc for proc in workers if ready_prefix_on_cmdline(proc)]\n    return len(ready_workers)\n\n\ndef get_num_workers_running(gunicorn_master_proc):\n    workers = psutil.Process(gunicorn_master_proc.pid).children()\n    return len(workers)\n\n\n@cli_utils.action_logging\ndef webserver(args):\n    print(settings.HEADER)\n\n    access_logfile = args.access_logfile or conf.get(\"webserver\", \"access_logfile\")\n    error_logfile = args.error_logfile or conf.get(\"webserver\", \"error_logfile\")\n    num_workers = args.workers or conf.get(\"webserver\", \"workers\")\n    worker_timeout = args.worker_timeout or conf.get(\n        \"webserver\", \"web_server_worker_timeout\"\n    )\n    ssl_cert = args.ssl_cert or conf.get(\"webserver\", \"web_server_ssl_cert\")\n    ssl_key = args.ssl_key or conf.get(\"webserver\", \"web_server_ssl_key\")\n    if not ssl_cert and ssl_key:\n        raise AirflowException(\n            \"An SSL certificate must also be provided for use with \" + ssl_key\n        )\n    if ssl_cert and not ssl_key:\n        raise AirflowException(\n            \"An SSL key must also be provided for use with \" + ssl_cert\n        )\n\n    if args.debug:\n        print(\n            \"Starting the web server on port {0} and host {1}.\".format(\n                args.port, args.hostname\n            )\n        )\n        app, _ = create_app(None, testing=conf.get(\"core\", \"unit_test_mode\"))\n        app.run(\n            debug=True,\n            use_reloader=False if app.config[\"TESTING\"] else True,\n            port=args.port,\n            host=args.hostname,\n            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,\n        )\n    else:\n        os.environ[\"SKIP_DAGS_PARSING\"] = \"True\"\n        app = cached_app(None)\n        pid, stdout, stderr, log_file = setup_locations(\n            \"webserver\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        os.environ.pop(\"SKIP_DAGS_PARSING\")\n        if args.daemon:\n            handle = setup_logging(log_file)\n            stdout = open(stdout, \"w+\")\n            stderr = open(stderr, \"w+\")\n\n        print(\n            textwrap.dedent(\n                \"\"\"\\\n                Running the Gunicorn Server with:\n                Workers: {num_workers} {workerclass}\n                Host: {hostname}:{port}\n                Timeout: {worker_timeout}\n                Logfiles: {access_logfile} {error_logfile}\n                =================================================================\\\n            \"\"\".format(\n                    num_workers=num_workers,\n                    workerclass=args.workerclass,\n                    hostname=args.hostname,\n                    port=args.port,\n                    worker_timeout=worker_timeout,\n                    access_logfile=access_logfile,\n                    error_logfile=error_logfile,\n                )\n            )\n        )\n\n        run_args = [\n            \"gunicorn\",\n            \"-w\",\n            str(num_workers),\n            \"-k\",\n            str(args.workerclass),\n            \"-t\",\n            str(worker_timeout),\n            \"-b\",\n            args.hostname + \":\" + str(args.port),\n            \"-n\",\n            \"airflow-webserver\",\n            \"-p\",\n            str(pid),\n            \"-c\",\n            \"python:airflow.www.gunicorn_config\",\n        ]\n\n        if args.access_logfile:\n            run_args += [\"--access-logfile\", str(args.access_logfile)]\n\n        if args.error_logfile:\n            run_args += [\"--error-logfile\", str(args.error_logfile)]\n\n        if args.daemon:\n            run_args += [\"-D\"]\n\n        if ssl_cert:\n            run_args += [\"--certfile\", ssl_cert, \"--keyfile\", ssl_key]\n\n        webserver_module = \"www\"\n        run_args += [\"airflow.\" + webserver_module + \".app:cached_app()\"]\n\n        gunicorn_master_proc = None\n\n        def kill_proc(dummy_signum, dummy_frame):\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n            sys.exit(0)\n\n        def monitor_gunicorn(gunicorn_master_proc):\n            # These run forever until SIG{INT, TERM, KILL, ...} signal is sent\n            if conf.getint(\"webserver\", \"worker_refresh_interval\") > 0:\n                master_timeout = conf.getint(\"webserver\", \"web_server_master_timeout\")\n                restart_workers(gunicorn_master_proc, num_workers, master_timeout)\n            else:\n                while gunicorn_master_proc.poll() is None:\n                    time.sleep(1)\n\n                sys.exit(gunicorn_master_proc.returncode)\n\n        if args.daemon:\n            base, ext = os.path.splitext(pid)\n            ctx = daemon.DaemonContext(\n                pidfile=TimeoutPIDLockFile(base + \"-monitor\" + ext, -1),\n                files_preserve=[handle],\n                stdout=stdout,\n                stderr=stderr,\n                signal_map={signal.SIGINT: kill_proc, signal.SIGTERM: kill_proc},\n            )\n            with ctx:\n                subprocess.Popen(run_args, close_fds=True)\n\n                # Reading pid file directly, since Popen#pid doesn't\n                # seem to return the right value with DaemonContext.\n                while True:\n                    try:\n                        with open(pid) as f:\n                            gunicorn_master_proc_pid = int(f.read())\n                            break\n                    except IOError:\n                        log.debug(\"Waiting for gunicorn's pid file to be created.\")\n                        time.sleep(0.1)\n\n                gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)\n                monitor_gunicorn(gunicorn_master_proc)\n\n            stdout.close()\n            stderr.close()\n        else:\n            gunicorn_master_proc = subprocess.Popen(run_args, close_fds=True)\n\n            signal.signal(signal.SIGINT, kill_proc)\n            signal.signal(signal.SIGTERM, kill_proc)\n\n            monitor_gunicorn(gunicorn_master_proc)\n\n\n@cli_utils.action_logging\ndef scheduler(args):\n    print(settings.HEADER)\n    job = jobs.SchedulerJob(\n        dag_id=args.dag_id,\n        subdir=process_subdir(args.subdir),\n        num_runs=args.num_runs,\n        do_pickle=args.do_pickle,\n    )\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"scheduler\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        handle = setup_logging(log_file)\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            files_preserve=[handle],\n            stdout=stdout,\n            stderr=stderr,\n        )\n        with ctx:\n            job.run()\n\n        stdout.close()\n        stderr.close()\n    else:\n        signal.signal(signal.SIGINT, sigint_handler)\n        signal.signal(signal.SIGTERM, sigint_handler)\n        signal.signal(signal.SIGQUIT, sigquit_handler)\n        job.run()\n\n\n@cli_utils.action_logging\ndef serve_logs(args):\n    print(\"Starting flask\")\n    import flask\n\n    flask_app = flask.Flask(__name__)\n\n    @flask_app.route(\"/log/<path:filename>\")\n    def serve_logs(filename):\n        log = os.path.expanduser(conf.get(\"core\", \"BASE_LOG_FOLDER\"))\n        return flask.send_from_directory(\n            log, filename, mimetype=\"application/json\", as_attachment=False\n        )\n\n    worker_log_server_port = int(conf.get(\"celery\", \"WORKER_LOG_SERVER_PORT\"))\n    flask_app.run(host=\"0.0.0.0\", port=worker_log_server_port)\n\n\n@cli_utils.action_logging\ndef worker(args):\n    env = os.environ.copy()\n    env[\"AIRFLOW_HOME\"] = settings.AIRFLOW_HOME\n\n    if not settings.validate_session():\n        log = LoggingMixin().log\n        log.error(\"Worker exiting... database connection precheck failed! \")\n        sys.exit(1)\n\n    # Celery worker\n    from airflow.executors.celery_executor import app as celery_app\n    from celery.bin import worker\n\n    autoscale = args.autoscale\n    if autoscale is None and conf.has_option(\"celery\", \"worker_autoscale\"):\n        autoscale = conf.get(\"celery\", \"worker_autoscale\")\n    worker = worker.worker(app=celery_app)\n    options = {\n        \"optimization\": \"fair\",\n        \"O\": \"fair\",\n        \"queues\": args.queues,\n        \"concurrency\": args.concurrency,\n        \"autoscale\": autoscale,\n        \"hostname\": args.celery_hostname,\n        \"loglevel\": conf.get(\"core\", \"LOGGING_LEVEL\"),\n    }\n\n    if conf.has_option(\"celery\", \"pool\"):\n        options[\"pool\"] = conf.get(\"celery\", \"pool\")\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"worker\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        handle = setup_logging(log_file)\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            files_preserve=[handle],\n            stdout=stdout,\n            stderr=stderr,\n        )\n        with ctx:\n            sp = subprocess.Popen([\"airflow\", \"serve_logs\"], env=env, close_fds=True)\n            worker.run(**options)\n            sp.kill()\n\n        stdout.close()\n        stderr.close()\n    else:\n        signal.signal(signal.SIGINT, sigint_handler)\n        signal.signal(signal.SIGTERM, sigint_handler)\n\n        sp = subprocess.Popen([\"airflow\", \"serve_logs\"], env=env, close_fds=True)\n\n        worker.run(**options)\n        sp.kill()\n\n\ndef initdb(args):\n    print(\"DB: \" + repr(settings.engine.url))\n    db.initdb()\n    print(\"Done.\")\n\n\ndef resetdb(args):\n    print(\"DB: \" + repr(settings.engine.url))\n    if (\n        args.yes\n        or input(\n            \"This will drop existing tables \" \"if they exist. Proceed? \" \"(y/n)\"\n        ).upper()\n        == \"Y\"\n    ):\n        db.resetdb()\n    else:\n        print(\"Bail.\")\n\n\n@cli_utils.action_logging\ndef upgradedb(args):\n    print(\"DB: \" + repr(settings.engine.url))\n    db.upgradedb()\n\n\n@cli_utils.action_logging\ndef version(args):\n    print(settings.HEADER + \"  v\" + airflow.__version__)\n\n\nalternative_conn_specs = [\n    \"conn_type\",\n    \"conn_host\",\n    \"conn_login\",\n    \"conn_password\",\n    \"conn_schema\",\n    \"conn_port\",\n]\n\n\n@cli_utils.action_logging\ndef connections(args):\n    if args.list:\n        # Check that no other flags were passed to the command\n        invalid_args = list()\n        for arg in [\"conn_id\", \"conn_uri\", \"conn_extra\"] + alternative_conn_specs:\n            if getattr(args, arg) is not None:\n                invalid_args.append(arg)\n        if invalid_args:\n            msg = (\n                \"\\n\\tThe following args are not compatible with the \"\n                + \"--list flag: {invalid!r}\\n\"\n            )\n            msg = msg.format(invalid=invalid_args)\n            print(msg)\n            return\n\n        with db.create_session() as session:\n            conns = session.query(\n                Connection.conn_id,\n                Connection.conn_type,\n                Connection.host,\n                Connection.port,\n                Connection.is_encrypted,\n                Connection.is_extra_encrypted,\n                Connection.extra,\n            ).all()\n            conns = [map(reprlib.repr, conn) for conn in conns]\n            msg = tabulate(\n                conns,\n                [\n                    \"Conn Id\",\n                    \"Conn Type\",\n                    \"Host\",\n                    \"Port\",\n                    \"Is Encrypted\",\n                    \"Is Extra Encrypted\",\n                    \"Extra\",\n                ],\n                tablefmt=\"fancy_grid\",\n            )\n            print(msg)\n            return\n\n    if args.delete:\n        # Check that only the `conn_id` arg was passed to the command\n        invalid_args = list()\n        for arg in [\"conn_uri\", \"conn_extra\"] + alternative_conn_specs:\n            if getattr(args, arg) is not None:\n                invalid_args.append(arg)\n        if invalid_args:\n            msg = (\n                \"\\n\\tThe following args are not compatible with the \"\n                + \"--delete flag: {invalid!r}\\n\"\n            )\n            msg = msg.format(invalid=invalid_args)\n            print(msg)\n            return\n\n        if args.conn_id is None:\n            print(\n                \"\\n\\tTo delete a connection, you Must provide a value for \"\n                + \"the --conn_id flag.\\n\"\n            )\n            return\n\n        with db.create_session() as session:\n            try:\n                to_delete = (\n                    session.query(Connection)\n                    .filter(Connection.conn_id == args.conn_id)\n                    .one()\n                )\n            except exc.NoResultFound:\n                msg = \"\\n\\tDid not find a connection with `conn_id`={conn_id}\\n\"\n                msg = msg.format(conn_id=args.conn_id)\n                print(msg)\n                return\n            except exc.MultipleResultsFound:\n                msg = (\n                    \"\\n\\tFound more than one connection with \" + \"`conn_id`={conn_id}\\n\"\n                )\n                msg = msg.format(conn_id=args.conn_id)\n                print(msg)\n                return\n            else:\n                deleted_conn_id = to_delete.conn_id\n                session.delete(to_delete)\n                msg = \"\\n\\tSuccessfully deleted `conn_id`={conn_id}\\n\"\n                msg = msg.format(conn_id=deleted_conn_id)\n                print(msg)\n            return\n\n    if args.add:\n        # Check that the conn_id and conn_uri args were passed to the command:\n        missing_args = list()\n        invalid_args = list()\n        if not args.conn_id:\n            missing_args.append(\"conn_id\")\n        if args.conn_uri:\n            for arg in alternative_conn_specs:\n                if getattr(args, arg) is not None:\n                    invalid_args.append(arg)\n        elif not args.conn_type:\n            missing_args.append(\"conn_uri or conn_type\")\n        if missing_args:\n            msg = (\n                \"\\n\\tThe following args are required to add a connection:\"\n                + \" {missing!r}\\n\".format(missing=missing_args)\n            )\n            print(msg)\n        if invalid_args:\n            msg = (\n                \"\\n\\tThe following args are not compatible with the \"\n                + \"--add flag and --conn_uri flag: {invalid!r}\\n\"\n            )\n            msg = msg.format(invalid=invalid_args)\n            print(msg)\n        if missing_args or invalid_args:\n            return\n\n        if args.conn_uri:\n            new_conn = Connection(conn_id=args.conn_id, uri=args.conn_uri)\n        else:\n            new_conn = Connection(\n                conn_id=args.conn_id,\n                conn_type=args.conn_type,\n                host=args.conn_host,\n                login=args.conn_login,\n                password=args.conn_password,\n                schema=args.conn_schema,\n                port=args.conn_port,\n            )\n        if args.conn_extra is not None:\n            new_conn.set_extra(args.conn_extra)\n\n        with db.create_session() as session:\n            if not (\n                session.query(Connection)\n                .filter(Connection.conn_id == new_conn.conn_id)\n                .first()\n            ):\n                session.add(new_conn)\n                msg = \"\\n\\tSuccessfully added `conn_id`={conn_id} : {uri}\\n\"\n                msg = msg.format(\n                    conn_id=new_conn.conn_id,\n                    uri=args.conn_uri\n                    or urlunparse(\n                        (\n                            args.conn_type,\n                            \"{login}:{password}@{host}:{port}\".format(\n                                login=args.conn_login or \"\",\n                                password=args.conn_password or \"\",\n                                host=args.conn_host or \"\",\n                                port=args.conn_port or \"\",\n                            ),\n                            args.conn_schema or \"\",\n                            \"\",\n                            \"\",\n                            \"\",\n                        )\n                    ),\n                )\n                print(msg)\n            else:\n                msg = \"\\n\\tA connection with `conn_id`={conn_id} already exists\\n\"\n                msg = msg.format(conn_id=new_conn.conn_id)\n                print(msg)\n\n        return\n\n\n@cli_utils.action_logging\ndef flower(args):\n    broka = conf.get(\"celery\", \"BROKER_URL\")\n    address = \"--address={}\".format(args.hostname)\n    port = \"--port={}\".format(args.port)\n    api = \"\"\n    if args.broker_api:\n        api = \"--broker_api=\" + args.broker_api\n\n    url_prefix = \"\"\n    if args.url_prefix:\n        url_prefix = \"--url-prefix=\" + args.url_prefix\n\n    basic_auth = \"\"\n    if args.basic_auth:\n        basic_auth = \"--basic_auth=\" + args.basic_auth\n\n    flower_conf = \"\"\n    if args.flower_conf:\n        flower_conf = \"--conf=\" + args.flower_conf\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"flower\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n        with ctx:\n            os.execvp(\n                \"flower\",\n                [\n                    \"flower\",\n                    \"-b\",\n                    broka,\n                    address,\n                    port,\n                    api,\n                    flower_conf,\n                    url_prefix,\n                    basic_auth,\n                ],\n            )\n\n        stdout.close()\n        stderr.close()\n    else:\n        signal.signal(signal.SIGINT, sigint_handler)\n        signal.signal(signal.SIGTERM, sigint_handler)\n\n        os.execvp(\n            \"flower\",\n            [\n                \"flower\",\n                \"-b\",\n                broka,\n                address,\n                port,\n                api,\n                flower_conf,\n                url_prefix,\n                basic_auth,\n            ],\n        )\n\n\n@cli_utils.action_logging\ndef kerberos(args):\n    print(settings.HEADER)\n    import airflow.security.kerberos\n\n    if args.daemon:\n        pid, stdout, stderr, log_file = setup_locations(\n            \"kerberos\", args.pid, args.stdout, args.stderr, args.log_file\n        )\n        stdout = open(stdout, \"w+\")\n        stderr = open(stderr, \"w+\")\n\n        ctx = daemon.DaemonContext(\n            pidfile=TimeoutPIDLockFile(pid, -1),\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n        with ctx:\n            airflow.security.kerberos.run(principal=args.principal, keytab=args.keytab)\n\n        stdout.close()\n        stderr.close()\n    else:\n        airflow.security.kerberos.run(principal=args.principal, keytab=args.keytab)\n\n\n@cli_utils.action_logging\ndef users(args):\n    if args.list:\n\n        appbuilder = cached_appbuilder()\n        users = appbuilder.sm.get_all_users()\n        fields = [\"id\", \"username\", \"email\", \"first_name\", \"last_name\", \"roles\"]\n        users = [[user.__getattribute__(field) for field in fields] for user in users]\n        msg = tabulate(\n            users,\n            [field.capitalize().replace(\"_\", \" \") for field in fields],\n            tablefmt=\"fancy_grid\",\n        )\n        print(msg)\n\n        return\n\n    elif args.create:\n        fields = {\n            \"role\": args.role,\n            \"username\": args.username,\n            \"email\": args.email,\n            \"firstname\": args.firstname,\n            \"lastname\": args.lastname,\n        }\n        empty_fields = [k for k, v in fields.items() if not v]\n        if empty_fields:\n            raise SystemExit(\n                \"Required arguments are missing: {}.\".format(\", \".join(empty_fields))\n            )\n\n        appbuilder = cached_appbuilder()\n        role = appbuilder.sm.find_role(args.role)\n        if not role:\n            raise SystemExit(\"{} is not a valid role.\".format(args.role))\n\n        if args.use_random_password:\n            password = \"\".join(random.choice(string.printable) for _ in range(16))\n        elif args.password:\n            password = args.password\n        else:\n            password = getpass.getpass(\"Password:\")\n            password_confirmation = getpass.getpass(\"Repeat for confirmation:\")\n            if password != password_confirmation:\n                raise SystemExit(\"Passwords did not match!\")\n\n        if appbuilder.sm.find_user(args.username):\n            print(\"{} already exist in the db\".format(args.username))\n            return\n        user = appbuilder.sm.add_user(\n            args.username, args.firstname, args.lastname, args.email, role, password\n        )\n        if user:\n            print(\"{} user {} created.\".format(args.role, args.username))\n        else:\n            raise SystemExit(\"Failed to create user.\")\n\n    elif args.delete:\n        if not args.username:\n            raise SystemExit(\"Required arguments are missing: username\")\n\n        appbuilder = cached_appbuilder()\n\n        try:\n            u = next(\n                u for u in appbuilder.sm.get_all_users() if u.username == args.username\n            )\n        except StopIteration:\n            raise SystemExit(\"{} is not a valid user.\".format(args.username))\n\n        if appbuilder.sm.del_register_user(u):\n            print(\"User {} deleted.\".format(args.username))\n        else:\n            raise SystemExit(\"Failed to delete user.\")\n\n    elif args.add_role or args.remove_role:\n        if args.add_role and args.remove_role:\n            raise SystemExit(\n                \"Conflicting args: --add-role and --remove-role\"\n                \" are mutually exclusive\"\n            )\n\n        if not args.username and not args.email:\n            raise SystemExit(\"Missing args: must supply one of --username or --email\")\n\n        if args.username and args.email:\n            raise SystemExit(\n                \"Conflicting args: must supply either --username\"\n                \" or --email, but not both\"\n            )\n        if not args.role:\n            raise SystemExit(\"Required args are missing: role\")\n\n        appbuilder = cached_appbuilder()\n        user = appbuilder.sm.find_user(\n            username=args.username\n        ) or appbuilder.sm.find_user(email=args.email)\n        if not user:\n            raise SystemExit(\n                'User \"{}\" does not exist'.format(args.username or args.email)\n            )\n\n        role = appbuilder.sm.find_role(args.role)\n        if not role:\n            raise SystemExit('\"{}\" is not a valid role.'.format(args.role))\n\n        if args.remove_role:\n            if role in user.roles:\n                user.roles = [r for r in user.roles if r != role]\n                appbuilder.sm.update_user(user)\n                print('User \"{}\" removed from role \"{}\".'.format(user, args.role))\n            else:\n                raise SystemExit(\n                    'User \"{}\" is not a member of role \"{}\".'.format(user, args.role)\n                )\n        elif args.add_role:\n            if role in user.roles:\n                raise SystemExit(\n                    'User \"{}\" is already a member of role \"{}\".'.format(\n                        user, args.role\n                    )\n                )\n            else:\n                user.roles.append(role)\n                appbuilder.sm.update_user(user)\n                print('User \"{}\" added to role \"{}\".'.format(user, args.role))\n    elif args.export:\n        appbuilder = cached_appbuilder()\n        users = appbuilder.sm.get_all_users()\n        fields = [\"id\", \"username\", \"email\", \"first_name\", \"last_name\", \"roles\"]\n\n        # In the User model the first and last name fields have underscores,\n        # but the corresponding parameters in the CLI don't\n        def remove_underscores(s):\n            return re.sub(\"_\", \"\", s)\n\n        users = [\n            {\n                remove_underscores(field): user.__getattribute__(field)\n                if field != \"roles\"\n                else [r.name for r in user.roles]\n                for field in fields\n            }\n            for user in users\n        ]\n\n        with open(args.export, \"w\") as f:\n            f.write(json.dumps(users, sort_keys=True, indent=4))\n            print(\"{} users successfully exported to {}\".format(len(users), f.name))\n\n    elif getattr(args, \"import\"):  # \"import\" is a reserved word\n        json_file = getattr(args, \"import\")\n        if not os.path.exists(json_file):\n            print(\"File '{}' does not exist\")\n            exit(1)\n\n        users_list = None\n        try:\n            with open(json_file, \"r\") as f:\n                users_list = json.loads(f.read())\n        except ValueError as e:\n            print(\"File '{}' is not valid JSON. Error: {}\".format(json_file, e))\n            exit(1)\n\n        users_created, users_updated = _import_users(users_list)\n        if users_created:\n            print(\n                \"Created the following users:\\n\\t{}\".format(\"\\n\\t\".join(users_created))\n            )\n\n        if users_updated:\n            print(\n                \"Updated the following users:\\n\\t{}\".format(\"\\n\\t\".join(users_updated))\n            )\n\n\ndef _import_users(users_list):\n    appbuilder = cached_appbuilder()\n    users_created = []\n    users_updated = []\n\n    for user in users_list:\n        roles = []\n        for rolename in user[\"roles\"]:\n            role = appbuilder.sm.find_role(rolename)\n            if not role:\n                print(\"Error: '{}' is not a valid role\".format(rolename))\n                exit(1)\n            else:\n                roles.append(role)\n\n        required_fields = [\"username\", \"firstname\", \"lastname\", \"email\", \"roles\"]\n        for field in required_fields:\n            if not user.get(field):\n                print(\n                    \"Error: '{}' is a required field, but was not \"\n                    \"specified\".format(field)\n                )\n                exit(1)\n\n        existing_user = appbuilder.sm.find_user(email=user[\"email\"])\n        if existing_user:\n            print(\"Found existing user with email '{}'\".format(user[\"email\"]))\n            existing_user.roles = roles\n            existing_user.first_name = user[\"firstname\"]\n            existing_user.last_name = user[\"lastname\"]\n\n            if existing_user.username != user[\"username\"]:\n                print(\n                    \"Error: Changing ther username is not allowed - \"\n                    \"please delete and recreate the user with \"\n                    \"email '{}'\".format(user[\"email\"])\n                )\n                exit(1)\n\n            appbuilder.sm.update_user(existing_user)\n            users_updated.append(user[\"email\"])\n        else:\n            print(\"Creating new user with email '{}'\".format(user[\"email\"]))\n            appbuilder.sm.add_user(\n                username=user[\"username\"],\n                first_name=user[\"firstname\"],\n                last_name=user[\"lastname\"],\n                email=user[\"email\"],\n                role=roles[0],  # add_user() requires exactly 1 role\n            )\n\n            if len(roles) > 1:\n                new_user = appbuilder.sm.find_user(email=user[\"email\"])\n                new_user.roles = roles\n                appbuilder.sm.update_user(new_user)\n\n            users_created.append(user[\"email\"])\n\n    return users_created, users_updated\n\n\n@cli_utils.action_logging\ndef roles(args):\n    if args.create and args.list:\n        raise AirflowException(\n            \"Please specify either --create or --list, \" \"but not both\"\n        )\n\n    appbuilder = cached_appbuilder()\n    if args.create:\n        for role_name in args.role:\n            appbuilder.sm.add_role(role_name)\n    elif args.list:\n        roles = appbuilder.sm.get_all_roles()\n        print(\"Existing roles:\\n\")\n        role_names = sorted([[r.name] for r in roles])\n        msg = tabulate(role_names, headers=[\"Role\"], tablefmt=\"fancy_grid\")\n        print(msg)\n\n\n@cli_utils.action_logging\ndef list_dag_runs(args, dag=None):\n    if dag:\n        args.dag_id = dag.dag_id\n\n    dagbag = DagBag()\n\n    if args.dag_id not in dagbag.dags:\n        error_message = \"Dag id {} not found\".format(args.dag_id)\n        raise AirflowException(error_message)\n\n    dag_runs = list()\n    state = args.state.lower() if args.state else None\n    for run in DagRun.find(\n        dag_id=args.dag_id, state=state, no_backfills=args.no_backfill\n    ):\n        dag_runs.append(\n            {\n                \"id\": run.id,\n                \"run_id\": run.run_id,\n                \"state\": run.state,\n                \"dag_id\": run.dag_id,\n                \"execution_date\": run.execution_date.isoformat(),\n                \"start_date\": ((run.start_date or \"\") and run.start_date.isoformat()),\n            }\n        )\n    if not dag_runs:\n        print(\"No dag runs for {dag_id}\".format(dag_id=args.dag_id))\n\n    s = textwrap.dedent(\n        \"\"\"\\n\n    {line}\n    DAG RUNS\n    {line}\n    {dag_run_header}\n    \"\"\"\n    )\n\n    dag_runs.sort(key=lambda x: x[\"execution_date\"], reverse=True)\n    dag_run_header = \"%-3s | %-20s | %-10s | %-20s | %-20s |\" % (\n        \"id\",\n        \"run_id\",\n        \"state\",\n        \"execution_date\",\n        \"state_date\",\n    )\n    print(s.format(dag_run_header=dag_run_header, line=\"-\" * 120))\n    for dag_run in dag_runs:\n        record = \"%-3s | %-20s | %-10s | %-20s | %-20s |\" % (\n            dag_run[\"id\"],\n            dag_run[\"run_id\"],\n            dag_run[\"state\"],\n            dag_run[\"execution_date\"],\n            dag_run[\"start_date\"],\n        )\n        print(record)\n\n\n@cli_utils.action_logging\ndef sync_perm(args):\n    appbuilder = cached_appbuilder()\n    print(\"Updating permission, view-menu for all existing roles\")\n    appbuilder.sm.sync_roles()\n    print(\"Updating permission on all DAG views\")\n    dags = DagBag().dags.values()\n    for dag in dags:\n        appbuilder.sm.sync_perm_for_dag(dag.dag_id, dag.access_control)\n\n\nclass Arg(object):\n    def __init__(\n        self,\n        flags=None,\n        help=None,\n        action=None,\n        default=None,\n        nargs=None,\n        type=None,\n        choices=None,\n        metavar=None,\n    ):\n        self.flags = flags\n        self.help = help\n        self.action = action\n        self.default = default\n        self.nargs = nargs\n        self.type = type\n        self.choices = choices\n        self.metavar = metavar\n\n\nclass CLIFactory(object):\n    args = {\n        # Shared\n        \"dag_id\": Arg((\"dag_id\",), \"The id of the dag\"),\n        \"task_id\": Arg((\"task_id\",), \"The id of the task\"),\n        \"execution_date\": Arg(\n            (\"execution_date\",), help=\"The execution date of the DAG\", type=parsedate\n        ),\n        \"task_regex\": Arg(\n            (\"-t\", \"--task_regex\"),\n            \"The regex to filter specific task_ids to backfill (optional)\",\n        ),\n        \"subdir\": Arg(\n            (\"-sd\", \"--subdir\"),\n            \"File location or directory from which to look for the dag. \"\n            \"Defaults to '[AIRFLOW_HOME]/dags' where [AIRFLOW_HOME] is the \"\n            \"value you set for 'AIRFLOW_HOME' config you set in 'airflow.cfg' \",\n            default=DAGS_FOLDER,\n        ),\n        \"start_date\": Arg(\n            (\"-s\", \"--start_date\"), \"Override start_date YYYY-MM-DD\", type=parsedate\n        ),\n        \"end_date\": Arg(\n            (\"-e\", \"--end_date\"), \"Override end_date YYYY-MM-DD\", type=parsedate\n        ),\n        \"dry_run\": Arg((\"-dr\", \"--dry_run\"), \"Perform a dry run\", \"store_true\"),\n        \"pid\": Arg((\"--pid\",), \"PID file location\", nargs=\"?\"),\n        \"daemon\": Arg(\n            (\"-D\", \"--daemon\"),\n            \"Daemonize instead of running \" \"in the foreground\",\n            \"store_true\",\n        ),\n        \"stderr\": Arg((\"--stderr\",), \"Redirect stderr to this file\"),\n        \"stdout\": Arg((\"--stdout\",), \"Redirect stdout to this file\"),\n        \"log_file\": Arg((\"-l\", \"--log-file\"), \"Location of the log file\"),\n        \"yes\": Arg(\n            (\"-y\", \"--yes\"),\n            \"Do not prompt to confirm reset. Use with care!\",\n            \"store_true\",\n            default=False,\n        ),\n        # list_dag_runs\n        \"no_backfill\": Arg(\n            (\"--no_backfill\",),\n            \"filter all the backfill dagruns given the dag id\",\n            \"store_true\",\n        ),\n        \"state\": Arg((\"--state\",), \"Only list the dag runs corresponding to the state\"),\n        # list_jobs\n        \"limit\": Arg((\"--limit\",), \"Return a limited number of records\"),\n        # backfill\n        \"mark_success\": Arg(\n            (\"-m\", \"--mark_success\"),\n            \"Mark jobs as succeeded without running them\",\n            \"store_true\",\n        ),\n        \"verbose\": Arg(\n            (\"-v\", \"--verbose\"), \"Make logging output more verbose\", \"store_true\"\n        ),\n        \"local\": Arg(\n            (\"-l\", \"--local\"), \"Run the task using the LocalExecutor\", \"store_true\"\n        ),\n        \"donot_pickle\": Arg(\n            (\"-x\", \"--donot_pickle\"),\n            (\n                \"Do not attempt to pickle the DAG object to send over \"\n                \"to the workers, just tell the workers to run their version \"\n                \"of the code.\"\n            ),\n            \"store_true\",\n        ),\n        \"bf_ignore_dependencies\": Arg(\n            (\"-i\", \"--ignore_dependencies\"),\n            (\n                \"Skip upstream tasks, run only the tasks \"\n                \"matching the regexp. Only works in conjunction \"\n                \"with task_regex\"\n            ),\n            \"store_true\",\n        ),\n        \"bf_ignore_first_depends_on_past\": Arg(\n            (\"-I\", \"--ignore_first_depends_on_past\"),\n            (\n                \"Ignores depends_on_past dependencies for the first \"\n                \"set of tasks only (subsequent executions in the backfill \"\n                \"DO respect depends_on_past).\"\n            ),\n            \"store_true\",\n        ),\n        \"pool\": Arg((\"--pool\",), \"Resource pool to use\"),\n        \"delay_on_limit\": Arg(\n            (\"--delay_on_limit\",),\n            help=(\n                \"Amount of time in seconds to wait when the limit \"\n                \"on maximum active dag runs (max_active_runs) has \"\n                \"been reached before trying to execute a dag run \"\n                \"again.\"\n            ),\n            type=float,\n            default=1.0,\n        ),\n        \"reset_dag_run\": Arg(\n            (\"--reset_dagruns\",),\n            (\n                \"if set, the backfill will delete existing \"\n                \"backfill-related DAG runs and start \"\n                \"anew with fresh, running DAG runs\"\n            ),\n            \"store_true\",\n        ),\n        \"rerun_failed_tasks\": Arg(\n            (\"--rerun_failed_tasks\",),\n            (\n                \"if set, the backfill will auto-rerun \"\n                \"all the failed tasks for the backfill date range \"\n                \"instead of throwing exceptions\"\n            ),\n            \"store_true\",\n        ),\n        \"run_backwards\": Arg(\n            (\n                \"-B\",\n                \"--run_backwards\",\n            ),\n            (\n                \"if set, the backfill will run tasks from the most \"\n                \"recent day first.  if there are tasks that depend_on_past \"\n                \"this option will throw an exception\"\n            ),\n            \"store_true\",\n        ),\n        # list_tasks\n        \"tree\": Arg((\"-t\", \"--tree\"), \"Tree view\", \"store_true\"),\n        # list_dags\n        \"report\": Arg((\"-r\", \"--report\"), \"Show DagBag loading report\", \"store_true\"),\n        # clear\n        \"upstream\": Arg((\"-u\", \"--upstream\"), \"Include upstream tasks\", \"store_true\"),\n        \"only_failed\": Arg((\"-f\", \"--only_failed\"), \"Only failed jobs\", \"store_true\"),\n        \"only_running\": Arg(\n            (\"-r\", \"--only_running\"), \"Only running jobs\", \"store_true\"\n        ),\n        \"downstream\": Arg(\n            (\"-d\", \"--downstream\"), \"Include downstream tasks\", \"store_true\"\n        ),\n        \"no_confirm\": Arg(\n            (\"-c\", \"--no_confirm\"), \"Do not request confirmation\", \"store_true\"\n        ),\n        \"exclude_subdags\": Arg(\n            (\"-x\", \"--exclude_subdags\"), \"Exclude subdags\", \"store_true\"\n        ),\n        \"exclude_parentdag\": Arg(\n            (\"-xp\", \"--exclude_parentdag\"),\n            \"Exclude ParentDAGS if the task cleared is a part of a SubDAG\",\n            \"store_true\",\n        ),\n        \"dag_regex\": Arg(\n            (\"-dx\", \"--dag_regex\"),\n            \"Search dag_id as regex instead of exact string\",\n            \"store_true\",\n        ),\n        # trigger_dag\n        \"run_id\": Arg((\"-r\", \"--run_id\"), \"Helps to identify this run\"),\n        \"conf\": Arg(\n            (\"-c\", \"--conf\"),\n            \"JSON string that gets pickled into the DagRun's conf attribute\",\n        ),\n        \"exec_date\": Arg(\n            (\"-e\", \"--exec_date\"), help=\"The execution date of the DAG\", type=parsedate\n        ),\n        # pool\n        \"pool_set\": Arg(\n            (\"-s\", \"--set\"),\n            nargs=3,\n            metavar=(\"NAME\", \"SLOT_COUNT\", \"POOL_DESCRIPTION\"),\n            help=\"Set pool slot count and description, respectively\",\n        ),\n        \"pool_get\": Arg((\"-g\", \"--get\"), metavar=\"NAME\", help=\"Get pool info\"),\n        \"pool_delete\": Arg((\"-x\", \"--delete\"), metavar=\"NAME\", help=\"Delete a pool\"),\n        \"pool_import\": Arg(\n            (\"-i\", \"--import\"), metavar=\"FILEPATH\", help=\"Import pool from JSON file\"\n        ),\n        \"pool_export\": Arg(\n            (\"-e\", \"--export\"), metavar=\"FILEPATH\", help=\"Export pool to JSON file\"\n        ),\n        # variables\n        \"set\": Arg(\n            (\"-s\", \"--set\"), nargs=2, metavar=(\"KEY\", \"VAL\"), help=\"Set a variable\"\n        ),\n        \"get\": Arg((\"-g\", \"--get\"), metavar=\"KEY\", help=\"Get value of a variable\"),\n        \"default\": Arg(\n            (\"-d\", \"--default\"),\n            metavar=\"VAL\",\n            default=None,\n            help=\"Default value returned if variable does not exist\",\n        ),\n        \"json\": Arg(\n            (\"-j\", \"--json\"), help=\"Deserialize JSON variable\", action=\"store_true\"\n        ),\n        \"var_import\": Arg(\n            (\"-i\", \"--import\"),\n            metavar=\"FILEPATH\",\n            help=\"Import variables from JSON file\",\n        ),\n        \"var_export\": Arg(\n            (\"-e\", \"--export\"), metavar=\"FILEPATH\", help=\"Export variables to JSON file\"\n        ),\n        \"var_delete\": Arg((\"-x\", \"--delete\"), metavar=\"KEY\", help=\"Delete a variable\"),\n        # kerberos\n        \"principal\": Arg((\"principal\",), \"kerberos principal\", nargs=\"?\"),\n        \"keytab\": Arg(\n            (\"-kt\", \"--keytab\"),\n            \"keytab\",\n            nargs=\"?\",\n            default=conf.get(\"kerberos\", \"keytab\"),\n        ),\n        # run\n        # TODO(aoen): \"force\" is a poor choice of name here since it implies it overrides\n        # all dependencies (not just past success), e.g. the ignore_depends_on_past\n        # dependency. This flag should be deprecated and renamed to 'ignore_ti_state' and\n        # the \"ignore_all_dependencies\" command should be called the\"force\" command\n        # instead.\n        \"interactive\": Arg(\n            (\"-int\", \"--interactive\"),\n            help=\"Do not capture standard output and error streams \"\n            \"(useful for interactive debugging)\",\n            action=\"store_true\",\n        ),\n        \"force\": Arg(\n            (\"-f\", \"--force\"),\n            \"Ignore previous task instance state, rerun regardless if task already \"\n            \"succeeded/failed\",\n            \"store_true\",\n        ),\n        \"raw\": Arg((\"-r\", \"--raw\"), argparse.SUPPRESS, \"store_true\"),\n        \"ignore_all_dependencies\": Arg(\n            (\"-A\", \"--ignore_all_dependencies\"),\n            \"Ignores all non-critical dependencies, including ignore_ti_state and \"\n            \"ignore_task_deps\",\n            \"store_true\",\n        ),\n        # TODO(aoen): ignore_dependencies is a poor choice of name here because it is too\n        # vague (e.g. a task being in the appropriate state to be run is also a dependency\n        # but is not ignored by this flag), the name 'ignore_task_dependencies' is\n        # slightly better (as it ignores all dependencies that are specific to the task),\n        # so deprecate the old command name and use this instead.\n        \"ignore_dependencies\": Arg(\n            (\"-i\", \"--ignore_dependencies\"),\n            \"Ignore task-specific dependencies, e.g. upstream, depends_on_past, and \"\n            \"retry delay dependencies\",\n            \"store_true\",\n        ),\n        \"ignore_depends_on_past\": Arg(\n            (\"-I\", \"--ignore_depends_on_past\"),\n            \"Ignore depends_on_past dependencies (but respect \"\n            \"upstream dependencies)\",\n            \"store_true\",\n        ),\n        \"ship_dag\": Arg(\n            (\"--ship_dag\",),\n            \"Pickles (serializes) the DAG and ships it to the worker\",\n            \"store_true\",\n        ),\n        \"pickle\": Arg(\n            (\"-p\", \"--pickle\"),\n            \"Serialized pickle object of the entire dag (used internally)\",\n        ),\n        \"job_id\": Arg((\"-j\", \"--job_id\"), argparse.SUPPRESS),\n        \"cfg_path\": Arg(\n            (\"--cfg_path\",), \"Path to config file to use instead of airflow.cfg\"\n        ),\n        # webserver\n        \"port\": Arg(\n            (\"-p\", \"--port\"),\n            default=conf.get(\"webserver\", \"WEB_SERVER_PORT\"),\n            type=int,\n            help=\"The port on which to run the server\",\n        ),\n        \"ssl_cert\": Arg(\n            (\"--ssl_cert\",),\n            default=conf.get(\"webserver\", \"WEB_SERVER_SSL_CERT\"),\n            help=\"Path to the SSL certificate for the webserver\",\n        ),\n        \"ssl_key\": Arg(\n            (\"--ssl_key\",),\n            default=conf.get(\"webserver\", \"WEB_SERVER_SSL_KEY\"),\n            help=\"Path to the key to use with the SSL certificate\",\n        ),\n        \"workers\": Arg(\n            (\"-w\", \"--workers\"),\n            default=conf.get(\"webserver\", \"WORKERS\"),\n            type=int,\n            help=\"Number of workers to run the webserver on\",\n        ),\n        \"workerclass\": Arg(\n            (\"-k\", \"--workerclass\"),\n            default=conf.get(\"webserver\", \"WORKER_CLASS\"),\n            choices=[\"sync\", \"eventlet\", \"gevent\", \"tornado\"],\n            help=\"The worker class to use for Gunicorn\",\n        ),\n        \"worker_timeout\": Arg(\n            (\"-t\", \"--worker_timeout\"),\n            default=conf.get(\"webserver\", \"WEB_SERVER_WORKER_TIMEOUT\"),\n            type=int,\n            help=\"The timeout for waiting on webserver workers\",\n        ),\n        \"hostname\": Arg(\n            (\"-hn\", \"--hostname\"),\n            default=conf.get(\"webserver\", \"WEB_SERVER_HOST\"),\n            help=\"Set the hostname on which to run the web server\",\n        ),\n        \"debug\": Arg(\n            (\"-d\", \"--debug\"),\n            \"Use the server that ships with Flask in debug mode\",\n            \"store_true\",\n        ),\n        \"access_logfile\": Arg(\n            (\"-A\", \"--access_logfile\"),\n            default=conf.get(\"webserver\", \"ACCESS_LOGFILE\"),\n            help=\"The logfile to store the webserver access log. Use '-' to print to \"\n            \"stderr.\",\n        ),\n        \"error_logfile\": Arg(\n            (\"-E\", \"--error_logfile\"),\n            default=conf.get(\"webserver\", \"ERROR_LOGFILE\"),\n            help=\"The logfile to store the webserver error log. Use '-' to print to \"\n            \"stderr.\",\n        ),\n        # scheduler\n        \"dag_id_opt\": Arg((\"-d\", \"--dag_id\"), help=\"The id of the dag to run\"),\n        \"num_runs\": Arg(\n            (\"-n\", \"--num_runs\"),\n            default=conf.getint(\"scheduler\", \"num_runs\"),\n            type=int,\n            help=\"Set the number of runs to execute before exiting\",\n        ),\n        # worker\n        \"do_pickle\": Arg(\n            (\"-p\", \"--do_pickle\"),\n            default=False,\n            help=(\n                \"Attempt to pickle the DAG object to send over \"\n                \"to the workers, instead of letting workers run their version \"\n                \"of the code.\"\n            ),\n            action=\"store_true\",\n        ),\n        \"queues\": Arg(\n            (\"-q\", \"--queues\"),\n            help=\"Comma delimited list of queues to serve\",\n            default=conf.get(\"celery\", \"DEFAULT_QUEUE\"),\n        ),\n        \"concurrency\": Arg(\n            (\"-c\", \"--concurrency\"),\n            type=int,\n            help=\"The number of worker processes\",\n            default=conf.get(\"celery\", \"worker_concurrency\"),\n        ),\n        \"celery_hostname\": Arg(\n            (\"-cn\", \"--celery_hostname\"),\n            help=(\n                \"Set the hostname of celery worker \"\n                \"if you have multiple workers on a single machine.\"\n            ),\n        ),\n        # flower\n        \"broker_api\": Arg((\"-a\", \"--broker_api\"), help=\"Broker api\"),\n        \"flower_hostname\": Arg(\n            (\"-hn\", \"--hostname\"),\n            default=conf.get(\"celery\", \"FLOWER_HOST\"),\n            help=\"Set the hostname on which to run the server\",\n        ),\n        \"flower_port\": Arg(\n            (\"-p\", \"--port\"),\n            default=conf.get(\"celery\", \"FLOWER_PORT\"),\n            type=int,\n            help=\"The port on which to run the server\",\n        ),\n        \"flower_conf\": Arg(\n            (\"-fc\", \"--flower_conf\"), help=\"Configuration file for flower\"\n        ),\n        \"flower_url_prefix\": Arg(\n            (\"-u\", \"--url_prefix\"),\n            default=conf.get(\"celery\", \"FLOWER_URL_PREFIX\"),\n            help=\"URL prefix for Flower\",\n        ),\n        \"flower_basic_auth\": Arg(\n            (\"-ba\", \"--basic_auth\"),\n            default=conf.get(\"celery\", \"FLOWER_BASIC_AUTH\"),\n            help=(\n                \"Securing Flower with Basic Authentication. \"\n                \"Accepts user:password pairs separated by a comma. \"\n                \"Example: flower_basic_auth = user1:password1,user2:password2\"\n            ),\n        ),\n        \"task_params\": Arg(\n            (\"-tp\", \"--task_params\"), help=\"Sends a JSON params dict to the task\"\n        ),\n        \"post_mortem\": Arg(\n            (\"-pm\", \"--post_mortem\"),\n            action=\"store_true\",\n            help=\"Open debugger on uncaught exception\",\n        ),\n        # connections\n        \"list_connections\": Arg(\n            (\"-l\", \"--list\"), help=\"List all connections\", action=\"store_true\"\n        ),\n        \"add_connection\": Arg(\n            (\"-a\", \"--add\"), help=\"Add a connection\", action=\"store_true\"\n        ),\n        \"delete_connection\": Arg(\n            (\"-d\", \"--delete\"), help=\"Delete a connection\", action=\"store_true\"\n        ),\n        \"conn_id\": Arg(\n            (\"--conn_id\",),\n            help=\"Connection id, required to add/delete a connection\",\n            type=str,\n        ),\n        \"conn_uri\": Arg(\n            (\"--conn_uri\",),\n            help=\"Connection URI, required to add a connection without conn_type\",\n            type=str,\n        ),\n        \"conn_type\": Arg(\n            (\"--conn_type\",),\n            help=\"Connection type, required to add a connection without conn_uri\",\n            type=str,\n        ),\n        \"conn_host\": Arg(\n            (\"--conn_host\",),\n            help=\"Connection host, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_login\": Arg(\n            (\"--conn_login\",),\n            help=\"Connection login, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_password\": Arg(\n            (\"--conn_password\",),\n            help=\"Connection password, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_schema\": Arg(\n            (\"--conn_schema\",),\n            help=\"Connection schema, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_port\": Arg(\n            (\"--conn_port\",),\n            help=\"Connection port, optional when adding a connection\",\n            type=str,\n        ),\n        \"conn_extra\": Arg(\n            (\"--conn_extra\",),\n            help=\"Connection `Extra` field, optional when adding a connection\",\n            type=str,\n        ),\n        # users\n        \"username\": Arg(\n            (\"--username\",),\n            help=\"Username of the user, required to create/delete a user\",\n            type=str,\n        ),\n        \"firstname\": Arg(\n            (\"--firstname\",),\n            help=\"First name of the user, required to create a user\",\n            type=str,\n        ),\n        \"lastname\": Arg(\n            (\"--lastname\",),\n            help=\"Last name of the user, required to create a user\",\n            type=str,\n        ),\n        \"role\": Arg(\n            (\"--role\",),\n            help=\"Role of the user. Existing roles include Admin, \"\n            \"User, Op, Viewer, and Public. Required to create a user\",\n            type=str,\n        ),\n        \"email\": Arg(\n            (\"--email\",), help=\"Email of the user, required to create a user\", type=str\n        ),\n        \"password\": Arg(\n            (\"--password\",),\n            help=\"Password of the user, required to create a user \"\n            \"without --use_random_password\",\n            type=str,\n        ),\n        \"use_random_password\": Arg(\n            (\"--use_random_password\",),\n            help=\"Do not prompt for password. Use random string instead.\"\n            \" Required to create a user without --password \",\n            default=False,\n            action=\"store_true\",\n        ),\n        \"list_users\": Arg((\"-l\", \"--list\"), help=\"List all users\", action=\"store_true\"),\n        \"create_user\": Arg(\n            (\"-c\", \"--create\"), help=\"Create a user\", action=\"store_true\"\n        ),\n        \"delete_user\": Arg(\n            (\"-d\", \"--delete\"), help=\"Delete a user\", action=\"store_true\"\n        ),\n        \"add_role\": Arg(\n            (\"--add-role\",), help=\"Add user to a role\", action=\"store_true\"\n        ),\n        \"remove_role\": Arg(\n            (\"--remove-role\",), help=\"Remove user from a role\", action=\"store_true\"\n        ),\n        \"user_import\": Arg(\n            (\"-i\", \"--import\"),\n            metavar=\"FILEPATH\",\n            help=\"Import users from JSON file. Example format:\"\n            + textwrap.dedent(\n                \"\"\"\n                    [\n                        {\n                            \"email\": \"foo@bar.org\",\n                            \"firstname\": \"Jon\",\n                            \"lastname\": \"Doe\",\n                            \"roles\": [\"Public\"],\n                            \"username\": \"jondoe\"\n                        }\n                    ]\"\"\"\n            ),\n        ),\n        \"user_export\": Arg(\n            (\"-e\", \"--export\"), metavar=\"FILEPATH\", help=\"Export users to JSON file\"\n        ),\n        # roles\n        \"create_role\": Arg(\n            (\"-c\", \"--create\"), help=\"Create a new role\", action=\"store_true\"\n        ),\n        \"list_roles\": Arg((\"-l\", \"--list\"), help=\"List roles\", action=\"store_true\"),\n        \"roles\": Arg((\"role\",), help=\"The name of a role\", nargs=\"*\"),\n        \"autoscale\": Arg(\n            (\"-a\", \"--autoscale\"),\n            help=\"Minimum and Maximum number of worker to autoscale\",\n        ),\n    }\n    subparsers = (\n        {\n            \"func\": backfill,\n            \"help\": \"Run subsections of a DAG for a specified date range. \"\n            \"If reset_dag_run option is used,\"\n            \" backfill will first prompt users whether airflow \"\n            \"should clear all the previous dag_run and task_instances \"\n            \"within the backfill date range. \"\n            \"If rerun_failed_tasks is used, backfill \"\n            \"will auto re-run the previous failed task instances\"\n            \" within the backfill date range.\",\n            \"args\": (\n                \"dag_id\",\n                \"task_regex\",\n                \"start_date\",\n                \"end_date\",\n                \"mark_success\",\n                \"local\",\n                \"donot_pickle\",\n                \"bf_ignore_dependencies\",\n                \"bf_ignore_first_depends_on_past\",\n                \"subdir\",\n                \"pool\",\n                \"delay_on_limit\",\n                \"dry_run\",\n                \"verbose\",\n                \"conf\",\n                \"reset_dag_run\",\n                \"rerun_failed_tasks\",\n                \"run_backwards\",\n            ),\n        },\n        {\n            \"func\": list_dag_runs,\n            \"help\": \"List dag runs given a DAG id. If state option is given, it will only\"\n            \"search for all the dagruns with the given state. \"\n            \"If no_backfill option is given, it will filter out\"\n            \"all backfill dagruns for given dag id.\",\n            \"args\": (\"dag_id\", \"no_backfill\", \"state\"),\n        },\n        {\n            \"func\": list_tasks,\n            \"help\": \"List the tasks within a DAG\",\n            \"args\": (\"dag_id\", \"tree\", \"subdir\"),\n        },\n        {\n            \"func\": list_jobs,\n            \"help\": \"List the jobs\",\n            \"args\": (\"dag_id_opt\", \"state\", \"limit\"),\n        },\n        {\n            \"func\": clear,\n            \"help\": \"Clear a set of task instance, as if they never ran\",\n            \"args\": (\n                \"dag_id\",\n                \"task_regex\",\n                \"start_date\",\n                \"end_date\",\n                \"subdir\",\n                \"upstream\",\n                \"downstream\",\n                \"no_confirm\",\n                \"only_failed\",\n                \"only_running\",\n                \"exclude_subdags\",\n                \"exclude_parentdag\",\n                \"dag_regex\",\n            ),\n        },\n        {\n            \"func\": pause,\n            \"help\": \"Pause a DAG\",\n            \"args\": (\"dag_id\", \"subdir\"),\n        },\n        {\n            \"func\": unpause,\n            \"help\": \"Resume a paused DAG\",\n            \"args\": (\"dag_id\", \"subdir\"),\n        },\n        {\n            \"func\": trigger_dag,\n            \"help\": \"Trigger a DAG run\",\n            \"args\": (\"dag_id\", \"subdir\", \"run_id\", \"conf\", \"exec_date\"),\n        },\n        {\n            \"func\": delete_dag,\n            \"help\": \"Delete all DB records related to the specified DAG\",\n            \"args\": (\n                \"dag_id\",\n                \"yes\",\n            ),\n        },\n        {\n            \"func\": pool,\n            \"help\": \"CRUD operations on pools\",\n            \"args\": (\n                \"pool_set\",\n                \"pool_get\",\n                \"pool_delete\",\n                \"pool_import\",\n                \"pool_export\",\n            ),\n        },\n        {\n            \"func\": variables,\n            \"help\": \"CRUD operations on variables\",\n            \"args\": (\n                \"set\",\n                \"get\",\n                \"json\",\n                \"default\",\n                \"var_import\",\n                \"var_export\",\n                \"var_delete\",\n            ),\n        },\n        {\n            \"func\": kerberos,\n            \"help\": \"Start a kerberos ticket renewer\",\n            \"args\": (\n                \"principal\",\n                \"keytab\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n            ),\n        },\n        {\n            \"func\": render,\n            \"help\": \"Render a task instance's template(s)\",\n            \"args\": (\"dag_id\", \"task_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": run,\n            \"help\": \"Run a single task instance\",\n            \"args\": (\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"subdir\",\n                \"mark_success\",\n                \"force\",\n                \"pool\",\n                \"cfg_path\",\n                \"local\",\n                \"raw\",\n                \"ignore_all_dependencies\",\n                \"ignore_dependencies\",\n                \"ignore_depends_on_past\",\n                \"ship_dag\",\n                \"pickle\",\n                \"job_id\",\n                \"interactive\",\n            ),\n        },\n        {\n            \"func\": initdb,\n            \"help\": \"Initialize the metadata database\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": list_dags,\n            \"help\": \"List all the DAGs\",\n            \"args\": (\"subdir\", \"report\"),\n        },\n        {\n            \"func\": dag_state,\n            \"help\": \"Get the status of a dag run\",\n            \"args\": (\"dag_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": task_failed_deps,\n            \"help\": (\n                \"Returns the unmet dependencies for a task instance from the perspective \"\n                \"of the scheduler. In other words, why a task instance doesn't get \"\n                \"scheduled and then queued by the scheduler, and then run by an \"\n                \"executor).\"\n            ),\n            \"args\": (\"dag_id\", \"task_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": task_state,\n            \"help\": \"Get the status of a task instance\",\n            \"args\": (\"dag_id\", \"task_id\", \"execution_date\", \"subdir\"),\n        },\n        {\n            \"func\": serve_logs,\n            \"help\": \"Serve logs generate by worker\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": test,\n            \"help\": (\n                \"Test a task instance. This will run a task without checking for \"\n                \"dependencies or recording its state in the database.\"\n            ),\n            \"args\": (\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"subdir\",\n                \"dry_run\",\n                \"task_params\",\n                \"post_mortem\",\n            ),\n        },\n        {\n            \"func\": webserver,\n            \"help\": \"Start a Airflow webserver instance\",\n            \"args\": (\n                \"port\",\n                \"workers\",\n                \"workerclass\",\n                \"worker_timeout\",\n                \"hostname\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"access_logfile\",\n                \"error_logfile\",\n                \"log_file\",\n                \"ssl_cert\",\n                \"ssl_key\",\n                \"debug\",\n            ),\n        },\n        {\n            \"func\": resetdb,\n            \"help\": \"Burn down and rebuild the metadata database\",\n            \"args\": (\"yes\",),\n        },\n        {\n            \"func\": upgradedb,\n            \"help\": \"Upgrade the metadata database to latest version\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": scheduler,\n            \"help\": \"Start a scheduler instance\",\n            \"args\": (\n                \"dag_id_opt\",\n                \"subdir\",\n                \"num_runs\",\n                \"do_pickle\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n            ),\n        },\n        {\n            \"func\": worker,\n            \"help\": \"Start a Celery worker node\",\n            \"args\": (\n                \"do_pickle\",\n                \"queues\",\n                \"concurrency\",\n                \"celery_hostname\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n                \"autoscale\",\n            ),\n        },\n        {\n            \"func\": flower,\n            \"help\": \"Start a Celery Flower\",\n            \"args\": (\n                \"flower_hostname\",\n                \"flower_port\",\n                \"flower_conf\",\n                \"flower_url_prefix\",\n                \"flower_basic_auth\",\n                \"broker_api\",\n                \"pid\",\n                \"daemon\",\n                \"stdout\",\n                \"stderr\",\n                \"log_file\",\n            ),\n        },\n        {\n            \"func\": version,\n            \"help\": \"Show the version\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": connections,\n            \"help\": \"List/Add/Delete connections\",\n            \"args\": (\n                \"list_connections\",\n                \"add_connection\",\n                \"delete_connection\",\n                \"conn_id\",\n                \"conn_uri\",\n                \"conn_extra\",\n            )\n            + tuple(alternative_conn_specs),\n        },\n        {\n            \"func\": users,\n            \"help\": \"List/Create/Delete/Update users\",\n            \"args\": (\n                \"list_users\",\n                \"create_user\",\n                \"delete_user\",\n                \"add_role\",\n                \"remove_role\",\n                \"user_import\",\n                \"user_export\",\n                \"username\",\n                \"email\",\n                \"firstname\",\n                \"lastname\",\n                \"role\",\n                \"password\",\n                \"use_random_password\",\n            ),\n        },\n        {\n            \"func\": roles,\n            \"help\": \"Create/List roles\",\n            \"args\": (\"create_role\", \"list_roles\", \"roles\"),\n        },\n        {\n            \"func\": sync_perm,\n            \"help\": \"Update permissions for existing roles and DAGs.\",\n            \"args\": tuple(),\n        },\n        {\n            \"func\": next_execution,\n            \"help\": \"Get the next execution datetime of a DAG.\",\n            \"args\": (\"dag_id\", \"subdir\"),\n        },\n        {\n            \"func\": rotate_fernet_key,\n            \"help\": \"Rotate all encrypted connection credentials and variables; see \"\n            \"https://airflow.readthedocs.io/en/stable/howto/secure-connections.html\"\n            \"#rotating-encryption-keys.\",\n            \"args\": (),\n        },\n    )\n    subparsers_dict = {sp[\"func\"].__name__: sp for sp in subparsers}\n    dag_subparsers = (\n        \"list_tasks\",\n        \"backfill\",\n        \"test\",\n        \"run\",\n        \"pause\",\n        \"unpause\",\n        \"list_dag_runs\",\n    )\n\n    @classmethod\n    def get_parser(cls, dag_parser=False):\n        parser = argparse.ArgumentParser()\n        subparsers = parser.add_subparsers(help=\"sub-command help\", dest=\"subcommand\")\n        subparsers.required = True\n\n        subparser_list = (\n            cls.dag_subparsers if dag_parser else cls.subparsers_dict.keys()\n        )\n        for sub in sorted(subparser_list):\n            sub = cls.subparsers_dict[sub]\n            sp = subparsers.add_parser(sub[\"func\"].__name__, help=sub[\"help\"])\n            sp.formatter_class = RawTextHelpFormatter\n            for arg in sub[\"args\"]:\n                if \"dag_id\" in arg and dag_parser:\n                    continue\n                arg = cls.args[arg]\n                kwargs = {f: v for f, v in vars(arg).items() if f != \"flags\" and v}\n                sp.add_argument(*arg.flags, **kwargs)\n            sp.set_defaults(func=sub[\"func\"])\n        return parser\n\n\ndef get_parser():\n    return CLIFactory.get_parser()\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "package": ["import importlib", "import logging", "import os", "import subprocess", "import textwrap", "import random", "import string", "from importlib import import_module", "import getpass", "import reprlib", "import argparse", "from argparse import RawTextHelpFormatter", "from builtins import input", "from airflow.utils.timezone import parse as parsedate", "import json", "from tabulate import tabulate", "import daemon", "from daemon.pidfile import TimeoutPIDLockFile", "import signal", "import sys", "import threading", "import traceback", "import time", "import psutil", "import re", "from urllib.parse import urlunparse", "from typing import Any", "import airflow", "from airflow import api", "from airflow import jobs, settings", "from airflow import configuration as conf", "from airflow.exceptions import AirflowException, AirflowWebServerTimeout", "from airflow.executors import get_default_executor", "from airflow.models import (", "from airflow.ti_deps.dep_context import DepContext, SCHEDULER_DEPS", "from airflow.utils import cli as cli_utils, db", "from airflow.utils.net import get_hostname", "from airflow.utils.log.logging_mixin import (", "from airflow.www.app import cached_app, create_app, cached_appbuilder", "from sqlalchemy.orm import exc", "import flask", "from airflow.executors.celery_executor import app as celery_app", "from celery.bin import worker", "import airflow.security.kerberos"], "function": ["def sigint_handler(sig, frame):\n", "def sigquit_handler(sig, frame):\n", "def setup_logging(filename):\n", "def setup_locations(process, pid=None, stdout=None, stderr=None, log=None):\n", "def process_subdir(subdir):\n", "def get_dag(args):\n", "def get_dags(args):\n", "def backfill(args, dag=None):\n", "def trigger_dag(args):\n", "def delete_dag(args):\n", "def pool(args):\n", "    def _tabulate(pools):\n", "def pool_import_helper(filepath):\n", "def pool_export_helper(filepath):\n", "def variables(args):\n", "def import_helper(filepath):\n", "def export_helper(filepath):\n", "def pause(args, dag=None):\n", "def unpause(args, dag=None):\n", "def set_is_paused(is_paused, args, dag=None):\n", "def _run(args, dag, ti):\n", "def run(args, dag=None):\n", "def task_failed_deps(args):\n", "def task_state(args):\n", "def dag_state(args):\n", "def next_execution(args):\n", "def rotate_fernet_key(args):\n", "def list_dags(args):\n", "def list_tasks(args, dag=None):\n", "def list_jobs(args, dag=None):\n", "def test(args, dag=None):\n", "def render(args):\n", "def clear(args):\n", "def get_num_ready_workers_running(gunicorn_master_proc):\n", "    def ready_prefix_on_cmdline(proc):\n", "def get_num_workers_running(gunicorn_master_proc):\n", "def webserver(args):\n", "def scheduler(args):\n", "def serve_logs(args):\n", "    def serve_logs(filename):\n", "def worker(args):\n", "def initdb(args):\n", "def resetdb(args):\n", "def upgradedb(args):\n", "def version(args):\n", "def connections(args):\n", "def flower(args):\n", "def kerberos(args):\n", "def users(args):\n", "def _import_users(users_list):\n", "def roles(args):\n", "def list_dag_runs(args, dag=None):\n", "def sync_perm(args):\n", "class Arg(object):\n", "class CLIFactory(object):\n", "    def get_parser(cls, dag_parser=False):\n", "def get_parser():\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_translate_hook.py", "func_name": "CloudTranslateHook.get_conn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client", "docstring_tokens": ["Retrieves", "connection", "to", "Cloud", "Translate"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L34-L43", "partition": "test", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom google.cloud.translate_v2 import Client\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n\nclass CloudTranslateHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud translate APIs.\n    \"\"\"\n\n    _client = None\n\n    def __init__(self, gcp_conn_id=\"google_cloud_default\"):\n        super().__init__(gcp_conn_id)\n\n    def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )\n", "levels": [0, 1], "package": ["from google.cloud.translate_v2 import Client", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook"], "function": ["class CloudTranslateHook(GoogleCloudBaseHook):\n", "    def __init__(self, gcp_conn_id=\"google_cloud_default\"):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_translate_hook.py", "func_name": "CloudTranslateHook.translate", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.", "docstring_tokens": ["Translate", "a", "string", "or", "list", "of", "strings", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L45-L97", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom google.cloud.translate_v2 import Client\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n\nclass CloudTranslateHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud translate APIs.\n    \"\"\"\n\n    _client = None\n\n    def __init__(self, gcp_conn_id=\"google_cloud_default\"):\n        super().__init__(gcp_conn_id)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client\n", "levels": [0, 1, 1], "package": ["from google.cloud.translate_v2 import Client", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook"], "function": ["class CloudTranslateHook(GoogleCloudBaseHook):\n", "    def __init__(self, gcp_conn_id=\"google_cloud_default\"):\n", "    def get_conn(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.get_instance", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict", "docstring_tokens": ["Retrieves", "a", "resource", "containing", "information", "about", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L97-L112", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.create_instance", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Creates", "a", "new", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L115-L133", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.patch_instance", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Updates", "settings", "of", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L136-L160", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.delete_instance", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None", "docstring_tokens": ["Deletes", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L163-L180", "partition": "test", "up_fun_num": 6, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.get_database", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict", "docstring_tokens": ["Retrieves", "a", "database", "resource", "from", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L183-L202", "partition": "test", "up_fun_num": 7, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.create_database", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Creates", "a", "new", "database", "inside", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L205-L226", "partition": "test", "up_fun_num": 8, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.patch_database", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Updates", "a", "database", "resource", "inside", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L229-L256", "partition": "test", "up_fun_num": 9, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.delete_database", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Deletes", "a", "database", "from", "a", "Cloud", "SQL", "instance", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L259-L279", "partition": "test", "up_fun_num": 10, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlHook.export_instance", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Exports", "data", "from", "a", "Cloud", "SQL", "instance", "to", "a", "Cloud", "Storage", "bucket", "as", "a", "SQL", "dump", "or", "CSV", "file", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L282-L310", "partition": "test", "up_fun_num": 11, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlProxyRunner.start_proxy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!", "docstring_tokens": ["Starts", "Cloud", "SQL", "Proxy", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L524-L565", "partition": "test", "up_fun_num": 19, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlProxyRunner.stop_proxy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Stops running proxy.\n\n        You should stop the proxy after you stop using it.", "docstring_tokens": ["Stops", "running", "proxy", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L567-L599", "partition": "test", "up_fun_num": 20, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlProxyRunner.get_proxy_version", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns version of the Cloud SQL Proxy.", "docstring_tokens": ["Returns", "version", "of", "the", "Cloud", "SQL", "Proxy", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L601-L615", "partition": "test", "up_fun_num": 21, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.create_connection", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).", "docstring_tokens": ["Create", "connection", "in", "the", "Connection", "table", "according", "to", "whether", "it", "uses", "proxy", "TCP", "UNIX", "sockets", "SSL", ".", "Connection", "ID", "will", "be", "randomly", "generated", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L895-L908", "partition": "test", "up_fun_num": 34, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.retrieve_connection", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).", "docstring_tokens": ["Retrieves", "the", "dynamically", "created", "connection", "from", "the", "Connection", "table", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923", "partition": "test", "up_fun_num": 35, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.delete_connection", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).", "docstring_tokens": ["Delete", "the", "dynamically", "created", "connection", "from", "the", "Connection", "table", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L926-L941", "partition": "test", "up_fun_num": 36, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.get_sqlproxy_runner", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner", "docstring_tokens": ["Retrieve", "Cloud", "SQL", "Proxy", "runner", ".", "It", "is", "used", "to", "manage", "the", "proxy", "lifecycle", "per", "task", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L943-L959", "partition": "test", "up_fun_num": 37, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.get_database_hook", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.", "docstring_tokens": ["Retrieve", "database", "hook", ".", "This", "is", "the", "actual", "Postgres", "or", "MySQL", "database", "hook", "that", "uses", "proxy", "or", "connects", "directly", "to", "the", "Google", "Cloud", "SQL", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L961-L972", "partition": "test", "up_fun_num": 38, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def cleanup_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.cleanup_database_hook", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Clean up database hook after it was used.", "docstring_tokens": ["Clean", "up", "database", "hook", "after", "it", "was", "used", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L974-L982", "partition": "test", "up_fun_num": 39, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind((\"127.0.0.1\", 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def reserve_free_tcp_port(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_sql_hook.py", "func_name": "CloudSqlDatabaseHook.reserve_free_tcp_port", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Reserve free TCP port to be used by Cloud SQL Proxy", "docstring_tokens": ["Reserve", "free", "TCP", "port", "to", "be", "used", "by", "Cloud", "SQL", "Proxy"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990", "partition": "test", "up_fun_num": 40, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport errno\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport string\n\nimport socket\nimport platform\nimport subprocess\nimport time\nimport uuid\nimport os.path\n\nfrom googleapiclient.errors import HttpError\nfrom subprocess import Popen, PIPE\nfrom urllib.parse import quote_plus\n\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException, LoggingMixin\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Number of retries - used by googleapiclient method calls to perform retries\n# For requests that are \"retriable\"\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.mysql_hook import MySqlHook\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import Connection\nfrom airflow.utils.db import provide_session\n\nUNIX_PATH_MAX = 108\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\nclass CloudSqlOperationStatus:\n    PENDING = \"PENDING\"\n    RUNNING = \"RUNNING\"\n    DONE = \"DONE\"\n    UNKNOWN = \"UNKNOWN\"\n\n\n# noinspection PyAbstractClass\nclass CloudSqlHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for Google Cloud SQL APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud SQL.\n\n        :return: Google Cloud SQL services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"sqladmin\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .instances()\n            .get(project=project_id, instance=instance)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .insert(project=project_id, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .patch(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .instances()\n            .delete(\n                project=project_id,\n                instance=instance,\n            )\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .databases()\n            .get(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .insert(project=project_id, instance=instance, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .patch(project=project_id, instance=instance, database=database, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .databases()\n            .delete(project=project_id, instance=instance, database=database)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(\n            project_id=project_id, operation_name=operation_name\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .export(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Exporting instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def import_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Imports data into a Cloud SQL instance from a SQL dump or CSV file in\n        Cloud Storage.\n\n        :param instance: Database instance ID. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = (\n                self.get_conn()\n                .instances()\n                .import_(project=project_id, instance=instance, body=body)\n                .execute(num_retries=self.num_retries)\n            )\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(\n                project_id=project_id, operation_name=operation_name\n            )\n        except HttpError as ex:\n            raise AirflowException(\n                \"Importing instance {} failed: {}\".format(instance, ex.content)\n            )\n\n    def _wait_for_operation_to_complete(self, project_id, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param project_id: Project ID of the project that contains the instance.\n        :type project_id: str\n        :param operation_name: Name of the operation.\n        :type operation_name: str\n        :return: None\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    project=project_id,\n                    operation=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"status\") == CloudSqlOperationStatus.DONE:\n                error = operation_response.get(\"error\")\n                if error:\n                    # Extracting the errors list as string and trimming square braces\n                    error_msg = str(error.get(\"errors\"))[1:-1]\n                    raise AirflowException(error_msg)\n                # No meaningful info to return from the response in case of success\n                return\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n\n\nCLOUD_SQL_PROXY_DOWNLOAD_URL = \"https://dl.google.com/cloudsql/cloud_sql_proxy.{}.{}\"\nCLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL = (\n    \"https://storage.googleapis.com/cloudsql-proxy/{}/cloud_sql_proxy.{}.{}\"\n)\n\nGCP_CREDENTIALS_KEY_PATH = \"extra__google_cloud_platform__key_path\"\nGCP_CREDENTIALS_KEYFILE_DICT = \"extra__google_cloud_platform__keyfile_dict\"\n\n\nclass CloudSqlProxyRunner(LoggingMixin):\n    \"\"\"\n    Downloads and runs cloud-sql-proxy as subprocess of the Python process.\n\n    The cloud-sql-proxy needs to be downloaded and started before we can connect\n    to the Google Cloud SQL instance via database connection. It establishes\n    secure tunnel connection to the database. It authorizes using the\n    GCP credentials that are passed by the configuration.\n\n    More details about the proxy can be found here:\n    https://cloud.google.com/sql/docs/mysql/sql-proxy\n\n    \"\"\"\n\n    def __init__(\n        self,\n        path_prefix,\n        instance_specification,\n        gcp_conn_id=\"google_cloud_default\",\n        project_id=None,\n        sql_proxy_version=None,\n        sql_proxy_binary_path=None,\n    ):\n        \"\"\"\n        Creates the proxy runner class.\n\n        :param path_prefix: Unique path prefix where proxy will be downloaded and\n            directories created for unix sockets.\n        :type path_prefix: str\n        :param instance_specification: Specification of the instance to connect the\n            proxy to. It should be specified in the form that is described in\n            https://cloud.google.com/sql/docs/mysql/sql-proxy#multiple-instances in\n            -instances parameter (typically in the form of ``<project>:<region>:<instance>``\n            for UNIX socket connections and in the form of\n            ``<project>:<region>:<instance>=tcp:<port>`` for TCP connections.\n        :type instance_specification: str\n        :param gcp_conn_id: Id of Google Cloud Platform connection to use for\n            authentication\n        :type gcp_conn_id: str\n        :param project_id: Optional id of the GCP project to connect to - it overwrites\n            default project id taken from the GCP connection.\n        :type project_id: str\n        :param sql_proxy_version: Specific version of SQL proxy to download\n            (for example 'v1.13'). By default latest version is downloaded.\n        :type sql_proxy_version: str\n        :param sql_proxy_binary_path: If specified, then proxy will be\n            used from the path specified rather than dynamically generated. This means\n            that if the binary is not present in that path it will also be downloaded.\n        :type sql_proxy_binary_path: str\n        \"\"\"\n        super().__init__()\n        self.path_prefix = path_prefix\n        if not self.path_prefix:\n            raise AirflowException(\"The path_prefix must not be empty!\")\n        self.sql_proxy_was_downloaded = False\n        self.sql_proxy_version = sql_proxy_version\n        self.download_sql_proxy_dir = None\n        self.sql_proxy_process = None\n        self.instance_specification = instance_specification\n        self.project_id = project_id\n        self.gcp_conn_id = gcp_conn_id\n        self.command_line_parameters = []\n        self.cloud_sql_proxy_socket_directory = self.path_prefix\n        self.sql_proxy_path = (\n            sql_proxy_binary_path\n            if sql_proxy_binary_path\n            else self.path_prefix + \"_cloud_sql_proxy\"\n        )\n        self.credentials_path = self.path_prefix + \"_credentials.json\"\n        self._build_command_line_parameters()\n\n    def _build_command_line_parameters(self):\n        self.command_line_parameters.extend(\n            [\"-dir\", self.cloud_sql_proxy_socket_directory]\n        )\n        self.command_line_parameters.extend([\"-instances\", self.instance_specification])\n\n    @staticmethod\n    def _is_os_64bit():\n        return platform.machine().endswith(\"64\")\n\n    def _download_sql_proxy_if_needed(self):\n        if os.path.isfile(self.sql_proxy_path):\n            self.log.info(\"cloud-sql-proxy is already present\")\n            return\n        system = platform.system().lower()\n        processor = \"amd64\" if CloudSqlProxyRunner._is_os_64bit() else \"386\"\n        if not self.sql_proxy_version:\n            download_url = CLOUD_SQL_PROXY_DOWNLOAD_URL.format(system, processor)\n        else:\n            download_url = CLOUD_SQL_PROXY_VERSION_DOWNLOAD_URL.format(\n                self.sql_proxy_version, system, processor\n            )\n        proxy_path_tmp = self.sql_proxy_path + \".tmp\"\n        self.log.info(\n            \"Downloading cloud_sql_proxy from %s to %s\", download_url, proxy_path_tmp\n        )\n        r = requests.get(download_url, allow_redirects=True)\n        # Downloading to .tmp file first to avoid case where partially downloaded\n        # binary is used by parallel operator which uses the same fixed binary path\n        with open(proxy_path_tmp, \"wb\") as f:\n            f.write(r.content)\n        if r.status_code != 200:\n            raise AirflowException(\n                \"The cloud-sql-proxy could not be downloaded. Status code = {}. \"\n                \"Reason = {}\".format(r.status_code, r.reason)\n            )\n        self.log.info(\n            \"Moving sql_proxy binary from %s to %s\", proxy_path_tmp, self.sql_proxy_path\n        )\n        shutil.move(proxy_path_tmp, self.sql_proxy_path)\n        os.chmod(self.sql_proxy_path, 0o744)  # Set executable bit\n        self.sql_proxy_was_downloaded = True\n\n    @provide_session\n    def _get_credential_parameters(self, session):\n        connection = (\n            session.query(Connection)\n            .filter(Connection.conn_id == self.gcp_conn_id)\n            .first()\n        )\n        session.expunge_all()\n        if GCP_CREDENTIALS_KEY_PATH in connection.extra_dejson:\n            credential_params = [\n                \"-credential_file\",\n                connection.extra_dejson[GCP_CREDENTIALS_KEY_PATH],\n            ]\n        elif GCP_CREDENTIALS_KEYFILE_DICT in connection.extra_dejson:\n            credential_file_content = json.loads(\n                connection.extra_dejson[GCP_CREDENTIALS_KEYFILE_DICT]\n            )\n            self.log.info(\"Saving credentials to %s\", self.credentials_path)\n            with open(self.credentials_path, \"w\") as f:\n                json.dump(credential_file_content, f)\n            credential_params = [\"-credential_file\", self.credentials_path]\n        else:\n            self.log.info(\n                \"The credentials are not supplied by neither key_path nor \"\n                \"keyfile_dict of the gcp connection %s. Falling back to \"\n                \"default activated account\",\n                self.gcp_conn_id,\n            )\n            credential_params = []\n\n        if not self.instance_specification:\n            project_id = connection.extra_dejson.get(\n                \"extra__google_cloud_platform__project\"\n            )\n            if self.project_id:\n                project_id = self.project_id\n            if not project_id:\n                raise AirflowException(\n                    \"For forwarding all instances, the project id \"\n                    \"for GCP should be provided either \"\n                    \"by project_id extra in the GCP connection or by \"\n                    \"project_id provided in the operator.\"\n                )\n            credential_params.extend([\"-projects\", project_id])\n        return credential_params\n\n    def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\n                \"The sql proxy is already running: {}\".format(self.sql_proxy_process)\n            )\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\n                    \"Creating directory %s\", self.cloud_sql_proxy_socket_directory\n                )\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(\n                command_to_run, stdin=PIPE, stdout=PIPE, stderr=PIPE\n            )\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode(\"utf-8\")\n                return_code = self.sql_proxy_process.poll()\n                if line == \"\" and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code\n                        )\n                    )\n                if line != \"\":\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(line)\n                    )\n                if \"Ready for new connections\" in line:\n                    return\n\n    def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\n                \"Stopping the cloud_sql_proxy pid: %s\", self.sql_proxy_process.pid\n            )\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\n            \"Removing the socket directory: %s\", self.cloud_sql_proxy_socket_directory\n        )\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\n                \"Skipped removing proxy - it was not downloaded: %s\",\n                self.sql_proxy_path,\n            )\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\n                \"Removing generated credentials file %s\", self.credentials_path\n            )\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)\n\n    def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend([\"--version\"])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode(\"utf-8\")\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None\n\n    def get_socket_path(self):\n        \"\"\"\n        Retrieves UNIX socket path used by Cloud SQL Proxy.\n\n        :return: The dynamically generated path for the socket created by the proxy.\n        :rtype: str\n        \"\"\"\n        return self.cloud_sql_proxy_socket_directory + \"/\" + self.instance_specification\n\n\nCONNECTION_URIS = {\n    \"postgres\": {\n        \"proxy\": {\n            \"tcp\": \"postgresql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"postgresql://{user}:{password}@{socket_path}/{database}\",\n        },\n        \"public\": {\n            \"ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"sslmode=verify-ca&\"\n            \"sslcert={client_cert_file}&\"\n            \"sslkey={client_key_file}&\"\n            \"sslrootcert={server_ca_file}\",\n            \"non-ssl\": \"postgresql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n    \"mysql\": {\n        \"proxy\": {\n            \"tcp\": \"mysql://{user}:{password}@127.0.0.1:{proxy_port}/{database}\",\n            \"socket\": \"mysql://{user}:{password}@localhost/{database}?\"\n            \"unix_socket={socket_path}\",\n        },\n        \"public\": {\n            \"ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}?\"\n            \"ssl={ssl_spec}\",\n            \"non-ssl\": \"mysql://{user}:{password}@{public_ip}:{public_port}/{database}\",\n        },\n    },\n}\n\nCLOUD_SQL_VALID_DATABASE_TYPES = [\"postgres\", \"mysql\"]\n\n\n# noinspection PyAbstractClass\nclass CloudSqlDatabaseHook(BaseHook):\n    \"\"\"\n    Serves DB connection configuration for Google Cloud SQL (Connections\n    of *gcpcloudsql://* type).\n\n    The hook is a \"meta\" one. It does not perform an actual connection.\n    It is there to retrieve all the parameters configured in gcpcloudsql:// connection,\n    start/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\n    connection in the database and return an actual Postgres or MySQL hook.\n    The returned Postgres/MySQL hooks are using direct connection or Cloud SQL\n    Proxy socket/TCP as configured.\n\n    Main parameters of the hook are retrieved from the standard URI components:\n\n    * **user** - User name to authenticate to the database (from login of the URI).\n    * **password** - Password to authenticate to the database (from password of the URI).\n    * **public_ip** - IP to connect to for public connection (from host of the URI).\n    * **public_port** - Port to connect to for public connection (from port of the URI).\n    * **database** - Database to connect to (from schema of the URI).\n\n    Remaining parameters are retrieved from the extras (URI query parameters):\n\n    * **project_id** - Optional, Google Cloud Platform project where the Cloud SQL\n       instance exists. If missing, default project id passed is used.\n    * **instance** -  Name of the instance of the Cloud SQL database instance.\n    * **location** - The location of the Cloud SQL instance (for example europe-west1).\n    * **database_type** - The type of the database instance (MySQL or Postgres).\n    * **use_proxy** - (default False) Whether SQL proxy should be used to connect to Cloud\n      SQL DB.\n    * **use_ssl** - (default False) Whether SSL should be used to connect to Cloud SQL DB.\n      You cannot use proxy and SSL together.\n    * **sql_proxy_use_tcp** - (default False) If set to true, TCP is used to connect via\n      proxy, otherwise UNIX sockets are used.\n    * **sql_proxy_binary_path** - Optional path to Cloud SQL Proxy binary. If the binary\n      is not specified or the binary is not present, it is automatically downloaded.\n    * **sql_proxy_version** -  Specific version of the proxy to download (for example\n      v1.13). If not specified, the latest version is downloaded.\n    * **sslcert** - Path to client certificate to authenticate when SSL is used.\n    * **sslkey** - Path to client private key to authenticate when SSL is used.\n    * **sslrootcert** - Path to server's certificate to authenticate when SSL is used.\n\n    :param gcp_cloudsql_conn_id: URL of the connection\n    :type gcp_cloudsql_conn_id: str\n    :param default_gcp_project_id: Default project id used if project_id not specified\n           in the connection URL\n    :type default_gcp_project_id: str\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self,\n        gcp_cloudsql_conn_id=\"google_cloud_sql_default\",\n        default_gcp_project_id=None,\n    ):\n        super().__init__(source=None)\n        self.gcp_cloudsql_conn_id = gcp_cloudsql_conn_id\n        self.cloudsql_connection = self.get_connection(self.gcp_cloudsql_conn_id)\n        self.extras = self.cloudsql_connection.extra_dejson\n        self.project_id = self.extras.get(\"project_id\", default_gcp_project_id)\n        self.instance = self.extras.get(\"instance\")\n        self.database = self.cloudsql_connection.schema\n        self.location = self.extras.get(\"location\")\n        self.database_type = self.extras.get(\"database_type\")\n        self.use_proxy = self._get_bool(self.extras.get(\"use_proxy\", \"False\"))\n        self.use_ssl = self._get_bool(self.extras.get(\"use_ssl\", \"False\"))\n        self.sql_proxy_use_tcp = self._get_bool(\n            self.extras.get(\"sql_proxy_use_tcp\", \"False\")\n        )\n        self.sql_proxy_version = self.extras.get(\"sql_proxy_version\")\n        self.sql_proxy_binary_path = self.extras.get(\"sql_proxy_binary_path\")\n        self.user = self.cloudsql_connection.login\n        self.password = self.cloudsql_connection.password\n        self.public_ip = self.cloudsql_connection.host\n        self.public_port = self.cloudsql_connection.port\n        self.sslcert = self.extras.get(\"sslcert\")\n        self.sslkey = self.extras.get(\"sslkey\")\n        self.sslrootcert = self.extras.get(\"sslrootcert\")\n        # Port and socket path and db_hook are automatically generated\n        self.sql_proxy_tcp_port = None\n        self.sql_proxy_unique_path = None\n        self.db_hook = None\n        self.reserved_tcp_socket = None\n        # Generated based on clock + clock sequence. Unique per host (!).\n        # This is important as different hosts share the database\n        self.db_conn_id = str(uuid.uuid1())\n        self._validate_inputs()\n\n    @staticmethod\n    def _get_bool(val):\n        if val == \"False\":\n            return False\n        return val\n\n    @staticmethod\n    def _check_ssl_file(file_to_check, name):\n        if not file_to_check:\n            raise AirflowException(\n                \"SSL connections requires {name} to be set\".format(name=name)\n            )\n        if not os.path.isfile(file_to_check):\n            raise AirflowException(\n                \"The {file_to_check} must be a readable file\".format(\n                    file_to_check=file_to_check\n                )\n            )\n\n    def _validate_inputs(self):\n        if self.project_id == \"\":\n            raise AirflowException(\"The required extra 'project_id' is empty\")\n        if not self.location:\n            raise AirflowException(\"The required extra 'location' is empty or None\")\n        if not self.instance:\n            raise AirflowException(\"The required extra 'instance' is empty or None\")\n        if self.database_type not in CLOUD_SQL_VALID_DATABASE_TYPES:\n            raise AirflowException(\n                \"Invalid database type '{}'. Must be one of {}\".format(\n                    self.database_type, CLOUD_SQL_VALID_DATABASE_TYPES\n                )\n            )\n        if self.use_proxy and self.use_ssl:\n            raise AirflowException(\n                \"Cloud SQL Proxy does not support SSL connections.\"\n                \" SSL is not needed as Cloud SQL Proxy \"\n                \"provides encryption on its own\"\n            )\n\n    def validate_ssl_certs(self):\n        if self.use_ssl:\n            self._check_ssl_file(self.sslcert, \"sslcert\")\n            self._check_ssl_file(self.sslkey, \"sslkey\")\n            self._check_ssl_file(self.sslrootcert, \"sslrootcert\")\n\n    def validate_socket_path_length(self):\n        if self.use_proxy and not self.sql_proxy_use_tcp:\n            if self.database_type == \"postgres\":\n                suffix = \"/.s.PGSQL.5432\"\n            else:\n                suffix = \"\"\n            expected_path = \"{}/{}:{}:{}{}\".format(\n                self._generate_unique_path(),\n                self.project_id,\n                self.instance,\n                self.database,\n                suffix,\n            )\n            if len(expected_path) > UNIX_PATH_MAX:\n                self.log.info(\n                    \"Too long (%s) path: %s\", len(expected_path), expected_path\n                )\n                raise AirflowException(\n                    \"The UNIX socket path length cannot exceed {} characters \"\n                    \"on Linux system. Either use shorter instance/database \"\n                    \"name or switch to TCP connection. \"\n                    \"The socket path for Cloud SQL proxy is now:\"\n                    \"{}\".format(UNIX_PATH_MAX, expected_path)\n                )\n\n    @staticmethod\n    def _generate_unique_path():\n        # We are not using mkdtemp here as the path generated with mkdtemp\n        # can be close to 60 characters and there is a limitation in\n        # length of socket path to around 100 characters in total.\n        # We append project/location/instance to it later and postgres\n        # appends its own prefix, so we chose a shorter \"/tmp/[8 random characters]\" -\n        random.seed()\n        while True:\n            candidate = \"/tmp/\" + \"\".join(\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(8)\n            )\n            if not os.path.exists(candidate):\n                return candidate\n\n    @staticmethod\n    def _quote(value):\n        return quote_plus(value) if value else None\n\n    def _generate_connection_uri(self):\n        if self.use_proxy:\n            if self.sql_proxy_use_tcp:\n                if not self.sql_proxy_tcp_port:\n                    self.reserve_free_tcp_port()\n            if not self.sql_proxy_unique_path:\n                self.sql_proxy_unique_path = self._generate_unique_path()\n\n        database_uris = CONNECTION_URIS[self.database_type]\n        ssl_spec = None\n        socket_path = None\n        if self.use_proxy:\n            proxy_uris = database_uris[\"proxy\"]\n            if self.sql_proxy_use_tcp:\n                format_string = proxy_uris[\"tcp\"]\n            else:\n                format_string = proxy_uris[\"socket\"]\n                socket_path = \"{sql_proxy_socket_path}/{instance_socket_name}\".format(\n                    sql_proxy_socket_path=self.sql_proxy_unique_path,\n                    instance_socket_name=self._get_instance_socket_name(),\n                )\n        else:\n            public_uris = database_uris[\"public\"]\n            if self.use_ssl:\n                format_string = public_uris[\"ssl\"]\n                ssl_spec = {\n                    \"cert\": self.sslcert,\n                    \"key\": self.sslkey,\n                    \"ca\": self.sslrootcert,\n                }\n            else:\n                format_string = public_uris[\"non-ssl\"]\n        if not self.user:\n            raise AirflowException(\"The login parameter needs to be set in connection\")\n        if not self.public_ip:\n            raise AirflowException(\n                \"The location parameter needs to be set in connection\"\n            )\n        if not self.password:\n            raise AirflowException(\n                \"The password parameter needs to be set in connection\"\n            )\n        if not self.database:\n            raise AirflowException(\n                \"The database parameter needs to be set in connection\"\n            )\n\n        connection_uri = format_string.format(\n            user=quote_plus(self.user) if self.user else \"\",\n            password=quote_plus(self.password) if self.password else \"\",\n            database=quote_plus(self.database) if self.database else \"\",\n            public_ip=self.public_ip,\n            public_port=self.public_port,\n            proxy_port=self.sql_proxy_tcp_port,\n            socket_path=self._quote(socket_path),\n            ssl_spec=self._quote(json.dumps(ssl_spec)) if ssl_spec else \"\",\n            client_cert_file=self._quote(self.sslcert) if self.sslcert else \"\",\n            client_key_file=self._quote(self.sslkey) if self.sslcert else \"\",\n            server_ca_file=self._quote(self.sslrootcert if self.sslcert else \"\"),\n        )\n        self.log.info(\n            \"DB connection URI %s\",\n            connection_uri.replace(\n                quote_plus(self.password) if self.password else \"PASSWORD\",\n                \"XXXXXXXXXXXX\",\n            ),\n        )\n        return connection_uri\n\n    def _get_instance_socket_name(self):\n        return self.project_id + \":\" + self.location + \":\" + self.instance\n\n    def _get_sqlproxy_instance_specification(self):\n        instance_specification = self._get_instance_socket_name()\n        if self.sql_proxy_use_tcp:\n            instance_specification += \"=tcp:\" + str(self.sql_proxy_tcp_port)\n        return instance_specification\n\n    @provide_session\n    def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()\n\n    @provide_session\n    def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            return connections[0]\n        return None\n\n    @provide_session\n    def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id\n        )\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\n                \"Proxy runner can only be retrieved in case of use_proxy = True\"\n            )\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path,\n        )\n\n    def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            self.db_hook = PostgresHook(\n                postgres_conn_id=self.db_conn_id, schema=self.database\n            )\n        else:\n            self.db_hook = MySqlHook(\n                mysql_conn_id=self.db_conn_id, schema=self.database\n            )\n        return self.db_hook\n\n    def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == \"postgres\":\n            if (\n                hasattr(self.db_hook, \"conn\")\n                and self.db_hook.conn\n                and self.db_hook.conn.notices\n            ):\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)\n\n    def free_reserved_port(self):\n        \"\"\"\n        Free TCP port. Makes it immediately ready to be used by Cloud SQL Proxy.\n        \"\"\"\n        if self.reserved_tcp_socket:\n            self.reserved_tcp_socket.close()\n            self.reserved_tcp_socket = None\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["import errno", "import json", "import os", "import random", "import re", "import shutil", "import string", "import socket", "import platform", "import subprocess", "import time", "import uuid", "import os.path", "from googleapiclient.errors import HttpError", "from subprocess import Popen, PIPE", "from urllib.parse import quote_plus", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException, LoggingMixin", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.mysql_hook import MySqlHook", "from airflow.hooks.postgres_hook import PostgresHook", "from airflow.models import Connection", "from airflow.utils.db import provide_session"], "function": ["class CloudSqlOperationStatus:\n", "class CloudSqlHook(GoogleCloudBaseHook):\n", "    def get_conn(self):\n", "    def get_instance(self, instance, project_id=None):\n", "    def create_instance(self, body, project_id=None):\n", "    def patch_instance(self, body, instance, project_id=None):\n", "    def delete_instance(self, instance, project_id=None):\n", "    def get_database(self, instance, database, project_id=None):\n", "    def create_database(self, instance, body, project_id=None):\n", "    def patch_database(self, instance, database, body, project_id=None):\n", "    def delete_database(self, instance, database, project_id=None):\n", "    def export_instance(self, instance, body, project_id=None):\n", "    def import_instance(self, instance, body, project_id=None):\n", "    def _wait_for_operation_to_complete(self, project_id, operation_name):\n", "class CloudSqlProxyRunner(LoggingMixin):\n", "    def _build_command_line_parameters(self):\n", "    def _is_os_64bit():\n", "    def _download_sql_proxy_if_needed(self):\n", "    def _get_credential_parameters(self, session):\n", "    def start_proxy(self):\n", "    def stop_proxy(self):\n", "    def get_proxy_version(self):\n", "    def get_socket_path(self):\n", "class CloudSqlDatabaseHook(BaseHook):\n", "    def _get_bool(val):\n", "    def _check_ssl_file(file_to_check, name):\n", "    def _validate_inputs(self):\n", "    def validate_ssl_certs(self):\n", "    def validate_socket_path_length(self):\n", "    def _generate_unique_path():\n", "    def _quote(value):\n", "    def _generate_connection_uri(self):\n", "    def _get_instance_socket_name(self):\n", "    def _get_sqlproxy_instance_specification(self):\n", "    def create_connection(self, session=None):\n", "    def retrieve_connection(self, session=None):\n", "    def delete_connection(self, session=None):\n", "    def get_sqlproxy_runner(self):\n", "    def get_database_hook(self):\n", "    def cleanup_database_hook(self):\n", "    def free_reserved_port(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/operators/mlengine_operator.py", "func_name": "_normalize_mlengine_job_id", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.", "docstring_tokens": ["Replaces", "invalid", "MLEngine", "job_id", "characters", "with", "_", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mlengine_operator.py#L29-L62", "partition": "test", "up_fun_num": 0, "context": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the 'License'); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport re\n\nfrom googleapiclient.errors import HttpError\n\nfrom airflow.contrib.hooks.gcp_mlengine_hook import MLEngineHook\nfrom airflow.exceptions import AirflowException\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nfrom airflow.utils.log.logging_mixin import LoggingMixin\n\nlog = LoggingMixin().log\n\n\nclass MLEngineBatchPredictionOperator(BaseOperator):\n    \"\"\"\n    Start a Google Cloud ML Engine prediction job.\n\n    NOTE: For model origin, users should consider exactly one from the\n    three options below:\n\n    1. Populate ``uri`` field only, which should be a GCS location that\n       points to a tensorflow savedModel directory.\n    2. Populate ``model_name`` field only, which refers to an existing\n       model, and the default version of the model will be used.\n    3. Populate both ``model_name`` and ``version_name`` fields, which\n       refers to a specific version of a specific model.\n\n    In options 2 and 3, both model and version name should contain the\n    minimal identifier. For instance, call::\n\n        MLEngineBatchPredictionOperator(\n            ...,\n            model_name='my_model',\n            version_name='my_version',\n            ...)\n\n    if the desired model version is\n    ``projects/my_project/models/my_model/versions/my_version``.\n\n    See https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs\n    for further documentation on the parameters.\n\n    :param project_id: The Google Cloud project name where the\n        prediction job is submitted. (templated)\n    :type project_id: str\n\n    :param job_id: A unique id for the prediction job on Google Cloud\n        ML Engine. (templated)\n    :type job_id: str\n\n    :param data_format: The format of the input data.\n        It will default to 'DATA_FORMAT_UNSPECIFIED' if is not provided\n        or is not one of [\"TEXT\", \"TF_RECORD\", \"TF_RECORD_GZIP\"].\n    :type data_format: str\n\n    :param input_paths: A list of GCS paths of input data for batch\n        prediction. Accepting wildcard operator ``*``, but only at the end. (templated)\n    :type input_paths: list[str]\n\n    :param output_path: The GCS path where the prediction results are\n        written to. (templated)\n    :type output_path: str\n\n    :param region: The Google Compute Engine region to run the\n        prediction job in. (templated)\n    :type region: str\n\n    :param model_name: The Google Cloud ML Engine model to use for prediction.\n        If version_name is not provided, the default version of this\n        model will be used.\n        Should not be None if version_name is provided.\n        Should be None if uri is provided. (templated)\n    :type model_name: str\n\n    :param version_name: The Google Cloud ML Engine model version to use for\n        prediction.\n        Should be None if uri is provided. (templated)\n    :type version_name: str\n\n    :param uri: The GCS path of the saved model to use for prediction.\n        Should be None if model_name is provided.\n        It should be a GCS path pointing to a tensorflow SavedModel. (templated)\n    :type uri: str\n\n    :param max_worker_count: The maximum number of workers to be used\n        for parallel processing. Defaults to 10 if not specified.\n    :type max_worker_count: int\n\n    :param runtime_version: The Google Cloud ML Engine runtime version to use\n        for batch prediction.\n    :type runtime_version: str\n\n    :param gcp_conn_id: The connection ID used for connection to Google\n        Cloud Platform.\n    :type gcp_conn_id: str\n\n    :param delegate_to: The account to impersonate, if any.\n        For this to work, the service account making the request must\n        have domain-wide delegation enabled.\n    :type delegate_to: str\n\n    :raises: ``ValueError``: if a unique model/version origin cannot be\n        determined.\n    \"\"\"\n\n    template_fields = [\n        \"_project_id\",\n        \"_job_id\",\n        \"_region\",\n        \"_input_paths\",\n        \"_output_path\",\n        \"_model_name\",\n        \"_version_name\",\n        \"_uri\",\n    ]\n\n    @apply_defaults\n    def __init__(\n        self,\n        project_id,\n        job_id,\n        region,\n        data_format,\n        input_paths,\n        output_path,\n        model_name=None,\n        version_name=None,\n        uri=None,\n        max_worker_count=None,\n        runtime_version=None,\n        gcp_conn_id=\"google_cloud_default\",\n        delegate_to=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n\n        self._project_id = project_id\n        self._job_id = job_id\n        self._region = region\n        self._data_format = data_format\n        self._input_paths = input_paths\n        self._output_path = output_path\n        self._model_name = model_name\n        self._version_name = version_name\n        self._uri = uri\n        self._max_worker_count = max_worker_count\n        self._runtime_version = runtime_version\n        self._gcp_conn_id = gcp_conn_id\n        self._delegate_to = delegate_to\n\n        if not self._project_id:\n            raise AirflowException(\"Google Cloud project id is required.\")\n        if not self._job_id:\n            raise AirflowException(\n                \"An unique job id is required for Google MLEngine prediction \" \"job.\"\n            )\n\n        if self._uri:\n            if self._model_name or self._version_name:\n                raise AirflowException(\n                    \"Ambiguous model origin: Both uri and \"\n                    \"model/version name are provided.\"\n                )\n\n        if self._version_name and not self._model_name:\n            raise AirflowException(\n                \"Missing model: Batch prediction expects \"\n                \"a model name when a version name is provided.\"\n            )\n\n        if not (self._uri or self._model_name):\n            raise AirflowException(\n                \"Missing model origin: Batch prediction expects a model, \"\n                \"a model & version combination, or a URI to a savedModel.\"\n            )\n\n    def execute(self, context):\n        job_id = _normalize_mlengine_job_id(self._job_id)\n        prediction_request = {\n            \"jobId\": job_id,\n            \"predictionInput\": {\n                \"dataFormat\": self._data_format,\n                \"inputPaths\": self._input_paths,\n                \"outputPath\": self._output_path,\n                \"region\": self._region,\n            },\n        }\n\n        if self._uri:\n            prediction_request[\"predictionInput\"][\"uri\"] = self._uri\n        elif self._model_name:\n            origin_name = \"projects/{}/models/{}\".format(\n                self._project_id, self._model_name\n            )\n            if not self._version_name:\n                prediction_request[\"predictionInput\"][\"modelName\"] = origin_name\n            else:\n                prediction_request[\"predictionInput\"][\n                    \"versionName\"\n                ] = origin_name + \"/versions/{}\".format(self._version_name)\n\n        if self._max_worker_count:\n            prediction_request[\"predictionInput\"][\n                \"maxWorkerCount\"\n            ] = self._max_worker_count\n\n        if self._runtime_version:\n            prediction_request[\"predictionInput\"][\n                \"runtimeVersion\"\n            ] = self._runtime_version\n\n        hook = MLEngineHook(self._gcp_conn_id, self._delegate_to)\n\n        # Helper method to check if the existing job's prediction input is the\n        # same as the request we get here.\n        def check_existing_job(existing_job):\n            return (\n                existing_job.get(\"predictionInput\", None)\n                == prediction_request[\"predictionInput\"]\n            )\n\n        try:\n            finished_prediction_job = hook.create_job(\n                self._project_id, prediction_request, check_existing_job\n            )\n        except HttpError:\n            raise\n\n        if finished_prediction_job[\"state\"] != \"SUCCEEDED\":\n            self.log.error(\n                \"MLEngine batch prediction job failed: %s\", str(finished_prediction_job)\n            )\n            raise RuntimeError(finished_prediction_job[\"errorMessage\"])\n\n        return finished_prediction_job[\"predictionOutput\"]\n\n\nclass MLEngineModelOperator(BaseOperator):\n    \"\"\"\n    Operator for managing a Google Cloud ML Engine model.\n\n    :param project_id: The Google Cloud project name to which MLEngine\n        model belongs. (templated)\n    :type project_id: str\n    :param model: A dictionary containing the information about the model.\n        If the `operation` is `create`, then the `model` parameter should\n        contain all the information about this model such as `name`.\n\n        If the `operation` is `get`, the `model` parameter\n        should contain the `name` of the model.\n    :type model: dict\n    :param operation: The operation to perform. Available operations are:\n\n        * ``create``: Creates a new model as provided by the `model` parameter.\n        * ``get``: Gets a particular model where the name is specified in `model`.\n    :type operation: str\n    :param gcp_conn_id: The connection ID to use when fetching connection info.\n    :type gcp_conn_id: str\n    :param delegate_to: The account to impersonate, if any.\n        For this to work, the service account making the request must have\n        domain-wide delegation enabled.\n    :type delegate_to: str\n    \"\"\"\n\n    template_fields = [\n        \"_model\",\n    ]\n\n    @apply_defaults\n    def __init__(\n        self,\n        project_id,\n        model,\n        operation=\"create\",\n        gcp_conn_id=\"google_cloud_default\",\n        delegate_to=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self._project_id = project_id\n        self._model = model\n        self._operation = operation\n        self._gcp_conn_id = gcp_conn_id\n        self._delegate_to = delegate_to\n\n    def execute(self, context):\n        hook = MLEngineHook(\n            gcp_conn_id=self._gcp_conn_id, delegate_to=self._delegate_to\n        )\n        if self._operation == \"create\":\n            return hook.create_model(self._project_id, self._model)\n        elif self._operation == \"get\":\n            return hook.get_model(self._project_id, self._model[\"name\"])\n        else:\n            raise ValueError(\"Unknown operation: {}\".format(self._operation))\n\n\nclass MLEngineVersionOperator(BaseOperator):\n    \"\"\"\n    Operator for managing a Google Cloud ML Engine version.\n\n    :param project_id: The Google Cloud project name to which MLEngine\n        model belongs.\n    :type project_id: str\n\n    :param model_name: The name of the Google Cloud ML Engine model that the version\n        belongs to. (templated)\n    :type model_name: str\n\n    :param version_name: A name to use for the version being operated upon.\n        If not None and the `version` argument is None or does not have a value for\n        the `name` key, then this will be populated in the payload for the\n        `name` key. (templated)\n    :type version_name: str\n\n    :param version: A dictionary containing the information about the version.\n        If the `operation` is `create`, `version` should contain all the\n        information about this version such as name, and deploymentUrl.\n        If the `operation` is `get` or `delete`, the `version` parameter\n        should contain the `name` of the version.\n        If it is None, the only `operation` possible would be `list`. (templated)\n    :type version: dict\n\n    :param operation: The operation to perform. Available operations are:\n\n        *   ``create``: Creates a new version in the model specified by `model_name`,\n            in which case the `version` parameter should contain all the\n            information to create that version\n            (e.g. `name`, `deploymentUrl`).\n\n        *   ``get``: Gets full information of a particular version in the model\n            specified by `model_name`.\n            The name of the version should be specified in the `version`\n            parameter.\n\n        *   ``list``: Lists all available versions of the model specified\n            by `model_name`.\n\n        *   ``delete``: Deletes the version specified in `version` parameter from the\n            model specified by `model_name`).\n            The name of the version should be specified in the `version`\n            parameter.\n    :type operation: str\n\n    :param gcp_conn_id: The connection ID to use when fetching connection info.\n    :type gcp_conn_id: str\n\n    :param delegate_to: The account to impersonate, if any.\n        For this to work, the service account making the request must have\n        domain-wide delegation enabled.\n    :type delegate_to: str\n    \"\"\"\n\n    template_fields = [\n        \"_model_name\",\n        \"_version_name\",\n        \"_version\",\n    ]\n\n    @apply_defaults\n    def __init__(\n        self,\n        project_id,\n        model_name,\n        version_name=None,\n        version=None,\n        operation=\"create\",\n        gcp_conn_id=\"google_cloud_default\",\n        delegate_to=None,\n        *args,\n        **kwargs\n    ):\n\n        super().__init__(*args, **kwargs)\n        self._project_id = project_id\n        self._model_name = model_name\n        self._version_name = version_name\n        self._version = version or {}\n        self._operation = operation\n        self._gcp_conn_id = gcp_conn_id\n        self._delegate_to = delegate_to\n\n    def execute(self, context):\n        if \"name\" not in self._version:\n            self._version[\"name\"] = self._version_name\n\n        hook = MLEngineHook(\n            gcp_conn_id=self._gcp_conn_id, delegate_to=self._delegate_to\n        )\n\n        if self._operation == \"create\":\n            if not self._version:\n                raise ValueError(\n                    \"version attribute of {} could not \"\n                    \"be empty\".format(self.__class__.__name__)\n                )\n            return hook.create_version(\n                self._project_id, self._model_name, self._version\n            )\n        elif self._operation == \"set_default\":\n            return hook.set_default_version(\n                self._project_id, self._model_name, self._version[\"name\"]\n            )\n        elif self._operation == \"list\":\n            return hook.list_versions(self._project_id, self._model_name)\n        elif self._operation == \"delete\":\n            return hook.delete_version(\n                self._project_id, self._model_name, self._version[\"name\"]\n            )\n        else:\n            raise ValueError(\"Unknown operation: {}\".format(self._operation))\n\n\nclass MLEngineTrainingOperator(BaseOperator):\n    \"\"\"\n    Operator for launching a MLEngine training job.\n\n    :param project_id: The Google Cloud project name within which MLEngine\n        training job should run (templated).\n    :type project_id: str\n\n    :param job_id: A unique templated id for the submitted Google MLEngine\n        training job. (templated)\n    :type job_id: str\n\n    :param package_uris: A list of package locations for MLEngine training job,\n        which should include the main training program + any additional\n        dependencies. (templated)\n    :type package_uris: str\n\n    :param training_python_module: The Python module name to run within MLEngine\n        training job after installing 'package_uris' packages. (templated)\n    :type training_python_module: str\n\n    :param training_args: A list of templated command line arguments to pass to\n        the MLEngine training program. (templated)\n    :type training_args: str\n\n    :param region: The Google Compute Engine region to run the MLEngine training\n        job in (templated).\n    :type region: str\n\n    :param scale_tier: Resource tier for MLEngine training job. (templated)\n    :type scale_tier: str\n\n    :param master_type: Cloud ML Engine machine name.\n        Must be set when scale_tier is CUSTOM. (templated)\n    :type master_type: str\n\n    :param runtime_version: The Google Cloud ML runtime version to use for\n        training. (templated)\n    :type runtime_version: str\n\n    :param python_version: The version of Python used in training. (templated)\n    :type python_version: str\n\n    :param job_dir: A Google Cloud Storage path in which to store training\n        outputs and other data needed for training. (templated)\n    :type job_dir: str\n\n    :param gcp_conn_id: The connection ID to use when fetching connection info.\n    :type gcp_conn_id: str\n\n    :param delegate_to: The account to impersonate, if any.\n        For this to work, the service account making the request must have\n        domain-wide delegation enabled.\n    :type delegate_to: str\n\n    :param mode: Can be one of 'DRY_RUN'/'CLOUD'. In 'DRY_RUN' mode, no real\n        training job will be launched, but the MLEngine training job request\n        will be printed out. In 'CLOUD' mode, a real MLEngine training job\n        creation request will be issued.\n    :type mode: str\n    \"\"\"\n\n    template_fields = [\n        \"_project_id\",\n        \"_job_id\",\n        \"_package_uris\",\n        \"_training_python_module\",\n        \"_training_args\",\n        \"_region\",\n        \"_scale_tier\",\n        \"_master_type\",\n        \"_runtime_version\",\n        \"_python_version\",\n        \"_job_dir\",\n    ]\n\n    @apply_defaults\n    def __init__(\n        self,\n        project_id,\n        job_id,\n        package_uris,\n        training_python_module,\n        training_args,\n        region,\n        scale_tier=None,\n        master_type=None,\n        runtime_version=None,\n        python_version=None,\n        job_dir=None,\n        gcp_conn_id=\"google_cloud_default\",\n        delegate_to=None,\n        mode=\"PRODUCTION\",\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self._project_id = project_id\n        self._job_id = job_id\n        self._package_uris = package_uris\n        self._training_python_module = training_python_module\n        self._training_args = training_args\n        self._region = region\n        self._scale_tier = scale_tier\n        self._master_type = master_type\n        self._runtime_version = runtime_version\n        self._python_version = python_version\n        self._job_dir = job_dir\n        self._gcp_conn_id = gcp_conn_id\n        self._delegate_to = delegate_to\n        self._mode = mode\n\n        if not self._project_id:\n            raise AirflowException(\"Google Cloud project id is required.\")\n        if not self._job_id:\n            raise AirflowException(\n                \"An unique job id is required for Google MLEngine training \" \"job.\"\n            )\n        if not package_uris:\n            raise AirflowException(\n                \"At least one python package is required for MLEngine \" \"Training job.\"\n            )\n        if not training_python_module:\n            raise AirflowException(\n                \"Python module name to run after installing required \"\n                \"packages is required.\"\n            )\n        if not self._region:\n            raise AirflowException(\"Google Compute Engine region is required.\")\n        if (\n            self._scale_tier is not None\n            and self._scale_tier.upper() == \"CUSTOM\"\n            and not self._master_type\n        ):\n            raise AirflowException(\"master_type must be set when scale_tier is CUSTOM\")\n\n    def execute(self, context):\n        job_id = _normalize_mlengine_job_id(self._job_id)\n        training_request = {\n            \"jobId\": job_id,\n            \"trainingInput\": {\n                \"scaleTier\": self._scale_tier,\n                \"packageUris\": self._package_uris,\n                \"pythonModule\": self._training_python_module,\n                \"region\": self._region,\n                \"args\": self._training_args,\n            },\n        }\n\n        if self._runtime_version:\n            training_request[\"trainingInput\"][\"runtimeVersion\"] = self._runtime_version\n\n        if self._python_version:\n            training_request[\"trainingInput\"][\"pythonVersion\"] = self._python_version\n\n        if self._job_dir:\n            training_request[\"trainingInput\"][\"jobDir\"] = self._job_dir\n\n        if self._scale_tier is not None and self._scale_tier.upper() == \"CUSTOM\":\n            training_request[\"trainingInput\"][\"masterType\"] = self._master_type\n\n        if self._mode == \"DRY_RUN\":\n            self.log.info(\"In dry_run mode.\")\n            self.log.info(\"MLEngine Training job request is: %s\", training_request)\n            return\n\n        hook = MLEngineHook(\n            gcp_conn_id=self._gcp_conn_id, delegate_to=self._delegate_to\n        )\n\n        # Helper method to check if the existing job's training input is the\n        # same as the request we get here.\n        def check_existing_job(existing_job):\n            return (\n                existing_job.get(\"trainingInput\", None)\n                == training_request[\"trainingInput\"]\n            )\n\n        try:\n            finished_training_job = hook.create_job(\n                self._project_id, training_request, check_existing_job\n            )\n        except HttpError:\n            raise\n\n        if finished_training_job[\"state\"] != \"SUCCEEDED\":\n            self.log.error(\n                \"MLEngine training job failed: %s\", str(finished_training_job)\n            )\n            raise RuntimeError(finished_training_job[\"errorMessage\"])\n", "levels": [0, 1, 2, 0, 1, 0, 1, 0, 1, 2], "package": ["import re", "from googleapiclient.errors import HttpError", "from airflow.contrib.hooks.gcp_mlengine_hook import MLEngineHook", "from airflow.exceptions import AirflowException", "from airflow.models import BaseOperator", "from airflow.utils.decorators import apply_defaults", "from airflow.utils.log.logging_mixin import LoggingMixin"], "function": ["class MLEngineBatchPredictionOperator(BaseOperator):\n", "    def execute(self, context):\n", "        def check_existing_job(existing_job):\n", "class MLEngineModelOperator(BaseOperator):\n", "    def execute(self, context):\n", "class MLEngineVersionOperator(BaseOperator):\n", "    def execute(self, context):\n", "class MLEngineTrainingOperator(BaseOperator):\n", "    def execute(self, context):\n", "        def check_existing_job(existing_job):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/sensors/ftp_sensor.py", "func_name": "FTPSensor._get_error_code", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Extract error code from ftp exception", "docstring_tokens": ["Extract", "error", "code", "from", "ftp", "exception"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/ftp_sensor.py#L69-L76", "partition": "test", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport ftplib\nimport re\n\nfrom airflow.contrib.hooks.ftp_hook import FTPHook, FTPSHook\nfrom airflow.sensors.base_sensor_operator import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass FTPSensor(BaseSensorOperator):\n    \"\"\"\n    Waits for a file or directory to be present on FTP.\n    \"\"\"\n\n    template_fields = (\"path\",)\n\n    \"\"\"Errors that are transient in nature, and where action can be retried\"\"\"\n    transient_errors = [421, 425, 426, 434, 450, 451, 452]\n\n    error_code_pattern = re.compile(r\"([\\d]+)\")\n\n    @apply_defaults\n    def __init__(\n        self,\n        path,\n        ftp_conn_id=\"ftp_default\",\n        fail_on_transient_errors=True,\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Create a new FTP sensor\n\n        :param path: Remote file or directory path\n        :type path: str\n        :param fail_on_transient_errors: Fail on all errors,\n            including 4xx transient errors. Default True.\n        :type fail_on_transient_errors: bool\n        :param ftp_conn_id: The connection to run the sensor against\n        :type ftp_conn_id: str\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n\n        self.path = path\n        self.ftp_conn_id = ftp_conn_id\n        self.fail_on_transient_errors = fail_on_transient_errors\n\n    def _create_hook(self):\n        \"\"\"Return connection hook.\"\"\"\n        return FTPHook(ftp_conn_id=self.ftp_conn_id)\n\n    def poke(self, context):\n        with self._create_hook() as hook:\n            self.log.info(\"Poking for %s\", self.path)\n            try:\n                hook.get_mod_time(self.path)\n            except ftplib.error_perm as e:\n                self.log.info(\"Ftp error encountered: %s\", str(e))\n                error_code = self._get_error_code(e)\n                if (error_code != 550) and (\n                    self.fail_on_transient_errors\n                    or (error_code not in self.transient_errors)\n                ):\n                    raise e\n\n                return False\n\n            return True\n\n\nclass FTPSSensor(FTPSensor):\n    \"\"\"Waits for a file or directory to be present on FTP over SSL.\"\"\"\n\n    def _create_hook(self):\n        \"\"\"Return connection hook.\"\"\"\n        return FTPSHook(ftp_conn_id=self.ftp_conn_id)\n", "levels": [0, 1, 1, 0, 1], "package": ["import ftplib", "import re", "from airflow.contrib.hooks.ftp_hook import FTPHook, FTPSHook", "from airflow.sensors.base_sensor_operator import BaseSensorOperator", "from airflow.utils.decorators import apply_defaults"], "function": ["class FTPSensor(BaseSensorOperator):\n", "    def _create_hook(self):\n", "    def poke(self, context):\n", "class FTPSSensor(FTPSensor):\n", "    def _create_hook(self):\n"]}
{"repo": "apache/airflow", "path": "scripts/perf/scheduler_ops_metrics.py", "func_name": "clear_dag_runs", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Remove any existing DAG runs for the perf test DAGs.", "docstring_tokens": ["Remove", "any", "existing", "DAG", "runs", "for", "the", "perf", "test", "DAGs", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L138-L148", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\nimport pandas as pd\nimport sys\n\nfrom airflow import configuration, settings\nfrom airflow.jobs import SchedulerJob\nfrom airflow.models import DagBag, DagModel, DagRun, TaskInstance\nfrom airflow.utils import timezone\nfrom airflow.utils.state import State\n\nSUBDIR = \"scripts/perf/dags\"\nDAG_IDS = [\"perf_dag_1\", \"perf_dag_2\"]\nMAX_RUNTIME_SECS = 6\n\n\nclass SchedulerMetricsJob(SchedulerJob):\n    \"\"\"\n    This class extends SchedulerJob to instrument the execution performance of\n    task instances contained in each DAG. We want to know if any DAG\n    is starved of resources, and this will be reflected in the stats printed\n    out at the end of the test run. The following metrics will be instrumented\n    for each task instance (dag_id, task_id, execution_date) tuple:\n\n    1. Queuing delay - time taken from starting the executor to the task\n       instance to be added to the executor queue.\n    2. Start delay - time taken from starting the executor to the task instance\n       to start execution.\n    3. Land time - time taken from starting the executor to task instance\n       completion.\n    4. Duration - time taken for executing the task instance.\n\n    The DAGs implement bash operators that call the system wait command. This\n    is representative of typical operators run on Airflow - queries that are\n    run on remote systems and spend the majority of their time on I/O wait.\n\n    To Run:\n        $ python scripts/perf/scheduler_ops_metrics.py [timeout]\n\n    You can specify timeout in seconds as an optional parameter.\n    Its default value is 6 seconds.\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"SchedulerMetricsJob\"}\n\n    def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [\n            (\n                ti.dag_id,\n                ti.task_id,\n                ti.execution_date,\n                (ti.queued_dttm - self.start_date).total_seconds(),\n                (ti.start_date - self.start_date).total_seconds(),\n                (ti.end_date - self.start_date).total_seconds(),\n                ti.duration,\n            )\n            for ti in successful_tis\n        ]\n        ti_perf_df = pd.DataFrame(\n            ti_perf,\n            columns=[\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"queue_delay\",\n                \"start_delay\",\n                \"land_time\",\n                \"duration\",\n            ],\n        )\n\n        print(\"Performance Results\")\n        print(\"###################\")\n        for dag_id in DAG_IDS:\n            print(\"DAG {}\".format(dag_id))\n            print(ti_perf_df[ti_perf_df[\"dag_id\"] == dag_id])\n        print(\"###################\")\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(\n                pd.DataFrame(\n                    [\n                        (ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                        for ti in filter(lambda x: x.state != State.SUCCESS, tis)\n                    ],\n                    columns=[\"dag_id\", \"task_id\", \"execution_date\", \"state\"],\n                )\n            )\n\n        session.commit()\n\n    def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session.query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum(\n            [\n                (timezone.utcnow() - task.start_date).days\n                for dag in dags\n                for task in dag.tasks\n            ]\n        )\n\n        if (\n            len(successful_tis) == num_task_instances\n            or (timezone.utcnow() - self.start_date).total_seconds() > MAX_RUNTIME_SECS\n        ):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()\n\n\ndef clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n    for ti in tis:\n        logging.info(\"Deleting TaskInstance :: {}\".format(ti))\n        session.delete(ti)\n    session.commit()\n\n\ndef set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info(\"Setting DAG :: {} is_paused={}\".format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()\n\n\ndef main():\n    global MAX_RUNTIME_SECS\n    if len(sys.argv) > 1:\n        try:\n            max_runtime_secs = int(sys.argv[1])\n            if max_runtime_secs < 1:\n                raise ValueError\n            MAX_RUNTIME_SECS = max_runtime_secs\n        except ValueError:\n            logging.error(\"Specify a positive integer for timeout.\")\n            sys.exit(1)\n\n    configuration.load_test_config()\n\n    set_dags_paused_state(False)\n    clear_dag_runs()\n    clear_dag_task_instances()\n\n    job = SchedulerMetricsJob(dag_ids=DAG_IDS, subdir=SUBDIR)\n    job.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 1, 1, 0, 0, 0], "package": ["import logging", "import pandas as pd", "import sys", "from airflow import configuration, settings", "from airflow.jobs import SchedulerJob", "from airflow.models import DagBag, DagModel, DagRun, TaskInstance", "from airflow.utils import timezone", "from airflow.utils.state import State"], "function": ["class SchedulerMetricsJob(SchedulerJob):\n", "    def print_stats(self):\n", "    def heartbeat(self):\n", "def clear_dag_task_instances():\n", "def set_dags_paused_state(is_paused):\n", "def main():\n"]}
{"repo": "apache/airflow", "path": "scripts/perf/scheduler_ops_metrics.py", "func_name": "clear_dag_task_instances", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Remove any existing task instances for the perf test DAGs.", "docstring_tokens": ["Remove", "any", "existing", "task", "instances", "for", "the", "perf", "test", "DAGs", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L151-L166", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\nimport pandas as pd\nimport sys\n\nfrom airflow import configuration, settings\nfrom airflow.jobs import SchedulerJob\nfrom airflow.models import DagBag, DagModel, DagRun, TaskInstance\nfrom airflow.utils import timezone\nfrom airflow.utils.state import State\n\nSUBDIR = \"scripts/perf/dags\"\nDAG_IDS = [\"perf_dag_1\", \"perf_dag_2\"]\nMAX_RUNTIME_SECS = 6\n\n\nclass SchedulerMetricsJob(SchedulerJob):\n    \"\"\"\n    This class extends SchedulerJob to instrument the execution performance of\n    task instances contained in each DAG. We want to know if any DAG\n    is starved of resources, and this will be reflected in the stats printed\n    out at the end of the test run. The following metrics will be instrumented\n    for each task instance (dag_id, task_id, execution_date) tuple:\n\n    1. Queuing delay - time taken from starting the executor to the task\n       instance to be added to the executor queue.\n    2. Start delay - time taken from starting the executor to the task instance\n       to start execution.\n    3. Land time - time taken from starting the executor to task instance\n       completion.\n    4. Duration - time taken for executing the task instance.\n\n    The DAGs implement bash operators that call the system wait command. This\n    is representative of typical operators run on Airflow - queries that are\n    run on remote systems and spend the majority of their time on I/O wait.\n\n    To Run:\n        $ python scripts/perf/scheduler_ops_metrics.py [timeout]\n\n    You can specify timeout in seconds as an optional parameter.\n    Its default value is 6 seconds.\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"SchedulerMetricsJob\"}\n\n    def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [\n            (\n                ti.dag_id,\n                ti.task_id,\n                ti.execution_date,\n                (ti.queued_dttm - self.start_date).total_seconds(),\n                (ti.start_date - self.start_date).total_seconds(),\n                (ti.end_date - self.start_date).total_seconds(),\n                ti.duration,\n            )\n            for ti in successful_tis\n        ]\n        ti_perf_df = pd.DataFrame(\n            ti_perf,\n            columns=[\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"queue_delay\",\n                \"start_delay\",\n                \"land_time\",\n                \"duration\",\n            ],\n        )\n\n        print(\"Performance Results\")\n        print(\"###################\")\n        for dag_id in DAG_IDS:\n            print(\"DAG {}\".format(dag_id))\n            print(ti_perf_df[ti_perf_df[\"dag_id\"] == dag_id])\n        print(\"###################\")\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(\n                pd.DataFrame(\n                    [\n                        (ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                        for ti in filter(lambda x: x.state != State.SUCCESS, tis)\n                    ],\n                    columns=[\"dag_id\", \"task_id\", \"execution_date\", \"state\"],\n                )\n            )\n\n        session.commit()\n\n    def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session.query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum(\n            [\n                (timezone.utcnow() - task.start_date).days\n                for dag in dags\n                for task in dag.tasks\n            ]\n        )\n\n        if (\n            len(successful_tis) == num_task_instances\n            or (timezone.utcnow() - self.start_date).total_seconds() > MAX_RUNTIME_SECS\n        ):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()\n\n\ndef clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = (\n        session.query(DagRun)\n        .filter(\n            DagRun.dag_id.in_(DAG_IDS),\n        )\n        .all()\n    )\n    for dr in drs:\n        logging.info(\"Deleting DagRun :: {}\".format(dr))\n        session.delete(dr)\n\n\ndef set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info(\"Setting DAG :: {} is_paused={}\".format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()\n\n\ndef main():\n    global MAX_RUNTIME_SECS\n    if len(sys.argv) > 1:\n        try:\n            max_runtime_secs = int(sys.argv[1])\n            if max_runtime_secs < 1:\n                raise ValueError\n            MAX_RUNTIME_SECS = max_runtime_secs\n        except ValueError:\n            logging.error(\"Specify a positive integer for timeout.\")\n            sys.exit(1)\n\n    configuration.load_test_config()\n\n    set_dags_paused_state(False)\n    clear_dag_runs()\n    clear_dag_task_instances()\n\n    job = SchedulerMetricsJob(dag_ids=DAG_IDS, subdir=SUBDIR)\n    job.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 1, 1, 0, 0, 0], "package": ["import logging", "import pandas as pd", "import sys", "from airflow import configuration, settings", "from airflow.jobs import SchedulerJob", "from airflow.models import DagBag, DagModel, DagRun, TaskInstance", "from airflow.utils import timezone", "from airflow.utils.state import State"], "function": ["class SchedulerMetricsJob(SchedulerJob):\n", "    def print_stats(self):\n", "    def heartbeat(self):\n", "def clear_dag_runs():\n", "def set_dags_paused_state(is_paused):\n", "def main():\n"]}
{"repo": "apache/airflow", "path": "scripts/perf/scheduler_ops_metrics.py", "func_name": "set_dags_paused_state", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Toggle the pause state of the DAGs in the test.", "docstring_tokens": ["Toggle", "the", "pause", "state", "of", "the", "DAGs", "in", "the", "test", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L169-L179", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\nimport pandas as pd\nimport sys\n\nfrom airflow import configuration, settings\nfrom airflow.jobs import SchedulerJob\nfrom airflow.models import DagBag, DagModel, DagRun, TaskInstance\nfrom airflow.utils import timezone\nfrom airflow.utils.state import State\n\nSUBDIR = \"scripts/perf/dags\"\nDAG_IDS = [\"perf_dag_1\", \"perf_dag_2\"]\nMAX_RUNTIME_SECS = 6\n\n\nclass SchedulerMetricsJob(SchedulerJob):\n    \"\"\"\n    This class extends SchedulerJob to instrument the execution performance of\n    task instances contained in each DAG. We want to know if any DAG\n    is starved of resources, and this will be reflected in the stats printed\n    out at the end of the test run. The following metrics will be instrumented\n    for each task instance (dag_id, task_id, execution_date) tuple:\n\n    1. Queuing delay - time taken from starting the executor to the task\n       instance to be added to the executor queue.\n    2. Start delay - time taken from starting the executor to the task instance\n       to start execution.\n    3. Land time - time taken from starting the executor to task instance\n       completion.\n    4. Duration - time taken for executing the task instance.\n\n    The DAGs implement bash operators that call the system wait command. This\n    is representative of typical operators run on Airflow - queries that are\n    run on remote systems and spend the majority of their time on I/O wait.\n\n    To Run:\n        $ python scripts/perf/scheduler_ops_metrics.py [timeout]\n\n    You can specify timeout in seconds as an optional parameter.\n    Its default value is 6 seconds.\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"SchedulerMetricsJob\"}\n\n    def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [\n            (\n                ti.dag_id,\n                ti.task_id,\n                ti.execution_date,\n                (ti.queued_dttm - self.start_date).total_seconds(),\n                (ti.start_date - self.start_date).total_seconds(),\n                (ti.end_date - self.start_date).total_seconds(),\n                ti.duration,\n            )\n            for ti in successful_tis\n        ]\n        ti_perf_df = pd.DataFrame(\n            ti_perf,\n            columns=[\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"queue_delay\",\n                \"start_delay\",\n                \"land_time\",\n                \"duration\",\n            ],\n        )\n\n        print(\"Performance Results\")\n        print(\"###################\")\n        for dag_id in DAG_IDS:\n            print(\"DAG {}\".format(dag_id))\n            print(ti_perf_df[ti_perf_df[\"dag_id\"] == dag_id])\n        print(\"###################\")\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(\n                pd.DataFrame(\n                    [\n                        (ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                        for ti in filter(lambda x: x.state != State.SUCCESS, tis)\n                    ],\n                    columns=[\"dag_id\", \"task_id\", \"execution_date\", \"state\"],\n                )\n            )\n\n        session.commit()\n\n    def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session.query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum(\n            [\n                (timezone.utcnow() - task.start_date).days\n                for dag in dags\n                for task in dag.tasks\n            ]\n        )\n\n        if (\n            len(successful_tis) == num_task_instances\n            or (timezone.utcnow() - self.start_date).total_seconds() > MAX_RUNTIME_SECS\n        ):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()\n\n\ndef clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = (\n        session.query(DagRun)\n        .filter(\n            DagRun.dag_id.in_(DAG_IDS),\n        )\n        .all()\n    )\n    for dr in drs:\n        logging.info(\"Deleting DagRun :: {}\".format(dr))\n        session.delete(dr)\n\n\ndef clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n    for ti in tis:\n        logging.info(\"Deleting TaskInstance :: {}\".format(ti))\n        session.delete(ti)\n    session.commit()\n\n\ndef main():\n    global MAX_RUNTIME_SECS\n    if len(sys.argv) > 1:\n        try:\n            max_runtime_secs = int(sys.argv[1])\n            if max_runtime_secs < 1:\n                raise ValueError\n            MAX_RUNTIME_SECS = max_runtime_secs\n        except ValueError:\n            logging.error(\"Specify a positive integer for timeout.\")\n            sys.exit(1)\n\n    configuration.load_test_config()\n\n    set_dags_paused_state(False)\n    clear_dag_runs()\n    clear_dag_task_instances()\n\n    job = SchedulerMetricsJob(dag_ids=DAG_IDS, subdir=SUBDIR)\n    job.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 1, 1, 0, 0, 0], "package": ["import logging", "import pandas as pd", "import sys", "from airflow import configuration, settings", "from airflow.jobs import SchedulerJob", "from airflow.models import DagBag, DagModel, DagRun, TaskInstance", "from airflow.utils import timezone", "from airflow.utils.state import State"], "function": ["class SchedulerMetricsJob(SchedulerJob):\n", "    def print_stats(self):\n", "    def heartbeat(self):\n", "def clear_dag_runs():\n", "def clear_dag_task_instances():\n", "def main():\n"]}
{"repo": "apache/airflow", "path": "scripts/perf/scheduler_ops_metrics.py", "func_name": "SchedulerMetricsJob.print_stats", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Print operational metrics for the scheduler test.", "docstring_tokens": ["Print", "operational", "metrics", "for", "the", "scheduler", "test", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L65-L101", "partition": "test", "up_fun_num": 1, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\nimport pandas as pd\nimport sys\n\nfrom airflow import configuration, settings\nfrom airflow.jobs import SchedulerJob\nfrom airflow.models import DagBag, DagModel, DagRun, TaskInstance\nfrom airflow.utils import timezone\nfrom airflow.utils.state import State\n\nSUBDIR = \"scripts/perf/dags\"\nDAG_IDS = [\"perf_dag_1\", \"perf_dag_2\"]\nMAX_RUNTIME_SECS = 6\n\n\nclass SchedulerMetricsJob(SchedulerJob):\n    \"\"\"\n    This class extends SchedulerJob to instrument the execution performance of\n    task instances contained in each DAG. We want to know if any DAG\n    is starved of resources, and this will be reflected in the stats printed\n    out at the end of the test run. The following metrics will be instrumented\n    for each task instance (dag_id, task_id, execution_date) tuple:\n\n    1. Queuing delay - time taken from starting the executor to the task\n       instance to be added to the executor queue.\n    2. Start delay - time taken from starting the executor to the task instance\n       to start execution.\n    3. Land time - time taken from starting the executor to task instance\n       completion.\n    4. Duration - time taken for executing the task instance.\n\n    The DAGs implement bash operators that call the system wait command. This\n    is representative of typical operators run on Airflow - queries that are\n    run on remote systems and spend the majority of their time on I/O wait.\n\n    To Run:\n        $ python scripts/perf/scheduler_ops_metrics.py [timeout]\n\n    You can specify timeout in seconds as an optional parameter.\n    Its default value is 6 seconds.\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"SchedulerMetricsJob\"}\n\n    def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session.query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum(\n            [\n                (timezone.utcnow() - task.start_date).days\n                for dag in dags\n                for task in dag.tasks\n            ]\n        )\n\n        if (\n            len(successful_tis) == num_task_instances\n            or (timezone.utcnow() - self.start_date).total_seconds() > MAX_RUNTIME_SECS\n        ):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()\n\n\ndef clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = (\n        session.query(DagRun)\n        .filter(\n            DagRun.dag_id.in_(DAG_IDS),\n        )\n        .all()\n    )\n    for dr in drs:\n        logging.info(\"Deleting DagRun :: {}\".format(dr))\n        session.delete(dr)\n\n\ndef clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n    for ti in tis:\n        logging.info(\"Deleting TaskInstance :: {}\".format(ti))\n        session.delete(ti)\n    session.commit()\n\n\ndef set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info(\"Setting DAG :: {} is_paused={}\".format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()\n\n\ndef main():\n    global MAX_RUNTIME_SECS\n    if len(sys.argv) > 1:\n        try:\n            max_runtime_secs = int(sys.argv[1])\n            if max_runtime_secs < 1:\n                raise ValueError\n            MAX_RUNTIME_SECS = max_runtime_secs\n        except ValueError:\n            logging.error(\"Specify a positive integer for timeout.\")\n            sys.exit(1)\n\n    configuration.load_test_config()\n\n    set_dags_paused_state(False)\n    clear_dag_runs()\n    clear_dag_task_instances()\n\n    job = SchedulerMetricsJob(dag_ids=DAG_IDS, subdir=SUBDIR)\n    job.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 1, 0, 0, 0, 0], "package": ["import logging", "import pandas as pd", "import sys", "from airflow import configuration, settings", "from airflow.jobs import SchedulerJob", "from airflow.models import DagBag, DagModel, DagRun, TaskInstance", "from airflow.utils import timezone", "from airflow.utils.state import State"], "function": ["class SchedulerMetricsJob(SchedulerJob):\n", "    def heartbeat(self):\n", "def clear_dag_runs():\n", "def clear_dag_task_instances():\n", "def set_dags_paused_state(is_paused):\n", "def main():\n"]}
{"repo": "apache/airflow", "path": "scripts/perf/scheduler_ops_metrics.py", "func_name": "SchedulerMetricsJob.heartbeat", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Override the scheduler heartbeat to determine when the test is complete", "docstring_tokens": ["Override", "the", "scheduler", "heartbeat", "to", "determine", "when", "the", "test", "is", "complete"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L103-L135", "partition": "test", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport logging\nimport pandas as pd\nimport sys\n\nfrom airflow import configuration, settings\nfrom airflow.jobs import SchedulerJob\nfrom airflow.models import DagBag, DagModel, DagRun, TaskInstance\nfrom airflow.utils import timezone\nfrom airflow.utils.state import State\n\nSUBDIR = \"scripts/perf/dags\"\nDAG_IDS = [\"perf_dag_1\", \"perf_dag_2\"]\nMAX_RUNTIME_SECS = 6\n\n\nclass SchedulerMetricsJob(SchedulerJob):\n    \"\"\"\n    This class extends SchedulerJob to instrument the execution performance of\n    task instances contained in each DAG. We want to know if any DAG\n    is starved of resources, and this will be reflected in the stats printed\n    out at the end of the test run. The following metrics will be instrumented\n    for each task instance (dag_id, task_id, execution_date) tuple:\n\n    1. Queuing delay - time taken from starting the executor to the task\n       instance to be added to the executor queue.\n    2. Start delay - time taken from starting the executor to the task instance\n       to start execution.\n    3. Land time - time taken from starting the executor to task instance\n       completion.\n    4. Duration - time taken for executing the task instance.\n\n    The DAGs implement bash operators that call the system wait command. This\n    is representative of typical operators run on Airflow - queries that are\n    run on remote systems and spend the majority of their time on I/O wait.\n\n    To Run:\n        $ python scripts/perf/scheduler_ops_metrics.py [timeout]\n\n    You can specify timeout in seconds as an optional parameter.\n    Its default value is 6 seconds.\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"SchedulerMetricsJob\"}\n\n    def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [\n            (\n                ti.dag_id,\n                ti.task_id,\n                ti.execution_date,\n                (ti.queued_dttm - self.start_date).total_seconds(),\n                (ti.start_date - self.start_date).total_seconds(),\n                (ti.end_date - self.start_date).total_seconds(),\n                ti.duration,\n            )\n            for ti in successful_tis\n        ]\n        ti_perf_df = pd.DataFrame(\n            ti_perf,\n            columns=[\n                \"dag_id\",\n                \"task_id\",\n                \"execution_date\",\n                \"queue_delay\",\n                \"start_delay\",\n                \"land_time\",\n                \"duration\",\n            ],\n        )\n\n        print(\"Performance Results\")\n        print(\"###################\")\n        for dag_id in DAG_IDS:\n            print(\"DAG {}\".format(dag_id))\n            print(ti_perf_df[ti_perf_df[\"dag_id\"] == dag_id])\n        print(\"###################\")\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(\n                pd.DataFrame(\n                    [\n                        (ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                        for ti in filter(lambda x: x.state != State.SUCCESS, tis)\n                    ],\n                    columns=[\"dag_id\", \"task_id\", \"execution_date\", \"state\"],\n                )\n            )\n\n        session.commit()\n\n\ndef clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = (\n        session.query(DagRun)\n        .filter(\n            DagRun.dag_id.in_(DAG_IDS),\n        )\n        .all()\n    )\n    for dr in drs:\n        logging.info(\"Deleting DagRun :: {}\".format(dr))\n        session.delete(dr)\n\n\ndef clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = session.query(TI).filter(TI.dag_id.in_(DAG_IDS)).all()\n    for ti in tis:\n        logging.info(\"Deleting TaskInstance :: {}\".format(ti))\n        session.delete(ti)\n    session.commit()\n\n\ndef set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info(\"Setting DAG :: {} is_paused={}\".format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()\n\n\ndef main():\n    global MAX_RUNTIME_SECS\n    if len(sys.argv) > 1:\n        try:\n            max_runtime_secs = int(sys.argv[1])\n            if max_runtime_secs < 1:\n                raise ValueError\n            MAX_RUNTIME_SECS = max_runtime_secs\n        except ValueError:\n            logging.error(\"Specify a positive integer for timeout.\")\n            sys.exit(1)\n\n    configuration.load_test_config()\n\n    set_dags_paused_state(False)\n    clear_dag_runs()\n    clear_dag_task_instances()\n\n    job = SchedulerMetricsJob(dag_ids=DAG_IDS, subdir=SUBDIR)\n    job.run()\n\n\nif __name__ == \"__main__\":\n    main()\n", "levels": [0, 1, 0, 0, 0, 0], "package": ["import logging", "import pandas as pd", "import sys", "from airflow import configuration, settings", "from airflow.jobs import SchedulerJob", "from airflow.models import DagBag, DagModel, DagRun, TaskInstance", "from airflow.utils import timezone", "from airflow.utils.state import State"], "function": ["class SchedulerMetricsJob(SchedulerJob):\n", "    def print_stats(self):\n", "def clear_dag_runs():\n", "def clear_dag_task_instances():\n", "def set_dags_paused_state(is_paused):\n", "def main():\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/aws_lambda_hook.py", "func_name": "AwsLambdaHook.invoke_lambda", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Invoke Lambda Function", "docstring_tokens": ["Invoke", "Lambda", "Function"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_lambda_hook.py#L53-L68", "partition": "test", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom airflow.contrib.hooks.aws_hook import AwsHook\n\n\nclass AwsLambdaHook(AwsHook):\n    \"\"\"\n    Interact with AWS Lambda\n\n    :param function_name: AWS Lambda Function Name\n    :type function_name: str\n    :param region_name: AWS Region Name (example: us-west-2)\n    :type region_name: str\n    :param log_type: Tail Invocation Request\n    :type log_type: str\n    :param qualifier: AWS Lambda Function Version or Alias Name\n    :type qualifier: str\n    :param invocation_type: AWS Lambda Invocation Type (RequestResponse, Event etc)\n    :type invocation_type: str\n    \"\"\"\n\n    def __init__(\n        self,\n        function_name,\n        region_name=None,\n        log_type=\"None\",\n        qualifier=\"$LATEST\",\n        invocation_type=\"RequestResponse\",\n        *args,\n        **kwargs\n    ):\n        self.function_name = function_name\n        self.region_name = region_name\n        self.log_type = log_type\n        self.invocation_type = invocation_type\n        self.qualifier = qualifier\n        super().__init__(*args, **kwargs)\n\n    def get_conn(self):\n        self.conn = self.get_client_type(\"lambda\", self.region_name)\n        return self.conn\n", "levels": [0, 1], "package": ["from airflow.contrib.hooks.aws_hook import AwsHook"], "function": ["class AwsLambdaHook(AwsHook):\n", "    def get_conn(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/utils/mlengine_operator_utils.py", "func_name": "create_evaluate_ops", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)", "docstring_tokens": ["Creates", "Operators", "needed", "for", "model", "evaluation", "and", "returns", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/mlengine_operator_utils.py#L32-L246", "partition": "test", "up_fun_num": 0, "context": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the 'License'); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport json\nimport os\nimport re\n\nimport dill\n\nfrom airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook\nfrom airflow.contrib.operators.mlengine_operator import MLEngineBatchPredictionOperator\nfrom airflow.contrib.operators.dataflow_operator import DataFlowPythonOperator\nfrom airflow.exceptions import AirflowException\nfrom airflow.operators.python_operator import PythonOperator\nfrom urllib.parse import urlsplit\n", "levels": [], "package": ["import base64", "import json", "import os", "import re", "import dill", "from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook", "from airflow.contrib.operators.mlengine_operator import MLEngineBatchPredictionOperator", "from airflow.contrib.operators.dataflow_operator import DataFlowPythonOperator", "from airflow.exceptions import AirflowException", "from airflow.operators.python_operator import PythonOperator", "from urllib.parse import urlsplit"], "function": []}
{"repo": "apache/airflow", "path": "airflow/utils/file.py", "func_name": "mkdirs", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int", "docstring_tokens": ["Creates", "the", "directory", "specified", "by", "path", "creating", "intermediate", "directories", "as", "necessary", ".", "If", "directory", "already", "exists", "this", "is", "a", "no", "-", "op", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/file.py#L42-L59", "partition": "test", "up_fun_num": 1, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport errno\nimport os\nimport shutil\nfrom tempfile import mkdtemp\n\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef TemporaryDirectory(suffix=\"\", prefix=None, dir=None):\n    name = mkdtemp(suffix=suffix, prefix=prefix, dir=dir)\n    try:\n        yield name\n    finally:\n        try:\n            shutil.rmtree(name)\n        except OSError as e:\n            # ENOENT - no such file or directory\n            if e.errno != errno.ENOENT:\n                raise e\n", "levels": [0], "package": ["import errno", "import os", "import shutil", "from tempfile import mkdtemp", "from contextlib import contextmanager"], "function": ["def TemporaryDirectory(suffix=\"\", prefix=None, dir=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/operators/check_operator.py", "func_name": "_convert_to_float_if_possible", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str", "docstring_tokens": ["A", "small", "helper", "function", "to", "convert", "a", "string", "to", "a", "numeric", "value", "if", "appropriate"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/check_operator.py#L98-L110", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str, zip\nfrom typing import Optional, Any, Iterable, Dict, SupportsAbs\n\nfrom airflow.exceptions import AirflowException\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass CheckOperator(BaseOperator):\n    \"\"\"\n    Performs checks against a db. The ``CheckOperator`` expects\n    a sql query that will return a single row. Each value on that\n    first row is evaluated using python ``bool`` casting. If any of the\n    values return ``False`` the check is failed and errors out.\n\n    Note that Python bool casting evals the following as ``False``:\n\n    * ``False``\n    * ``0``\n    * Empty string (``\"\"``)\n    * Empty list (``[]``)\n    * Empty dictionary or set (``{}``)\n\n    Given a query like ``SELECT COUNT(*) FROM foo``, it will fail only if\n    the count ``== 0``. You can craft much more complex query that could,\n    for instance, check that the table has the same number of rows as\n    the source table upstream, or that the count of today's partition is\n    greater than yesterday's partition, or that a set of metrics are less\n    than 3 standard deviation for the 7 day average.\n\n    This operator can be used as a data quality check in your pipeline, and\n    depending on where you put it in your DAG, you have the choice to\n    stop the critical path, preventing from\n    publishing dubious data, or on the side and receive email alerts\n    without stopping the progress of the DAG.\n\n    Note that this is an abstract class and get_db_hook\n    needs to be defined. Whereas a get_db_hook is hook that gets a\n    single record from an external source.\n\n    :param sql: the sql to be executed. (templated)\n    :type sql: str\n    \"\"\"\n\n    template_fields = (\"sql\",)  # type: Iterable[str]\n    template_ext = (\n        \".hql\",\n        \".sql\",\n    )  # type: Iterable[str]\n    ui_color = \"#fff7e6\"\n\n    @apply_defaults\n    def __init__(\n        self,\n        sql,  # type: str\n        conn_id=None,  # type: Optional[str]\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.conn_id = conn_id\n        self.sql = sql\n\n    def execute(self, context=None):\n        self.log.info(\"Executing SQL check: %s\", self.sql)\n        records = self.get_db_hook().get_first(self.sql)\n\n        self.log.info(\"Record: %s\", records)\n        if not records:\n            raise AirflowException(\"The query returned None\")\n        elif not all([bool(r) for r in records]):\n            raise AirflowException(\n                \"Test failed.\\nQuery:\\n{query}\\nResults:\\n{records!s}\".format(\n                    query=self.sql, records=records\n                )\n            )\n\n        self.log.info(\"Success.\")\n\n    def get_db_hook(self):\n        return BaseHook.get_hook(conn_id=self.conn_id)\n\n\nclass ValueCheckOperator(BaseOperator):\n    \"\"\"\n    Performs a simple value check using sql code.\n\n    Note that this is an abstract class and get_db_hook\n    needs to be defined. Whereas a get_db_hook is hook that gets a\n    single record from an external source.\n\n    :param sql: the sql to be executed. (templated)\n    :type sql: str\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"ValueCheckOperator\"}\n    template_fields = (\n        \"sql\",\n        \"pass_value\",\n    )  # type: Iterable[str]\n    template_ext = (\n        \".hql\",\n        \".sql\",\n    )  # type: Iterable[str]\n    ui_color = \"#fff7e6\"\n\n    @apply_defaults\n    def __init__(\n        self,\n        sql,  # type: str\n        pass_value,  # type: Any\n        tolerance=None,  # type: Any\n        conn_id=None,  # type: Optional[str]\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.sql = sql\n        self.conn_id = conn_id\n        self.pass_value = str(pass_value)\n        tol = _convert_to_float_if_possible(tolerance)\n        self.tol = tol if isinstance(tol, float) else None\n        self.has_tolerance = self.tol is not None\n\n    def execute(self, context=None):\n        self.log.info(\"Executing SQL check: %s\", self.sql)\n        records = self.get_db_hook().get_first(self.sql)\n\n        if not records:\n            raise AirflowException(\"The query returned None\")\n\n        pass_value_conv = _convert_to_float_if_possible(self.pass_value)\n        is_numeric_value_check = isinstance(pass_value_conv, float)\n\n        tolerance_pct_str = str(self.tol * 100) + \"%\" if self.has_tolerance else None\n        error_msg = (\n            \"Test failed.\\nPass value:{pass_value_conv}\\n\"\n            \"Tolerance:{tolerance_pct_str}\\n\"\n            \"Query:\\n{sql}\\nResults:\\n{records!s}\"\n        ).format(\n            pass_value_conv=pass_value_conv,\n            tolerance_pct_str=tolerance_pct_str,\n            sql=self.sql,\n            records=records,\n        )\n\n        if not is_numeric_value_check:\n            tests = self._get_string_matches(records, pass_value_conv)\n        elif is_numeric_value_check:\n            try:\n                numeric_records = self._to_float(records)\n            except (ValueError, TypeError):\n                raise AirflowException(\n                    \"Converting a result to float failed.\\n{}\".format(error_msg)\n                )\n            tests = self._get_numeric_matches(numeric_records, pass_value_conv)\n        else:\n            tests = []\n\n        if not all(tests):\n            raise AirflowException(error_msg)\n\n    def _to_float(self, records):\n        return [float(record) for record in records]\n\n    def _get_string_matches(self, records, pass_value_conv):\n        return [str(record) == pass_value_conv for record in records]\n\n    def _get_numeric_matches(self, numeric_records, numeric_pass_value_conv):\n        if self.has_tolerance:\n            return [\n                numeric_pass_value_conv * (1 - self.tol)\n                <= record\n                <= numeric_pass_value_conv * (1 + self.tol)\n                for record in numeric_records\n            ]\n\n        return [record == numeric_pass_value_conv for record in numeric_records]\n\n    def get_db_hook(self):\n        return BaseHook.get_hook(conn_id=self.conn_id)\n\n\nclass IntervalCheckOperator(BaseOperator):\n    \"\"\"\n    Checks that the values of metrics given as SQL expressions are within\n    a certain tolerance of the ones from days_back before.\n\n    Note that this is an abstract class and get_db_hook\n    needs to be defined. Whereas a get_db_hook is hook that gets a\n    single record from an external source.\n\n    :param table: the table name\n    :type table: str\n    :param days_back: number of days between ds and the ds we want to check\n        against. Defaults to 7 days\n    :type days_back: int\n    :param ratio_formula: which formula to use to compute the ratio between\n        the two metrics. Assuming cur is the metric of today and ref is\n        the metric to today - days_back.\n\n        max_over_min: computes max(cur, ref) / min(cur, ref)\n        relative_diff: computes abs(cur-ref) / ref\n\n        Default: 'max_over_min'\n    :type ratio_formula: str\n    :param ignore_zero: whether we should ignore zero metrics\n    :type ignore_zero: bool\n    :param metrics_threshold: a dictionary of ratios indexed by metrics\n    :type metrics_threshold: dict\n    \"\"\"\n\n    __mapper_args__ = {\"polymorphic_identity\": \"IntervalCheckOperator\"}\n    template_fields = (\"sql1\", \"sql2\")  # type: Iterable[str]\n    template_ext = (\n        \".hql\",\n        \".sql\",\n    )  # type: Iterable[str]\n    ui_color = \"#fff7e6\"\n\n    ratio_formulas = {\n        \"max_over_min\": lambda cur, ref: float(max(cur, ref)) / min(cur, ref),\n        \"relative_diff\": lambda cur, ref: float(abs(cur - ref)) / ref,\n    }\n\n    @apply_defaults\n    def __init__(\n        self,\n        table,  # type: str\n        metrics_thresholds,  # type: Dict[str, int]\n        date_filter_column=\"ds\",  # type: Optional[str]\n        days_back=-7,  # type: SupportsAbs[int]\n        ratio_formula=\"max_over_min\",  # type: Optional[str]\n        ignore_zero=True,  # type: Optional[bool]\n        conn_id=None,  # type: Optional[str]\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        if ratio_formula not in self.ratio_formulas:\n            msg_template = (\n                \"Invalid diff_method: {diff_method}. \"\n                \"Supported diff methods are: {diff_methods}\"\n            )\n\n            raise AirflowException(\n                msg_template.format(\n                    diff_method=ratio_formula, diff_methods=self.ratio_formulas\n                )\n            )\n        self.ratio_formula = ratio_formula\n        self.ignore_zero = ignore_zero\n        self.table = table\n        self.metrics_thresholds = metrics_thresholds\n        self.metrics_sorted = sorted(metrics_thresholds.keys())\n        self.date_filter_column = date_filter_column\n        self.days_back = -abs(days_back)\n        self.conn_id = conn_id\n        sqlexp = \", \".join(self.metrics_sorted)\n        sqlt = \"SELECT {sqlexp} FROM {table} WHERE {date_filter_column}=\".format(\n            sqlexp=sqlexp, table=table, date_filter_column=date_filter_column\n        )\n\n        self.sql1 = sqlt + \"'{{ ds }}'\"\n        self.sql2 = sqlt + \"'{{ macros.ds_add(ds, \" + str(self.days_back) + \") }}'\"\n\n    def execute(self, context=None):\n        hook = self.get_db_hook()\n        self.log.info(\"Using ratio formula: %s\", self.ratio_formula)\n        self.log.info(\"Executing SQL check: %s\", self.sql2)\n        row2 = hook.get_first(self.sql2)\n        self.log.info(\"Executing SQL check: %s\", self.sql1)\n        row1 = hook.get_first(self.sql1)\n\n        if not row2:\n            raise AirflowException(\"The query {} returned None\".format(self.sql2))\n        if not row1:\n            raise AirflowException(\"The query {} returned None\".format(self.sql1))\n\n        current = dict(zip(self.metrics_sorted, row1))\n        reference = dict(zip(self.metrics_sorted, row2))\n\n        ratios = {}\n        test_results = {}\n\n        for m in self.metrics_sorted:\n            cur = current[m]\n            ref = reference[m]\n            threshold = self.metrics_thresholds[m]\n            if cur == 0 or ref == 0:\n                ratios[m] = None\n                test_results[m] = self.ignore_zero\n            else:\n                ratios[m] = self.ratio_formulas[self.ratio_formula](\n                    current[m], reference[m]\n                )\n                test_results[m] = ratios[m] < threshold\n\n            self.log.info(\n                (\n                    \"Current metric for %s: %s\\n\"\n                    \"Past metric for %s: %s\\n\"\n                    \"Ratio for %s: %s\\n\"\n                    \"Threshold: %s\\n\"\n                ),\n                m,\n                cur,\n                m,\n                ref,\n                m,\n                ratios[m],\n                threshold,\n            )\n\n        if not all(test_results.values()):\n            failed_tests = [it[0] for it in test_results.items() if not it[1]]\n            j = len(failed_tests)\n            n = len(self.metrics_sorted)\n            self.log.warning(\"The following %s tests out of %s failed:\", j, n)\n            for k in failed_tests:\n                self.log.warning(\n                    \"'%s' check failed. %s is above %s\",\n                    k,\n                    ratios[k],\n                    self.metrics_thresholds[k],\n                )\n            raise AirflowException(\n                \"The following tests have failed:\\n {0}\".format(\n                    \", \".join(sorted(failed_tests))\n                )\n            )\n\n        self.log.info(\"All tests have passed\")\n\n    def get_db_hook(self):\n        return BaseHook.get_hook(conn_id=self.conn_id)\n", "levels": [0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1], "package": ["from builtins import str, zip", "from typing import Optional, Any, Iterable, Dict, SupportsAbs", "from airflow.exceptions import AirflowException", "from airflow.hooks.base_hook import BaseHook", "from airflow.models import BaseOperator", "from airflow.utils.decorators import apply_defaults"], "function": ["class CheckOperator(BaseOperator):\n", "    def execute(self, context=None):\n", "    def get_db_hook(self):\n", "class ValueCheckOperator(BaseOperator):\n", "    def execute(self, context=None):\n", "    def _to_float(self, records):\n", "    def _get_string_matches(self, records, pass_value_conv):\n", "    def _get_numeric_matches(self, numeric_records, numeric_pass_value_conv):\n", "    def get_db_hook(self):\n", "class IntervalCheckOperator(BaseOperator):\n", "    def execute(self, context=None):\n", "    def get_db_hook(self):\n"]}
{"repo": "apache/airflow", "path": "airflow/utils/timezone.py", "func_name": "make_aware", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone", "docstring_tokens": ["Make", "a", "naive", "datetime", ".", "datetime", "in", "a", "given", "time", "zone", "aware", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L98-L128", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport datetime as dt\nimport pendulum\n\nfrom airflow.settings import TIMEZONE\n\n\n# UTC time zone as a tzinfo instance.\nutc = pendulum.timezone(\"UTC\")\n\n\ndef is_localized(value):\n    \"\"\"\n    Determine if a given datetime.datetime is aware.\n    The concept is defined in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    Assuming value.tzinfo is either None or a proper datetime.tzinfo,\n    value.utcoffset() implements the appropriate logic.\n    \"\"\"\n    return value.utcoffset() is not None\n\n\ndef is_naive(value):\n    \"\"\"\n    Determine if a given datetime.datetime is naive.\n    The concept is defined in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    Assuming value.tzinfo is either None or a proper datetime.tzinfo,\n    value.utcoffset() implements the appropriate logic.\n    \"\"\"\n    return value.utcoffset() is None\n\n\ndef utcnow():\n    \"\"\"\n    Get the current date and time in UTC\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime.utcnow()\n    d = d.replace(tzinfo=utc)\n\n    return d\n\n\ndef utc_epoch():\n    \"\"\"\n    Gets the epoch in the users timezone\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime(1970, 1, 1)\n    d = d.replace(tzinfo=utc)\n\n    return d\n\n\ndef convert_to_utc(value):\n    \"\"\"\n    Returns the datetime with the default timezone added if timezone\n    information was not associated\n    :param value: datetime\n    :return: datetime with tzinfo\n    \"\"\"\n    if not value:\n        return value\n\n    if not is_localized(value):\n        value = pendulum.instance(value, TIMEZONE)\n\n    return value.astimezone(utc)\n\n\ndef make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(\n        o.year, o.month, o.day, o.hour, o.minute, o.second, o.microsecond\n    )\n\n    return naive\n\n\ndef datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if \"tzinfo\" not in kwargs:\n        kwargs[\"tzinfo\"] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)\n\n\ndef parse(string, timezone=None):\n    \"\"\"\n    Parse a time string and return an aware datetime\n    :param string: time string\n    \"\"\"\n    return pendulum.parse(string, tz=timezone or TIMEZONE)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import datetime as dt", "import pendulum", "from airflow.settings import TIMEZONE"], "function": ["def is_localized(value):\n", "def is_naive(value):\n", "def utcnow():\n", "def utc_epoch():\n", "def convert_to_utc(value):\n", "def make_naive(value, timezone=None):\n", "def datetime(*args, **kwargs):\n", "def parse(string, timezone=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/utils/timezone.py", "func_name": "make_naive", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime", "docstring_tokens": ["Make", "an", "aware", "datetime", ".", "datetime", "naive", "in", "a", "given", "time", "zone", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L131-L157", "partition": "test", "up_fun_num": 6, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport datetime as dt\nimport pendulum\n\nfrom airflow.settings import TIMEZONE\n\n\n# UTC time zone as a tzinfo instance.\nutc = pendulum.timezone(\"UTC\")\n\n\ndef is_localized(value):\n    \"\"\"\n    Determine if a given datetime.datetime is aware.\n    The concept is defined in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    Assuming value.tzinfo is either None or a proper datetime.tzinfo,\n    value.utcoffset() implements the appropriate logic.\n    \"\"\"\n    return value.utcoffset() is not None\n\n\ndef is_naive(value):\n    \"\"\"\n    Determine if a given datetime.datetime is naive.\n    The concept is defined in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    Assuming value.tzinfo is either None or a proper datetime.tzinfo,\n    value.utcoffset() implements the appropriate logic.\n    \"\"\"\n    return value.utcoffset() is None\n\n\ndef utcnow():\n    \"\"\"\n    Get the current date and time in UTC\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime.utcnow()\n    d = d.replace(tzinfo=utc)\n\n    return d\n\n\ndef utc_epoch():\n    \"\"\"\n    Gets the epoch in the users timezone\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime(1970, 1, 1)\n    d = d.replace(tzinfo=utc)\n\n    return d\n\n\ndef convert_to_utc(value):\n    \"\"\"\n    Returns the datetime with the default timezone added if timezone\n    information was not associated\n    :param value: datetime\n    :return: datetime with tzinfo\n    \"\"\"\n    if not value:\n        return value\n\n    if not is_localized(value):\n        value = pendulum.instance(value, TIMEZONE)\n\n    return value.astimezone(utc)\n\n\ndef make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, \"fold\"):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, \"localize\"):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, \"convert\"):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)\n\n\ndef datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if \"tzinfo\" not in kwargs:\n        kwargs[\"tzinfo\"] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)\n\n\ndef parse(string, timezone=None):\n    \"\"\"\n    Parse a time string and return an aware datetime\n    :param string: time string\n    \"\"\"\n    return pendulum.parse(string, tz=timezone or TIMEZONE)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import datetime as dt", "import pendulum", "from airflow.settings import TIMEZONE"], "function": ["def is_localized(value):\n", "def is_naive(value):\n", "def utcnow():\n", "def utc_epoch():\n", "def convert_to_utc(value):\n", "def make_aware(value, timezone=None):\n", "def datetime(*args, **kwargs):\n", "def parse(string, timezone=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/utils/timezone.py", "func_name": "datetime", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime", "docstring_tokens": ["Wrapper", "around", "datetime", ".", "datetime", "that", "adds", "settings", ".", "TIMEZONE", "if", "tzinfo", "not", "specified"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L160-L169", "partition": "test", "up_fun_num": 7, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport datetime as dt\nimport pendulum\n\nfrom airflow.settings import TIMEZONE\n\n\n# UTC time zone as a tzinfo instance.\nutc = pendulum.timezone(\"UTC\")\n\n\ndef is_localized(value):\n    \"\"\"\n    Determine if a given datetime.datetime is aware.\n    The concept is defined in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    Assuming value.tzinfo is either None or a proper datetime.tzinfo,\n    value.utcoffset() implements the appropriate logic.\n    \"\"\"\n    return value.utcoffset() is not None\n\n\ndef is_naive(value):\n    \"\"\"\n    Determine if a given datetime.datetime is naive.\n    The concept is defined in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    Assuming value.tzinfo is either None or a proper datetime.tzinfo,\n    value.utcoffset() implements the appropriate logic.\n    \"\"\"\n    return value.utcoffset() is None\n\n\ndef utcnow():\n    \"\"\"\n    Get the current date and time in UTC\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime.utcnow()\n    d = d.replace(tzinfo=utc)\n\n    return d\n\n\ndef utc_epoch():\n    \"\"\"\n    Gets the epoch in the users timezone\n    :return:\n    \"\"\"\n\n    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n    # instead of a Timezone. This is not pickable and also creates issues\n    # when using replace()\n    d = dt.datetime(1970, 1, 1)\n    d = d.replace(tzinfo=utc)\n\n    return d\n\n\ndef convert_to_utc(value):\n    \"\"\"\n    Returns the datetime with the default timezone added if timezone\n    information was not associated\n    :param value: datetime\n    :return: datetime with tzinfo\n    \"\"\"\n    if not value:\n        return value\n\n    if not is_localized(value):\n        value = pendulum.instance(value, TIMEZONE)\n\n    return value.astimezone(utc)\n\n\ndef make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, \"fold\"):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, \"localize\"):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, \"convert\"):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)\n\n\ndef make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(\n        o.year, o.month, o.day, o.hour, o.minute, o.second, o.microsecond\n    )\n\n    return naive\n\n\ndef parse(string, timezone=None):\n    \"\"\"\n    Parse a time string and return an aware datetime\n    :param string: time string\n    \"\"\"\n    return pendulum.parse(string, tz=timezone or TIMEZONE)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import datetime as dt", "import pendulum", "from airflow.settings import TIMEZONE"], "function": ["def is_localized(value):\n", "def is_naive(value):\n", "def utcnow():\n", "def utc_epoch():\n", "def convert_to_utc(value):\n", "def make_aware(value, timezone=None):\n", "def make_naive(value, timezone=None):\n", "def parse(string, timezone=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/druid_hook.py", "func_name": "DruidDbApiHook.get_conn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Establish a connection to druid broker.", "docstring_tokens": ["Establish", "a", "connection", "to", "druid", "broker", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/druid_hook.py#L127-L139", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport requests\nimport time\n\nfrom pydruid.db import connect\n\nfrom airflow.exceptions import AirflowException\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass DruidHook(BaseHook):\n    \"\"\"\n    Connection to Druid overlord for ingestion\n\n    :param druid_ingest_conn_id: The connection id to the Druid overlord machine\n                                 which accepts index jobs\n    :type druid_ingest_conn_id: str\n    :param timeout: The interval between polling\n                    the Druid job for the status of the ingestion job.\n                    Must be greater than or equal to 1\n    :type timeout: int\n    :param max_ingestion_time: The maximum ingestion time before assuming the job failed\n    :type max_ingestion_time: int\n    \"\"\"\n\n    def __init__(\n        self,\n        druid_ingest_conn_id=\"druid_ingest_default\",\n        timeout=1,\n        max_ingestion_time=None,\n    ):\n\n        self.druid_ingest_conn_id = druid_ingest_conn_id\n        self.timeout = timeout\n        self.max_ingestion_time = max_ingestion_time\n        self.header = {\"content-type\": \"application/json\"}\n\n        if self.timeout < 1:\n            raise ValueError(\"Druid timeout should be equal or greater than 1\")\n\n    def get_conn_url(self):\n        conn = self.get_connection(self.druid_ingest_conn_id)\n        host = conn.host\n        port = conn.port\n        conn_type = \"http\" if not conn.conn_type else conn.conn_type\n        endpoint = conn.extra_dejson.get(\"endpoint\", \"\")\n        return \"{conn_type}://{host}:{port}/{endpoint}\".format(\n            conn_type=conn_type, host=host, port=port, endpoint=endpoint\n        )\n\n    def submit_indexing_job(self, json_index_spec):\n        url = self.get_conn_url()\n\n        self.log.info(\"Druid ingestion spec: %s\", json_index_spec)\n        req_index = requests.post(url, json=json_index_spec, headers=self.header)\n        if req_index.status_code != 200:\n            raise AirflowException(\n                \"Did not get 200 when \" \"submitting the Druid job to {}\".format(url)\n            )\n\n        req_json = req_index.json()\n        # Wait until the job is completed\n        druid_task_id = req_json[\"task\"]\n        self.log.info(\"Druid indexing task-id: %s\", druid_task_id)\n\n        running = True\n\n        sec = 0\n        while running:\n            req_status = requests.get(\"{0}/{1}/status\".format(url, druid_task_id))\n\n            self.log.info(\"Job still running for %s seconds...\", sec)\n\n            if self.max_ingestion_time and sec > self.max_ingestion_time:\n                # ensure that the job gets killed if the max ingestion time is exceeded\n                requests.post(\"{0}/{1}/shutdown\".format(url, druid_task_id))\n                raise AirflowException(\n                    \"Druid ingestion took more than \" \"%s seconds\",\n                    self.max_ingestion_time,\n                )\n\n            time.sleep(self.timeout)\n\n            sec = sec + self.timeout\n\n            status = req_status.json()[\"status\"][\"status\"]\n            if status == \"RUNNING\":\n                running = True\n            elif status == \"SUCCESS\":\n                running = False  # Great success!\n            elif status == \"FAILED\":\n                raise AirflowException(\n                    \"Druid indexing job failed, \" \"check console for more info\"\n                )\n            else:\n                raise AirflowException(\n                    \"Could not get status of the job, got %s\", status\n                )\n\n        self.log.info(\"Successful index\")\n\n\nclass DruidDbApiHook(DbApiHook):\n    \"\"\"\n    Interact with Druid broker\n\n    This hook is purely for users to query druid broker.\n    For ingestion, please use druidHook.\n    \"\"\"\n\n    conn_name_attr = \"druid_broker_conn_id\"\n    default_conn_name = \"druid_broker_default\"\n    supports_autocommit = False\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def get_uri(self):\n        \"\"\"\n        Get the connection uri for druid broker.\n\n        e.g: druid://localhost:8082/druid/v2/sql/\n        \"\"\"\n        conn = self.get_connection(getattr(self, self.conn_name_attr))\n        host = conn.host\n        if conn.port is not None:\n            host += \":{port}\".format(port=conn.port)\n        conn_type = \"druid\" if not conn.conn_type else conn.conn_type\n        endpoint = conn.extra_dejson.get(\"endpoint\", \"druid/v2/sql\")\n        return \"{conn_type}://{host}/{endpoint}\".format(\n            conn_type=conn_type, host=host, endpoint=endpoint\n        )\n\n    def set_autocommit(self, conn, autocommit):\n        raise NotImplementedError()\n\n    def get_pandas_df(self, sql, parameters=None):\n        raise NotImplementedError()\n\n    def insert_rows(self, table, rows, target_fields=None, commit_every=1000):\n        raise NotImplementedError()\n", "levels": [0, 1, 1, 0, 1, 1, 1, 1, 1], "package": ["import requests", "import time", "from pydruid.db import connect", "from airflow.exceptions import AirflowException", "from airflow.hooks.base_hook import BaseHook", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class DruidHook(BaseHook):\n", "    def get_conn_url(self):\n", "    def submit_indexing_job(self, json_index_spec):\n", "class DruidDbApiHook(DbApiHook):\n", "    def __init__(self, *args, **kwargs):\n", "    def get_uri(self):\n", "    def set_autocommit(self, conn, autocommit):\n", "    def get_pandas_df(self, sql, parameters=None):\n", "    def insert_rows(self, table, rows, target_fields=None, commit_every=1000):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/http_hook.py", "func_name": "HttpHook.get_conn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict", "docstring_tokens": ["Returns", "http", "session", "for", "use", "with", "requests"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L53-L83", "partition": "test", "up_fun_num": 1, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nimport requests\nimport tenacity\n\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.exceptions import AirflowException\n\n\nclass HttpHook(BaseHook):\n    \"\"\"\n    Interact with HTTP servers.\n\n    :param http_conn_id: connection that has the base API url i.e https://www.google.com/\n        and optional authentication credentials. Default headers can also be specified in\n        the Extra field in json format.\n    :type http_conn_id: str\n    :param method: the API method to be called\n    :type method: str\n    \"\"\"\n\n    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n        self.http_conn_id = http_conn_id\n        self.method = method\n        self.base_url = None\n        self._retry_obj = None\n\n    # headers may be passed through directly or in the \"extra\" field in the connection\n    # definition\n\n    def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if (\n            self.base_url\n            and not self.base_url.endswith(\"/\")\n            and endpoint\n            and not endpoint.startswith(\"/\")\n        ):\n            url = self.base_url + \"/\" + endpoint\n        else:\n            url = (self.base_url or \"\") + (endpoint or \"\")\n\n        req = None\n        if self.method == \"GET\":\n            # GET uses params\n            req = requests.Request(self.method, url, params=data, headers=headers)\n        elif self.method == \"HEAD\":\n            # HEAD doesn't use params\n            req = requests.Request(self.method, url, headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method, url, data=data, headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)\n\n    def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in [\"GET\", \"HEAD\"]:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)\n\n    def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True),\n            )\n\n            if extra_options.get(\"check_response\", True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + \" Tenacity will retry to execute the operation\")\n            raise ex\n\n    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n        \"\"\"\n        Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n        connectors which might be disturbed by intermittent issues and should not\n        instantly fail.\n\n        :param _retry_args: Arguments which define the retry behaviour.\n            See Tenacity documentation at https://github.com/jd/tenacity\n        :type _retry_args: dict\n\n\n        :Example::\n\n            hook = HttpHook(http_conn_id='my_conn',method='GET')\n            retry_args = dict(\n                 wait=tenacity.wait_exponential(),\n                 stop=tenacity.stop_after_attempt(10),\n                 retry=requests.exceptions.ConnectionError\n             )\n             hook.run_with_advanced_retry(\n                     endpoint='v1/test',\n                     _retry_args=retry_args\n                 )\n        \"\"\"\n        self._retry_obj = tenacity.Retrying(**_retry_args)\n\n        self._retry_obj(self.run, *args, **kwargs)\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["from builtins import str", "import requests", "import tenacity", "from airflow.hooks.base_hook import BaseHook", "from airflow.exceptions import AirflowException"], "function": ["class HttpHook(BaseHook):\n", "    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n", "    def run(self, endpoint, data=None, headers=None, extra_options=None):\n", "    def check_response(self, response):\n", "    def run_and_check(self, session, prepped_request, extra_options):\n", "    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/http_hook.py", "func_name": "HttpHook.run", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict", "docstring_tokens": ["Performs", "the", "request"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L85-L131", "partition": "test", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nimport requests\nimport tenacity\n\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.exceptions import AirflowException\n\n\nclass HttpHook(BaseHook):\n    \"\"\"\n    Interact with HTTP servers.\n\n    :param http_conn_id: connection that has the base API url i.e https://www.google.com/\n        and optional authentication credentials. Default headers can also be specified in\n        the Extra field in json format.\n    :type http_conn_id: str\n    :param method: the API method to be called\n    :type method: str\n    \"\"\"\n\n    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n        self.http_conn_id = http_conn_id\n        self.method = method\n        self.base_url = None\n        self._retry_obj = None\n\n    # headers may be passed through directly or in the \"extra\" field in the connection\n    # definition\n    def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn(\n                        \"Connection to %s has invalid extra field.\", conn.host\n                    )\n        if headers:\n            session.headers.update(headers)\n\n        return session\n\n    def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in [\"GET\", \"HEAD\"]:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)\n\n    def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True),\n            )\n\n            if extra_options.get(\"check_response\", True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + \" Tenacity will retry to execute the operation\")\n            raise ex\n\n    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n        \"\"\"\n        Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n        connectors which might be disturbed by intermittent issues and should not\n        instantly fail.\n\n        :param _retry_args: Arguments which define the retry behaviour.\n            See Tenacity documentation at https://github.com/jd/tenacity\n        :type _retry_args: dict\n\n\n        :Example::\n\n            hook = HttpHook(http_conn_id='my_conn',method='GET')\n            retry_args = dict(\n                 wait=tenacity.wait_exponential(),\n                 stop=tenacity.stop_after_attempt(10),\n                 retry=requests.exceptions.ConnectionError\n             )\n             hook.run_with_advanced_retry(\n                     endpoint='v1/test',\n                     _retry_args=retry_args\n                 )\n        \"\"\"\n        self._retry_obj = tenacity.Retrying(**_retry_args)\n\n        self._retry_obj(self.run, *args, **kwargs)\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["from builtins import str", "import requests", "import tenacity", "from airflow.hooks.base_hook import BaseHook", "from airflow.exceptions import AirflowException"], "function": ["class HttpHook(BaseHook):\n", "    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n", "    def get_conn(self, headers=None):\n", "    def check_response(self, response):\n", "    def run_and_check(self, session, prepped_request, extra_options):\n", "    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/http_hook.py", "func_name": "HttpHook.check_response", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response", "docstring_tokens": ["Checks", "the", "status", "code", "and", "raise", "an", "AirflowException", "exception", "on", "non", "2XX", "or", "3XX", "status", "codes"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L133-L147", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nimport requests\nimport tenacity\n\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.exceptions import AirflowException\n\n\nclass HttpHook(BaseHook):\n    \"\"\"\n    Interact with HTTP servers.\n\n    :param http_conn_id: connection that has the base API url i.e https://www.google.com/\n        and optional authentication credentials. Default headers can also be specified in\n        the Extra field in json format.\n    :type http_conn_id: str\n    :param method: the API method to be called\n    :type method: str\n    \"\"\"\n\n    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n        self.http_conn_id = http_conn_id\n        self.method = method\n        self.base_url = None\n        self._retry_obj = None\n\n    # headers may be passed through directly or in the \"extra\" field in the connection\n    # definition\n    def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn(\n                        \"Connection to %s has invalid extra field.\", conn.host\n                    )\n        if headers:\n            session.headers.update(headers)\n\n        return session\n\n    def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if (\n            self.base_url\n            and not self.base_url.endswith(\"/\")\n            and endpoint\n            and not endpoint.startswith(\"/\")\n        ):\n            url = self.base_url + \"/\" + endpoint\n        else:\n            url = (self.base_url or \"\") + (endpoint or \"\")\n\n        req = None\n        if self.method == \"GET\":\n            # GET uses params\n            req = requests.Request(self.method, url, params=data, headers=headers)\n        elif self.method == \"HEAD\":\n            # HEAD doesn't use params\n            req = requests.Request(self.method, url, headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method, url, data=data, headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)\n\n    def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True),\n            )\n\n            if extra_options.get(\"check_response\", True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + \" Tenacity will retry to execute the operation\")\n            raise ex\n\n    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n        \"\"\"\n        Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n        connectors which might be disturbed by intermittent issues and should not\n        instantly fail.\n\n        :param _retry_args: Arguments which define the retry behaviour.\n            See Tenacity documentation at https://github.com/jd/tenacity\n        :type _retry_args: dict\n\n\n        :Example::\n\n            hook = HttpHook(http_conn_id='my_conn',method='GET')\n            retry_args = dict(\n                 wait=tenacity.wait_exponential(),\n                 stop=tenacity.stop_after_attempt(10),\n                 retry=requests.exceptions.ConnectionError\n             )\n             hook.run_with_advanced_retry(\n                     endpoint='v1/test',\n                     _retry_args=retry_args\n                 )\n        \"\"\"\n        self._retry_obj = tenacity.Retrying(**_retry_args)\n\n        self._retry_obj(self.run, *args, **kwargs)\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["from builtins import str", "import requests", "import tenacity", "from airflow.hooks.base_hook import BaseHook", "from airflow.exceptions import AirflowException"], "function": ["class HttpHook(BaseHook):\n", "    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n", "    def get_conn(self, headers=None):\n", "    def run(self, endpoint, data=None, headers=None, extra_options=None):\n", "    def run_and_check(self, session, prepped_request, extra_options):\n", "    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/http_hook.py", "func_name": "HttpHook.run_and_check", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict", "docstring_tokens": ["Grabs", "extra", "options", "like", "timeout", "and", "actually", "runs", "the", "request", "checking", "for", "the", "result"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nimport requests\nimport tenacity\n\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.exceptions import AirflowException\n\n\nclass HttpHook(BaseHook):\n    \"\"\"\n    Interact with HTTP servers.\n\n    :param http_conn_id: connection that has the base API url i.e https://www.google.com/\n        and optional authentication credentials. Default headers can also be specified in\n        the Extra field in json format.\n    :type http_conn_id: str\n    :param method: the API method to be called\n    :type method: str\n    \"\"\"\n\n    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n        self.http_conn_id = http_conn_id\n        self.method = method\n        self.base_url = None\n        self._retry_obj = None\n\n    # headers may be passed through directly or in the \"extra\" field in the connection\n    # definition\n    def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn(\n                        \"Connection to %s has invalid extra field.\", conn.host\n                    )\n        if headers:\n            session.headers.update(headers)\n\n        return session\n\n    def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if (\n            self.base_url\n            and not self.base_url.endswith(\"/\")\n            and endpoint\n            and not endpoint.startswith(\"/\")\n        ):\n            url = self.base_url + \"/\" + endpoint\n        else:\n            url = (self.base_url or \"\") + (endpoint or \"\")\n\n        req = None\n        if self.method == \"GET\":\n            # GET uses params\n            req = requests.Request(self.method, url, params=data, headers=headers)\n        elif self.method == \"HEAD\":\n            # HEAD doesn't use params\n            req = requests.Request(self.method, url, headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method, url, data=data, headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)\n\n    def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in [\"GET\", \"HEAD\"]:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)\n\n    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n        \"\"\"\n        Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n        connectors which might be disturbed by intermittent issues and should not\n        instantly fail.\n\n        :param _retry_args: Arguments which define the retry behaviour.\n            See Tenacity documentation at https://github.com/jd/tenacity\n        :type _retry_args: dict\n\n\n        :Example::\n\n            hook = HttpHook(http_conn_id='my_conn',method='GET')\n            retry_args = dict(\n                 wait=tenacity.wait_exponential(),\n                 stop=tenacity.stop_after_attempt(10),\n                 retry=requests.exceptions.ConnectionError\n             )\n             hook.run_with_advanced_retry(\n                     endpoint='v1/test',\n                     _retry_args=retry_args\n                 )\n        \"\"\"\n        self._retry_obj = tenacity.Retrying(**_retry_args)\n\n        self._retry_obj(self.run, *args, **kwargs)\n", "levels": [0, 1, 1, 1, 1, 1], "package": ["from builtins import str", "import requests", "import tenacity", "from airflow.hooks.base_hook import BaseHook", "from airflow.exceptions import AirflowException"], "function": ["class HttpHook(BaseHook):\n", "    def __init__(self, method=\"POST\", http_conn_id=\"http_default\"):\n", "    def get_conn(self, headers=None):\n", "    def run(self, endpoint, data=None, headers=None, extra_options=None):\n", "    def check_response(self, response):\n", "    def run_with_advanced_retry(self, _retry_args, *args, **kwargs):\n"]}
{"repo": "apache/airflow", "path": "airflow/utils/db.py", "func_name": "create_session", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Contextmanager that will create and teardown a session.", "docstring_tokens": ["Contextmanager", "that", "will", "create", "and", "teardown", "a", "session", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L32-L44", "partition": "test", "up_fun_num": 0, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom functools import wraps\n\nimport os\nimport contextlib\n\nfrom airflow import settings\nfrom airflow.utils.log.logging_mixin import LoggingMixin\n\nlog = LoggingMixin().log\n\n\n@contextlib.contextmanager\ndef provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = \"session\"\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and func_params.index(\n            arg_session\n        ) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper\n\n\n@provide_session\ndef merge_conn(conn, session=None):\n    from airflow.models import Connection\n\n    if not session.query(Connection).filter(Connection.conn_id == conn.conn_id).first():\n        session.add(conn)\n        session.commit()\n\n\ndef initdb():\n    from airflow import models\n    from airflow.models import Connection\n\n    upgradedb()\n\n    merge_conn(\n        Connection(\n            conn_id=\"airflow_db\",\n            conn_type=\"mysql\",\n            host=\"mysql\",\n            login=\"root\",\n            password=\"\",\n            schema=\"airflow\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"local_mysql\",\n            conn_type=\"mysql\",\n            host=\"localhost\",\n            login=\"airflow\",\n            password=\"airflow\",\n            schema=\"airflow\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"presto_default\",\n            conn_type=\"presto\",\n            host=\"localhost\",\n            schema=\"hive\",\n            port=3400,\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"google_cloud_default\",\n            conn_type=\"google_cloud_platform\",\n            schema=\"default\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"hive_cli_default\",\n            conn_type=\"hive_cli\",\n            port=10000,\n            host=\"localhost\",\n            extra='{\"use_beeline\": true, \"auth\": \"\"}',\n            schema=\"default\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"hiveserver2_default\",\n            conn_type=\"hiveserver2\",\n            host=\"localhost\",\n            schema=\"default\",\n            port=10000,\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"metastore_default\",\n            conn_type=\"hive_metastore\",\n            host=\"localhost\",\n            extra='{\"authMechanism\": \"PLAIN\"}',\n            port=9083,\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"mongo_default\", conn_type=\"mongo\", host=\"mongo\", port=27017)\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"mysql_default\",\n            conn_type=\"mysql\",\n            login=\"root\",\n            schema=\"airflow\",\n            host=\"mysql\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"postgres_default\",\n            conn_type=\"postgres\",\n            login=\"postgres\",\n            password=\"airflow\",\n            schema=\"airflow\",\n            host=\"postgres\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"sqlite_default\", conn_type=\"sqlite\", host=\"/tmp/sqlite_default.db\"\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"http_default\", conn_type=\"http\", host=\"https://www.google.com/\"\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"mssql_default\", conn_type=\"mssql\", host=\"localhost\", port=1433\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"vertica_default\", conn_type=\"vertica\", host=\"localhost\", port=5433\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"wasb_default\", conn_type=\"wasb\", extra='{\"sas_token\": null}'\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"webhdfs_default\", conn_type=\"hdfs\", host=\"localhost\", port=50070\n        )\n    )\n    merge_conn(Connection(conn_id=\"ssh_default\", conn_type=\"ssh\", host=\"localhost\"))\n    merge_conn(\n        Connection(\n            conn_id=\"sftp_default\",\n            conn_type=\"sftp\",\n            host=\"localhost\",\n            port=22,\n            login=\"airflow\",\n            extra=\"\"\"\n                {\"key_file\": \"~/.ssh/id_rsa\", \"no_host_key_check\": true}\n            \"\"\",\n        )\n    )\n    merge_conn(Connection(conn_id=\"fs_default\", conn_type=\"fs\", extra='{\"path\": \"/\"}'))\n    merge_conn(\n        Connection(\n            conn_id=\"aws_default\", conn_type=\"aws\", extra='{\"region_name\": \"us-east-1\"}'\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"spark_default\",\n            conn_type=\"spark\",\n            host=\"yarn\",\n            extra='{\"queue\": \"root.default\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"druid_broker_default\",\n            conn_type=\"druid\",\n            host=\"druid-broker\",\n            port=8082,\n            extra='{\"endpoint\": \"druid/v2/sql\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"druid_ingest_default\",\n            conn_type=\"druid\",\n            host=\"druid-overlord\",\n            port=8081,\n            extra='{\"endpoint\": \"druid/indexer/v1/task\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"redis_default\",\n            conn_type=\"redis\",\n            host=\"redis\",\n            port=6379,\n            extra='{\"db\": 0}',\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"sqoop_default\", conn_type=\"sqoop\", host=\"rmdbs\", extra=\"\")\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"emr_default\",\n            conn_type=\"emr\",\n            extra=\"\"\"\n                {   \"Name\": \"default_job_flow_name\",\n                    \"LogUri\": \"s3://my-emr-log-bucket/default_job_flow_location\",\n                    \"ReleaseLabel\": \"emr-4.6.0\",\n                    \"Instances\": {\n                        \"Ec2KeyName\": \"mykey\",\n                        \"Ec2SubnetId\": \"somesubnet\",\n                        \"InstanceGroups\": [\n                            {\n                                \"Name\": \"Master nodes\",\n                                \"Market\": \"ON_DEMAND\",\n                                \"InstanceRole\": \"MASTER\",\n                                \"InstanceType\": \"r3.2xlarge\",\n                                \"InstanceCount\": 1\n                            },\n                            {\n                                \"Name\": \"Slave nodes\",\n                                \"Market\": \"ON_DEMAND\",\n                                \"InstanceRole\": \"CORE\",\n                                \"InstanceType\": \"r3.2xlarge\",\n                                \"InstanceCount\": 1\n                            }\n                        ],\n                        \"TerminationProtected\": false,\n                        \"KeepJobFlowAliveWhenNoSteps\": false\n                    },\n                    \"Applications\":[\n                        { \"Name\": \"Spark\" }\n                    ],\n                    \"VisibleToAllUsers\": true,\n                    \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n                    \"ServiceRole\": \"EMR_DefaultRole\",\n                    \"Tags\": [\n                        {\n                            \"Key\": \"app\",\n                            \"Value\": \"analytics\"\n                        },\n                        {\n                            \"Key\": \"environment\",\n                            \"Value\": \"development\"\n                        }\n                    ]\n                }\n            \"\"\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"databricks_default\", conn_type=\"databricks\", host=\"localhost\"\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"qubole_default\", conn_type=\"qubole\", host=\"localhost\")\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"segment_default\",\n            conn_type=\"segment\",\n            extra='{\"write_key\": \"my-segment-write-key\"}',\n        )\n    ),\n    merge_conn(\n        Connection(\n            conn_id=\"azure_data_lake_default\",\n            conn_type=\"azure_data_lake\",\n            extra='{\"tenant\": \"<TENANT>\", \"account_name\": \"<ACCOUNTNAME>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"azure_cosmos_default\",\n            conn_type=\"azure_cosmos\",\n            extra='{\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"<COLLECTION_NAME>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"azure_container_instances_default\",\n            conn_type=\"azure_container_instances\",\n            extra='{\"tenantId\": \"<TENANT>\", \"subscriptionId\": \"<SUBSCRIPTION ID>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"cassandra_default\",\n            conn_type=\"cassandra\",\n            host=\"cassandra\",\n            port=9042,\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"dingding_default\", conn_type=\"http\", host=\"\", password=\"\")\n    )\n    merge_conn(\n        Connection(conn_id=\"opsgenie_default\", conn_type=\"http\", host=\"\", password=\"\")\n    )\n\n    dagbag = models.DagBag()\n    # Save individual DAGs in the ORM\n    for dag in dagbag.dags.values():\n        dag.sync_to_db()\n    # Deactivate the unknown ones\n    models.DAG.deactivate_unknown_dags(dagbag.dags.keys())\n\n    from flask_appbuilder.models.sqla import Base\n\n    Base.metadata.create_all(settings.engine)\n\n\ndef upgradedb():\n    # alembic adds significant import time, so we import it lazily\n    from alembic import command\n    from alembic.config import Config\n\n    log.info(\"Creating tables\")\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    package_dir = os.path.normpath(os.path.join(current_dir, \"..\"))\n    directory = os.path.join(package_dir, \"migrations\")\n    config = Config(os.path.join(package_dir, \"alembic.ini\"))\n    config.set_main_option(\"script_location\", directory.replace(\"%\", \"%%\"))\n    config.set_main_option(\n        \"sqlalchemy.url\", settings.SQL_ALCHEMY_CONN.replace(\"%\", \"%%\")\n    )\n    command.upgrade(config, \"heads\")\n\n\ndef resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()\n", "levels": [0, 1, 0, 0, 0, 0], "package": ["from functools import wraps", "import os", "import contextlib", "from airflow import settings", "from airflow.utils.log.logging_mixin import LoggingMixin", "from airflow.models import Connection", "from airflow import models", "from airflow.models import Connection", "from flask_appbuilder.models.sqla import Base", "from alembic import command", "from alembic.config import Config", "from airflow import models", "from alembic.migration import MigrationContext", "from flask_appbuilder.models.sqla import Base"], "function": ["def provide_session(func):\n", "    def wrapper(*args, **kwargs):\n", "def merge_conn(conn, session=None):\n", "def initdb():\n", "def upgradedb():\n", "def resetdb():\n"]}
{"repo": "apache/airflow", "path": "airflow/utils/db.py", "func_name": "provide_session", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.", "docstring_tokens": ["Function", "decorator", "that", "provides", "a", "session", "if", "it", "isn", "t", "provided", ".", "If", "you", "want", "to", "reuse", "a", "session", "or", "run", "the", "function", "as", "part", "of", "a", "database", "transaction", "you", "pass", "it", "to", "the", "function", "if", "not", "this", "wrapper", "will", "create", "one", "and", "close", "it", "for", "you", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L47-L70", "partition": "test", "up_fun_num": 1, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom functools import wraps\n\nimport os\nimport contextlib\n\nfrom airflow import settings\nfrom airflow.utils.log.logging_mixin import LoggingMixin\n\nlog = LoggingMixin().log\n\n\n@contextlib.contextmanager\ndef create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n\n@provide_session\ndef merge_conn(conn, session=None):\n    from airflow.models import Connection\n\n    if not session.query(Connection).filter(Connection.conn_id == conn.conn_id).first():\n        session.add(conn)\n        session.commit()\n\n\ndef initdb():\n    from airflow import models\n    from airflow.models import Connection\n\n    upgradedb()\n\n    merge_conn(\n        Connection(\n            conn_id=\"airflow_db\",\n            conn_type=\"mysql\",\n            host=\"mysql\",\n            login=\"root\",\n            password=\"\",\n            schema=\"airflow\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"local_mysql\",\n            conn_type=\"mysql\",\n            host=\"localhost\",\n            login=\"airflow\",\n            password=\"airflow\",\n            schema=\"airflow\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"presto_default\",\n            conn_type=\"presto\",\n            host=\"localhost\",\n            schema=\"hive\",\n            port=3400,\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"google_cloud_default\",\n            conn_type=\"google_cloud_platform\",\n            schema=\"default\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"hive_cli_default\",\n            conn_type=\"hive_cli\",\n            port=10000,\n            host=\"localhost\",\n            extra='{\"use_beeline\": true, \"auth\": \"\"}',\n            schema=\"default\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"hiveserver2_default\",\n            conn_type=\"hiveserver2\",\n            host=\"localhost\",\n            schema=\"default\",\n            port=10000,\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"metastore_default\",\n            conn_type=\"hive_metastore\",\n            host=\"localhost\",\n            extra='{\"authMechanism\": \"PLAIN\"}',\n            port=9083,\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"mongo_default\", conn_type=\"mongo\", host=\"mongo\", port=27017)\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"mysql_default\",\n            conn_type=\"mysql\",\n            login=\"root\",\n            schema=\"airflow\",\n            host=\"mysql\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"postgres_default\",\n            conn_type=\"postgres\",\n            login=\"postgres\",\n            password=\"airflow\",\n            schema=\"airflow\",\n            host=\"postgres\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"sqlite_default\", conn_type=\"sqlite\", host=\"/tmp/sqlite_default.db\"\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"http_default\", conn_type=\"http\", host=\"https://www.google.com/\"\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"mssql_default\", conn_type=\"mssql\", host=\"localhost\", port=1433\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"vertica_default\", conn_type=\"vertica\", host=\"localhost\", port=5433\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"wasb_default\", conn_type=\"wasb\", extra='{\"sas_token\": null}'\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"webhdfs_default\", conn_type=\"hdfs\", host=\"localhost\", port=50070\n        )\n    )\n    merge_conn(Connection(conn_id=\"ssh_default\", conn_type=\"ssh\", host=\"localhost\"))\n    merge_conn(\n        Connection(\n            conn_id=\"sftp_default\",\n            conn_type=\"sftp\",\n            host=\"localhost\",\n            port=22,\n            login=\"airflow\",\n            extra=\"\"\"\n                {\"key_file\": \"~/.ssh/id_rsa\", \"no_host_key_check\": true}\n            \"\"\",\n        )\n    )\n    merge_conn(Connection(conn_id=\"fs_default\", conn_type=\"fs\", extra='{\"path\": \"/\"}'))\n    merge_conn(\n        Connection(\n            conn_id=\"aws_default\", conn_type=\"aws\", extra='{\"region_name\": \"us-east-1\"}'\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"spark_default\",\n            conn_type=\"spark\",\n            host=\"yarn\",\n            extra='{\"queue\": \"root.default\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"druid_broker_default\",\n            conn_type=\"druid\",\n            host=\"druid-broker\",\n            port=8082,\n            extra='{\"endpoint\": \"druid/v2/sql\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"druid_ingest_default\",\n            conn_type=\"druid\",\n            host=\"druid-overlord\",\n            port=8081,\n            extra='{\"endpoint\": \"druid/indexer/v1/task\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"redis_default\",\n            conn_type=\"redis\",\n            host=\"redis\",\n            port=6379,\n            extra='{\"db\": 0}',\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"sqoop_default\", conn_type=\"sqoop\", host=\"rmdbs\", extra=\"\")\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"emr_default\",\n            conn_type=\"emr\",\n            extra=\"\"\"\n                {   \"Name\": \"default_job_flow_name\",\n                    \"LogUri\": \"s3://my-emr-log-bucket/default_job_flow_location\",\n                    \"ReleaseLabel\": \"emr-4.6.0\",\n                    \"Instances\": {\n                        \"Ec2KeyName\": \"mykey\",\n                        \"Ec2SubnetId\": \"somesubnet\",\n                        \"InstanceGroups\": [\n                            {\n                                \"Name\": \"Master nodes\",\n                                \"Market\": \"ON_DEMAND\",\n                                \"InstanceRole\": \"MASTER\",\n                                \"InstanceType\": \"r3.2xlarge\",\n                                \"InstanceCount\": 1\n                            },\n                            {\n                                \"Name\": \"Slave nodes\",\n                                \"Market\": \"ON_DEMAND\",\n                                \"InstanceRole\": \"CORE\",\n                                \"InstanceType\": \"r3.2xlarge\",\n                                \"InstanceCount\": 1\n                            }\n                        ],\n                        \"TerminationProtected\": false,\n                        \"KeepJobFlowAliveWhenNoSteps\": false\n                    },\n                    \"Applications\":[\n                        { \"Name\": \"Spark\" }\n                    ],\n                    \"VisibleToAllUsers\": true,\n                    \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n                    \"ServiceRole\": \"EMR_DefaultRole\",\n                    \"Tags\": [\n                        {\n                            \"Key\": \"app\",\n                            \"Value\": \"analytics\"\n                        },\n                        {\n                            \"Key\": \"environment\",\n                            \"Value\": \"development\"\n                        }\n                    ]\n                }\n            \"\"\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"databricks_default\", conn_type=\"databricks\", host=\"localhost\"\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"qubole_default\", conn_type=\"qubole\", host=\"localhost\")\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"segment_default\",\n            conn_type=\"segment\",\n            extra='{\"write_key\": \"my-segment-write-key\"}',\n        )\n    ),\n    merge_conn(\n        Connection(\n            conn_id=\"azure_data_lake_default\",\n            conn_type=\"azure_data_lake\",\n            extra='{\"tenant\": \"<TENANT>\", \"account_name\": \"<ACCOUNTNAME>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"azure_cosmos_default\",\n            conn_type=\"azure_cosmos\",\n            extra='{\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"<COLLECTION_NAME>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"azure_container_instances_default\",\n            conn_type=\"azure_container_instances\",\n            extra='{\"tenantId\": \"<TENANT>\", \"subscriptionId\": \"<SUBSCRIPTION ID>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"cassandra_default\",\n            conn_type=\"cassandra\",\n            host=\"cassandra\",\n            port=9042,\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"dingding_default\", conn_type=\"http\", host=\"\", password=\"\")\n    )\n    merge_conn(\n        Connection(conn_id=\"opsgenie_default\", conn_type=\"http\", host=\"\", password=\"\")\n    )\n\n    dagbag = models.DagBag()\n    # Save individual DAGs in the ORM\n    for dag in dagbag.dags.values():\n        dag.sync_to_db()\n    # Deactivate the unknown ones\n    models.DAG.deactivate_unknown_dags(dagbag.dags.keys())\n\n    from flask_appbuilder.models.sqla import Base\n\n    Base.metadata.create_all(settings.engine)\n\n\ndef upgradedb():\n    # alembic adds significant import time, so we import it lazily\n    from alembic import command\n    from alembic.config import Config\n\n    log.info(\"Creating tables\")\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    package_dir = os.path.normpath(os.path.join(current_dir, \"..\"))\n    directory = os.path.join(package_dir, \"migrations\")\n    config = Config(os.path.join(package_dir, \"alembic.ini\"))\n    config.set_main_option(\"script_location\", directory.replace(\"%\", \"%%\"))\n    config.set_main_option(\n        \"sqlalchemy.url\", settings.SQL_ALCHEMY_CONN.replace(\"%\", \"%%\")\n    )\n    command.upgrade(config, \"heads\")\n\n\ndef resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()\n", "levels": [0, 0, 0, 0, 0], "package": ["from functools import wraps", "import os", "import contextlib", "from airflow import settings", "from airflow.utils.log.logging_mixin import LoggingMixin", "from airflow.models import Connection", "from airflow import models", "from airflow.models import Connection", "from flask_appbuilder.models.sqla import Base", "from alembic import command", "from alembic.config import Config", "from airflow import models", "from alembic.migration import MigrationContext", "from flask_appbuilder.models.sqla import Base"], "function": ["def create_session():\n", "def merge_conn(conn, session=None):\n", "def initdb():\n", "def upgradedb():\n", "def resetdb():\n"]}
{"repo": "apache/airflow", "path": "airflow/utils/db.py", "func_name": "resetdb", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Clear out the database", "docstring_tokens": ["Clear", "out", "the", "database"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L312-L331", "partition": "test", "up_fun_num": 6, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom functools import wraps\n\nimport os\nimport contextlib\n\nfrom airflow import settings\nfrom airflow.utils.log.logging_mixin import LoggingMixin\n\nlog = LoggingMixin().log\n\n\n@contextlib.contextmanager\ndef create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n\ndef provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = \"session\"\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and func_params.index(\n            arg_session\n        ) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper\n\n\n@provide_session\ndef merge_conn(conn, session=None):\n    from airflow.models import Connection\n\n    if not session.query(Connection).filter(Connection.conn_id == conn.conn_id).first():\n        session.add(conn)\n        session.commit()\n\n\ndef initdb():\n    from airflow import models\n    from airflow.models import Connection\n\n    upgradedb()\n\n    merge_conn(\n        Connection(\n            conn_id=\"airflow_db\",\n            conn_type=\"mysql\",\n            host=\"mysql\",\n            login=\"root\",\n            password=\"\",\n            schema=\"airflow\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"local_mysql\",\n            conn_type=\"mysql\",\n            host=\"localhost\",\n            login=\"airflow\",\n            password=\"airflow\",\n            schema=\"airflow\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"presto_default\",\n            conn_type=\"presto\",\n            host=\"localhost\",\n            schema=\"hive\",\n            port=3400,\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"google_cloud_default\",\n            conn_type=\"google_cloud_platform\",\n            schema=\"default\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"hive_cli_default\",\n            conn_type=\"hive_cli\",\n            port=10000,\n            host=\"localhost\",\n            extra='{\"use_beeline\": true, \"auth\": \"\"}',\n            schema=\"default\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"hiveserver2_default\",\n            conn_type=\"hiveserver2\",\n            host=\"localhost\",\n            schema=\"default\",\n            port=10000,\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"metastore_default\",\n            conn_type=\"hive_metastore\",\n            host=\"localhost\",\n            extra='{\"authMechanism\": \"PLAIN\"}',\n            port=9083,\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"mongo_default\", conn_type=\"mongo\", host=\"mongo\", port=27017)\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"mysql_default\",\n            conn_type=\"mysql\",\n            login=\"root\",\n            schema=\"airflow\",\n            host=\"mysql\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"postgres_default\",\n            conn_type=\"postgres\",\n            login=\"postgres\",\n            password=\"airflow\",\n            schema=\"airflow\",\n            host=\"postgres\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"sqlite_default\", conn_type=\"sqlite\", host=\"/tmp/sqlite_default.db\"\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"http_default\", conn_type=\"http\", host=\"https://www.google.com/\"\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"mssql_default\", conn_type=\"mssql\", host=\"localhost\", port=1433\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"vertica_default\", conn_type=\"vertica\", host=\"localhost\", port=5433\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"wasb_default\", conn_type=\"wasb\", extra='{\"sas_token\": null}'\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"webhdfs_default\", conn_type=\"hdfs\", host=\"localhost\", port=50070\n        )\n    )\n    merge_conn(Connection(conn_id=\"ssh_default\", conn_type=\"ssh\", host=\"localhost\"))\n    merge_conn(\n        Connection(\n            conn_id=\"sftp_default\",\n            conn_type=\"sftp\",\n            host=\"localhost\",\n            port=22,\n            login=\"airflow\",\n            extra=\"\"\"\n                {\"key_file\": \"~/.ssh/id_rsa\", \"no_host_key_check\": true}\n            \"\"\",\n        )\n    )\n    merge_conn(Connection(conn_id=\"fs_default\", conn_type=\"fs\", extra='{\"path\": \"/\"}'))\n    merge_conn(\n        Connection(\n            conn_id=\"aws_default\", conn_type=\"aws\", extra='{\"region_name\": \"us-east-1\"}'\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"spark_default\",\n            conn_type=\"spark\",\n            host=\"yarn\",\n            extra='{\"queue\": \"root.default\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"druid_broker_default\",\n            conn_type=\"druid\",\n            host=\"druid-broker\",\n            port=8082,\n            extra='{\"endpoint\": \"druid/v2/sql\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"druid_ingest_default\",\n            conn_type=\"druid\",\n            host=\"druid-overlord\",\n            port=8081,\n            extra='{\"endpoint\": \"druid/indexer/v1/task\"}',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"redis_default\",\n            conn_type=\"redis\",\n            host=\"redis\",\n            port=6379,\n            extra='{\"db\": 0}',\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"sqoop_default\", conn_type=\"sqoop\", host=\"rmdbs\", extra=\"\")\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"emr_default\",\n            conn_type=\"emr\",\n            extra=\"\"\"\n                {   \"Name\": \"default_job_flow_name\",\n                    \"LogUri\": \"s3://my-emr-log-bucket/default_job_flow_location\",\n                    \"ReleaseLabel\": \"emr-4.6.0\",\n                    \"Instances\": {\n                        \"Ec2KeyName\": \"mykey\",\n                        \"Ec2SubnetId\": \"somesubnet\",\n                        \"InstanceGroups\": [\n                            {\n                                \"Name\": \"Master nodes\",\n                                \"Market\": \"ON_DEMAND\",\n                                \"InstanceRole\": \"MASTER\",\n                                \"InstanceType\": \"r3.2xlarge\",\n                                \"InstanceCount\": 1\n                            },\n                            {\n                                \"Name\": \"Slave nodes\",\n                                \"Market\": \"ON_DEMAND\",\n                                \"InstanceRole\": \"CORE\",\n                                \"InstanceType\": \"r3.2xlarge\",\n                                \"InstanceCount\": 1\n                            }\n                        ],\n                        \"TerminationProtected\": false,\n                        \"KeepJobFlowAliveWhenNoSteps\": false\n                    },\n                    \"Applications\":[\n                        { \"Name\": \"Spark\" }\n                    ],\n                    \"VisibleToAllUsers\": true,\n                    \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n                    \"ServiceRole\": \"EMR_DefaultRole\",\n                    \"Tags\": [\n                        {\n                            \"Key\": \"app\",\n                            \"Value\": \"analytics\"\n                        },\n                        {\n                            \"Key\": \"environment\",\n                            \"Value\": \"development\"\n                        }\n                    ]\n                }\n            \"\"\",\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"databricks_default\", conn_type=\"databricks\", host=\"localhost\"\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"qubole_default\", conn_type=\"qubole\", host=\"localhost\")\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"segment_default\",\n            conn_type=\"segment\",\n            extra='{\"write_key\": \"my-segment-write-key\"}',\n        )\n    ),\n    merge_conn(\n        Connection(\n            conn_id=\"azure_data_lake_default\",\n            conn_type=\"azure_data_lake\",\n            extra='{\"tenant\": \"<TENANT>\", \"account_name\": \"<ACCOUNTNAME>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"azure_cosmos_default\",\n            conn_type=\"azure_cosmos\",\n            extra='{\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"<COLLECTION_NAME>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"azure_container_instances_default\",\n            conn_type=\"azure_container_instances\",\n            extra='{\"tenantId\": \"<TENANT>\", \"subscriptionId\": \"<SUBSCRIPTION ID>\" }',\n        )\n    )\n    merge_conn(\n        Connection(\n            conn_id=\"cassandra_default\",\n            conn_type=\"cassandra\",\n            host=\"cassandra\",\n            port=9042,\n        )\n    )\n    merge_conn(\n        Connection(conn_id=\"dingding_default\", conn_type=\"http\", host=\"\", password=\"\")\n    )\n    merge_conn(\n        Connection(conn_id=\"opsgenie_default\", conn_type=\"http\", host=\"\", password=\"\")\n    )\n\n    dagbag = models.DagBag()\n    # Save individual DAGs in the ORM\n    for dag in dagbag.dags.values():\n        dag.sync_to_db()\n    # Deactivate the unknown ones\n    models.DAG.deactivate_unknown_dags(dagbag.dags.keys())\n\n    from flask_appbuilder.models.sqla import Base\n\n    Base.metadata.create_all(settings.engine)\n\n\ndef upgradedb():\n    # alembic adds significant import time, so we import it lazily\n    from alembic import command\n    from alembic.config import Config\n\n    log.info(\"Creating tables\")\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    package_dir = os.path.normpath(os.path.join(current_dir, \"..\"))\n    directory = os.path.join(package_dir, \"migrations\")\n    config = Config(os.path.join(package_dir, \"alembic.ini\"))\n    config.set_main_option(\"script_location\", directory.replace(\"%\", \"%%\"))\n    config.set_main_option(\n        \"sqlalchemy.url\", settings.SQL_ALCHEMY_CONN.replace(\"%\", \"%%\")\n    )\n    command.upgrade(config, \"heads\")\n", "levels": [0, 0, 1, 0, 0, 0], "package": ["from functools import wraps", "import os", "import contextlib", "from airflow import settings", "from airflow.utils.log.logging_mixin import LoggingMixin", "from airflow.models import Connection", "from airflow import models", "from airflow.models import Connection", "from flask_appbuilder.models.sqla import Base", "from alembic import command", "from alembic.config import Config"], "function": ["def create_session():\n", "def provide_session(func):\n", "    def wrapper(*args, **kwargs):\n", "def merge_conn(conn, session=None):\n", "def initdb():\n", "def upgradedb():\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/presto_hook.py", "func_name": "PrestoHook._get_pretty_exception_message", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Parses some DatabaseError to provide a better error message", "docstring_tokens": ["Parses", "some", "DatabaseError", "to", "provide", "a", "better", "error", "message"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L67-L78", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nfrom pyhive import presto\nfrom pyhive.exc import DatabaseError\nfrom requests.auth import HTTPBasicAuth\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PrestoException(Exception):\n    pass\n\n\nclass PrestoHook(DbApiHook):\n    \"\"\"\n    Interact with Presto through PyHive!\n\n    >>> ph = PrestoHook()\n    >>> sql = \"SELECT count(1) AS num FROM airflow.static_babynames\"\n    >>> ph.get_records(sql)\n    [[340698]]\n    \"\"\"\n\n    conn_name_attr = \"presto_conn_id\"\n    default_conn_name = \"presto_default\"\n\n    def get_conn(self):\n        \"\"\"Returns a connection object\"\"\"\n        db = self.get_connection(self.presto_conn_id)\n        reqkwargs = None\n        if db.password is not None:\n            reqkwargs = {\"auth\": HTTPBasicAuth(db.login, db.password)}\n        return presto.connect(\n            host=db.host,\n            port=db.port,\n            username=db.login,\n            source=db.extra_dejson.get(\"source\", \"airflow\"),\n            protocol=db.extra_dejson.get(\"protocol\", \"http\"),\n            catalog=db.extra_dejson.get(\"catalog\", \"hive\"),\n            requests_kwargs=reqkwargs,\n            schema=db.schema,\n        )\n\n    @staticmethod\n    def _strip_sql(sql):\n        return sql.strip().rstrip(\";\")\n\n    @staticmethod\n    def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_first(self, hql, parameters=None):\n        \"\"\"\n        Returns only the first row, regardless of how many rows the query\n        returns.\n        \"\"\"\n        try:\n            return super().get_first(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df\n\n    def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)\n\n    # TODO Enable commit_every once PyHive supports transaction.\n    # Unfortunately, PyHive 0.5.1 doesn't support transaction for now,\n    # whereas Presto 0.132+ does.\n    def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import str", "from pyhive import presto", "from pyhive.exc import DatabaseError", "from requests.auth import HTTPBasicAuth", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PrestoException(Exception):\n", "class PrestoHook(DbApiHook):\n", "    def get_conn(self):\n", "    def _strip_sql(sql):\n", "    def get_records(self, hql, parameters=None):\n", "    def get_first(self, hql, parameters=None):\n", "    def get_pandas_df(self, hql, parameters=None):\n", "    def run(self, hql, parameters=None):\n", "    def insert_rows(self, table, rows, target_fields=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/presto_hook.py", "func_name": "PrestoHook.get_records", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Get a set of records from Presto", "docstring_tokens": ["Get", "a", "set", "of", "records", "from", "Presto"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L80-L88", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nfrom pyhive import presto\nfrom pyhive.exc import DatabaseError\nfrom requests.auth import HTTPBasicAuth\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PrestoException(Exception):\n    pass\n\n\nclass PrestoHook(DbApiHook):\n    \"\"\"\n    Interact with Presto through PyHive!\n\n    >>> ph = PrestoHook()\n    >>> sql = \"SELECT count(1) AS num FROM airflow.static_babynames\"\n    >>> ph.get_records(sql)\n    [[340698]]\n    \"\"\"\n\n    conn_name_attr = \"presto_conn_id\"\n    default_conn_name = \"presto_default\"\n\n    def get_conn(self):\n        \"\"\"Returns a connection object\"\"\"\n        db = self.get_connection(self.presto_conn_id)\n        reqkwargs = None\n        if db.password is not None:\n            reqkwargs = {\"auth\": HTTPBasicAuth(db.login, db.password)}\n        return presto.connect(\n            host=db.host,\n            port=db.port,\n            username=db.login,\n            source=db.extra_dejson.get(\"source\", \"airflow\"),\n            protocol=db.extra_dejson.get(\"protocol\", \"http\"),\n            catalog=db.extra_dejson.get(\"catalog\", \"hive\"),\n            requests_kwargs=reqkwargs,\n            schema=db.schema,\n        )\n\n    @staticmethod\n    def _strip_sql(sql):\n        return sql.strip().rstrip(\";\")\n\n    @staticmethod\n    def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (\n            hasattr(e, \"message\")\n            and \"errorName\" in e.message\n            and \"message\" in e.message\n        ):\n            return \"{name}: {message}\".format(\n                name=e.message[\"errorName\"], message=e.message[\"message\"]\n            )\n        else:\n            return str(e)\n\n    def get_first(self, hql, parameters=None):\n        \"\"\"\n        Returns only the first row, regardless of how many rows the query\n        returns.\n        \"\"\"\n        try:\n            return super().get_first(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df\n\n    def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)\n\n    # TODO Enable commit_every once PyHive supports transaction.\n    # Unfortunately, PyHive 0.5.1 doesn't support transaction for now,\n    # whereas Presto 0.132+ does.\n    def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import str", "from pyhive import presto", "from pyhive.exc import DatabaseError", "from requests.auth import HTTPBasicAuth", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PrestoException(Exception):\n", "class PrestoHook(DbApiHook):\n", "    def get_conn(self):\n", "    def _strip_sql(sql):\n", "    def _get_pretty_exception_message(e):\n", "    def get_first(self, hql, parameters=None):\n", "    def get_pandas_df(self, hql, parameters=None):\n", "    def run(self, hql, parameters=None):\n", "    def insert_rows(self, table, rows, target_fields=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/presto_hook.py", "func_name": "PrestoHook.get_pandas_df", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Get a pandas dataframe from a sql query.", "docstring_tokens": ["Get", "a", "pandas", "dataframe", "from", "a", "sql", "query", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L101-L118", "partition": "test", "up_fun_num": 7, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nfrom pyhive import presto\nfrom pyhive.exc import DatabaseError\nfrom requests.auth import HTTPBasicAuth\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PrestoException(Exception):\n    pass\n\n\nclass PrestoHook(DbApiHook):\n    \"\"\"\n    Interact with Presto through PyHive!\n\n    >>> ph = PrestoHook()\n    >>> sql = \"SELECT count(1) AS num FROM airflow.static_babynames\"\n    >>> ph.get_records(sql)\n    [[340698]]\n    \"\"\"\n\n    conn_name_attr = \"presto_conn_id\"\n    default_conn_name = \"presto_default\"\n\n    def get_conn(self):\n        \"\"\"Returns a connection object\"\"\"\n        db = self.get_connection(self.presto_conn_id)\n        reqkwargs = None\n        if db.password is not None:\n            reqkwargs = {\"auth\": HTTPBasicAuth(db.login, db.password)}\n        return presto.connect(\n            host=db.host,\n            port=db.port,\n            username=db.login,\n            source=db.extra_dejson.get(\"source\", \"airflow\"),\n            protocol=db.extra_dejson.get(\"protocol\", \"http\"),\n            catalog=db.extra_dejson.get(\"catalog\", \"hive\"),\n            requests_kwargs=reqkwargs,\n            schema=db.schema,\n        )\n\n    @staticmethod\n    def _strip_sql(sql):\n        return sql.strip().rstrip(\";\")\n\n    @staticmethod\n    def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (\n            hasattr(e, \"message\")\n            and \"errorName\" in e.message\n            and \"message\" in e.message\n        ):\n            return \"{name}: {message}\".format(\n                name=e.message[\"errorName\"], message=e.message[\"message\"]\n            )\n        else:\n            return str(e)\n\n    def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_first(self, hql, parameters=None):\n        \"\"\"\n        Returns only the first row, regardless of how many rows the query\n        returns.\n        \"\"\"\n        try:\n            return super().get_first(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)\n\n    # TODO Enable commit_every once PyHive supports transaction.\n    # Unfortunately, PyHive 0.5.1 doesn't support transaction for now,\n    # whereas Presto 0.132+ does.\n    def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import str", "from pyhive import presto", "from pyhive.exc import DatabaseError", "from requests.auth import HTTPBasicAuth", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PrestoException(Exception):\n", "class PrestoHook(DbApiHook):\n", "    def get_conn(self):\n", "    def _strip_sql(sql):\n", "    def _get_pretty_exception_message(e):\n", "    def get_records(self, hql, parameters=None):\n", "    def get_first(self, hql, parameters=None):\n", "    def run(self, hql, parameters=None):\n", "    def insert_rows(self, table, rows, target_fields=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/presto_hook.py", "func_name": "PrestoHook.run", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Execute the statement against Presto. Can be used to create views.", "docstring_tokens": ["Execute", "the", "statement", "against", "Presto", ".", "Can", "be", "used", "to", "create", "views", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L120-L124", "partition": "test", "up_fun_num": 8, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nfrom pyhive import presto\nfrom pyhive.exc import DatabaseError\nfrom requests.auth import HTTPBasicAuth\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PrestoException(Exception):\n    pass\n\n\nclass PrestoHook(DbApiHook):\n    \"\"\"\n    Interact with Presto through PyHive!\n\n    >>> ph = PrestoHook()\n    >>> sql = \"SELECT count(1) AS num FROM airflow.static_babynames\"\n    >>> ph.get_records(sql)\n    [[340698]]\n    \"\"\"\n\n    conn_name_attr = \"presto_conn_id\"\n    default_conn_name = \"presto_default\"\n\n    def get_conn(self):\n        \"\"\"Returns a connection object\"\"\"\n        db = self.get_connection(self.presto_conn_id)\n        reqkwargs = None\n        if db.password is not None:\n            reqkwargs = {\"auth\": HTTPBasicAuth(db.login, db.password)}\n        return presto.connect(\n            host=db.host,\n            port=db.port,\n            username=db.login,\n            source=db.extra_dejson.get(\"source\", \"airflow\"),\n            protocol=db.extra_dejson.get(\"protocol\", \"http\"),\n            catalog=db.extra_dejson.get(\"catalog\", \"hive\"),\n            requests_kwargs=reqkwargs,\n            schema=db.schema,\n        )\n\n    @staticmethod\n    def _strip_sql(sql):\n        return sql.strip().rstrip(\";\")\n\n    @staticmethod\n    def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (\n            hasattr(e, \"message\")\n            and \"errorName\" in e.message\n            and \"message\" in e.message\n        ):\n            return \"{name}: {message}\".format(\n                name=e.message[\"errorName\"], message=e.message[\"message\"]\n            )\n        else:\n            return str(e)\n\n    def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_first(self, hql, parameters=None):\n        \"\"\"\n        Returns only the first row, regardless of how many rows the query\n        returns.\n        \"\"\"\n        try:\n            return super().get_first(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df\n\n    # TODO Enable commit_every once PyHive supports transaction.\n    # Unfortunately, PyHive 0.5.1 doesn't support transaction for now,\n    # whereas Presto 0.132+ does.\n    def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import str", "from pyhive import presto", "from pyhive.exc import DatabaseError", "from requests.auth import HTTPBasicAuth", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PrestoException(Exception):\n", "class PrestoHook(DbApiHook):\n", "    def get_conn(self):\n", "    def _strip_sql(sql):\n", "    def _get_pretty_exception_message(e):\n", "    def get_records(self, hql, parameters=None):\n", "    def get_first(self, hql, parameters=None):\n", "    def get_pandas_df(self, hql, parameters=None):\n", "    def insert_rows(self, table, rows, target_fields=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/hooks/presto_hook.py", "func_name": "PrestoHook.insert_rows", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings", "docstring_tokens": ["A", "generic", "way", "to", "insert", "a", "set", "of", "tuples", "into", "a", "table", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L129-L140", "partition": "test", "up_fun_num": 9, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom builtins import str\n\nfrom pyhive import presto\nfrom pyhive.exc import DatabaseError\nfrom requests.auth import HTTPBasicAuth\n\nfrom airflow.hooks.dbapi_hook import DbApiHook\n\n\nclass PrestoException(Exception):\n    pass\n\n\nclass PrestoHook(DbApiHook):\n    \"\"\"\n    Interact with Presto through PyHive!\n\n    >>> ph = PrestoHook()\n    >>> sql = \"SELECT count(1) AS num FROM airflow.static_babynames\"\n    >>> ph.get_records(sql)\n    [[340698]]\n    \"\"\"\n\n    conn_name_attr = \"presto_conn_id\"\n    default_conn_name = \"presto_default\"\n\n    def get_conn(self):\n        \"\"\"Returns a connection object\"\"\"\n        db = self.get_connection(self.presto_conn_id)\n        reqkwargs = None\n        if db.password is not None:\n            reqkwargs = {\"auth\": HTTPBasicAuth(db.login, db.password)}\n        return presto.connect(\n            host=db.host,\n            port=db.port,\n            username=db.login,\n            source=db.extra_dejson.get(\"source\", \"airflow\"),\n            protocol=db.extra_dejson.get(\"protocol\", \"http\"),\n            catalog=db.extra_dejson.get(\"catalog\", \"hive\"),\n            requests_kwargs=reqkwargs,\n            schema=db.schema,\n        )\n\n    @staticmethod\n    def _strip_sql(sql):\n        return sql.strip().rstrip(\";\")\n\n    @staticmethod\n    def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (\n            hasattr(e, \"message\")\n            and \"errorName\" in e.message\n            and \"message\" in e.message\n        ):\n            return \"{name}: {message}\".format(\n                name=e.message[\"errorName\"], message=e.message[\"message\"]\n            )\n        else:\n            return str(e)\n\n    def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_first(self, hql, parameters=None):\n        \"\"\"\n        Returns only the first row, regardless of how many rows the query\n        returns.\n        \"\"\"\n        try:\n            return super().get_first(self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n\n    def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df\n\n    def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)\n\n    # TODO Enable commit_every once PyHive supports transaction.\n    # Unfortunately, PyHive 0.5.1 doesn't support transaction for now,\n    # whereas Presto 0.132+ does.\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1], "package": ["from builtins import str", "from pyhive import presto", "from pyhive.exc import DatabaseError", "from requests.auth import HTTPBasicAuth", "from airflow.hooks.dbapi_hook import DbApiHook"], "function": ["class PrestoException(Exception):\n", "class PrestoHook(DbApiHook):\n", "    def get_conn(self):\n", "    def _strip_sql(sql):\n", "    def _get_pretty_exception_message(e):\n", "    def get_records(self, hql, parameters=None):\n", "    def get_first(self, hql, parameters=None):\n", "    def get_pandas_df(self, hql, parameters=None):\n", "    def run(self, hql, parameters=None):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.get_conn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Return a cosmos db client.", "docstring_tokens": ["Return", "a", "cosmos", "db", "client", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L50-L60", "partition": "test", "up_fun_num": 2, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.does_collection_exist", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks if a collection exists in CosmosDB.", "docstring_tokens": ["Checks", "if", "a", "collection", "exists", "in", "CosmosDB", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L82-L99", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.create_collection", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a new collection in the CosmosDB database.", "docstring_tokens": ["Creates", "a", "new", "collection", "in", "the", "CosmosDB", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L101-L122", "partition": "test", "up_fun_num": 6, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.does_database_exist", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks if a database exists in CosmosDB.", "docstring_tokens": ["Checks", "if", "a", "database", "exists", "in", "CosmosDB", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L124-L140", "partition": "test", "up_fun_num": 7, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.create_database", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a new database in CosmosDB.", "docstring_tokens": ["Creates", "a", "new", "database", "in", "CosmosDB", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L142-L160", "partition": "test", "up_fun_num": 8, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.delete_database", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Deletes an existing database in CosmosDB.", "docstring_tokens": ["Deletes", "an", "existing", "database", "in", "CosmosDB", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L162-L169", "partition": "test", "up_fun_num": 9, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.delete_collection", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Deletes an existing collection in the CosmosDB database.", "docstring_tokens": ["Deletes", "an", "existing", "collection", "in", "the", "CosmosDB", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L171-L179", "partition": "test", "up_fun_num": 10, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.insert_documents", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Insert a list of new documents into an existing collection in the CosmosDB database.", "docstring_tokens": ["Insert", "a", "list", "of", "new", "documents", "into", "an", "existing", "collection", "in", "the", "CosmosDB", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L208-L224", "partition": "test", "up_fun_num": 12, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.delete_document", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Delete an existing document out of a collection in the CosmosDB database.", "docstring_tokens": ["Delete", "an", "existing", "document", "out", "of", "a", "collection", "in", "the", "CosmosDB", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L226-L237", "partition": "test", "up_fun_num": 13, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.get_document", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Get a document from an existing collection in the CosmosDB database.", "docstring_tokens": ["Get", "a", "document", "from", "an", "existing", "collection", "in", "the", "CosmosDB", "database", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L239-L253", "partition": "test", "up_fun_num": 14, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_documents(\n        self, sql_string, database_name=None, collection_name=None, partition_key=None\n    ):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {\"query\": sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                ),\n                query,\n                partition_key,\n            )\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.get_documents", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Get a list of documents from an existing collection in the CosmosDB database via SQL query.", "docstring_tokens": ["Get", "a", "list", "of", "documents", "from", "an", "existing", "collection", "in", "the", "CosmosDB", "database", "via", "SQL", "query", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275", "partition": "test", "up_fun_num": 15, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport azure.cosmos.cosmos_client as cosmos_client\nfrom azure.cosmos.errors import HTTPFailure\nimport uuid\n\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.hooks.base_hook import BaseHook\n\n\nclass AzureCosmosDBHook(BaseHook):\n    \"\"\"\n    Interacts with Azure CosmosDB.\n\n    login should be the endpoint uri, password should be the master key\n    optionally, you can use the following extras to default these values\n    {\"database_name\": \"<DATABASE_NAME>\", \"collection_name\": \"COLLECTION_NAME\"}.\n\n    :param azure_cosmos_conn_id: Reference to the Azure CosmosDB connection.\n    :type azure_cosmos_conn_id: str\n    \"\"\"\n\n    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n        self.conn_id = azure_cosmos_conn_id\n        self.connection = self.get_connection(self.conn_id)\n        self.extras = self.connection.extra_dejson\n\n        self.endpoint_uri = self.connection.login\n        self.master_key = self.connection.password\n        self.default_database_name = self.extras.get(\"database_name\")\n        self.default_collection_name = self.extras.get(\"collection_name\")\n        self.cosmos_client = None\n\n    def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(\n            self.endpoint_uri, {\"masterKey\": self.master_key}\n        )\n\n        return self.cosmos_client\n\n    def __get_database_name(self, database_name=None):\n        db_name = database_name\n        if db_name is None:\n            db_name = self.default_database_name\n\n        if db_name is None:\n            raise AirflowBadRequest(\"Database name must be specified\")\n\n        return db_name\n\n    def __get_collection_name(self, collection_name=None):\n        coll_name = collection_name\n        if coll_name is None:\n            coll_name = self.default_collection_name\n\n        if coll_name is None:\n            raise AirflowBadRequest(\"Collection name must be specified\")\n\n        return coll_name\n\n    def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n        if len(existing_container) == 0:\n            return False\n\n        return True\n\n    def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(\n            self.get_conn().QueryContainers(\n                get_database_link(self.__get_database_name(database_name)),\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": collection_name}],\n                },\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name},\n            )\n\n    def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n        if len(existing_database) == 0:\n            return False\n\n        return True\n\n    def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(\n            self.get_conn().QueryDatabases(\n                {\n                    \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                    \"parameters\": [{\"name\": \"@id\", \"value\": database_name}],\n                }\n            )\n        )\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})\n\n    def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))\n\n    def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(\n                self.__get_database_name(database_name), collection_name\n            )\n        )\n\n    def upsert_document(\n        self, document, database_name=None, collection_name=None, document_id=None\n    ):\n        \"\"\"\n        Inserts a new document (or updates an existing one) into an existing\n        collection in the CosmosDB database.\n        \"\"\"\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if \"id\" in document:\n            if document[\"id\"] is None:\n                document[\"id\"] = document_id\n        else:\n            document[\"id\"] = document_id\n\n        created_document = self.get_conn().CreateItem(\n            get_collection_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n            ),\n            document,\n        )\n\n        return created_document\n\n    def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name),\n                    ),\n                    single_document,\n                )\n            )\n\n        return created_documents\n\n    def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id,\n            )\n        )\n\n    def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id,\n                )\n            )\n        except HTTPFailure:\n            return None\n\n\ndef get_database_link(database_id):\n    return \"dbs/\" + database_id\n\n\ndef get_collection_link(database_id, collection_id):\n    return get_database_link(database_id) + \"/colls/\" + collection_id\n\n\ndef get_document_link(database_id, collection_id, document_id):\n    return get_collection_link(database_id, collection_id) + \"/docs/\" + document_id\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], "package": ["import azure.cosmos.cosmos_client as cosmos_client", "from azure.cosmos.errors import HTTPFailure", "import uuid", "from airflow.exceptions import AirflowBadRequest", "from airflow.hooks.base_hook import BaseHook"], "function": ["class AzureCosmosDBHook(BaseHook):\n", "    def __init__(self, azure_cosmos_conn_id=\"azure_cosmos_default\"):\n", "    def get_conn(self):\n", "    def __get_database_name(self, database_name=None):\n", "    def __get_collection_name(self, collection_name=None):\n", "    def does_collection_exist(self, collection_name, database_name=None):\n", "    def create_collection(self, collection_name, database_name=None):\n", "    def does_database_exist(self, database_name):\n", "    def create_database(self, database_name):\n", "    def delete_database(self, database_name):\n", "    def delete_collection(self, collection_name, database_name=None):\n", "    def insert_documents(self, documents, database_name=None, collection_name=None):\n", "    def delete_document(self, document_id, database_name=None, collection_name=None):\n", "    def get_document(self, document_id, database_name=None, collection_name=None):\n", "def get_database_link(database_id):\n", "def get_collection_link(database_id, collection_id):\n", "def get_document_link(database_id, collection_id, document_id):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_function_hook.py", "func_name": "GcfHook.get_function", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict", "docstring_tokens": ["Returns", "the", "Cloud", "Function", "with", "the", "given", "name", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L76-L86", "partition": "test", "up_fun_num": 3, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport time\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\n# noinspection PyAbstractClass\nclass GcfHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for the Google Cloud Functions APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    @staticmethod\n    def _full_location(project_id, location):\n        \"\"\"\n        Retrieve full location of the function in the form of\n        projects/<GCP_PROJECT_ID>/locations/<GCP_LOCATION>\n\n        :param project_id: The Google Cloud Project project_id where the function belongs.\n        :type project_id: str\n        :param location: The location where the function is created.\n        :type location: str\n        :return:\n        \"\"\"\n        return \"projects/{}/locations/{}\".format(project_id, location)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves the connection to Cloud Functions.\n\n        :return: Google Cloud Function services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"cloudfunctions\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .create(location=self._full_location(project_id, location), body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .patch(updateMask=\",\".join(update_mask), name=name, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .generateUploadUrl(parent=self._full_location(project_id, location))\n            .execute(num_retries=self.num_retries)\n        )\n        upload_url = response.get(\"uploadUrl\")\n        with open(zip_path, \"rb\") as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    \"Content-type\": \"application/zip\",\n                    \"x-goog-content-length-range\": \"0,104857600\",\n                },\n            )\n        return upload_url\n\n    def delete_function(self, name):\n        \"\"\"\n        Deletes the specified Cloud Function.\n\n        :param name: The name of the function.\n        :type name: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .delete(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    def _wait_for_operation_to_complete(self, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param operation_name: The name of the operation.\n        :type operation_name: str\n        :return: The response returned by the operation.\n        :rtype: dict\n        :exception: AirflowException in case error is returned.\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    name=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n                # Note, according to documentation always either response or error is\n                # set when \"done\" == True\n                if error:\n                    raise AirflowException(str(error))\n                return response\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook"], "function": ["class GcfHook(GoogleCloudBaseHook):\n", "    def _full_location(project_id, location):\n", "    def get_conn(self):\n", "    def create_new_function(self, location, body, project_id=None):\n", "    def update_function(self, name, body, update_mask):\n", "    def upload_function_zip(self, location, zip_path, project_id=None):\n", "    def delete_function(self, name):\n", "    def _wait_for_operation_to_complete(self, operation_name):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_function_hook.py", "func_name": "GcfHook.create_new_function", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None", "docstring_tokens": ["Creates", "a", "new", "function", "in", "Cloud", "Function", "in", "the", "location", "specified", "in", "the", "body", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L89-L107", "partition": "test", "up_fun_num": 4, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport time\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\n# noinspection PyAbstractClass\nclass GcfHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for the Google Cloud Functions APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    @staticmethod\n    def _full_location(project_id, location):\n        \"\"\"\n        Retrieve full location of the function in the form of\n        projects/<GCP_PROJECT_ID>/locations/<GCP_LOCATION>\n\n        :param project_id: The Google Cloud Project project_id where the function belongs.\n        :type project_id: str\n        :param location: The location where the function is created.\n        :type location: str\n        :return:\n        \"\"\"\n        return \"projects/{}/locations/{}\".format(project_id, location)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves the connection to Cloud Functions.\n\n        :return: Google Cloud Function services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"cloudfunctions\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .get(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .patch(updateMask=\",\".join(update_mask), name=name, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .generateUploadUrl(parent=self._full_location(project_id, location))\n            .execute(num_retries=self.num_retries)\n        )\n        upload_url = response.get(\"uploadUrl\")\n        with open(zip_path, \"rb\") as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    \"Content-type\": \"application/zip\",\n                    \"x-goog-content-length-range\": \"0,104857600\",\n                },\n            )\n        return upload_url\n\n    def delete_function(self, name):\n        \"\"\"\n        Deletes the specified Cloud Function.\n\n        :param name: The name of the function.\n        :type name: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .delete(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    def _wait_for_operation_to_complete(self, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param operation_name: The name of the operation.\n        :type operation_name: str\n        :return: The response returned by the operation.\n        :rtype: dict\n        :exception: AirflowException in case error is returned.\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    name=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n                # Note, according to documentation always either response or error is\n                # set when \"done\" == True\n                if error:\n                    raise AirflowException(str(error))\n                return response\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook"], "function": ["class GcfHook(GoogleCloudBaseHook):\n", "    def _full_location(project_id, location):\n", "    def get_conn(self):\n", "    def get_function(self, name):\n", "    def update_function(self, name, body, update_mask):\n", "    def upload_function_zip(self, location, zip_path, project_id=None):\n", "    def delete_function(self, name):\n", "    def _wait_for_operation_to_complete(self, operation_name):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_function_hook.py", "func_name": "GcfHook.update_function", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None", "docstring_tokens": ["Updates", "Cloud", "Functions", "according", "to", "the", "specified", "update", "mask", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L109-L127", "partition": "test", "up_fun_num": 5, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport time\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\n# noinspection PyAbstractClass\nclass GcfHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for the Google Cloud Functions APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    @staticmethod\n    def _full_location(project_id, location):\n        \"\"\"\n        Retrieve full location of the function in the form of\n        projects/<GCP_PROJECT_ID>/locations/<GCP_LOCATION>\n\n        :param project_id: The Google Cloud Project project_id where the function belongs.\n        :type project_id: str\n        :param location: The location where the function is created.\n        :type location: str\n        :return:\n        \"\"\"\n        return \"projects/{}/locations/{}\".format(project_id, location)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves the connection to Cloud Functions.\n\n        :return: Google Cloud Function services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"cloudfunctions\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .get(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .create(location=self._full_location(project_id, location), body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .generateUploadUrl(parent=self._full_location(project_id, location))\n            .execute(num_retries=self.num_retries)\n        )\n        upload_url = response.get(\"uploadUrl\")\n        with open(zip_path, \"rb\") as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    \"Content-type\": \"application/zip\",\n                    \"x-goog-content-length-range\": \"0,104857600\",\n                },\n            )\n        return upload_url\n\n    def delete_function(self, name):\n        \"\"\"\n        Deletes the specified Cloud Function.\n\n        :param name: The name of the function.\n        :type name: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .delete(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    def _wait_for_operation_to_complete(self, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param operation_name: The name of the operation.\n        :type operation_name: str\n        :return: The response returned by the operation.\n        :rtype: dict\n        :exception: AirflowException in case error is returned.\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    name=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n                # Note, according to documentation always either response or error is\n                # set when \"done\" == True\n                if error:\n                    raise AirflowException(str(error))\n                return response\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook"], "function": ["class GcfHook(GoogleCloudBaseHook):\n", "    def _full_location(project_id, location):\n", "    def get_conn(self):\n", "    def get_function(self, name):\n", "    def create_new_function(self, location, body, project_id=None):\n", "    def upload_function_zip(self, location, zip_path, project_id=None):\n", "    def delete_function(self, name):\n", "    def _wait_for_operation_to_complete(self, operation_name):\n"]}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_function_hook.py", "func_name": "GcfHook.upload_function_zip", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.", "docstring_tokens": ["Uploads", "zip", "file", "with", "sources", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L130-L159", "partition": "test", "up_fun_num": 6, "context": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport time\nimport requests\nfrom googleapiclient.discovery import build\n\nfrom airflow import AirflowException\nfrom airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n\n# Time to sleep between active checks of the operation results\nTIME_TO_SLEEP_IN_SECONDS = 1\n\n\n# noinspection PyAbstractClass\nclass GcfHook(GoogleCloudBaseHook):\n    \"\"\"\n    Hook for the Google Cloud Functions APIs.\n\n    All the methods in the hook where project_id is used must be called with\n    keyword arguments rather than positional.\n    \"\"\"\n\n    _conn = None\n\n    def __init__(\n        self, api_version, gcp_conn_id=\"google_cloud_default\", delegate_to=None\n    ):\n        super().__init__(gcp_conn_id, delegate_to)\n        self.api_version = api_version\n        self.num_retries = self._get_field(\"num_retries\", 5)\n\n    @staticmethod\n    def _full_location(project_id, location):\n        \"\"\"\n        Retrieve full location of the function in the form of\n        projects/<GCP_PROJECT_ID>/locations/<GCP_LOCATION>\n\n        :param project_id: The Google Cloud Project project_id where the function belongs.\n        :type project_id: str\n        :param location: The location where the function is created.\n        :type location: str\n        :return:\n        \"\"\"\n        return \"projects/{}/locations/{}\".format(project_id, location)\n\n    def get_conn(self):\n        \"\"\"\n        Retrieves the connection to Cloud Functions.\n\n        :return: Google Cloud Function services object.\n        :rtype: dict\n        \"\"\"\n        if not self._conn:\n            http_authorized = self._authorize()\n            self._conn = build(\n                \"cloudfunctions\",\n                self.api_version,\n                http=http_authorized,\n                cache_discovery=False,\n            )\n        return self._conn\n\n    def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .get(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .create(location=self._full_location(project_id, location), body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .patch(updateMask=\",\".join(update_mask), name=name, body=body)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    @GoogleCloudBaseHook.fallback_to_default_project_id\n    def delete_function(self, name):\n        \"\"\"\n        Deletes the specified Cloud Function.\n\n        :param name: The name of the function.\n        :type name: str\n        :return: None\n        \"\"\"\n        response = (\n            self.get_conn()\n            .projects()\n            .locations()\n            .functions()\n            .delete(name=name)\n            .execute(num_retries=self.num_retries)\n        )\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)\n\n    def _wait_for_operation_to_complete(self, operation_name):\n        \"\"\"\n        Waits for the named operation to complete - checks status of the\n        asynchronous call.\n\n        :param operation_name: The name of the operation.\n        :type operation_name: str\n        :return: The response returned by the operation.\n        :rtype: dict\n        :exception: AirflowException in case error is returned.\n        \"\"\"\n        service = self.get_conn()\n        while True:\n            operation_response = (\n                service.operations()\n                .get(\n                    name=operation_name,\n                )\n                .execute(num_retries=self.num_retries)\n            )\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n                # Note, according to documentation always either response or error is\n                # set when \"done\" == True\n                if error:\n                    raise AirflowException(str(error))\n                return response\n            time.sleep(TIME_TO_SLEEP_IN_SECONDS)\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1], "package": ["import time", "import requests", "from googleapiclient.discovery import build", "from airflow import AirflowException", "from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook"], "function": ["class GcfHook(GoogleCloudBaseHook):\n", "    def _full_location(project_id, location):\n", "    def get_conn(self):\n", "    def get_function(self, name):\n", "    def create_new_function(self, location, body, project_id=None):\n", "    def update_function(self, name, body, update_mask):\n", "    def delete_function(self, name):\n", "    def _wait_for_operation_to_complete(self, operation_name):\n"]}
