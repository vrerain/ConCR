{"repo": "susam/ice", "path": "ice.py", "func_name": "Response.set_cookie", "original_string": "def set_cookie(self, name, value, attrs={}):\n        \"\"\"Add a Set-Cookie header to response object.\n\n        For a description about cookie attribute values, see\n        https://docs.python.org/3/library/http.cookies.html#http.cookies.Morsel.\n\n        Arguments:\n          name (str): Name of the cookie\n          value (str): Value of the cookie\n          attrs (dict): Dicitionary with cookie attribute keys and\n                        values.\n        \"\"\"\n        cookie = http.cookies.SimpleCookie()\n        cookie[name] = value\n        for key, value in attrs.items():\n            cookie[name][key] = value\n        self.add_header('Set-Cookie', cookie[name].OutputString())", "language": "python", "code": "def set_cookie(self, name, value, attrs={}):\n        \"\"\"Add a Set-Cookie header to response object.\n\n        For a description about cookie attribute values, see\n        https://docs.python.org/3/library/http.cookies.html#http.cookies.Morsel.\n\n        Arguments:\n          name (str): Name of the cookie\n          value (str): Value of the cookie\n          attrs (dict): Dicitionary with cookie attribute keys and\n                        values.\n        \"\"\"\n        cookie = http.cookies.SimpleCookie()\n        cookie[name] = value\n        for key, value in attrs.items():\n            cookie[name][key] = value\n        self.add_header('Set-Cookie', cookie[name].OutputString())", "code_tokens": ["def", "set_cookie", "(", "self", ",", "name", ",", "value", ",", "attrs", "=", "{", "}", ")", ":", "cookie", "=", "http", ".", "cookies", ".", "SimpleCookie", "(", ")", "cookie", "[", "name", "]", "=", "value", "for", "key", ",", "value", "in", "attrs", ".", "items", "(", ")", ":", "cookie", "[", "name", "]", "[", "key", "]", "=", "value", "self", ".", "add_header", "(", "'Set-Cookie'", ",", "cookie", "[", "name", "]", ".", "OutputString", "(", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "532e685c504ea96f9e42833594585159ac1d2068", "url": "https://github.com/susam/ice/blob/532e685c504ea96f9e42833594585159ac1d2068/ice.py#L794-L810", "partition": "test"}
{"repo": "CalebBell/thermo", "path": "thermo/refractivity.py", "func_name": "refractive_index", "original_string": "def refractive_index(CASRN, T=None, AvailableMethods=False, Method=None,\n                     full_info=True):\n    r'''This function handles the retrieval of a chemical's refractive\n    index. Lookup is based on CASRNs. Will automatically select a data source\n    to use if no Method is provided; returns None if the data is not available.\n\n    Function has data for approximately 4500 chemicals.\n\n    Parameters\n    ----------\n    CASRN : string\n        CASRN [-]\n\n    Returns\n    -------\n    RI : float\n        Refractive Index on the Na D line, [-]\n    T : float, only returned if full_info == True\n        Temperature at which refractive index reading was made\n    methods : list, only returned if AvailableMethods == True\n        List of methods which can be used to obtain RI with the given inputs\n\n    Other Parameters\n    ----------------\n    Method : string, optional\n        A string for the method name to use, as defined by constants in\n        RI_methods\n    AvailableMethods : bool, optional\n        If True, function will determine which methods can be used to obtain\n        RI for the desired chemical, and will return methods instead of RI\n    full_info : bool, optional\n        If True, function will return the temperature at which the refractive\n        index reading was made\n\n    Notes\n    -----\n    Only one source is available in this function. It is:\n\n        * 'CRC', a compillation of Organic RI data in [1]_.\n\n    Examples\n    --------\n    >>> refractive_index(CASRN='64-17-5')\n    (1.3611, 293.15)\n\n    References\n    ----------\n    .. [1] Haynes, W.M., Thomas J. Bruno, and David R. Lide. CRC Handbook of\n       Chemistry and Physics, 95E. Boca Raton, FL: CRC press, 2014.\n    '''\n    def list_methods():\n        methods = []\n        if CASRN in CRC_RI_organic.index:\n            methods.append(CRC)\n        methods.append(NONE)\n        return methods\n    if AvailableMethods:\n        return list_methods()\n    if not Method:\n        Method = list_methods()[0]\n\n    if Method == CRC:\n        _RI = float(CRC_RI_organic.at[CASRN, 'RI'])\n        if full_info:\n            _T = float(CRC_RI_organic.at[CASRN, 'RIT'])\n    elif Method == NONE:\n        _RI, _T = None, None\n    else:\n        raise Exception('Failure in in function')\n    if full_info:\n        return _RI, _T\n    else:\n        return _RI", "language": "python", "code": "def refractive_index(CASRN, T=None, AvailableMethods=False, Method=None,\n                     full_info=True):\n    r'''This function handles the retrieval of a chemical's refractive\n    index. Lookup is based on CASRNs. Will automatically select a data source\n    to use if no Method is provided; returns None if the data is not available.\n\n    Function has data for approximately 4500 chemicals.\n\n    Parameters\n    ----------\n    CASRN : string\n        CASRN [-]\n\n    Returns\n    -------\n    RI : float\n        Refractive Index on the Na D line, [-]\n    T : float, only returned if full_info == True\n        Temperature at which refractive index reading was made\n    methods : list, only returned if AvailableMethods == True\n        List of methods which can be used to obtain RI with the given inputs\n\n    Other Parameters\n    ----------------\n    Method : string, optional\n        A string for the method name to use, as defined by constants in\n        RI_methods\n    AvailableMethods : bool, optional\n        If True, function will determine which methods can be used to obtain\n        RI for the desired chemical, and will return methods instead of RI\n    full_info : bool, optional\n        If True, function will return the temperature at which the refractive\n        index reading was made\n\n    Notes\n    -----\n    Only one source is available in this function. It is:\n\n        * 'CRC', a compillation of Organic RI data in [1]_.\n\n    Examples\n    --------\n    >>> refractive_index(CASRN='64-17-5')\n    (1.3611, 293.15)\n\n    References\n    ----------\n    .. [1] Haynes, W.M., Thomas J. Bruno, and David R. Lide. CRC Handbook of\n       Chemistry and Physics, 95E. Boca Raton, FL: CRC press, 2014.\n    '''\n    def list_methods():\n        methods = []\n        if CASRN in CRC_RI_organic.index:\n            methods.append(CRC)\n        methods.append(NONE)\n        return methods\n    if AvailableMethods:\n        return list_methods()\n    if not Method:\n        Method = list_methods()[0]\n\n    if Method == CRC:\n        _RI = float(CRC_RI_organic.at[CASRN, 'RI'])\n        if full_info:\n            _T = float(CRC_RI_organic.at[CASRN, 'RIT'])\n    elif Method == NONE:\n        _RI, _T = None, None\n    else:\n        raise Exception('Failure in in function')\n    if full_info:\n        return _RI, _T\n    else:\n        return _RI", "code_tokens": ["def", "refractive_index", "(", "CASRN", ",", "T", "=", "None", ",", "AvailableMethods", "=", "False", ",", "Method", "=", "None", ",", "full_info", "=", "True", ")", ":", "def", "list_methods", "(", ")", ":", "methods", "=", "[", "]", "if", "CASRN", "in", "CRC_RI_organic", ".", "index", ":", "methods", ".", "append", "(", "CRC", ")", "methods", ".", "append", "(", "NONE", ")", "return", "methods", "if", "AvailableMethods", ":", "return", "list_methods", "(", ")", "if", "not", "Method", ":", "Method", "=", "list_methods", "(", ")", "[", "0", "]", "if", "Method", "==", "CRC", ":", "_RI", "=", "float", "(", "CRC_RI_organic", ".", "at", "[", "CASRN", ",", "'RI'", "]", ")", "if", "full_info", ":", "_T", "=", "float", "(", "CRC_RI_organic", ".", "at", "[", "CASRN", ",", "'RIT'", "]", ")", "elif", "Method", "==", "NONE", ":", "_RI", ",", "_T", "=", "None", ",", "None", "else", ":", "raise", "Exception", "(", "'Failure in in function'", ")", "if", "full_info", ":", "return", "_RI", ",", "_T", "else", ":", "return", "_RI"], "docstring": "", "docstring_tokens": [], "sha": "3857ed023a3e64fd3039a32d53576c24990ef1c3", "url": "https://github.com/CalebBell/thermo/blob/3857ed023a3e64fd3039a32d53576c24990ef1c3/thermo/refractivity.py#L44-L116", "partition": "valid"}
{"repo": "fumitoh/modelx", "path": "modelx/core/space.py", "func_name": "BaseSpaceImpl.get_dynspace", "original_string": "def get_dynspace(self, args, kwargs=None):\n        \"\"\"Create a dynamic root space\n\n        Called from interface methods\n        \"\"\"\n\n        node = get_node(self, *convert_args(args, kwargs))\n        key = node[KEY]\n\n        if key in self.param_spaces:\n            return self.param_spaces[key]\n\n        else:\n            last_self = self.system.self\n            self.system.self = self\n\n            try:\n                space_args = self.eval_formula(node)\n\n            finally:\n                self.system.self = last_self\n\n            if space_args is None:\n                space_args = {\"bases\": [self]}  # Default\n            else:\n                if \"bases\" in space_args:\n                    bases = get_impls(space_args[\"bases\"])\n                    if isinstance(bases, StaticSpaceImpl):\n                        space_args[\"bases\"] = [bases]\n                    elif bases is None:\n                        space_args[\"bases\"] = [self]  # Default\n                    else:\n                        space_args[\"bases\"] = bases\n                else:\n                    space_args[\"bases\"] = [self]\n\n            space_args[\"arguments\"] = node_get_args(node)\n            space = self._new_dynspace(**space_args)\n            self.param_spaces[key] = space\n            space.inherit(clear_value=False)\n            return space", "language": "python", "code": "def get_dynspace(self, args, kwargs=None):\n        \"\"\"Create a dynamic root space\n\n        Called from interface methods\n        \"\"\"\n\n        node = get_node(self, *convert_args(args, kwargs))\n        key = node[KEY]\n\n        if key in self.param_spaces:\n            return self.param_spaces[key]\n\n        else:\n            last_self = self.system.self\n            self.system.self = self\n\n            try:\n                space_args = self.eval_formula(node)\n\n            finally:\n                self.system.self = last_self\n\n            if space_args is None:\n                space_args = {\"bases\": [self]}  # Default\n            else:\n                if \"bases\" in space_args:\n                    bases = get_impls(space_args[\"bases\"])\n                    if isinstance(bases, StaticSpaceImpl):\n                        space_args[\"bases\"] = [bases]\n                    elif bases is None:\n                        space_args[\"bases\"] = [self]  # Default\n                    else:\n                        space_args[\"bases\"] = bases\n                else:\n                    space_args[\"bases\"] = [self]\n\n            space_args[\"arguments\"] = node_get_args(node)\n            space = self._new_dynspace(**space_args)\n            self.param_spaces[key] = space\n            space.inherit(clear_value=False)\n            return space", "code_tokens": ["def", "get_dynspace", "(", "self", ",", "args", ",", "kwargs", "=", "None", ")", ":", "node", "=", "get_node", "(", "self", ",", "*", "convert_args", "(", "args", ",", "kwargs", ")", ")", "key", "=", "node", "[", "KEY", "]", "if", "key", "in", "self", ".", "param_spaces", ":", "return", "self", ".", "param_spaces", "[", "key", "]", "else", ":", "last_self", "=", "self", ".", "system", ".", "self", "self", ".", "system", ".", "self", "=", "self", "try", ":", "space_args", "=", "self", ".", "eval_formula", "(", "node", ")", "finally", ":", "self", ".", "system", ".", "self", "=", "last_self", "if", "space_args", "is", "None", ":", "space_args", "=", "{", "\"bases\"", ":", "[", "self", "]", "}", "# Default", "else", ":", "if", "\"bases\"", "in", "space_args", ":", "bases", "=", "get_impls", "(", "space_args", "[", "\"bases\"", "]", ")", "if", "isinstance", "(", "bases", ",", "StaticSpaceImpl", ")", ":", "space_args", "[", "\"bases\"", "]", "=", "[", "bases", "]", "elif", "bases", "is", "None", ":", "space_args", "[", "\"bases\"", "]", "=", "[", "self", "]", "# Default", "else", ":", "space_args", "[", "\"bases\"", "]", "=", "bases", "else", ":", "space_args", "[", "\"bases\"", "]", "=", "[", "self", "]", "space_args", "[", "\"arguments\"", "]", "=", "node_get_args", "(", "node", ")", "space", "=", "self", ".", "_new_dynspace", "(", "*", "*", "space_args", ")", "self", ".", "param_spaces", "[", "key", "]", "=", "space", "space", ".", "inherit", "(", "clear_value", "=", "False", ")", "return", "space"], "docstring": "", "docstring_tokens": [], "sha": "0180da34d052c44fb94dab9e115e218bbebfc9c3", "url": "https://github.com/fumitoh/modelx/blob/0180da34d052c44fb94dab9e115e218bbebfc9c3/modelx/core/space.py#L1004-L1044", "partition": "valid"}
{"repo": "abau171/highfive", "path": "highfive/jobs.py", "func_name": "JobSet._load_job", "original_string": "def _load_job(self):\n        \"\"\"\n        If there is still a job in the job iterator, loads it and increments\n        the active job count.\n        \"\"\"\n\n        try:\n            next_job = next(self._jobs)\n        except StopIteration:\n            self._on_deck = None\n        else:\n            if not isinstance(next_job, Job):\n                next_job = DefaultJob(next_job)\n            self._on_deck = next_job\n            self._active_jobs += 1", "language": "python", "code": "def _load_job(self):\n        \"\"\"\n        If there is still a job in the job iterator, loads it and increments\n        the active job count.\n        \"\"\"\n\n        try:\n            next_job = next(self._jobs)\n        except StopIteration:\n            self._on_deck = None\n        else:\n            if not isinstance(next_job, Job):\n                next_job = DefaultJob(next_job)\n            self._on_deck = next_job\n            self._active_jobs += 1", "code_tokens": ["def", "_load_job", "(", "self", ")", ":", "try", ":", "next_job", "=", "next", "(", "self", ".", "_jobs", ")", "except", "StopIteration", ":", "self", ".", "_on_deck", "=", "None", "else", ":", "if", "not", "isinstance", "(", "next_job", ",", "Job", ")", ":", "next_job", "=", "DefaultJob", "(", "next_job", ")", "self", ".", "_on_deck", "=", "next_job", "self", ".", "_active_jobs", "+=", "1"], "docstring": "", "docstring_tokens": [], "sha": "07b3829331072035ab100d1d66deca3e8f3f372a", "url": "https://github.com/abau171/highfive/blob/07b3829331072035ab100d1d66deca3e8f3f372a/highfive/jobs.py#L235-L249", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink_cli/main.py", "func_name": "setup_args", "original_string": "def setup_args(parser, config_files=[], ignore_unknown=False):\n    \"\"\"Parses arguments.\"\"\"\n    global args\n    arglist = sys.argv[1:]\n\n    # Load arguments from config files\n    for config_file in filter(os.path.isfile, config_files):\n        arglist.insert(0, \"@\" + config_file)\n\n    args, unknown = parser.parse_known_args(arglist)\n    if unknown and not ignore_unknown:\n        msg = gettext('unrecognized arguments: %s')\n        parser.error(msg % ' '.join(unknown))\n\n    # Force lowercase to allow case-insensitive lookup\n    if args.stream:\n        args.stream = [stream.lower() for stream in args.stream]\n\n    if not args.url and args.url_param:\n        args.url = args.url_param", "language": "python", "code": "def setup_args(parser, config_files=[], ignore_unknown=False):\n    \"\"\"Parses arguments.\"\"\"\n    global args\n    arglist = sys.argv[1:]\n\n    # Load arguments from config files\n    for config_file in filter(os.path.isfile, config_files):\n        arglist.insert(0, \"@\" + config_file)\n\n    args, unknown = parser.parse_known_args(arglist)\n    if unknown and not ignore_unknown:\n        msg = gettext('unrecognized arguments: %s')\n        parser.error(msg % ' '.join(unknown))\n\n    # Force lowercase to allow case-insensitive lookup\n    if args.stream:\n        args.stream = [stream.lower() for stream in args.stream]\n\n    if not args.url and args.url_param:\n        args.url = args.url_param", "code_tokens": ["def", "setup_args", "(", "parser", ",", "config_files", "=", "[", "]", ",", "ignore_unknown", "=", "False", ")", ":", "global", "args", "arglist", "=", "sys", ".", "argv", "[", "1", ":", "]", "# Load arguments from config files", "for", "config_file", "in", "filter", "(", "os", ".", "path", ".", "isfile", ",", "config_files", ")", ":", "arglist", ".", "insert", "(", "0", ",", "\"@\"", "+", "config_file", ")", "args", ",", "unknown", "=", "parser", ".", "parse_known_args", "(", "arglist", ")", "if", "unknown", "and", "not", "ignore_unknown", ":", "msg", "=", "gettext", "(", "'unrecognized arguments: %s'", ")", "parser", ".", "error", "(", "msg", "%", "' '", ".", "join", "(", "unknown", ")", ")", "# Force lowercase to allow case-insensitive lookup", "if", "args", ".", "stream", ":", "args", ".", "stream", "=", "[", "stream", ".", "lower", "(", ")", "for", "stream", "in", "args", ".", "stream", "]", "if", "not", "args", ".", "url", "and", "args", ".", "url_param", ":", "args", ".", "url", "=", "args", ".", "url_param"], "docstring": "", "docstring_tokens": [], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/main.py#L663-L682", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/parallel/controller/dictdb.py", "func_name": "DictDB.drop_matching_records", "original_string": "def drop_matching_records(self, check):\n        \"\"\"Remove a record from the DB.\"\"\"\n        matches = self._match(check)\n        for m in matches:\n            del self._records[m['msg_id']]", "language": "python", "code": "def drop_matching_records(self, check):\n        \"\"\"Remove a record from the DB.\"\"\"\n        matches = self._match(check)\n        for m in matches:\n            del self._records[m['msg_id']]", "code_tokens": ["def", "drop_matching_records", "(", "self", ",", "check", ")", ":", "matches", "=", "self", ".", "_match", "(", "check", ")", "for", "m", "in", "matches", ":", "del", "self", ".", "_records", "[", "m", "[", "'msg_id'", "]", "]"], "docstring": "", "docstring_tokens": [], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/parallel/controller/dictdb.py#L150-L154", "partition": "test"}
{"repo": "briandilley/ebs-deploy", "path": "ebs_deploy/__init__.py", "func_name": "EbsHelper.get_environments", "original_string": "def get_environments(self):\n        \"\"\"\n        Returns the environments\n        \"\"\"\n        response = self.ebs.describe_environments(application_name=self.app_name, include_deleted=False)\n        return response['DescribeEnvironmentsResponse']['DescribeEnvironmentsResult']['Environments']", "language": "python", "code": "def get_environments(self):\n        \"\"\"\n        Returns the environments\n        \"\"\"\n        response = self.ebs.describe_environments(application_name=self.app_name, include_deleted=False)\n        return response['DescribeEnvironmentsResponse']['DescribeEnvironmentsResult']['Environments']", "code_tokens": ["def", "get_environments", "(", "self", ")", ":", "response", "=", "self", ".", "ebs", ".", "describe_environments", "(", "application_name", "=", "self", ".", "app_name", ",", "include_deleted", "=", "False", ")", "return", "response", "[", "'DescribeEnvironmentsResponse'", "]", "[", "'DescribeEnvironmentsResult'", "]", "[", "'Environments'", "]"], "docstring": "", "docstring_tokens": [], "sha": "4178c9c1282a9025fb987dab3470bea28c202e10", "url": "https://github.com/briandilley/ebs-deploy/blob/4178c9c1282a9025fb987dab3470bea28c202e10/ebs_deploy/__init__.py#L343-L348", "partition": "valid"}
{"repo": "openstax/cnx-publishing", "path": "cnxpublishing/db.py", "func_name": "_dissect_roles", "original_string": "def _dissect_roles(metadata):\n    \"\"\"Given a model's ``metadata``, iterate over the roles.\n    Return values are the role identifier and role type as a tuple.\n    \"\"\"\n    for role_key in cnxepub.ATTRIBUTED_ROLE_KEYS:\n        for user in metadata.get(role_key, []):\n            if user['type'] != 'cnx-id':\n                raise ValueError(\"Archive only accepts Connexions users.\")\n            uid = parse_user_uri(user['id'])\n            yield uid, role_key\n    raise StopIteration()", "language": "python", "code": "def _dissect_roles(metadata):\n    \"\"\"Given a model's ``metadata``, iterate over the roles.\n    Return values are the role identifier and role type as a tuple.\n    \"\"\"\n    for role_key in cnxepub.ATTRIBUTED_ROLE_KEYS:\n        for user in metadata.get(role_key, []):\n            if user['type'] != 'cnx-id':\n                raise ValueError(\"Archive only accepts Connexions users.\")\n            uid = parse_user_uri(user['id'])\n            yield uid, role_key\n    raise StopIteration()", "code_tokens": ["def", "_dissect_roles", "(", "metadata", ")", ":", "for", "role_key", "in", "cnxepub", ".", "ATTRIBUTED_ROLE_KEYS", ":", "for", "user", "in", "metadata", ".", "get", "(", "role_key", ",", "[", "]", ")", ":", "if", "user", "[", "'type'", "]", "!=", "'cnx-id'", ":", "raise", "ValueError", "(", "\"Archive only accepts Connexions users.\"", ")", "uid", "=", "parse_user_uri", "(", "user", "[", "'id'", "]", ")", "yield", "uid", ",", "role_key", "raise", "StopIteration", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "f55b4a2c45d8618737288f1b74b4139d5ac74154", "url": "https://github.com/openstax/cnx-publishing/blob/f55b4a2c45d8618737288f1b74b4139d5ac74154/cnxpublishing/db.py#L106-L116", "partition": "valid"}
{"repo": "andsor/pypercolate", "path": "percolate/hpc.py", "func_name": "finalize_canonical_averages", "original_string": "def finalize_canonical_averages(\n    number_of_nodes, ps, canonical_averages, alpha,\n):\n    \"\"\"\n    Finalize canonical averages\n    \"\"\"\n\n    spanning_cluster = (\n        (\n            'percolation_probability_mean' in\n            canonical_averages.dtype.names\n        ) and\n        'percolation_probability_m2' in canonical_averages.dtype.names\n    )\n\n    # append values of p as an additional field\n    ret = np.empty_like(\n        canonical_averages,\n        dtype=finalized_canonical_averages_dtype(\n            spanning_cluster=spanning_cluster\n        ),\n    )\n\n    n = canonical_averages['number_of_runs']\n    sqrt_n = np.sqrt(canonical_averages['number_of_runs'])\n\n    ret['number_of_runs'] = n\n    ret['p'] = ps\n    ret['alpha'] = alpha\n\n    def _transform(\n        original_key, final_key=None, normalize=False, transpose=False,\n    ):\n        if final_key is None:\n            final_key = original_key\n        keys_mean = [\n            '{}_mean'.format(key)\n            for key in [original_key, final_key]\n        ]\n        keys_std = [\n            '{}_m2'.format(original_key),\n            '{}_std'.format(final_key),\n        ]\n        key_ci = '{}_ci'.format(final_key)\n\n        # calculate sample mean\n        ret[keys_mean[1]] = canonical_averages[keys_mean[0]]\n        if normalize:\n            ret[keys_mean[1]] /= number_of_nodes\n\n        # calculate sample standard deviation\n        array = canonical_averages[keys_std[0]]\n        result = np.sqrt(\n            (array.T if transpose else array) / (n - 1)\n        )\n        ret[keys_std[1]] = (\n            result.T if transpose else result\n        )\n        if normalize:\n            ret[keys_std[1]] /= number_of_nodes\n\n        # calculate standard normal confidence interval\n        array = ret[keys_std[1]]\n        scale = (array.T if transpose else array) / sqrt_n\n        array = ret[keys_mean[1]]\n        mean = (array.T if transpose else array)\n        result = scipy.stats.t.interval(\n            1 - alpha,\n            df=n - 1,\n            loc=mean,\n            scale=scale,\n        )\n        (\n            ret[key_ci][..., 0], ret[key_ci][..., 1]\n        ) = ([my_array.T for my_array in result] if transpose else result)\n\n    if spanning_cluster:\n        _transform('percolation_probability')\n\n    _transform('max_cluster_size', 'percolation_strength', normalize=True)\n    _transform('moments', normalize=True, transpose=True)\n\n    return ret", "language": "python", "code": "def finalize_canonical_averages(\n    number_of_nodes, ps, canonical_averages, alpha,\n):\n    \"\"\"\n    Finalize canonical averages\n    \"\"\"\n\n    spanning_cluster = (\n        (\n            'percolation_probability_mean' in\n            canonical_averages.dtype.names\n        ) and\n        'percolation_probability_m2' in canonical_averages.dtype.names\n    )\n\n    # append values of p as an additional field\n    ret = np.empty_like(\n        canonical_averages,\n        dtype=finalized_canonical_averages_dtype(\n            spanning_cluster=spanning_cluster\n        ),\n    )\n\n    n = canonical_averages['number_of_runs']\n    sqrt_n = np.sqrt(canonical_averages['number_of_runs'])\n\n    ret['number_of_runs'] = n\n    ret['p'] = ps\n    ret['alpha'] = alpha\n\n    def _transform(\n        original_key, final_key=None, normalize=False, transpose=False,\n    ):\n        if final_key is None:\n            final_key = original_key\n        keys_mean = [\n            '{}_mean'.format(key)\n            for key in [original_key, final_key]\n        ]\n        keys_std = [\n            '{}_m2'.format(original_key),\n            '{}_std'.format(final_key),\n        ]\n        key_ci = '{}_ci'.format(final_key)\n\n        # calculate sample mean\n        ret[keys_mean[1]] = canonical_averages[keys_mean[0]]\n        if normalize:\n            ret[keys_mean[1]] /= number_of_nodes\n\n        # calculate sample standard deviation\n        array = canonical_averages[keys_std[0]]\n        result = np.sqrt(\n            (array.T if transpose else array) / (n - 1)\n        )\n        ret[keys_std[1]] = (\n            result.T if transpose else result\n        )\n        if normalize:\n            ret[keys_std[1]] /= number_of_nodes\n\n        # calculate standard normal confidence interval\n        array = ret[keys_std[1]]\n        scale = (array.T if transpose else array) / sqrt_n\n        array = ret[keys_mean[1]]\n        mean = (array.T if transpose else array)\n        result = scipy.stats.t.interval(\n            1 - alpha,\n            df=n - 1,\n            loc=mean,\n            scale=scale,\n        )\n        (\n            ret[key_ci][..., 0], ret[key_ci][..., 1]\n        ) = ([my_array.T for my_array in result] if transpose else result)\n\n    if spanning_cluster:\n        _transform('percolation_probability')\n\n    _transform('max_cluster_size', 'percolation_strength', normalize=True)\n    _transform('moments', normalize=True, transpose=True)\n\n    return ret", "code_tokens": ["def", "finalize_canonical_averages", "(", "number_of_nodes", ",", "ps", ",", "canonical_averages", ",", "alpha", ",", ")", ":", "spanning_cluster", "=", "(", "(", "'percolation_probability_mean'", "in", "canonical_averages", ".", "dtype", ".", "names", ")", "and", "'percolation_probability_m2'", "in", "canonical_averages", ".", "dtype", ".", "names", ")", "# append values of p as an additional field", "ret", "=", "np", ".", "empty_like", "(", "canonical_averages", ",", "dtype", "=", "finalized_canonical_averages_dtype", "(", "spanning_cluster", "=", "spanning_cluster", ")", ",", ")", "n", "=", "canonical_averages", "[", "'number_of_runs'", "]", "sqrt_n", "=", "np", ".", "sqrt", "(", "canonical_averages", "[", "'number_of_runs'", "]", ")", "ret", "[", "'number_of_runs'", "]", "=", "n", "ret", "[", "'p'", "]", "=", "ps", "ret", "[", "'alpha'", "]", "=", "alpha", "def", "_transform", "(", "original_key", ",", "final_key", "=", "None", ",", "normalize", "=", "False", ",", "transpose", "=", "False", ",", ")", ":", "if", "final_key", "is", "None", ":", "final_key", "=", "original_key", "keys_mean", "=", "[", "'{}_mean'", ".", "format", "(", "key", ")", "for", "key", "in", "[", "original_key", ",", "final_key", "]", "]", "keys_std", "=", "[", "'{}_m2'", ".", "format", "(", "original_key", ")", ",", "'{}_std'", ".", "format", "(", "final_key", ")", ",", "]", "key_ci", "=", "'{}_ci'", ".", "format", "(", "final_key", ")", "# calculate sample mean", "ret", "[", "keys_mean", "[", "1", "]", "]", "=", "canonical_averages", "[", "keys_mean", "[", "0", "]", "]", "if", "normalize", ":", "ret", "[", "keys_mean", "[", "1", "]", "]", "/=", "number_of_nodes", "# calculate sample standard deviation", "array", "=", "canonical_averages", "[", "keys_std", "[", "0", "]", "]", "result", "=", "np", ".", "sqrt", "(", "(", "array", ".", "T", "if", "transpose", "else", "array", ")", "/", "(", "n", "-", "1", ")", ")", "ret", "[", "keys_std", "[", "1", "]", "]", "=", "(", "result", ".", "T", "if", "transpose", "else", "result", ")", "if", "normalize", ":", "ret", "[", "keys_std", "[", "1", "]", "]", "/=", "number_of_nodes", "# calculate standard normal confidence interval", "array", "=", "ret", "[", "keys_std", "[", "1", "]", "]", "scale", "=", "(", "array", ".", "T", "if", "transpose", "else", "array", ")", "/", "sqrt_n", "array", "=", "ret", "[", "keys_mean", "[", "1", "]", "]", "mean", "=", "(", "array", ".", "T", "if", "transpose", "else", "array", ")", "result", "=", "scipy", ".", "stats", ".", "t", ".", "interval", "(", "1", "-", "alpha", ",", "df", "=", "n", "-", "1", ",", "loc", "=", "mean", ",", "scale", "=", "scale", ",", ")", "(", "ret", "[", "key_ci", "]", "[", "...", ",", "0", "]", ",", "ret", "[", "key_ci", "]", "[", "...", ",", "1", "]", ")", "=", "(", "[", "my_array", ".", "T", "for", "my_array", "in", "result", "]", "if", "transpose", "else", "result", ")", "if", "spanning_cluster", ":", "_transform", "(", "'percolation_probability'", ")", "_transform", "(", "'max_cluster_size'", ",", "'percolation_strength'", ",", "normalize", "=", "True", ")", "_transform", "(", "'moments'", ",", "normalize", "=", "True", ",", "transpose", "=", "True", ")", "return", "ret"], "docstring": "", "docstring_tokens": [], "sha": "92478c1fc4d4ff5ae157f7607fd74f6f9ec360ac", "url": "https://github.com/andsor/pypercolate/blob/92478c1fc4d4ff5ae157f7607fd74f6f9ec360ac/percolate/hpc.py#L752-L834", "partition": "valid"}
{"repo": "ActivisionGameScience/assertpy", "path": "assertpy/assertpy.py", "func_name": "AssertionBuilder.is_less_than", "original_string": "def is_less_than(self, other):\n        \"\"\"Asserts that val is numeric and is less than other.\"\"\"\n        self._validate_compareable(other)\n        if self.val >= other:\n            if type(self.val) is datetime.datetime:\n                self._err('Expected <%s> to be less than <%s>, but was not.' % (self.val.strftime('%Y-%m-%d %H:%M:%S'), other.strftime('%Y-%m-%d %H:%M:%S')))\n            else:\n                self._err('Expected <%s> to be less than <%s>, but was not.' % (self.val, other))\n        return self", "language": "python", "code": "def is_less_than(self, other):\n        \"\"\"Asserts that val is numeric and is less than other.\"\"\"\n        self._validate_compareable(other)\n        if self.val >= other:\n            if type(self.val) is datetime.datetime:\n                self._err('Expected <%s> to be less than <%s>, but was not.' % (self.val.strftime('%Y-%m-%d %H:%M:%S'), other.strftime('%Y-%m-%d %H:%M:%S')))\n            else:\n                self._err('Expected <%s> to be less than <%s>, but was not.' % (self.val, other))\n        return self", "code_tokens": ["def", "is_less_than", "(", "self", ",", "other", ")", ":", "self", ".", "_validate_compareable", "(", "other", ")", "if", "self", ".", "val", ">=", "other", ":", "if", "type", "(", "self", ".", "val", ")", "is", "datetime", ".", "datetime", ":", "self", ".", "_err", "(", "'Expected <%s> to be less than <%s>, but was not.'", "%", "(", "self", ".", "val", ".", "strftime", "(", "'%Y-%m-%d %H:%M:%S'", ")", ",", "other", ".", "strftime", "(", "'%Y-%m-%d %H:%M:%S'", ")", ")", ")", "else", ":", "self", ".", "_err", "(", "'Expected <%s> to be less than <%s>, but was not.'", "%", "(", "self", ".", "val", ",", "other", ")", ")", "return", "self"], "docstring": "", "docstring_tokens": [], "sha": "08d799cdb01f9a25d3e20672efac991c7bc26d79", "url": "https://github.com/ActivisionGameScience/assertpy/blob/08d799cdb01f9a25d3e20672efac991c7bc26d79/assertpy/assertpy.py#L479-L487", "partition": "valid"}
{"repo": "Jajcus/pyxmpp2", "path": "pyxmpp2/cache.py", "func_name": "CacheFetcher.got_it", "original_string": "def got_it(self, value, state = \"new\"):\n        \"\"\"Handle a successfull retrieval and call apriopriate handler.\n\n        Should be called when retrieval succeeds.\n\n        Do nothing when the fetcher is not active any more (after\n        one of handlers was already called).\n\n        :Parameters:\n            - `value`: fetched object.\n            - `state`: initial state of the object.\n        :Types:\n            - `value`: any\n            - `state`: `str`\"\"\"\n        if not self.active:\n            return\n        item = CacheItem(self.address, value, self._item_freshness_period,\n                self._item_expiration_period, self._item_purge_period, state)\n        self._object_handler(item.address, item.value, item.state)\n        self.cache.add_item(item)\n        self._deactivate()", "language": "python", "code": "def got_it(self, value, state = \"new\"):\n        \"\"\"Handle a successfull retrieval and call apriopriate handler.\n\n        Should be called when retrieval succeeds.\n\n        Do nothing when the fetcher is not active any more (after\n        one of handlers was already called).\n\n        :Parameters:\n            - `value`: fetched object.\n            - `state`: initial state of the object.\n        :Types:\n            - `value`: any\n            - `state`: `str`\"\"\"\n        if not self.active:\n            return\n        item = CacheItem(self.address, value, self._item_freshness_period,\n                self._item_expiration_period, self._item_purge_period, state)\n        self._object_handler(item.address, item.value, item.state)\n        self.cache.add_item(item)\n        self._deactivate()", "code_tokens": ["def", "got_it", "(", "self", ",", "value", ",", "state", "=", "\"new\"", ")", ":", "if", "not", "self", ".", "active", ":", "return", "item", "=", "CacheItem", "(", "self", ".", "address", ",", "value", ",", "self", ".", "_item_freshness_period", ",", "self", ".", "_item_expiration_period", ",", "self", ".", "_item_purge_period", ",", "state", ")", "self", ".", "_object_handler", "(", "item", ".", "address", ",", "item", ".", "value", ",", "item", ".", "state", ")", "self", ".", "cache", ".", "add_item", "(", "item", ")", "self", ".", "_deactivate", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "14a40a3950910a9cd008b55f0d8905aa0186ce18", "url": "https://github.com/Jajcus/pyxmpp2/blob/14a40a3950910a9cd008b55f0d8905aa0186ce18/pyxmpp2/cache.py#L228-L248", "partition": "valid"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/generator/inspect.py", "func_name": "NextflowInspector._get_log_lines", "original_string": "def _get_log_lines(self, n=300):\n        \"\"\"Returns a list with the last ``n`` lines of the nextflow log file\n\n        Parameters\n        ----------\n        n : int\n            Number of last lines from the log file\n\n        Returns\n        -------\n        list\n            List of strings with the nextflow log\n        \"\"\"\n\n        with open(self.log_file) as fh:\n            last_lines = fh.readlines()[-n:]\n\n        return last_lines", "language": "python", "code": "def _get_log_lines(self, n=300):\n        \"\"\"Returns a list with the last ``n`` lines of the nextflow log file\n\n        Parameters\n        ----------\n        n : int\n            Number of last lines from the log file\n\n        Returns\n        -------\n        list\n            List of strings with the nextflow log\n        \"\"\"\n\n        with open(self.log_file) as fh:\n            last_lines = fh.readlines()[-n:]\n\n        return last_lines", "code_tokens": ["def", "_get_log_lines", "(", "self", ",", "n", "=", "300", ")", ":", "with", "open", "(", "self", ".", "log_file", ")", "as", "fh", ":", "last_lines", "=", "fh", ".", "readlines", "(", ")", "[", "-", "n", ":", "]", "return", "last_lines"], "docstring": "", "docstring_tokens": [], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/inspect.py#L1356-L1373", "partition": "test"}
{"repo": "goose3/goose3", "path": "goose3/extractors/images.py", "func_name": "ImageExtractor.check_link_tag", "original_string": "def check_link_tag(self):\n        \"\"\"\\\n        checks to see if we were able to\n        find open link_src on this page\n        \"\"\"\n        node = self.article.raw_doc\n        meta = self.parser.getElementsByTag(node, tag='link', attr='rel', value='image_src')\n        for item in meta:\n            src = self.parser.getAttribute(item, attr='href')\n            if src:\n                return self.get_image(src, extraction_type='linktag')\n        return None", "language": "python", "code": "def check_link_tag(self):\n        \"\"\"\\\n        checks to see if we were able to\n        find open link_src on this page\n        \"\"\"\n        node = self.article.raw_doc\n        meta = self.parser.getElementsByTag(node, tag='link', attr='rel', value='image_src')\n        for item in meta:\n            src = self.parser.getAttribute(item, attr='href')\n            if src:\n                return self.get_image(src, extraction_type='linktag')\n        return None", "code_tokens": ["def", "check_link_tag", "(", "self", ")", ":", "node", "=", "self", ".", "article", ".", "raw_doc", "meta", "=", "self", ".", "parser", ".", "getElementsByTag", "(", "node", ",", "tag", "=", "'link'", ",", "attr", "=", "'rel'", ",", "value", "=", "'image_src'", ")", "for", "item", "in", "meta", ":", "src", "=", "self", ".", "parser", ".", "getAttribute", "(", "item", ",", "attr", "=", "'href'", ")", "if", "src", ":", "return", "self", ".", "get_image", "(", "src", ",", "extraction_type", "=", "'linktag'", ")", "return", "None"], "docstring": "", "docstring_tokens": [], "sha": "e6994b1b1826af2720a091d1bff5ca15594f558d", "url": "https://github.com/goose3/goose3/blob/e6994b1b1826af2720a091d1bff5ca15594f558d/goose3/extractors/images.py#L303-L314", "partition": "valid"}
{"repo": "gears/gears", "path": "gears/environment.py", "func_name": "Environment.paths", "original_string": "def paths(self):\n        \"\"\"The list of search paths. It is built from registered finders, which\n        has ``paths`` property. Can be useful for compilers to resolve internal\n        dependencies.\n        \"\"\"\n        if not hasattr(self, '_paths'):\n            paths = []\n            for finder in self.finders:\n                if hasattr(finder, 'paths'):\n                    paths.extend(finder.paths)\n            self._paths = paths\n        return self._paths", "language": "python", "code": "def paths(self):\n        \"\"\"The list of search paths. It is built from registered finders, which\n        has ``paths`` property. Can be useful for compilers to resolve internal\n        dependencies.\n        \"\"\"\n        if not hasattr(self, '_paths'):\n            paths = []\n            for finder in self.finders:\n                if hasattr(finder, 'paths'):\n                    paths.extend(finder.paths)\n            self._paths = paths\n        return self._paths", "code_tokens": ["def", "paths", "(", "self", ")", ":", "if", "not", "hasattr", "(", "self", ",", "'_paths'", ")", ":", "paths", "=", "[", "]", "for", "finder", "in", "self", ".", "finders", ":", "if", "hasattr", "(", "finder", ",", "'paths'", ")", ":", "paths", ".", "extend", "(", "finder", ".", "paths", ")", "self", ".", "_paths", "=", "paths", "return", "self", ".", "_paths"], "docstring": "", "docstring_tokens": [], "sha": "5729c2525a8c04c185e998bd9a86233708972921", "url": "https://github.com/gears/gears/blob/5729c2525a8c04c185e998bd9a86233708972921/gears/environment.py#L294-L305", "partition": "test"}
{"repo": "greenbender/pynntp", "path": "nntp/nntp.py", "func_name": "NNTPClient.list_overview_fmt_gen", "original_string": "def list_overview_fmt_gen(self):\n        \"\"\"Generator for the LIST OVERVIEW.FMT\n\n        See list_overview_fmt() for more information.\n\n        Yields:\n            An element in the list returned by list_overview_fmt().\n        \"\"\"\n        code, message = self.command(\"LIST OVERVIEW.FMT\")\n        if code != 215:\n            raise NNTPReplyError(code, message)\n\n        for line in self.info_gen(code, message):\n            try:\n                name, suffix = line.rstrip().split(\":\")\n            except ValueError:\n                raise NNTPDataError(\"Invalid LIST OVERVIEW.FMT\")\n            if suffix and not name:\n                name, suffix = suffix, name\n            if suffix and suffix != \"full\":\n                raise NNTPDataError(\"Invalid LIST OVERVIEW.FMT\")\n            yield (name, suffix == \"full\")", "language": "python", "code": "def list_overview_fmt_gen(self):\n        \"\"\"Generator for the LIST OVERVIEW.FMT\n\n        See list_overview_fmt() for more information.\n\n        Yields:\n            An element in the list returned by list_overview_fmt().\n        \"\"\"\n        code, message = self.command(\"LIST OVERVIEW.FMT\")\n        if code != 215:\n            raise NNTPReplyError(code, message)\n\n        for line in self.info_gen(code, message):\n            try:\n                name, suffix = line.rstrip().split(\":\")\n            except ValueError:\n                raise NNTPDataError(\"Invalid LIST OVERVIEW.FMT\")\n            if suffix and not name:\n                name, suffix = suffix, name\n            if suffix and suffix != \"full\":\n                raise NNTPDataError(\"Invalid LIST OVERVIEW.FMT\")\n            yield (name, suffix == \"full\")", "code_tokens": ["def", "list_overview_fmt_gen", "(", "self", ")", ":", "code", ",", "message", "=", "self", ".", "command", "(", "\"LIST OVERVIEW.FMT\"", ")", "if", "code", "!=", "215", ":", "raise", "NNTPReplyError", "(", "code", ",", "message", ")", "for", "line", "in", "self", ".", "info_gen", "(", "code", ",", "message", ")", ":", "try", ":", "name", ",", "suffix", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "\":\"", ")", "except", "ValueError", ":", "raise", "NNTPDataError", "(", "\"Invalid LIST OVERVIEW.FMT\"", ")", "if", "suffix", "and", "not", "name", ":", "name", ",", "suffix", "=", "suffix", ",", "name", "if", "suffix", "and", "suffix", "!=", "\"full\"", ":", "raise", "NNTPDataError", "(", "\"Invalid LIST OVERVIEW.FMT\"", ")", "yield", "(", "name", ",", "suffix", "==", "\"full\"", ")"], "docstring": "", "docstring_tokens": [], "sha": "991a76331cdf5d8f9dbf5b18f6e29adc80749a2f", "url": "https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/nntp.py#L860-L881", "partition": "test"}
{"repo": "ingolemo/python-lenses", "path": "examples/naughts_and_crosses.py", "func_name": "play", "original_string": "def play():\n    'Play a game of naughts and crosses against the computer.'\n    ai = {'X': player_move, 'O': random_move}\n    board = Board()\n    while not board.winner:\n        x, y = ai[board.player](board)\n        board = board.make_move(x, y)\n    print(board, end='\\n\\n')\n    print(board.winner)", "language": "python", "code": "def play():\n    'Play a game of naughts and crosses against the computer.'\n    ai = {'X': player_move, 'O': random_move}\n    board = Board()\n    while not board.winner:\n        x, y = ai[board.player](board)\n        board = board.make_move(x, y)\n    print(board, end='\\n\\n')\n    print(board.winner)", "code_tokens": ["def", "play", "(", ")", ":", "ai", "=", "{", "'X'", ":", "player_move", ",", "'O'", ":", "random_move", "}", "board", "=", "Board", "(", ")", "while", "not", "board", ".", "winner", ":", "x", ",", "y", "=", "ai", "[", "board", ".", "player", "]", "(", "board", ")", "board", "=", "board", ".", "make_move", "(", "x", ",", "y", ")", "print", "(", "board", ",", "end", "=", "'\\n\\n'", ")", "print", "(", "board", ".", "winner", ")"], "docstring": "", "docstring_tokens": [], "sha": "a3a6ed0a31f6674451e542e7380a8aa16e6f8edf", "url": "https://github.com/ingolemo/python-lenses/blob/a3a6ed0a31f6674451e542e7380a8aa16e6f8edf/examples/naughts_and_crosses.py#L102-L110", "partition": "test"}
{"repo": "ihgazni2/elist", "path": "elist/elist.py", "func_name": "PointerCache.child_begin_handler", "original_string": "def child_begin_handler(self,scache,*args):\n        '''\n            _creat_child_desc\n            update depth,parent_breadth_path,parent_path,sib_seq,path,lsib_path,rsib_path,lcin_path,rcin_path\n        '''\n        pdesc = self.pdesc\n        depth = scache.depth\n        sib_seq = self.sib_seq\n        sibs_len = self.sibs_len\n        pdesc_level = scache.pdesc_level\n        desc = copy.deepcopy(pdesc)\n        desc = reset_parent_desc_template(desc)\n        desc['depth'] = depth\n        desc['parent_breadth_path'] = copy.deepcopy(desc['breadth_path'])\n        desc['sib_seq'] = sib_seq\n        desc['parent_path'] = copy.deepcopy(desc['path'])\n        desc['path'].append(sib_seq)\n        update_desc_lsib_path(desc)\n        update_desc_rsib_path(desc,sibs_len)\n        if(depth == 1):\n            pass\n        else:\n            update_desc_lcin_path(desc,pdesc_level)\n            update_desc_rcin_path(desc,sibs_len,pdesc_level)\n        return(desc)", "language": "python", "code": "def child_begin_handler(self,scache,*args):\n        '''\n            _creat_child_desc\n            update depth,parent_breadth_path,parent_path,sib_seq,path,lsib_path,rsib_path,lcin_path,rcin_path\n        '''\n        pdesc = self.pdesc\n        depth = scache.depth\n        sib_seq = self.sib_seq\n        sibs_len = self.sibs_len\n        pdesc_level = scache.pdesc_level\n        desc = copy.deepcopy(pdesc)\n        desc = reset_parent_desc_template(desc)\n        desc['depth'] = depth\n        desc['parent_breadth_path'] = copy.deepcopy(desc['breadth_path'])\n        desc['sib_seq'] = sib_seq\n        desc['parent_path'] = copy.deepcopy(desc['path'])\n        desc['path'].append(sib_seq)\n        update_desc_lsib_path(desc)\n        update_desc_rsib_path(desc,sibs_len)\n        if(depth == 1):\n            pass\n        else:\n            update_desc_lcin_path(desc,pdesc_level)\n            update_desc_rcin_path(desc,sibs_len,pdesc_level)\n        return(desc)", "code_tokens": ["def", "child_begin_handler", "(", "self", ",", "scache", ",", "*", "args", ")", ":", "pdesc", "=", "self", ".", "pdesc", "depth", "=", "scache", ".", "depth", "sib_seq", "=", "self", ".", "sib_seq", "sibs_len", "=", "self", ".", "sibs_len", "pdesc_level", "=", "scache", ".", "pdesc_level", "desc", "=", "copy", ".", "deepcopy", "(", "pdesc", ")", "desc", "=", "reset_parent_desc_template", "(", "desc", ")", "desc", "[", "'depth'", "]", "=", "depth", "desc", "[", "'parent_breadth_path'", "]", "=", "copy", ".", "deepcopy", "(", "desc", "[", "'breadth_path'", "]", ")", "desc", "[", "'sib_seq'", "]", "=", "sib_seq", "desc", "[", "'parent_path'", "]", "=", "copy", ".", "deepcopy", "(", "desc", "[", "'path'", "]", ")", "desc", "[", "'path'", "]", ".", "append", "(", "sib_seq", ")", "update_desc_lsib_path", "(", "desc", ")", "update_desc_rsib_path", "(", "desc", ",", "sibs_len", ")", "if", "(", "depth", "==", "1", ")", ":", "pass", "else", ":", "update_desc_lcin_path", "(", "desc", ",", "pdesc_level", ")", "update_desc_rcin_path", "(", "desc", ",", "sibs_len", ",", "pdesc_level", ")", "return", "(", "desc", ")"], "docstring": "", "docstring_tokens": [], "sha": "8c07b5029bda34ead60ce10335ceb145f209263c", "url": "https://github.com/ihgazni2/elist/blob/8c07b5029bda34ead60ce10335ceb145f209263c/elist/elist.py#L6069-L6093", "partition": "valid"}
{"repo": "apache/incubator-heron", "path": "heron/statemgrs/src/python/statemanager.py", "func_name": "StateManager.establish_ssh_tunnel", "original_string": "def establish_ssh_tunnel(self):\n    \"\"\"\n    Establish an ssh tunnel for each local host and port\n    that can be used to communicate with the state host.\n    \"\"\"\n    localportlist = []\n    for (host, port) in self.hostportlist:\n      localport = self.pick_unused_port()\n      self.tunnel.append(subprocess.Popen(\n          ('ssh', self.tunnelhost, '-NL127.0.0.1:%d:%s:%d' % (localport, host, port))))\n      localportlist.append(('127.0.0.1', localport))\n    return localportlist", "language": "python", "code": "def establish_ssh_tunnel(self):\n    \"\"\"\n    Establish an ssh tunnel for each local host and port\n    that can be used to communicate with the state host.\n    \"\"\"\n    localportlist = []\n    for (host, port) in self.hostportlist:\n      localport = self.pick_unused_port()\n      self.tunnel.append(subprocess.Popen(\n          ('ssh', self.tunnelhost, '-NL127.0.0.1:%d:%s:%d' % (localport, host, port))))\n      localportlist.append(('127.0.0.1', localport))\n    return localportlist", "code_tokens": ["def", "establish_ssh_tunnel", "(", "self", ")", ":", "localportlist", "=", "[", "]", "for", "(", "host", ",", "port", ")", "in", "self", ".", "hostportlist", ":", "localport", "=", "self", ".", "pick_unused_port", "(", ")", "self", ".", "tunnel", ".", "append", "(", "subprocess", ".", "Popen", "(", "(", "'ssh'", ",", "self", ".", "tunnelhost", ",", "'-NL127.0.0.1:%d:%s:%d'", "%", "(", "localport", ",", "host", ",", "port", ")", ")", ")", ")", "localportlist", ".", "append", "(", "(", "'127.0.0.1'", ",", "localport", ")", ")", "return", "localportlist"], "docstring": "", "docstring_tokens": [], "sha": "ad10325a0febe89ad337e561ebcbe37ec5d9a5ac", "url": "https://github.com/apache/incubator-heron/blob/ad10325a0febe89ad337e561ebcbe37ec5d9a5ac/heron/statemgrs/src/python/statemanager.py#L110-L121", "partition": "valid"}
{"repo": "numenta/nupic", "path": "src/nupic/regions/record_sensor.py", "func_name": "RecordSensor.applyFilters", "original_string": "def applyFilters(self, data):\n    \"\"\"\n    Apply pre-encoding filters. These filters may modify or add data. If a \n    filter needs another record (e.g. a delta filter) it will request another \n    record by returning False and the current record will be skipped (but will \n    still be given to all filters).\n\n    We have to be very careful about resets. A filter may add a reset,\n    but other filters should not see the added reset, each filter sees\n    the original reset value, and we keep track of whether any filter\n    adds a reset.\n\n    :param data: (dict) The data that will be processed by the filter.\n    :returns: (tuple) with the data processed by the filter and a boolean to\n              know whether or not the filter needs mode data.\n    \"\"\"\n\n    if self.verbosity > 0:\n      print \"RecordSensor got data: %s\" % data\n\n    allFiltersHaveEnoughData = True\n    if len(self.preEncodingFilters) > 0:\n      originalReset = data['_reset']\n      actualReset = originalReset\n      for f in self.preEncodingFilters:\n        # if filter needs more data, it returns False\n        filterHasEnoughData = f.process(data)\n        allFiltersHaveEnoughData = (allFiltersHaveEnoughData\n                                    and filterHasEnoughData)\n        actualReset = actualReset or data['_reset']\n        data['_reset'] = originalReset\n      data['_reset'] = actualReset\n\n    return data, allFiltersHaveEnoughData", "language": "python", "code": "def applyFilters(self, data):\n    \"\"\"\n    Apply pre-encoding filters. These filters may modify or add data. If a \n    filter needs another record (e.g. a delta filter) it will request another \n    record by returning False and the current record will be skipped (but will \n    still be given to all filters).\n\n    We have to be very careful about resets. A filter may add a reset,\n    but other filters should not see the added reset, each filter sees\n    the original reset value, and we keep track of whether any filter\n    adds a reset.\n\n    :param data: (dict) The data that will be processed by the filter.\n    :returns: (tuple) with the data processed by the filter and a boolean to\n              know whether or not the filter needs mode data.\n    \"\"\"\n\n    if self.verbosity > 0:\n      print \"RecordSensor got data: %s\" % data\n\n    allFiltersHaveEnoughData = True\n    if len(self.preEncodingFilters) > 0:\n      originalReset = data['_reset']\n      actualReset = originalReset\n      for f in self.preEncodingFilters:\n        # if filter needs more data, it returns False\n        filterHasEnoughData = f.process(data)\n        allFiltersHaveEnoughData = (allFiltersHaveEnoughData\n                                    and filterHasEnoughData)\n        actualReset = actualReset or data['_reset']\n        data['_reset'] = originalReset\n      data['_reset'] = actualReset\n\n    return data, allFiltersHaveEnoughData", "code_tokens": ["def", "applyFilters", "(", "self", ",", "data", ")", ":", "if", "self", ".", "verbosity", ">", "0", ":", "print", "\"RecordSensor got data: %s\"", "%", "data", "allFiltersHaveEnoughData", "=", "True", "if", "len", "(", "self", ".", "preEncodingFilters", ")", ">", "0", ":", "originalReset", "=", "data", "[", "'_reset'", "]", "actualReset", "=", "originalReset", "for", "f", "in", "self", ".", "preEncodingFilters", ":", "# if filter needs more data, it returns False", "filterHasEnoughData", "=", "f", ".", "process", "(", "data", ")", "allFiltersHaveEnoughData", "=", "(", "allFiltersHaveEnoughData", "and", "filterHasEnoughData", ")", "actualReset", "=", "actualReset", "or", "data", "[", "'_reset'", "]", "data", "[", "'_reset'", "]", "=", "originalReset", "data", "[", "'_reset'", "]", "=", "actualReset", "return", "data", ",", "allFiltersHaveEnoughData"], "docstring": "", "docstring_tokens": [], "sha": "5922fafffdccc8812e72b3324965ad2f7d4bbdad", "url": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/regions/record_sensor.py#L304-L337", "partition": "valid"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.validate_expression", "original_string": "def validate_expression(self, expression):\n        \"\"\"Validate an expression (may throw Exceptions)\"\"\"\n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)", "language": "python", "code": "def validate_expression(self, expression):\n        \"\"\"Validate an expression (may throw Exceptions)\"\"\"\n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)", "code_tokens": ["def", "validate_expression", "(", "self", ",", "expression", ")", ":", "# return self.evaluate(expression, 0, 2)", "vars", "=", "set", "(", "self", ".", "get_column_names", "(", ")", ")", "|", "set", "(", "self", ".", "variables", ".", "keys", "(", ")", ")", "funcs", "=", "set", "(", "expression_namespace", ".", "keys", "(", ")", ")", "return", "vaex", ".", "expresso", ".", "validate_expression", "(", "expression", ",", "vars", ",", "funcs", ")"], "docstring": "", "docstring_tokens": [], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L2713-L2718", "partition": "test"}
{"repo": "mar10/wsgidav", "path": "wsgidav/samples/virtual_dav_provider.py", "func_name": "VirtualResource.handle_move", "original_string": "def handle_move(self, dest_path):\n        \"\"\"Change semantic of MOVE to change resource tags.\"\"\"\n        # path and destPath must be '/by_tag/<tag>/<resname>'\n        if \"/by_tag/\" not in self.path:\n            raise DAVError(HTTP_FORBIDDEN)\n        if \"/by_tag/\" not in dest_path:\n            raise DAVError(HTTP_FORBIDDEN)\n        catType, tag, _rest = util.save_split(self.path.strip(\"/\"), \"/\", 2)\n        assert catType == \"by_tag\"\n        assert tag in self.data[\"tags\"]\n        self.data[\"tags\"].remove(tag)\n        catType, tag, _rest = util.save_split(dest_path.strip(\"/\"), \"/\", 2)\n        assert catType == \"by_tag\"\n        if tag not in self.data[\"tags\"]:\n            self.data[\"tags\"].append(tag)\n        return True", "language": "python", "code": "def handle_move(self, dest_path):\n        \"\"\"Change semantic of MOVE to change resource tags.\"\"\"\n        # path and destPath must be '/by_tag/<tag>/<resname>'\n        if \"/by_tag/\" not in self.path:\n            raise DAVError(HTTP_FORBIDDEN)\n        if \"/by_tag/\" not in dest_path:\n            raise DAVError(HTTP_FORBIDDEN)\n        catType, tag, _rest = util.save_split(self.path.strip(\"/\"), \"/\", 2)\n        assert catType == \"by_tag\"\n        assert tag in self.data[\"tags\"]\n        self.data[\"tags\"].remove(tag)\n        catType, tag, _rest = util.save_split(dest_path.strip(\"/\"), \"/\", 2)\n        assert catType == \"by_tag\"\n        if tag not in self.data[\"tags\"]:\n            self.data[\"tags\"].append(tag)\n        return True", "code_tokens": ["def", "handle_move", "(", "self", ",", "dest_path", ")", ":", "# path and destPath must be '/by_tag/<tag>/<resname>'", "if", "\"/by_tag/\"", "not", "in", "self", ".", "path", ":", "raise", "DAVError", "(", "HTTP_FORBIDDEN", ")", "if", "\"/by_tag/\"", "not", "in", "dest_path", ":", "raise", "DAVError", "(", "HTTP_FORBIDDEN", ")", "catType", ",", "tag", ",", "_rest", "=", "util", ".", "save_split", "(", "self", ".", "path", ".", "strip", "(", "\"/\"", ")", ",", "\"/\"", ",", "2", ")", "assert", "catType", "==", "\"by_tag\"", "assert", "tag", "in", "self", ".", "data", "[", "\"tags\"", "]", "self", ".", "data", "[", "\"tags\"", "]", ".", "remove", "(", "tag", ")", "catType", ",", "tag", ",", "_rest", "=", "util", ".", "save_split", "(", "dest_path", ".", "strip", "(", "\"/\"", ")", ",", "\"/\"", ",", "2", ")", "assert", "catType", "==", "\"by_tag\"", "if", "tag", "not", "in", "self", ".", "data", "[", "\"tags\"", "]", ":", "self", ".", "data", "[", "\"tags\"", "]", ".", "append", "(", "tag", ")", "return", "True"], "docstring": "", "docstring_tokens": [], "sha": "cec0d84222fc24bea01be1cea91729001963f172", "url": "https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/samples/virtual_dav_provider.py#L334-L349", "partition": "valid"}
{"repo": "edx/edx-enterprise", "path": "enterprise/api_client/lms.py", "func_name": "EnrollmentApiClient.has_course_mode", "original_string": "def has_course_mode(self, course_run_id, mode):\n        \"\"\"\n        Query the Enrollment API to see whether a course run has a given course mode available.\n\n        Arguments:\n            course_run_id (str): The string value of the course run's unique identifier\n\n        Returns:\n            bool: Whether the course run has the given mode avaialble for enrollment.\n\n        \"\"\"\n        course_modes = self.get_course_modes(course_run_id)\n        return any(course_mode for course_mode in course_modes if course_mode['slug'] == mode)", "language": "python", "code": "def has_course_mode(self, course_run_id, mode):\n        \"\"\"\n        Query the Enrollment API to see whether a course run has a given course mode available.\n\n        Arguments:\n            course_run_id (str): The string value of the course run's unique identifier\n\n        Returns:\n            bool: Whether the course run has the given mode avaialble for enrollment.\n\n        \"\"\"\n        course_modes = self.get_course_modes(course_run_id)\n        return any(course_mode for course_mode in course_modes if course_mode['slug'] == mode)", "code_tokens": ["def", "has_course_mode", "(", "self", ",", "course_run_id", ",", "mode", ")", ":", "course_modes", "=", "self", ".", "get_course_modes", "(", "course_run_id", ")", "return", "any", "(", "course_mode", "for", "course_mode", "in", "course_modes", "if", "course_mode", "[", "'slug'", "]", "==", "mode", ")"], "docstring": "", "docstring_tokens": [], "sha": "aea91379ab0a87cd3bc798961fce28b60ee49a80", "url": "https://github.com/edx/edx-enterprise/blob/aea91379ab0a87cd3bc798961fce28b60ee49a80/enterprise/api_client/lms.py#L198-L210", "partition": "valid"}
{"repo": "rwl/godot", "path": "godot/graph.py", "func_name": "Graph._maxiter_default", "original_string": "def _maxiter_default(self):\n        \"\"\" Trait initialiser.\n        \"\"\"\n        mode = self.mode\n        if mode == \"KK\":\n            return 100 * len(self.nodes)\n        elif mode == \"major\":\n            return 200\n        else:\n            return 600", "language": "python", "code": "def _maxiter_default(self):\n        \"\"\" Trait initialiser.\n        \"\"\"\n        mode = self.mode\n        if mode == \"KK\":\n            return 100 * len(self.nodes)\n        elif mode == \"major\":\n            return 200\n        else:\n            return 600", "code_tokens": ["def", "_maxiter_default", "(", "self", ")", ":", "mode", "=", "self", ".", "mode", "if", "mode", "==", "\"KK\"", ":", "return", "100", "*", "len", "(", "self", ".", "nodes", ")", "elif", "mode", "==", "\"major\"", ":", "return", "200", "else", ":", "return", "600"], "docstring": "", "docstring_tokens": [], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/graph.py#L1012-L1021", "partition": "test"}
{"repo": "DigitalGlobe/gbdxtools", "path": "gbdxtools/answerfactory.py", "func_name": "Recipe.get", "original_string": "def get(self, recipe_id):\n        '''\n        Retrieves an AnswerFactory Recipe by id\n\n        Args:\n            recipe_id The id of the recipe\n\n        Returns:\n            A JSON representation of the recipe\n        '''\n        self.logger.debug('Retrieving recipe by id: ' + recipe_id)\n        url = '%(base_url)s/recipe/%(recipe_id)s' % {\n            'base_url': self.base_url, 'recipe_id': recipe_id\n        }\n        r = self.gbdx_connection.get(url)\n        r.raise_for_status()\n        return r.json()", "language": "python", "code": "def get(self, recipe_id):\n        '''\n        Retrieves an AnswerFactory Recipe by id\n\n        Args:\n            recipe_id The id of the recipe\n\n        Returns:\n            A JSON representation of the recipe\n        '''\n        self.logger.debug('Retrieving recipe by id: ' + recipe_id)\n        url = '%(base_url)s/recipe/%(recipe_id)s' % {\n            'base_url': self.base_url, 'recipe_id': recipe_id\n        }\n        r = self.gbdx_connection.get(url)\n        r.raise_for_status()\n        return r.json()", "code_tokens": ["def", "get", "(", "self", ",", "recipe_id", ")", ":", "self", ".", "logger", ".", "debug", "(", "'Retrieving recipe by id: '", "+", "recipe_id", ")", "url", "=", "'%(base_url)s/recipe/%(recipe_id)s'", "%", "{", "'base_url'", ":", "self", ".", "base_url", ",", "'recipe_id'", ":", "recipe_id", "}", "r", "=", "self", ".", "gbdx_connection", ".", "get", "(", "url", ")", "r", ".", "raise_for_status", "(", ")", "return", "r", ".", "json", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "def62f8f2d77b168aa2bd115290aaa0f9a08a4bb", "url": "https://github.com/DigitalGlobe/gbdxtools/blob/def62f8f2d77b168aa2bd115290aaa0f9a08a4bb/gbdxtools/answerfactory.py#L32-L48", "partition": "valid"}
{"repo": "neherlab/treetime", "path": "treetime/treetime.py", "func_name": "TreeTime.print_lh", "original_string": "def print_lh(self, joint=True):\n        \"\"\"\n        Print the total likelihood of the tree given the constrained leaves\n\n        Parameters\n        ----------\n\n         joint : bool\n            If true, print joint LH, else print marginal LH\n\n        \"\"\"\n        try:\n            u_lh = self.tree.unconstrained_sequence_LH\n            if joint:\n                s_lh = self.tree.sequence_joint_LH\n                t_lh = self.tree.positional_joint_LH\n                c_lh = self.tree.coalescent_joint_LH\n            else:\n                s_lh = self.tree.sequence_marginal_LH\n                t_lh = self.tree.positional_marginal_LH\n                c_lh = 0\n\n            print (\"###  Tree Log-Likelihood  ###\\n\"\n                \" Sequence log-LH without constraints: \\t%1.3f\\n\"\n                \" Sequence log-LH with constraints:    \\t%1.3f\\n\"\n                \" TreeTime sequence log-LH:            \\t%1.3f\\n\"\n                \" Coalescent log-LH:                   \\t%1.3f\\n\"\n               \"#########################\"%(u_lh, s_lh,t_lh, c_lh))\n        except:\n            print(\"ERROR. Did you run the corresponding inference (joint/marginal)?\")", "language": "python", "code": "def print_lh(self, joint=True):\n        \"\"\"\n        Print the total likelihood of the tree given the constrained leaves\n\n        Parameters\n        ----------\n\n         joint : bool\n            If true, print joint LH, else print marginal LH\n\n        \"\"\"\n        try:\n            u_lh = self.tree.unconstrained_sequence_LH\n            if joint:\n                s_lh = self.tree.sequence_joint_LH\n                t_lh = self.tree.positional_joint_LH\n                c_lh = self.tree.coalescent_joint_LH\n            else:\n                s_lh = self.tree.sequence_marginal_LH\n                t_lh = self.tree.positional_marginal_LH\n                c_lh = 0\n\n            print (\"###  Tree Log-Likelihood  ###\\n\"\n                \" Sequence log-LH without constraints: \\t%1.3f\\n\"\n                \" Sequence log-LH with constraints:    \\t%1.3f\\n\"\n                \" TreeTime sequence log-LH:            \\t%1.3f\\n\"\n                \" Coalescent log-LH:                   \\t%1.3f\\n\"\n               \"#########################\"%(u_lh, s_lh,t_lh, c_lh))\n        except:\n            print(\"ERROR. Did you run the corresponding inference (joint/marginal)?\")", "code_tokens": ["def", "print_lh", "(", "self", ",", "joint", "=", "True", ")", ":", "try", ":", "u_lh", "=", "self", ".", "tree", ".", "unconstrained_sequence_LH", "if", "joint", ":", "s_lh", "=", "self", ".", "tree", ".", "sequence_joint_LH", "t_lh", "=", "self", ".", "tree", ".", "positional_joint_LH", "c_lh", "=", "self", ".", "tree", ".", "coalescent_joint_LH", "else", ":", "s_lh", "=", "self", ".", "tree", ".", "sequence_marginal_LH", "t_lh", "=", "self", ".", "tree", ".", "positional_marginal_LH", "c_lh", "=", "0", "print", "(", "\"###  Tree Log-Likelihood  ###\\n\"", "\" Sequence log-LH without constraints: \\t%1.3f\\n\"", "\" Sequence log-LH with constraints:    \\t%1.3f\\n\"", "\" TreeTime sequence log-LH:            \\t%1.3f\\n\"", "\" Coalescent log-LH:                   \\t%1.3f\\n\"", "\"#########################\"", "%", "(", "u_lh", ",", "s_lh", ",", "t_lh", ",", "c_lh", ")", ")", "except", ":", "print", "(", "\"ERROR. Did you run the corresponding inference (joint/marginal)?\"", ")"], "docstring": "", "docstring_tokens": [], "sha": "f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0", "url": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treetime.py#L655-L684", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/remo.py", "func_name": "ReMo._init_client", "original_string": "def _init_client(self, from_archive=False):\n        \"\"\"Init client\"\"\"\n\n        return ReMoClient(self.url, self.archive, from_archive)", "language": "python", "code": "def _init_client(self, from_archive=False):\n        \"\"\"Init client\"\"\"\n\n        return ReMoClient(self.url, self.archive, from_archive)", "code_tokens": ["def", "_init_client", "(", "self", ",", "from_archive", "=", "False", ")", ":", "return", "ReMoClient", "(", "self", ".", "url", ",", "self", ".", "archive", ",", "from_archive", ")"], "docstring": "", "docstring_tokens": [], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/remo.py#L216-L219", "partition": "test"}
{"repo": "wtsi-hgi/python-common", "path": "hgicommon/managers.py", "func_name": "TempManager.create_temp_directory", "original_string": "def create_temp_directory(self, **mkdtemp_kwargs) -> str:\n        \"\"\"\n        Creates a temp directory.\n        :param mkdtemp_kwargs: named arguments to be passed to `tempfile.mkdtemp`\n        :return: the location of the temp directory\n        \"\"\"\n        kwargs = {**self.default_mkdtemp_kwargs, **mkdtemp_kwargs}\n        location = tempfile.mkdtemp(**kwargs)\n        self._temp_directories.add(location)\n        return location", "language": "python", "code": "def create_temp_directory(self, **mkdtemp_kwargs) -> str:\n        \"\"\"\n        Creates a temp directory.\n        :param mkdtemp_kwargs: named arguments to be passed to `tempfile.mkdtemp`\n        :return: the location of the temp directory\n        \"\"\"\n        kwargs = {**self.default_mkdtemp_kwargs, **mkdtemp_kwargs}\n        location = tempfile.mkdtemp(**kwargs)\n        self._temp_directories.add(location)\n        return location", "code_tokens": ["def", "create_temp_directory", "(", "self", ",", "*", "*", "mkdtemp_kwargs", ")", "->", "str", ":", "kwargs", "=", "{", "*", "*", "self", ".", "default_mkdtemp_kwargs", ",", "*", "*", "mkdtemp_kwargs", "}", "location", "=", "tempfile", ".", "mkdtemp", "(", "*", "*", "kwargs", ")", "self", ".", "_temp_directories", ".", "add", "(", "location", ")", "return", "location"], "docstring": "", "docstring_tokens": [], "sha": "0376a6b574ff46e82e509e90b6cb3693a3dbb577", "url": "https://github.com/wtsi-hgi/python-common/blob/0376a6b574ff46e82e509e90b6cb3693a3dbb577/hgicommon/managers.py#L43-L52", "partition": "valid"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/werkzeug/wrappers.py", "func_name": "BaseRequest.get_data", "original_string": "def get_data(self, cache=True, as_text=False, parse_form_data=False):\n        \"\"\"This reads the buffered incoming data from the client into one\n        bytestring.  By default this is cached but that behavior can be\n        changed by setting `cache` to `False`.\n\n        Usually it's a bad idea to call this method without checking the\n        content length first as a client could send dozens of megabytes or more\n        to cause memory problems on the server.\n\n        Note that if the form data was already parsed this method will not\n        return anything as form data parsing does not cache the data like\n        this method does.  To implicitly invoke form data parsing function\n        set `parse_form_data` to `True`.  When this is done the return value\n        of this method will be an empty string if the form parser handles\n        the data.  This generally is not necessary as if the whole data is\n        cached (which is the default) the form parser will used the cached\n        data to parse the form data.  Please be generally aware of checking\n        the content length first in any case before calling this method\n        to avoid exhausting server memory.\n\n        If `as_text` is set to `True` the return value will be a decoded\n        unicode string.\n\n        .. versionadded:: 0.9\n        \"\"\"\n        rv = getattr(self, '_cached_data', None)\n        if rv is None:\n            if parse_form_data:\n                self._load_form_data()\n            rv = self.stream.read()\n            if cache:\n                self._cached_data = rv\n        if as_text:\n            rv = rv.decode(self.charset, self.encoding_errors)\n        return rv", "language": "python", "code": "def get_data(self, cache=True, as_text=False, parse_form_data=False):\n        \"\"\"This reads the buffered incoming data from the client into one\n        bytestring.  By default this is cached but that behavior can be\n        changed by setting `cache` to `False`.\n\n        Usually it's a bad idea to call this method without checking the\n        content length first as a client could send dozens of megabytes or more\n        to cause memory problems on the server.\n\n        Note that if the form data was already parsed this method will not\n        return anything as form data parsing does not cache the data like\n        this method does.  To implicitly invoke form data parsing function\n        set `parse_form_data` to `True`.  When this is done the return value\n        of this method will be an empty string if the form parser handles\n        the data.  This generally is not necessary as if the whole data is\n        cached (which is the default) the form parser will used the cached\n        data to parse the form data.  Please be generally aware of checking\n        the content length first in any case before calling this method\n        to avoid exhausting server memory.\n\n        If `as_text` is set to `True` the return value will be a decoded\n        unicode string.\n\n        .. versionadded:: 0.9\n        \"\"\"\n        rv = getattr(self, '_cached_data', None)\n        if rv is None:\n            if parse_form_data:\n                self._load_form_data()\n            rv = self.stream.read()\n            if cache:\n                self._cached_data = rv\n        if as_text:\n            rv = rv.decode(self.charset, self.encoding_errors)\n        return rv", "code_tokens": ["def", "get_data", "(", "self", ",", "cache", "=", "True", ",", "as_text", "=", "False", ",", "parse_form_data", "=", "False", ")", ":", "rv", "=", "getattr", "(", "self", ",", "'_cached_data'", ",", "None", ")", "if", "rv", "is", "None", ":", "if", "parse_form_data", ":", "self", ".", "_load_form_data", "(", ")", "rv", "=", "self", ".", "stream", ".", "read", "(", ")", "if", "cache", ":", "self", ".", "_cached_data", "=", "rv", "if", "as_text", ":", "rv", "=", "rv", ".", "decode", "(", "self", ".", "charset", ",", "self", ".", "encoding_errors", ")", "return", "rv"], "docstring": "", "docstring_tokens": [], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/werkzeug/wrappers.py#L444-L478", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/dirichlet_multinomial.py", "func_name": "DirichletMultinomial._variance_scale_term", "original_string": "def _variance_scale_term(self):\n    \"\"\"Helper to `_covariance` and `_variance` which computes a shared scale.\"\"\"\n    # Expand back the last dim so the shape of _variance_scale_term matches the\n    # shape of self.concentration.\n    c0 = self.total_concentration[..., tf.newaxis]\n    return tf.sqrt((1. + c0 / self.total_count[..., tf.newaxis]) / (1. + c0))", "language": "python", "code": "def _variance_scale_term(self):\n    \"\"\"Helper to `_covariance` and `_variance` which computes a shared scale.\"\"\"\n    # Expand back the last dim so the shape of _variance_scale_term matches the\n    # shape of self.concentration.\n    c0 = self.total_concentration[..., tf.newaxis]\n    return tf.sqrt((1. + c0 / self.total_count[..., tf.newaxis]) / (1. + c0))", "code_tokens": ["def", "_variance_scale_term", "(", "self", ")", ":", "# Expand back the last dim so the shape of _variance_scale_term matches the", "# shape of self.concentration.", "c0", "=", "self", ".", "total_concentration", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "tf", ".", "sqrt", "(", "(", "1.", "+", "c0", "/", "self", ".", "total_count", "[", "...", ",", "tf", ".", "newaxis", "]", ")", "/", "(", "1.", "+", "c0", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/dirichlet_multinomial.py#L322-L327", "partition": "test"}
{"repo": "pybel/pybel-tools", "path": "src/pybel_tools/analysis/neurommsig/algorithm.py", "func_name": "get_neurommsig_score", "original_string": "def get_neurommsig_score(graph: BELGraph,\n                         genes: List[Gene],\n                         ora_weight: Optional[float] = None,\n                         hub_weight: Optional[float] = None,\n                         top_percent: Optional[float] = None,\n                         topology_weight: Optional[float] = None) -> float:\n    \"\"\"Calculate the composite NeuroMMSig Score for a given list of genes.\n\n    :param graph: A BEL graph\n    :param genes: A list of gene nodes\n    :param ora_weight: The relative weight of the over-enrichment analysis score from\n     :py:func:`neurommsig_gene_ora`. Defaults to 1.0.\n    :param hub_weight: The relative weight of the hub analysis score from :py:func:`neurommsig_hubs`.\n     Defaults to 1.0.\n    :param top_percent: The percentage of top genes to use as hubs. Defaults to 5% (0.05).\n    :param topology_weight: The relative weight of the topolgical analysis core from\n     :py:func:`neurommsig_topology`. Defaults to 1.0.\n    :return: The NeuroMMSig composite score\n    \"\"\"\n    ora_weight = ora_weight or 1.0\n    hub_weight = hub_weight or 1.0\n    topology_weight = topology_weight or 1.0\n    total_weight = ora_weight + hub_weight + topology_weight\n\n    genes = list(genes)\n\n    ora_score = neurommsig_gene_ora(graph, genes)\n    hub_score = neurommsig_hubs(graph, genes, top_percent=top_percent)\n    topology_score = neurommsig_topology(graph, genes)\n\n    weighted_sum = (\n            ora_weight * ora_score +\n            hub_weight * hub_score +\n            topology_weight * topology_score\n    )\n\n    return weighted_sum / total_weight", "language": "python", "code": "def get_neurommsig_score(graph: BELGraph,\n                         genes: List[Gene],\n                         ora_weight: Optional[float] = None,\n                         hub_weight: Optional[float] = None,\n                         top_percent: Optional[float] = None,\n                         topology_weight: Optional[float] = None) -> float:\n    \"\"\"Calculate the composite NeuroMMSig Score for a given list of genes.\n\n    :param graph: A BEL graph\n    :param genes: A list of gene nodes\n    :param ora_weight: The relative weight of the over-enrichment analysis score from\n     :py:func:`neurommsig_gene_ora`. Defaults to 1.0.\n    :param hub_weight: The relative weight of the hub analysis score from :py:func:`neurommsig_hubs`.\n     Defaults to 1.0.\n    :param top_percent: The percentage of top genes to use as hubs. Defaults to 5% (0.05).\n    :param topology_weight: The relative weight of the topolgical analysis core from\n     :py:func:`neurommsig_topology`. Defaults to 1.0.\n    :return: The NeuroMMSig composite score\n    \"\"\"\n    ora_weight = ora_weight or 1.0\n    hub_weight = hub_weight or 1.0\n    topology_weight = topology_weight or 1.0\n    total_weight = ora_weight + hub_weight + topology_weight\n\n    genes = list(genes)\n\n    ora_score = neurommsig_gene_ora(graph, genes)\n    hub_score = neurommsig_hubs(graph, genes, top_percent=top_percent)\n    topology_score = neurommsig_topology(graph, genes)\n\n    weighted_sum = (\n            ora_weight * ora_score +\n            hub_weight * hub_score +\n            topology_weight * topology_score\n    )\n\n    return weighted_sum / total_weight", "code_tokens": ["def", "get_neurommsig_score", "(", "graph", ":", "BELGraph", ",", "genes", ":", "List", "[", "Gene", "]", ",", "ora_weight", ":", "Optional", "[", "float", "]", "=", "None", ",", "hub_weight", ":", "Optional", "[", "float", "]", "=", "None", ",", "top_percent", ":", "Optional", "[", "float", "]", "=", "None", ",", "topology_weight", ":", "Optional", "[", "float", "]", "=", "None", ")", "->", "float", ":", "ora_weight", "=", "ora_weight", "or", "1.0", "hub_weight", "=", "hub_weight", "or", "1.0", "topology_weight", "=", "topology_weight", "or", "1.0", "total_weight", "=", "ora_weight", "+", "hub_weight", "+", "topology_weight", "genes", "=", "list", "(", "genes", ")", "ora_score", "=", "neurommsig_gene_ora", "(", "graph", ",", "genes", ")", "hub_score", "=", "neurommsig_hubs", "(", "graph", ",", "genes", ",", "top_percent", "=", "top_percent", ")", "topology_score", "=", "neurommsig_topology", "(", "graph", ",", "genes", ")", "weighted_sum", "=", "(", "ora_weight", "*", "ora_score", "+", "hub_weight", "*", "hub_score", "+", "topology_weight", "*", "topology_score", ")", "return", "weighted_sum", "/", "total_weight"], "docstring": "", "docstring_tokens": [], "sha": "3491adea0ac4ee60f57275ef72f9b73da6dbfe0c", "url": "https://github.com/pybel/pybel-tools/blob/3491adea0ac4ee60f57275ef72f9b73da6dbfe0c/src/pybel_tools/analysis/neurommsig/algorithm.py#L127-L163", "partition": "valid"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/itsdangerous.py", "func_name": "base64_encode", "original_string": "def base64_encode(string):\n    \"\"\"base64 encodes a single bytestring (and is tolerant to getting\n    called with a unicode string).\n    The resulting bytestring is safe for putting into URLs.\n    \"\"\"\n    string = want_bytes(string)\n    return base64.urlsafe_b64encode(string).strip(b'=')", "language": "python", "code": "def base64_encode(string):\n    \"\"\"base64 encodes a single bytestring (and is tolerant to getting\n    called with a unicode string).\n    The resulting bytestring is safe for putting into URLs.\n    \"\"\"\n    string = want_bytes(string)\n    return base64.urlsafe_b64encode(string).strip(b'=')", "code_tokens": ["def", "base64_encode", "(", "string", ")", ":", "string", "=", "want_bytes", "(", "string", ")", "return", "base64", ".", "urlsafe_b64encode", "(", "string", ")", ".", "strip", "(", "b'='", ")"], "docstring": "", "docstring_tokens": [], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/itsdangerous.py#L201-L207", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "py2/h2o_ray.py", "func_name": "import_files", "original_string": "def import_files(self, path, timeoutSecs=180):\n    ''' \n    Import a file or files into h2o.  The 'file' parameter accepts a directory or a single file.\n    192.168.0.37:54323/ImportFiles.html?file=%2Fhome%2F0xdiag%2Fdatasets\n    '''\n    a = self.do_json_request('3/ImportFiles.json',\n        timeout=timeoutSecs,\n        params={\"path\": path}\n    )\n    verboseprint(\"\\nimport_files result:\", dump_json(a))\n    h2o_sandbox.check_sandbox_for_errors()\n    return a", "language": "python", "code": "def import_files(self, path, timeoutSecs=180):\n    ''' \n    Import a file or files into h2o.  The 'file' parameter accepts a directory or a single file.\n    192.168.0.37:54323/ImportFiles.html?file=%2Fhome%2F0xdiag%2Fdatasets\n    '''\n    a = self.do_json_request('3/ImportFiles.json',\n        timeout=timeoutSecs,\n        params={\"path\": path}\n    )\n    verboseprint(\"\\nimport_files result:\", dump_json(a))\n    h2o_sandbox.check_sandbox_for_errors()\n    return a", "code_tokens": ["def", "import_files", "(", "self", ",", "path", ",", "timeoutSecs", "=", "180", ")", ":", "a", "=", "self", ".", "do_json_request", "(", "'3/ImportFiles.json'", ",", "timeout", "=", "timeoutSecs", ",", "params", "=", "{", "\"path\"", ":", "path", "}", ")", "verboseprint", "(", "\"\\nimport_files result:\"", ",", "dump_json", "(", "a", ")", ")", "h2o_sandbox", ".", "check_sandbox_for_errors", "(", ")", "return", "a"], "docstring": "", "docstring_tokens": [], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/py2/h2o_ray.py#L82-L93", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/results.py", "func_name": "Analysis.arcs_unpredicted", "original_string": "def arcs_unpredicted(self):\n        \"\"\"Returns a sorted list of the executed arcs missing from the code.\"\"\"\n        possible = self.arc_possibilities()\n        executed = self.arcs_executed()\n        # Exclude arcs here which connect a line to itself.  They can occur\n        # in executed data in some cases.  This is where they can cause\n        # trouble, and here is where it's the least burden to remove them.\n        unpredicted = [\n            e for e in executed\n                if e not in possible\n                    and e[0] != e[1]\n            ]\n        return sorted(unpredicted)", "language": "python", "code": "def arcs_unpredicted(self):\n        \"\"\"Returns a sorted list of the executed arcs missing from the code.\"\"\"\n        possible = self.arc_possibilities()\n        executed = self.arcs_executed()\n        # Exclude arcs here which connect a line to itself.  They can occur\n        # in executed data in some cases.  This is where they can cause\n        # trouble, and here is where it's the least burden to remove them.\n        unpredicted = [\n            e for e in executed\n                if e not in possible\n                    and e[0] != e[1]\n            ]\n        return sorted(unpredicted)", "code_tokens": ["def", "arcs_unpredicted", "(", "self", ")", ":", "possible", "=", "self", ".", "arc_possibilities", "(", ")", "executed", "=", "self", ".", "arcs_executed", "(", ")", "# Exclude arcs here which connect a line to itself.  They can occur", "# in executed data in some cases.  This is where they can cause", "# trouble, and here is where it's the least burden to remove them.", "unpredicted", "=", "[", "e", "for", "e", "in", "executed", "if", "e", "not", "in", "possible", "and", "e", "[", "0", "]", "!=", "e", "[", "1", "]", "]", "return", "sorted", "(", "unpredicted", ")"], "docstring": "", "docstring_tokens": [], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/results.py#L129-L141", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-mgmt-containerregistry/azure/mgmt/containerregistry/container_registry_management_client.py", "func_name": "ContainerRegistryManagementClient.build_steps", "original_string": "def build_steps(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-02-01-preview: :class:`BuildStepsOperations<azure.mgmt.containerregistry.v2018_02_01_preview.operations.BuildStepsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('build_steps')\n        if api_version == '2018-02-01-preview':\n            from .v2018_02_01_preview.operations import BuildStepsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "language": "python", "code": "def build_steps(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-02-01-preview: :class:`BuildStepsOperations<azure.mgmt.containerregistry.v2018_02_01_preview.operations.BuildStepsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('build_steps')\n        if api_version == '2018-02-01-preview':\n            from .v2018_02_01_preview.operations import BuildStepsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "code_tokens": ["def", "build_steps", "(", "self", ")", ":", "api_version", "=", "self", ".", "_get_api_version", "(", "'build_steps'", ")", "if", "api_version", "==", "'2018-02-01-preview'", ":", "from", ".", "v2018_02_01_preview", ".", "operations", "import", "BuildStepsOperations", "as", "OperationClass", "else", ":", "raise", "NotImplementedError", "(", "\"APIVersion {} is not available\"", ".", "format", "(", "api_version", ")", ")", "return", "OperationClass", "(", "self", ".", "_client", ",", "self", ".", "config", ",", "Serializer", "(", "self", ".", "_models_dict", "(", "api_version", ")", ")", ",", "Deserializer", "(", "self", ".", "_models_dict", "(", "api_version", ")", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-containerregistry/azure/mgmt/containerregistry/container_registry_management_client.py#L118-L128", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/config/application.py", "func_name": "Application.generate_config_file", "original_string": "def generate_config_file(self):\n        \"\"\"generate default config file from Configurables\"\"\"\n        lines = [\"# Configuration file for %s.\"%self.name]\n        lines.append('')\n        lines.append('c = get_config()')\n        lines.append('')\n        for cls in self.classes:\n            lines.append(cls.class_config_section())\n        return '\\n'.join(lines)", "language": "python", "code": "def generate_config_file(self):\n        \"\"\"generate default config file from Configurables\"\"\"\n        lines = [\"# Configuration file for %s.\"%self.name]\n        lines.append('')\n        lines.append('c = get_config()')\n        lines.append('')\n        for cls in self.classes:\n            lines.append(cls.class_config_section())\n        return '\\n'.join(lines)", "code_tokens": ["def", "generate_config_file", "(", "self", ")", ":", "lines", "=", "[", "\"# Configuration file for %s.\"", "%", "self", ".", "name", "]", "lines", ".", "append", "(", "''", ")", "lines", ".", "append", "(", "'c = get_config()'", ")", "lines", ".", "append", "(", "''", ")", "for", "cls", "in", "self", ".", "classes", ":", "lines", ".", "append", "(", "cls", ".", "class_config_section", "(", ")", ")", "return", "'\\n'", ".", "join", "(", "lines", ")"], "docstring": "", "docstring_tokens": [], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/config/application.py#L462-L470", "partition": "test"}
{"repo": "buckket/twtxt", "path": "twtxt/parser.py", "func_name": "parse_tweets", "original_string": "def parse_tweets(raw_tweets, source, now=None):\n    \"\"\"\n        Parses a list of raw tweet lines from a twtxt file\n        and returns a list of :class:`Tweet` objects.\n\n        :param list raw_tweets: list of raw tweet lines\n        :param Source source: the source of the given tweets\n        :param Datetime now: the current datetime\n\n        :returns: a list of parsed tweets :class:`Tweet` objects\n        :rtype: list\n    \"\"\"\n    if now is None:\n        now = datetime.now(timezone.utc)\n\n    tweets = []\n    for line in raw_tweets:\n        try:\n            tweet = parse_tweet(line, source, now)\n        except (ValueError, OverflowError) as e:\n            logger.debug(\"{0} - {1}\".format(source.url, e))\n        else:\n            tweets.append(tweet)\n\n    return tweets", "language": "python", "code": "def parse_tweets(raw_tweets, source, now=None):\n    \"\"\"\n        Parses a list of raw tweet lines from a twtxt file\n        and returns a list of :class:`Tweet` objects.\n\n        :param list raw_tweets: list of raw tweet lines\n        :param Source source: the source of the given tweets\n        :param Datetime now: the current datetime\n\n        :returns: a list of parsed tweets :class:`Tweet` objects\n        :rtype: list\n    \"\"\"\n    if now is None:\n        now = datetime.now(timezone.utc)\n\n    tweets = []\n    for line in raw_tweets:\n        try:\n            tweet = parse_tweet(line, source, now)\n        except (ValueError, OverflowError) as e:\n            logger.debug(\"{0} - {1}\".format(source.url, e))\n        else:\n            tweets.append(tweet)\n\n    return tweets", "code_tokens": ["def", "parse_tweets", "(", "raw_tweets", ",", "source", ",", "now", "=", "None", ")", ":", "if", "now", "is", "None", ":", "now", "=", "datetime", ".", "now", "(", "timezone", ".", "utc", ")", "tweets", "=", "[", "]", "for", "line", "in", "raw_tweets", ":", "try", ":", "tweet", "=", "parse_tweet", "(", "line", ",", "source", ",", "now", ")", "except", "(", "ValueError", ",", "OverflowError", ")", "as", "e", ":", "logger", ".", "debug", "(", "\"{0} - {1}\"", ".", "format", "(", "source", ".", "url", ",", "e", ")", ")", "else", ":", "tweets", ".", "append", "(", "tweet", ")", "return", "tweets"], "docstring": "", "docstring_tokens": [], "sha": "6c8ad8ef3cbcf0dd335a12285d8b6bbdf93ce851", "url": "https://github.com/buckket/twtxt/blob/6c8ad8ef3cbcf0dd335a12285d8b6bbdf93ce851/twtxt/parser.py#L32-L56", "partition": "valid"}
{"repo": "google/grumpy", "path": "third_party/stdlib/urlparse.py", "func_name": "urldefrag", "original_string": "def urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n        return defrag, frag\n    else:\n        return url, ''", "language": "python", "code": "def urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n        return defrag, frag\n    else:\n        return url, ''", "code_tokens": ["def", "urldefrag", "(", "url", ")", ":", "if", "'#'", "in", "url", ":", "s", ",", "n", ",", "p", ",", "a", ",", "q", ",", "frag", "=", "urlparse", "(", "url", ")", "defrag", "=", "urlunparse", "(", "(", "s", ",", "n", ",", "p", ",", "a", ",", "q", ",", "''", ")", ")", "return", "defrag", ",", "frag", "else", ":", "return", "url", ",", "''"], "docstring": "", "docstring_tokens": [], "sha": "3ec87959189cfcdeae82eb68a47648ac25ceb10b", "url": "https://github.com/google/grumpy/blob/3ec87959189cfcdeae82eb68a47648ac25ceb10b/third_party/stdlib/urlparse.py#L425-L437", "partition": "valid"}
{"repo": "myint/autoflake", "path": "autoflake.py", "func_name": "filter_unused_variable", "original_string": "def filter_unused_variable(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"\n    if re.match(EXCEPT_REGEX, line):\n        return re.sub(r' as \\w+:$', ':', line, count=1)\n    elif multiline_statement(line, previous_line):\n        return line\n    elif line.count('=') == 1:\n        split_line = line.split('=')\n        assert len(split_line) == 2\n        value = split_line[1].lstrip()\n        if ',' in split_line[0]:\n            return line\n\n        if is_literal_or_name(value):\n            # Rather than removing the line, replace with it \"pass\" to avoid\n            # a possible hanging block with no body.\n            value = 'pass' + get_line_ending(line)\n\n        return get_indentation(line) + value\n    else:\n        return line", "language": "python", "code": "def filter_unused_variable(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"\n    if re.match(EXCEPT_REGEX, line):\n        return re.sub(r' as \\w+:$', ':', line, count=1)\n    elif multiline_statement(line, previous_line):\n        return line\n    elif line.count('=') == 1:\n        split_line = line.split('=')\n        assert len(split_line) == 2\n        value = split_line[1].lstrip()\n        if ',' in split_line[0]:\n            return line\n\n        if is_literal_or_name(value):\n            # Rather than removing the line, replace with it \"pass\" to avoid\n            # a possible hanging block with no body.\n            value = 'pass' + get_line_ending(line)\n\n        return get_indentation(line) + value\n    else:\n        return line", "code_tokens": ["def", "filter_unused_variable", "(", "line", ",", "previous_line", "=", "''", ")", ":", "if", "re", ".", "match", "(", "EXCEPT_REGEX", ",", "line", ")", ":", "return", "re", ".", "sub", "(", "r' as \\w+:$'", ",", "':'", ",", "line", ",", "count", "=", "1", ")", "elif", "multiline_statement", "(", "line", ",", "previous_line", ")", ":", "return", "line", "elif", "line", ".", "count", "(", "'='", ")", "==", "1", ":", "split_line", "=", "line", ".", "split", "(", "'='", ")", "assert", "len", "(", "split_line", ")", "==", "2", "value", "=", "split_line", "[", "1", "]", ".", "lstrip", "(", ")", "if", "','", "in", "split_line", "[", "0", "]", ":", "return", "line", "if", "is_literal_or_name", "(", "value", ")", ":", "# Rather than removing the line, replace with it \"pass\" to avoid", "# a possible hanging block with no body.", "value", "=", "'pass'", "+", "get_line_ending", "(", "line", ")", "return", "get_indentation", "(", "line", ")", "+", "value", "else", ":", "return", "line"], "docstring": "", "docstring_tokens": [], "sha": "68fea68646922b920d55975f9f2adaeafd84df4f", "url": "https://github.com/myint/autoflake/blob/68fea68646922b920d55975f9f2adaeafd84df4f/autoflake.py#L456-L476", "partition": "test"}
{"repo": "armet/python-armet", "path": "armet/http/request.py", "func_name": "Request.read", "original_string": "def read(self, deserialize=False, format=None):\n        \"\"\"Read and return the request data.\n\n        @param[in] deserialize\n            True to deserialize the resultant text using a determiend format\n            or the passed format.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n        \"\"\"\n\n        if deserialize:\n            data, _ = self.deserialize(format=format)\n            return data\n\n        content = self._read()\n\n        if not content:\n            return ''\n\n        if type(content) is six.binary_type:\n            content = content.decode(self.encoding)\n\n        return content", "language": "python", "code": "def read(self, deserialize=False, format=None):\n        \"\"\"Read and return the request data.\n\n        @param[in] deserialize\n            True to deserialize the resultant text using a determiend format\n            or the passed format.\n\n        @param[in] format\n            A specific format to deserialize in; if provided, no detection is\n            done. If not provided, the content-type header is looked at to\n            determine an appropriate deserializer.\n        \"\"\"\n\n        if deserialize:\n            data, _ = self.deserialize(format=format)\n            return data\n\n        content = self._read()\n\n        if not content:\n            return ''\n\n        if type(content) is six.binary_type:\n            content = content.decode(self.encoding)\n\n        return content", "code_tokens": ["def", "read", "(", "self", ",", "deserialize", "=", "False", ",", "format", "=", "None", ")", ":", "if", "deserialize", ":", "data", ",", "_", "=", "self", ".", "deserialize", "(", "format", "=", "format", ")", "return", "data", "content", "=", "self", ".", "_read", "(", ")", "if", "not", "content", ":", "return", "''", "if", "type", "(", "content", ")", "is", "six", ".", "binary_type", ":", "content", "=", "content", ".", "decode", "(", "self", ".", "encoding", ")", "return", "content"], "docstring": "", "docstring_tokens": [], "sha": "d61eca9082256cb1e7f7f3c7f2fbc4b697157de7", "url": "https://github.com/armet/python-armet/blob/d61eca9082256cb1e7f7f3c7f2fbc4b697157de7/armet/http/request.py#L192-L217", "partition": "valid"}
{"repo": "trailofbits/manticore", "path": "manticore/ethereum/detectors.py", "func_name": "DetectIntegerOverflow._signed_add_overflow", "original_string": "def _signed_add_overflow(state, a, b):\n        \"\"\"\n        Sign extend the value to 512 bits and check the result can be represented\n         in 256. Following there is a 32 bit excerpt of this condition:\n\n        a  +  b   -80000000 -3fffffff -00000001 +00000000 +00000001 +3fffffff +7fffffff\n        +80000000     True     True     True    False    False    False    False\n        +c0000001     True    False    False    False    False    False    False\n        +ffffffff     True    False    False    False    False    False    False\n        +00000000    False    False    False    False    False    False    False\n        +00000001    False    False    False    False    False    False     True\n        +3fffffff    False    False    False    False    False    False     True\n        +7fffffff    False    False    False    False     True     True     True\n        \"\"\"\n        add = Operators.SEXTEND(a, 256, 512) + Operators.SEXTEND(b, 256, 512)\n        cond = Operators.OR(add < -(1 << 255), add >= (1 << 255))\n        return cond", "language": "python", "code": "def _signed_add_overflow(state, a, b):\n        \"\"\"\n        Sign extend the value to 512 bits and check the result can be represented\n         in 256. Following there is a 32 bit excerpt of this condition:\n\n        a  +  b   -80000000 -3fffffff -00000001 +00000000 +00000001 +3fffffff +7fffffff\n        +80000000     True     True     True    False    False    False    False\n        +c0000001     True    False    False    False    False    False    False\n        +ffffffff     True    False    False    False    False    False    False\n        +00000000    False    False    False    False    False    False    False\n        +00000001    False    False    False    False    False    False     True\n        +3fffffff    False    False    False    False    False    False     True\n        +7fffffff    False    False    False    False     True     True     True\n        \"\"\"\n        add = Operators.SEXTEND(a, 256, 512) + Operators.SEXTEND(b, 256, 512)\n        cond = Operators.OR(add < -(1 << 255), add >= (1 << 255))\n        return cond", "code_tokens": ["def", "_signed_add_overflow", "(", "state", ",", "a", ",", "b", ")", ":", "add", "=", "Operators", ".", "SEXTEND", "(", "a", ",", "256", ",", "512", ")", "+", "Operators", ".", "SEXTEND", "(", "b", ",", "256", ",", "512", ")", "cond", "=", "Operators", ".", "OR", "(", "add", "<", "-", "(", "1", "<<", "255", ")", ",", "add", ">=", "(", "1", "<<", "255", ")", ")", "return", "cond"], "docstring": "", "docstring_tokens": [], "sha": "54c5a15b1119c523ae54c09972413e8b97f11629", "url": "https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/ethereum/detectors.py#L358-L374", "partition": "valid"}
{"repo": "xtuml/pyxtuml", "path": "xtuml/load.py", "func_name": "ModelLoader.t_LPAREN", "original_string": "def t_LPAREN(self, t):\n        r'\\('\n        t.endlexpos = t.lexpos + len(t.value)\n        return t", "language": "python", "code": "def t_LPAREN(self, t):\n        r'\\('\n        t.endlexpos = t.lexpos + len(t.value)\n        return t", "code_tokens": ["def", "t_LPAREN", "(", "self", ",", "t", ")", ":", "t", ".", "endlexpos", "=", "t", ".", "lexpos", "+", "len", "(", "t", ".", "value", ")", "return", "t"], "docstring": "", "docstring_tokens": [], "sha": "7dd9343b9a0191d1db1887ab9288d0a026608d9a", "url": "https://github.com/xtuml/pyxtuml/blob/7dd9343b9a0191d1db1887ab9288d0a026608d9a/xtuml/load.py#L477-L480", "partition": "test"}
{"repo": "memsql/memsql-python", "path": "memsql/common/sql_step_queue/queue.py", "func_name": "SQLStepQueue.start", "original_string": "def start(self, block=False, timeout=None, retry_interval=0.5, extra_predicate=None):\n        \"\"\"\n        Retrieve a task handler from the queue.\n\n        If block is True, this function will block until it is able to retrieve a task.\n        If block is True and timeout is a number it will block for at most <timeout> seconds.\n        retry_interval is the maximum time in seconds between successive retries.\n\n        extra_predicate\n        If extra_predicate is defined, it should be a tuple of (raw_predicate, predicate_args)\n        raw_predicate will be prefixed by AND, and inserted into the WHERE condition in the queries.\n        predicate_args will be sql escaped and formatted into raw_predicate.\n        \"\"\"\n        start = time.time()\n        while 1:\n            task_handler = self._dequeue_task(extra_predicate)\n            if task_handler is None and block:\n                if timeout is not None and (time.time() - start) > timeout:\n                    break\n                time.sleep(retry_interval * (random.random() + 0.1))\n            else:\n                break\n        return task_handler", "language": "python", "code": "def start(self, block=False, timeout=None, retry_interval=0.5, extra_predicate=None):\n        \"\"\"\n        Retrieve a task handler from the queue.\n\n        If block is True, this function will block until it is able to retrieve a task.\n        If block is True and timeout is a number it will block for at most <timeout> seconds.\n        retry_interval is the maximum time in seconds between successive retries.\n\n        extra_predicate\n        If extra_predicate is defined, it should be a tuple of (raw_predicate, predicate_args)\n        raw_predicate will be prefixed by AND, and inserted into the WHERE condition in the queries.\n        predicate_args will be sql escaped and formatted into raw_predicate.\n        \"\"\"\n        start = time.time()\n        while 1:\n            task_handler = self._dequeue_task(extra_predicate)\n            if task_handler is None and block:\n                if timeout is not None and (time.time() - start) > timeout:\n                    break\n                time.sleep(retry_interval * (random.random() + 0.1))\n            else:\n                break\n        return task_handler", "code_tokens": ["def", "start", "(", "self", ",", "block", "=", "False", ",", "timeout", "=", "None", ",", "retry_interval", "=", "0.5", ",", "extra_predicate", "=", "None", ")", ":", "start", "=", "time", ".", "time", "(", ")", "while", "1", ":", "task_handler", "=", "self", ".", "_dequeue_task", "(", "extra_predicate", ")", "if", "task_handler", "is", "None", "and", "block", ":", "if", "timeout", "is", "not", "None", "and", "(", "time", ".", "time", "(", ")", "-", "start", ")", ">", "timeout", ":", "break", "time", ".", "sleep", "(", "retry_interval", "*", "(", "random", ".", "random", "(", ")", "+", "0.1", ")", ")", "else", ":", "break", "return", "task_handler"], "docstring": "", "docstring_tokens": [], "sha": "aac223a1b937d5b348b42af3c601a6c685ca633a", "url": "https://github.com/memsql/memsql-python/blob/aac223a1b937d5b348b42af3c601a6c685ca633a/memsql/common/sql_step_queue/queue.py#L65-L87", "partition": "test"}
{"repo": "quantopian/pyfolio", "path": "pyfolio/risk.py", "func_name": "plot_sector_exposures_net", "original_string": "def plot_sector_exposures_net(net_exposures, sector_dict=None, ax=None):\n    \"\"\"\n    Plots output of compute_sector_exposures as line graphs\n\n    Parameters\n    ----------\n    net_exposures : arrays\n        Arrays of net sector exposures (output of compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if sector_dict is None:\n        sector_names = SECTORS.values()\n    else:\n        sector_names = sector_dict.values()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))\n\n    for i in range(len(net_exposures)):\n        ax.plot(net_exposures[i], color=color_list[i], alpha=0.8,\n                label=sector_names[i])\n    ax.set(title='Net exposures to sectors',\n           ylabel='Proportion of net exposure \\n in sectors')\n\n    return ax", "language": "python", "code": "def plot_sector_exposures_net(net_exposures, sector_dict=None, ax=None):\n    \"\"\"\n    Plots output of compute_sector_exposures as line graphs\n\n    Parameters\n    ----------\n    net_exposures : arrays\n        Arrays of net sector exposures (output of compute_sector_exposures).\n\n    sector_dict : dict or OrderedDict\n        Dictionary of all sectors\n        - See full description in compute_sector_exposures\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if sector_dict is None:\n        sector_names = SECTORS.values()\n    else:\n        sector_names = sector_dict.values()\n\n    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))\n\n    for i in range(len(net_exposures)):\n        ax.plot(net_exposures[i], color=color_list[i], alpha=0.8,\n                label=sector_names[i])\n    ax.set(title='Net exposures to sectors',\n           ylabel='Proportion of net exposure \\n in sectors')\n\n    return ax", "code_tokens": ["def", "plot_sector_exposures_net", "(", "net_exposures", ",", "sector_dict", "=", "None", ",", "ax", "=", "None", ")", ":", "if", "ax", "is", "None", ":", "ax", "=", "plt", ".", "gca", "(", ")", "if", "sector_dict", "is", "None", ":", "sector_names", "=", "SECTORS", ".", "values", "(", ")", "else", ":", "sector_names", "=", "sector_dict", ".", "values", "(", ")", "color_list", "=", "plt", ".", "cm", ".", "gist_rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "11", ")", ")", "for", "i", "in", "range", "(", "len", "(", "net_exposures", ")", ")", ":", "ax", ".", "plot", "(", "net_exposures", "[", "i", "]", ",", "color", "=", "color_list", "[", "i", "]", ",", "alpha", "=", "0.8", ",", "label", "=", "sector_names", "[", "i", "]", ")", "ax", ".", "set", "(", "title", "=", "'Net exposures to sectors'", ",", "ylabel", "=", "'Proportion of net exposure \\n in sectors'", ")", "return", "ax"], "docstring": "", "docstring_tokens": [], "sha": "712716ab0cdebbec9fabb25eea3bf40e4354749d", "url": "https://github.com/quantopian/pyfolio/blob/712716ab0cdebbec9fabb25eea3bf40e4354749d/pyfolio/risk.py#L247-L277", "partition": "valid"}
{"repo": "librosa/librosa", "path": "librosa/core/constantq.py", "func_name": "__cqt_filter_fft", "original_string": "def __cqt_filter_fft(sr, fmin, n_bins, bins_per_octave, tuning,\n                     filter_scale, norm, sparsity, hop_length=None,\n                     window='hann'):\n    '''Generate the frequency domain constant-Q filter basis.'''\n\n    basis, lengths = filters.constant_q(sr,\n                                        fmin=fmin,\n                                        n_bins=n_bins,\n                                        bins_per_octave=bins_per_octave,\n                                        tuning=tuning,\n                                        filter_scale=filter_scale,\n                                        norm=norm,\n                                        pad_fft=True,\n                                        window=window)\n\n    # Filters are padded up to the nearest integral power of 2\n    n_fft = basis.shape[1]\n\n    if (hop_length is not None and\n            n_fft < 2.0**(1 + np.ceil(np.log2(hop_length)))):\n\n        n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))\n\n    # re-normalize bases with respect to the FFT window length\n    basis *= lengths[:, np.newaxis] / float(n_fft)\n\n    # FFT and retain only the non-negative frequencies\n    fft = get_fftlib()\n    fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, :(n_fft // 2)+1]\n\n    # sparsify the basis\n    fft_basis = util.sparsify_rows(fft_basis, quantile=sparsity)\n\n    return fft_basis, n_fft, lengths", "language": "python", "code": "def __cqt_filter_fft(sr, fmin, n_bins, bins_per_octave, tuning,\n                     filter_scale, norm, sparsity, hop_length=None,\n                     window='hann'):\n    '''Generate the frequency domain constant-Q filter basis.'''\n\n    basis, lengths = filters.constant_q(sr,\n                                        fmin=fmin,\n                                        n_bins=n_bins,\n                                        bins_per_octave=bins_per_octave,\n                                        tuning=tuning,\n                                        filter_scale=filter_scale,\n                                        norm=norm,\n                                        pad_fft=True,\n                                        window=window)\n\n    # Filters are padded up to the nearest integral power of 2\n    n_fft = basis.shape[1]\n\n    if (hop_length is not None and\n            n_fft < 2.0**(1 + np.ceil(np.log2(hop_length)))):\n\n        n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))\n\n    # re-normalize bases with respect to the FFT window length\n    basis *= lengths[:, np.newaxis] / float(n_fft)\n\n    # FFT and retain only the non-negative frequencies\n    fft = get_fftlib()\n    fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, :(n_fft // 2)+1]\n\n    # sparsify the basis\n    fft_basis = util.sparsify_rows(fft_basis, quantile=sparsity)\n\n    return fft_basis, n_fft, lengths", "code_tokens": ["def", "__cqt_filter_fft", "(", "sr", ",", "fmin", ",", "n_bins", ",", "bins_per_octave", ",", "tuning", ",", "filter_scale", ",", "norm", ",", "sparsity", ",", "hop_length", "=", "None", ",", "window", "=", "'hann'", ")", ":", "basis", ",", "lengths", "=", "filters", ".", "constant_q", "(", "sr", ",", "fmin", "=", "fmin", ",", "n_bins", "=", "n_bins", ",", "bins_per_octave", "=", "bins_per_octave", ",", "tuning", "=", "tuning", ",", "filter_scale", "=", "filter_scale", ",", "norm", "=", "norm", ",", "pad_fft", "=", "True", ",", "window", "=", "window", ")", "# Filters are padded up to the nearest integral power of 2", "n_fft", "=", "basis", ".", "shape", "[", "1", "]", "if", "(", "hop_length", "is", "not", "None", "and", "n_fft", "<", "2.0", "**", "(", "1", "+", "np", ".", "ceil", "(", "np", ".", "log2", "(", "hop_length", ")", ")", ")", ")", ":", "n_fft", "=", "int", "(", "2.0", "**", "(", "1", "+", "np", ".", "ceil", "(", "np", ".", "log2", "(", "hop_length", ")", ")", ")", ")", "# re-normalize bases with respect to the FFT window length", "basis", "*=", "lengths", "[", ":", ",", "np", ".", "newaxis", "]", "/", "float", "(", "n_fft", ")", "# FFT and retain only the non-negative frequencies", "fft", "=", "get_fftlib", "(", ")", "fft_basis", "=", "fft", ".", "fft", "(", "basis", ",", "n", "=", "n_fft", ",", "axis", "=", "1", ")", "[", ":", ",", ":", "(", "n_fft", "//", "2", ")", "+", "1", "]", "# sparsify the basis", "fft_basis", "=", "util", ".", "sparsify_rows", "(", "fft_basis", ",", "quantile", "=", "sparsity", ")", "return", "fft_basis", ",", "n_fft", ",", "lengths"], "docstring": "", "docstring_tokens": [], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/constantq.py#L707-L740", "partition": "test"}
{"repo": "numenta/nupic", "path": "src/nupic/data/utils.py", "func_name": "parseBool", "original_string": "def parseBool(s):\n  \"\"\"\n  String to boolean\n\n  :param s: (string)\n  :return: (bool)\n  \"\"\"\n  l = s.lower()\n  if l in (\"true\", \"t\", \"1\"):\n    return True\n  if l in (\"false\", \"f\", \"0\"):\n    return False\n  raise Exception(\"Unable to convert string '%s' to a boolean value\" % s)", "language": "python", "code": "def parseBool(s):\n  \"\"\"\n  String to boolean\n\n  :param s: (string)\n  :return: (bool)\n  \"\"\"\n  l = s.lower()\n  if l in (\"true\", \"t\", \"1\"):\n    return True\n  if l in (\"false\", \"f\", \"0\"):\n    return False\n  raise Exception(\"Unable to convert string '%s' to a boolean value\" % s)", "code_tokens": ["def", "parseBool", "(", "s", ")", ":", "l", "=", "s", ".", "lower", "(", ")", "if", "l", "in", "(", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", ":", "return", "True", "if", "l", "in", "(", "\"false\"", ",", "\"f\"", ",", "\"0\"", ")", ":", "return", "False", "raise", "Exception", "(", "\"Unable to convert string '%s' to a boolean value\"", "%", "s", ")"], "docstring": "", "docstring_tokens": [], "sha": "5922fafffdccc8812e72b3324965ad2f7d4bbdad", "url": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/data/utils.py#L95-L107", "partition": "valid"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/mainwindow.py", "func_name": "MainWindow.find_slave_widgets", "original_string": "def find_slave_widgets(self,tab):\n        \"\"\"return all the frontends that do not own the kernel attached to the given widget/tab.\n\n            Only find frontends owned by the current application. Selection\n            based on connection file of the kernel.\n\n            This function does the conversion tabNumber/widget if needed.\n        \"\"\"\n        #convert from/to int/richIpythonWidget if needed\n        if isinstance(tab, int):\n            tab = self.tab_widget.widget(tab)\n        km=tab.kernel_manager\n\n        #build list of all widgets\n        widget_list = [self.tab_widget.widget(i) for i in range(self.tab_widget.count())]\n\n        # widget that are candidate not to be the owner of the kernel does have all the same port of the curent widget\n        filtered_widget_list = ( widget for widget in widget_list if\n                                widget.kernel_manager.connection_file == km.connection_file)\n        # Get a list of all widget owning the same kernel and removed it from\n        # the previous cadidate. (better using sets ?)\n        master_widget_list = self.find_master_tab(tab, as_list=True)\n        slave_list = [widget for widget in filtered_widget_list if widget not in master_widget_list]\n\n        return slave_list", "language": "python", "code": "def find_slave_widgets(self,tab):\n        \"\"\"return all the frontends that do not own the kernel attached to the given widget/tab.\n\n            Only find frontends owned by the current application. Selection\n            based on connection file of the kernel.\n\n            This function does the conversion tabNumber/widget if needed.\n        \"\"\"\n        #convert from/to int/richIpythonWidget if needed\n        if isinstance(tab, int):\n            tab = self.tab_widget.widget(tab)\n        km=tab.kernel_manager\n\n        #build list of all widgets\n        widget_list = [self.tab_widget.widget(i) for i in range(self.tab_widget.count())]\n\n        # widget that are candidate not to be the owner of the kernel does have all the same port of the curent widget\n        filtered_widget_list = ( widget for widget in widget_list if\n                                widget.kernel_manager.connection_file == km.connection_file)\n        # Get a list of all widget owning the same kernel and removed it from\n        # the previous cadidate. (better using sets ?)\n        master_widget_list = self.find_master_tab(tab, as_list=True)\n        slave_list = [widget for widget in filtered_widget_list if widget not in master_widget_list]\n\n        return slave_list", "code_tokens": ["def", "find_slave_widgets", "(", "self", ",", "tab", ")", ":", "#convert from/to int/richIpythonWidget if needed", "if", "isinstance", "(", "tab", ",", "int", ")", ":", "tab", "=", "self", ".", "tab_widget", ".", "widget", "(", "tab", ")", "km", "=", "tab", ".", "kernel_manager", "#build list of all widgets", "widget_list", "=", "[", "self", ".", "tab_widget", ".", "widget", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "tab_widget", ".", "count", "(", ")", ")", "]", "# widget that are candidate not to be the owner of the kernel does have all the same port of the curent widget", "filtered_widget_list", "=", "(", "widget", "for", "widget", "in", "widget_list", "if", "widget", ".", "kernel_manager", ".", "connection_file", "==", "km", ".", "connection_file", ")", "# Get a list of all widget owning the same kernel and removed it from", "# the previous cadidate. (better using sets ?)", "master_widget_list", "=", "self", ".", "find_master_tab", "(", "tab", ",", "as_list", "=", "True", ")", "slave_list", "=", "[", "widget", "for", "widget", "in", "filtered_widget_list", "if", "widget", "not", "in", "master_widget_list", "]", "return", "slave_list"], "docstring": "", "docstring_tokens": [], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/mainwindow.py#L307-L331", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._prm_store_from_dict", "original_string": "def _prm_store_from_dict(self, fullname, store_dict, hdf5_group, store_flags, kwargs):\n        \"\"\"Stores a `store_dict`\"\"\"\n        for key, data_to_store in store_dict.items():\n            # self._logger.log(1, 'SUB-Storing %s [%s]', key, str(store_dict[key]))\n            original_hdf5_group = None\n\n            flag = store_flags[key]\n\n            if '.' in key:\n                original_hdf5_group = hdf5_group\n                split_key = key.split('.')\n                key = split_key.pop()\n                for inner_key in split_key:\n                    hdf5_group, newly_created =  self._all_create_or_get_group(inner_key,\n                                                                                hdf5_group)\n                    if newly_created:\n                        setattr(hdf5_group._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                                HDF5StorageService.NESTED_GROUP)\n                    else:\n                        store_type = self._all_get_from_attrs(hdf5_group, HDF5StorageService.STORAGE_TYPE)\n                        if store_type != HDF5StorageService.NESTED_GROUP:\n                            raise ValueError('You want to nested results but `%s` is already '\n                                             'of type `%s`!' % (hdf5_group._v_name, store_type))\n\n            # Iterate through the data and store according to the storage flags\n            if key in hdf5_group:\n                # We won't change any data that is found on disk\n                self._logger.debug(\n                    'Found %s already in hdf5 node of %s, so I will ignore it.' %\n                    (key, fullname))\n                continue\n\n            if flag == HDF5StorageService.TABLE:\n                # self._logger.log(1, 'SUB-Storing %s TABLE', key)\n                self._prm_write_into_pytable(key, data_to_store, hdf5_group, fullname,\n                                             **kwargs)\n            elif flag == HDF5StorageService.DICT:\n                # self._logger.log(1, 'SUB-Storing %s DICT', key)\n                self._prm_write_dict_as_table(key, data_to_store, hdf5_group, fullname,\n                                              **kwargs)\n            elif flag == HDF5StorageService.ARRAY:\n                # self._logger.log(1, 'SUB-Storing %s ARRAY', key)\n                self._prm_write_into_array(key, data_to_store, hdf5_group, fullname,\n                                           **kwargs)\n            elif flag in (HDF5StorageService.CARRAY,\n                          HDF5StorageService.EARRAY,\n                          HDF5StorageService.VLARRAY):\n                self._prm_write_into_other_array(key, data_to_store,\n                                                 hdf5_group, fullname,\n                                                 flag=flag, **kwargs)\n            elif flag in (HDF5StorageService.SERIES,\n                          HDF5StorageService.FRAME,\n                          #  HDF5StorageService.PANEL\n                          ):\n                # self._logger.log(1, 'SUB-Storing %s PANDAS', key)\n                self._prm_write_pandas_data(key, data_to_store, hdf5_group, fullname,\n                                            flag, **kwargs)\n            elif flag == HDF5StorageService.SHARED_DATA:\n                pass  # Shared data needs to be explicitly created and is not stored on\n                # the fly\n            else:\n                raise RuntimeError('You shall not pass!')\n\n            if original_hdf5_group is not None:\n                hdf5_group = original_hdf5_group", "language": "python", "code": "def _prm_store_from_dict(self, fullname, store_dict, hdf5_group, store_flags, kwargs):\n        \"\"\"Stores a `store_dict`\"\"\"\n        for key, data_to_store in store_dict.items():\n            # self._logger.log(1, 'SUB-Storing %s [%s]', key, str(store_dict[key]))\n            original_hdf5_group = None\n\n            flag = store_flags[key]\n\n            if '.' in key:\n                original_hdf5_group = hdf5_group\n                split_key = key.split('.')\n                key = split_key.pop()\n                for inner_key in split_key:\n                    hdf5_group, newly_created =  self._all_create_or_get_group(inner_key,\n                                                                                hdf5_group)\n                    if newly_created:\n                        setattr(hdf5_group._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                                HDF5StorageService.NESTED_GROUP)\n                    else:\n                        store_type = self._all_get_from_attrs(hdf5_group, HDF5StorageService.STORAGE_TYPE)\n                        if store_type != HDF5StorageService.NESTED_GROUP:\n                            raise ValueError('You want to nested results but `%s` is already '\n                                             'of type `%s`!' % (hdf5_group._v_name, store_type))\n\n            # Iterate through the data and store according to the storage flags\n            if key in hdf5_group:\n                # We won't change any data that is found on disk\n                self._logger.debug(\n                    'Found %s already in hdf5 node of %s, so I will ignore it.' %\n                    (key, fullname))\n                continue\n\n            if flag == HDF5StorageService.TABLE:\n                # self._logger.log(1, 'SUB-Storing %s TABLE', key)\n                self._prm_write_into_pytable(key, data_to_store, hdf5_group, fullname,\n                                             **kwargs)\n            elif flag == HDF5StorageService.DICT:\n                # self._logger.log(1, 'SUB-Storing %s DICT', key)\n                self._prm_write_dict_as_table(key, data_to_store, hdf5_group, fullname,\n                                              **kwargs)\n            elif flag == HDF5StorageService.ARRAY:\n                # self._logger.log(1, 'SUB-Storing %s ARRAY', key)\n                self._prm_write_into_array(key, data_to_store, hdf5_group, fullname,\n                                           **kwargs)\n            elif flag in (HDF5StorageService.CARRAY,\n                          HDF5StorageService.EARRAY,\n                          HDF5StorageService.VLARRAY):\n                self._prm_write_into_other_array(key, data_to_store,\n                                                 hdf5_group, fullname,\n                                                 flag=flag, **kwargs)\n            elif flag in (HDF5StorageService.SERIES,\n                          HDF5StorageService.FRAME,\n                          #  HDF5StorageService.PANEL\n                          ):\n                # self._logger.log(1, 'SUB-Storing %s PANDAS', key)\n                self._prm_write_pandas_data(key, data_to_store, hdf5_group, fullname,\n                                            flag, **kwargs)\n            elif flag == HDF5StorageService.SHARED_DATA:\n                pass  # Shared data needs to be explicitly created and is not stored on\n                # the fly\n            else:\n                raise RuntimeError('You shall not pass!')\n\n            if original_hdf5_group is not None:\n                hdf5_group = original_hdf5_group", "code_tokens": ["def", "_prm_store_from_dict", "(", "self", ",", "fullname", ",", "store_dict", ",", "hdf5_group", ",", "store_flags", ",", "kwargs", ")", ":", "for", "key", ",", "data_to_store", "in", "store_dict", ".", "items", "(", ")", ":", "# self._logger.log(1, 'SUB-Storing %s [%s]', key, str(store_dict[key]))", "original_hdf5_group", "=", "None", "flag", "=", "store_flags", "[", "key", "]", "if", "'.'", "in", "key", ":", "original_hdf5_group", "=", "hdf5_group", "split_key", "=", "key", ".", "split", "(", "'.'", ")", "key", "=", "split_key", ".", "pop", "(", ")", "for", "inner_key", "in", "split_key", ":", "hdf5_group", ",", "newly_created", "=", "self", ".", "_all_create_or_get_group", "(", "inner_key", ",", "hdf5_group", ")", "if", "newly_created", ":", "setattr", "(", "hdf5_group", ".", "_v_attrs", ",", "HDF5StorageService", ".", "STORAGE_TYPE", ",", "HDF5StorageService", ".", "NESTED_GROUP", ")", "else", ":", "store_type", "=", "self", ".", "_all_get_from_attrs", "(", "hdf5_group", ",", "HDF5StorageService", ".", "STORAGE_TYPE", ")", "if", "store_type", "!=", "HDF5StorageService", ".", "NESTED_GROUP", ":", "raise", "ValueError", "(", "'You want to nested results but `%s` is already '", "'of type `%s`!'", "%", "(", "hdf5_group", ".", "_v_name", ",", "store_type", ")", ")", "# Iterate through the data and store according to the storage flags", "if", "key", "in", "hdf5_group", ":", "# We won't change any data that is found on disk", "self", ".", "_logger", ".", "debug", "(", "'Found %s already in hdf5 node of %s, so I will ignore it.'", "%", "(", "key", ",", "fullname", ")", ")", "continue", "if", "flag", "==", "HDF5StorageService", ".", "TABLE", ":", "# self._logger.log(1, 'SUB-Storing %s TABLE', key)", "self", ".", "_prm_write_into_pytable", "(", "key", ",", "data_to_store", ",", "hdf5_group", ",", "fullname", ",", "*", "*", "kwargs", ")", "elif", "flag", "==", "HDF5StorageService", ".", "DICT", ":", "# self._logger.log(1, 'SUB-Storing %s DICT', key)", "self", ".", "_prm_write_dict_as_table", "(", "key", ",", "data_to_store", ",", "hdf5_group", ",", "fullname", ",", "*", "*", "kwargs", ")", "elif", "flag", "==", "HDF5StorageService", ".", "ARRAY", ":", "# self._logger.log(1, 'SUB-Storing %s ARRAY', key)", "self", ".", "_prm_write_into_array", "(", "key", ",", "data_to_store", ",", "hdf5_group", ",", "fullname", ",", "*", "*", "kwargs", ")", "elif", "flag", "in", "(", "HDF5StorageService", ".", "CARRAY", ",", "HDF5StorageService", ".", "EARRAY", ",", "HDF5StorageService", ".", "VLARRAY", ")", ":", "self", ".", "_prm_write_into_other_array", "(", "key", ",", "data_to_store", ",", "hdf5_group", ",", "fullname", ",", "flag", "=", "flag", ",", "*", "*", "kwargs", ")", "elif", "flag", "in", "(", "HDF5StorageService", ".", "SERIES", ",", "HDF5StorageService", ".", "FRAME", ",", "#  HDF5StorageService.PANEL", ")", ":", "# self._logger.log(1, 'SUB-Storing %s PANDAS', key)", "self", ".", "_prm_write_pandas_data", "(", "key", ",", "data_to_store", ",", "hdf5_group", ",", "fullname", ",", "flag", ",", "*", "*", "kwargs", ")", "elif", "flag", "==", "HDF5StorageService", ".", "SHARED_DATA", ":", "pass", "# Shared data needs to be explicitly created and is not stored on", "# the fly", "else", ":", "raise", "RuntimeError", "(", "'You shall not pass!'", ")", "if", "original_hdf5_group", "is", "not", "None", ":", "hdf5_group", "=", "original_hdf5_group"], "docstring": "", "docstring_tokens": [], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L3782-L3846", "partition": "test"}
{"repo": "authomatic/authomatic", "path": "authomatic/providers/__init__.py", "func_name": "BaseProvider._http_status_in_category", "original_string": "def _http_status_in_category(status, category):\n        \"\"\"\n        Checks whether a HTTP status code is in the category denoted by the\n        hundreds digit.\n        \"\"\"\n\n        assert category < 10, 'HTTP status category must be a one-digit int!'\n        cat = category * 100\n        return status >= cat and status < cat + 100", "language": "python", "code": "def _http_status_in_category(status, category):\n        \"\"\"\n        Checks whether a HTTP status code is in the category denoted by the\n        hundreds digit.\n        \"\"\"\n\n        assert category < 10, 'HTTP status category must be a one-digit int!'\n        cat = category * 100\n        return status >= cat and status < cat + 100", "code_tokens": ["def", "_http_status_in_category", "(", "status", ",", "category", ")", ":", "assert", "category", "<", "10", ",", "'HTTP status category must be a one-digit int!'", "cat", "=", "category", "*", "100", "return", "status", ">=", "cat", "and", "status", "<", "cat", "+", "100"], "docstring": "", "docstring_tokens": [], "sha": "90a9ce60cc405ae8a2bf5c3713acd5d78579a04e", "url": "https://github.com/authomatic/authomatic/blob/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e/authomatic/providers/__init__.py#L533-L541", "partition": "test"}
{"repo": "ManiacalLabs/BiblioPixel", "path": "bibliopixel/builder/description.py", "func_name": "Description.clear", "original_string": "def clear(self):\n        \"\"\"Clear description to default values\"\"\"\n        self._desc = {}\n        for key, value in merge.DEFAULT_PROJECT.items():\n            if key not in self._HIDDEN:\n                self._desc[key] = type(value)()", "language": "python", "code": "def clear(self):\n        \"\"\"Clear description to default values\"\"\"\n        self._desc = {}\n        for key, value in merge.DEFAULT_PROJECT.items():\n            if key not in self._HIDDEN:\n                self._desc[key] = type(value)()", "code_tokens": ["def", "clear", "(", "self", ")", ":", "self", ".", "_desc", "=", "{", "}", "for", "key", ",", "value", "in", "merge", ".", "DEFAULT_PROJECT", ".", "items", "(", ")", ":", "if", "key", "not", "in", "self", ".", "_HIDDEN", ":", "self", ".", "_desc", "[", "key", "]", "=", "type", "(", "value", ")", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "fd97e6c651a4bbcade64733847f4eec8f7704b7c", "url": "https://github.com/ManiacalLabs/BiblioPixel/blob/fd97e6c651a4bbcade64733847f4eec8f7704b7c/bibliopixel/builder/description.py#L20-L25", "partition": "valid"}
{"repo": "refenv/cijoe", "path": "deprecated/modules/cij/liblight.py", "func_name": "Nvm.vblk_write", "original_string": "def vblk_write(self, address, meta=False):\n        \"\"\"nvm_vblk write\"\"\"\n        cmd = list()\n        if meta:\n            cmd.append(\"NVM_CLI_META_MODE=1\")\n        cmd += [\"nvm_vblk write\", self.envs[\"DEV_PATH\"], \"0x%x\" % address]\n        status, _, _ = cij.ssh.command(cmd, shell=True)\n        return status", "language": "python", "code": "def vblk_write(self, address, meta=False):\n        \"\"\"nvm_vblk write\"\"\"\n        cmd = list()\n        if meta:\n            cmd.append(\"NVM_CLI_META_MODE=1\")\n        cmd += [\"nvm_vblk write\", self.envs[\"DEV_PATH\"], \"0x%x\" % address]\n        status, _, _ = cij.ssh.command(cmd, shell=True)\n        return status", "code_tokens": ["def", "vblk_write", "(", "self", ",", "address", ",", "meta", "=", "False", ")", ":", "cmd", "=", "list", "(", ")", "if", "meta", ":", "cmd", ".", "append", "(", "\"NVM_CLI_META_MODE=1\"", ")", "cmd", "+=", "[", "\"nvm_vblk write\"", ",", "self", ".", "envs", "[", "\"DEV_PATH\"", "]", ",", "\"0x%x\"", "%", "address", "]", "status", ",", "_", ",", "_", "=", "cij", ".", "ssh", ".", "command", "(", "cmd", ",", "shell", "=", "True", ")", "return", "status"], "docstring": "", "docstring_tokens": [], "sha": "21d7b2ed4ff68e0a1457e7df2db27f6334f1a379", "url": "https://github.com/refenv/cijoe/blob/21d7b2ed4ff68e0a1457e7df2db27f6334f1a379/deprecated/modules/cij/liblight.py#L135-L142", "partition": "valid"}
{"repo": "Microsoft/botbuilder-python", "path": "libraries/botbuilder-core/botbuilder/core/bot_telemetry_client.py", "func_name": "BotTelemetryClient.track_exception", "original_string": "def track_exception(self, type: type = None, value : Exception =None, tb : traceback =None, \n                        properties: Dict[str, object]=None, measurements: Dict[str, object]=None) -> None:\n        \"\"\" \n        Send information about a single exception that occurred in the application.\n        :param type: the type of the exception that was thrown.\n        :param value: the exception that the client wants to send.\n        :param tb: the traceback information as returned by :func:`sys.exc_info`.\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        \"\"\"\n        raise NotImplementedError('BotTelemetryClient.track_request(): is not implemented.')", "language": "python", "code": "def track_exception(self, type: type = None, value : Exception =None, tb : traceback =None, \n                        properties: Dict[str, object]=None, measurements: Dict[str, object]=None) -> None:\n        \"\"\" \n        Send information about a single exception that occurred in the application.\n        :param type: the type of the exception that was thrown.\n        :param value: the exception that the client wants to send.\n        :param tb: the traceback information as returned by :func:`sys.exc_info`.\n        :param properties: the set of custom properties the client wants attached to this data item. (defaults to: None)\n        :param measurements: the set of custom measurements the client wants to attach to this data item. (defaults to: None)\n        \"\"\"\n        raise NotImplementedError('BotTelemetryClient.track_request(): is not implemented.')", "code_tokens": ["def", "track_exception", "(", "self", ",", "type", ":", "type", "=", "None", ",", "value", ":", "Exception", "=", "None", ",", "tb", ":", "traceback", "=", "None", ",", "properties", ":", "Dict", "[", "str", ",", "object", "]", "=", "None", ",", "measurements", ":", "Dict", "[", "str", ",", "object", "]", "=", "None", ")", "->", "None", ":", "raise", "NotImplementedError", "(", "'BotTelemetryClient.track_request(): is not implemented.'", ")"], "docstring": "", "docstring_tokens": [], "sha": "274663dd91c811bae6ac4488915ba5880771b0a7", "url": "https://github.com/Microsoft/botbuilder-python/blob/274663dd91c811bae6ac4488915ba5880771b0a7/libraries/botbuilder-core/botbuilder/core/bot_telemetry_client.py#L29-L39", "partition": "test"}
{"repo": "pmacosta/peng", "path": "docs/support/ptypes.py", "func_name": "engineering_notation_number", "original_string": "def engineering_notation_number(obj):\n    r\"\"\"\n    Validate if an object is an :ref:`EngineeringNotationNumber` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the name of the argument the\n     contract is attached to\n\n    :rtype: None\n    \"\"\"\n    try:\n        obj = obj.rstrip()\n        float(obj[:-1] if obj[-1] in _SUFFIX_TUPLE else obj)\n        return None\n    except (AttributeError, IndexError, ValueError):\n        # AttributeError: obj.rstrip(), object could not be a string\n        # IndexError: obj[-1], when an empty string\n        # ValueError: float(), when not a string representing a number\n        raise ValueError(pexdoc.pcontracts.get_exdesc())", "language": "python", "code": "def engineering_notation_number(obj):\n    r\"\"\"\n    Validate if an object is an :ref:`EngineeringNotationNumber` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the name of the argument the\n     contract is attached to\n\n    :rtype: None\n    \"\"\"\n    try:\n        obj = obj.rstrip()\n        float(obj[:-1] if obj[-1] in _SUFFIX_TUPLE else obj)\n        return None\n    except (AttributeError, IndexError, ValueError):\n        # AttributeError: obj.rstrip(), object could not be a string\n        # IndexError: obj[-1], when an empty string\n        # ValueError: float(), when not a string representing a number\n        raise ValueError(pexdoc.pcontracts.get_exdesc())", "code_tokens": ["def", "engineering_notation_number", "(", "obj", ")", ":", "try", ":", "obj", "=", "obj", ".", "rstrip", "(", ")", "float", "(", "obj", "[", ":", "-", "1", "]", "if", "obj", "[", "-", "1", "]", "in", "_SUFFIX_TUPLE", "else", "obj", ")", "return", "None", "except", "(", "AttributeError", ",", "IndexError", ",", "ValueError", ")", ":", "# AttributeError: obj.rstrip(), object could not be a string", "# IndexError: obj[-1], when an empty string", "# ValueError: float(), when not a string representing a number", "raise", "ValueError", "(", "pexdoc", ".", "pcontracts", ".", "get_exdesc", "(", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/docs/support/ptypes.py#L72-L93", "partition": "test"}
{"repo": "chrisspen/burlap", "path": "burlap/common.py", "func_name": "Satchel.has_changes", "original_string": "def has_changes(self):\n        \"\"\"\n        Returns true if at least one tracker detects a change.\n        \"\"\"\n        lm = self.last_manifest\n        for tracker in self.get_trackers():\n            last_thumbprint = lm['_tracker_%s' % tracker.get_natural_key_hash()]\n            if tracker.is_changed(last_thumbprint):\n                return True\n        return False", "language": "python", "code": "def has_changes(self):\n        \"\"\"\n        Returns true if at least one tracker detects a change.\n        \"\"\"\n        lm = self.last_manifest\n        for tracker in self.get_trackers():\n            last_thumbprint = lm['_tracker_%s' % tracker.get_natural_key_hash()]\n            if tracker.is_changed(last_thumbprint):\n                return True\n        return False", "code_tokens": ["def", "has_changes", "(", "self", ")", ":", "lm", "=", "self", ".", "last_manifest", "for", "tracker", "in", "self", ".", "get_trackers", "(", ")", ":", "last_thumbprint", "=", "lm", "[", "'_tracker_%s'", "%", "tracker", ".", "get_natural_key_hash", "(", ")", "]", "if", "tracker", ".", "is_changed", "(", "last_thumbprint", ")", ":", "return", "True", "return", "False"], "docstring": "", "docstring_tokens": [], "sha": "a92b0a8e5206850bb777c74af8421ea8b33779bd", "url": "https://github.com/chrisspen/burlap/blob/a92b0a8e5206850bb777c74af8421ea8b33779bd/burlap/common.py#L1445-L1454", "partition": "valid"}
{"repo": "adafruit/Adafruit_Python_PureIO", "path": "Adafruit_PureIO/smbus.py", "func_name": "SMBus.write_block_data", "original_string": "def write_block_data(self, addr, cmd, vals):\n        \"\"\"Write a block of data to the specified cmd register of the device.\n        The amount of data to write should be the first byte inside the vals\n        string/bytearray and that count of bytes of data to write should follow\n        it.\n        \"\"\"\n        # Just use the I2C block data write to write the provided values and\n        # their length as the first byte.\n        data = bytearray(len(vals)+1)\n        data[0] = len(vals) & 0xFF\n        data[1:] = vals[0:]\n        self.write_i2c_block_data(addr, cmd, data)", "language": "python", "code": "def write_block_data(self, addr, cmd, vals):\n        \"\"\"Write a block of data to the specified cmd register of the device.\n        The amount of data to write should be the first byte inside the vals\n        string/bytearray and that count of bytes of data to write should follow\n        it.\n        \"\"\"\n        # Just use the I2C block data write to write the provided values and\n        # their length as the first byte.\n        data = bytearray(len(vals)+1)\n        data[0] = len(vals) & 0xFF\n        data[1:] = vals[0:]\n        self.write_i2c_block_data(addr, cmd, data)", "code_tokens": ["def", "write_block_data", "(", "self", ",", "addr", ",", "cmd", ",", "vals", ")", ":", "# Just use the I2C block data write to write the provided values and", "# their length as the first byte.", "data", "=", "bytearray", "(", "len", "(", "vals", ")", "+", "1", ")", "data", "[", "0", "]", "=", "len", "(", "vals", ")", "&", "0xFF", "data", "[", "1", ":", "]", "=", "vals", "[", "0", ":", "]", "self", ".", "write_i2c_block_data", "(", "addr", ",", "cmd", ",", "data", ")"], "docstring": "", "docstring_tokens": [], "sha": "6f4976d91c52d70b67b28bba75a429b5328a52c1", "url": "https://github.com/adafruit/Adafruit_Python_PureIO/blob/6f4976d91c52d70b67b28bba75a429b5328a52c1/Adafruit_PureIO/smbus.py#L270-L281", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/serializer/hwt/serializer.py", "func_name": "HwtSerializer.asHdl", "original_string": "def asHdl(cls, obj, ctx: HwtSerializerCtx):\n        \"\"\"\n        Convert object to HDL string\n\n        :param obj: object to serialize\n        :param ctx: HwtSerializerCtx instance\n        \"\"\"\n        if isinstance(obj, RtlSignalBase):\n            return cls.SignalItem(obj, ctx)\n        elif isinstance(obj, Value):\n            return cls.Value(obj, ctx)\n        else:\n            try:\n                serFn = obj.asHwt\n            except AttributeError:\n                serFn = None\n            if serFn is not None:\n                return serFn(cls, ctx)\n\n            try:\n                serFn = getattr(cls, obj.__class__.__name__)\n            except AttributeError:\n                serFn = None\n\n            if serFn is not None:\n                return serFn(obj, ctx)\n\n            raise SerializerException(\"Not implemented for %r\" % (obj))", "language": "python", "code": "def asHdl(cls, obj, ctx: HwtSerializerCtx):\n        \"\"\"\n        Convert object to HDL string\n\n        :param obj: object to serialize\n        :param ctx: HwtSerializerCtx instance\n        \"\"\"\n        if isinstance(obj, RtlSignalBase):\n            return cls.SignalItem(obj, ctx)\n        elif isinstance(obj, Value):\n            return cls.Value(obj, ctx)\n        else:\n            try:\n                serFn = obj.asHwt\n            except AttributeError:\n                serFn = None\n            if serFn is not None:\n                return serFn(cls, ctx)\n\n            try:\n                serFn = getattr(cls, obj.__class__.__name__)\n            except AttributeError:\n                serFn = None\n\n            if serFn is not None:\n                return serFn(obj, ctx)\n\n            raise SerializerException(\"Not implemented for %r\" % (obj))", "code_tokens": ["def", "asHdl", "(", "cls", ",", "obj", ",", "ctx", ":", "HwtSerializerCtx", ")", ":", "if", "isinstance", "(", "obj", ",", "RtlSignalBase", ")", ":", "return", "cls", ".", "SignalItem", "(", "obj", ",", "ctx", ")", "elif", "isinstance", "(", "obj", ",", "Value", ")", ":", "return", "cls", ".", "Value", "(", "obj", ",", "ctx", ")", "else", ":", "try", ":", "serFn", "=", "obj", ".", "asHwt", "except", "AttributeError", ":", "serFn", "=", "None", "if", "serFn", "is", "not", "None", ":", "return", "serFn", "(", "cls", ",", "ctx", ")", "try", ":", "serFn", "=", "getattr", "(", "cls", ",", "obj", ".", "__class__", ".", "__name__", ")", "except", "AttributeError", ":", "serFn", "=", "None", "if", "serFn", "is", "not", "None", ":", "return", "serFn", "(", "obj", ",", "ctx", ")", "raise", "SerializerException", "(", "\"Not implemented for %r\"", "%", "(", "obj", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/serializer/hwt/serializer.py#L57-L84", "partition": "test"}
{"repo": "not-na/peng3d", "path": "peng3d/window.py", "func_name": "PengWindow.set2d", "original_string": "def set2d(self):\n        \"\"\"\n        Configures OpenGL to draw in 2D.\n        \n        Note that wireframe mode is always disabled in 2D-Mode, but can be re-enabled by calling ``glPolygonMode(GL_FRONT_AND_BACK, GL_LINE)``\\ .\n        \"\"\"\n        # Light\n        \n        glDisable(GL_LIGHTING)\n        \n        # To avoid accidental wireframe GUIs and fonts\n        glPolygonMode( GL_FRONT_AND_BACK, GL_FILL)\n        \n        width, height = self.get_size()\n        glDisable(GL_DEPTH_TEST)\n        glViewport(0, 0, width, height)\n        glMatrixMode(GL_PROJECTION)\n        glLoadIdentity()\n        glOrtho(0, width, 0, height, -1, 1)\n        glMatrixMode(GL_MODELVIEW)\n        glLoadIdentity()", "language": "python", "code": "def set2d(self):\n        \"\"\"\n        Configures OpenGL to draw in 2D.\n        \n        Note that wireframe mode is always disabled in 2D-Mode, but can be re-enabled by calling ``glPolygonMode(GL_FRONT_AND_BACK, GL_LINE)``\\ .\n        \"\"\"\n        # Light\n        \n        glDisable(GL_LIGHTING)\n        \n        # To avoid accidental wireframe GUIs and fonts\n        glPolygonMode( GL_FRONT_AND_BACK, GL_FILL)\n        \n        width, height = self.get_size()\n        glDisable(GL_DEPTH_TEST)\n        glViewport(0, 0, width, height)\n        glMatrixMode(GL_PROJECTION)\n        glLoadIdentity()\n        glOrtho(0, width, 0, height, -1, 1)\n        glMatrixMode(GL_MODELVIEW)\n        glLoadIdentity()", "code_tokens": ["def", "set2d", "(", "self", ")", ":", "# Light", "glDisable", "(", "GL_LIGHTING", ")", "# To avoid accidental wireframe GUIs and fonts", "glPolygonMode", "(", "GL_FRONT_AND_BACK", ",", "GL_FILL", ")", "width", ",", "height", "=", "self", ".", "get_size", "(", ")", "glDisable", "(", "GL_DEPTH_TEST", ")", "glViewport", "(", "0", ",", "0", ",", "width", ",", "height", ")", "glMatrixMode", "(", "GL_PROJECTION", ")", "glLoadIdentity", "(", ")", "glOrtho", "(", "0", ",", "width", ",", "0", ",", "height", ",", "-", "1", ",", "1", ")", "glMatrixMode", "(", "GL_MODELVIEW", ")", "glLoadIdentity", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "1151be665b26cc8a479f6307086ba919e4d32d85", "url": "https://github.com/not-na/peng3d/blob/1151be665b26cc8a479f6307086ba919e4d32d85/peng3d/window.py#L336-L356", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/database.py", "func_name": "Whois.get_expiration_date", "original_string": "def get_expiration_date(self):\n        \"\"\"\n        Get the expiration date from the database.\n\n        :return: The expiration date from the database.\n        :rtype: str|None\n        \"\"\"\n\n        if self._authorization() and self.is_in_database() and not self.is_time_older():\n            # * We are authorized to work.\n            # and\n            # * The element we are testing is in the database.\n            # and\n            # * The expiration date is in the future.\n\n            # We get the expiration date from the database.\n            result = PyFunceble.INTERN[\"whois_db\"][PyFunceble.INTERN[\"file_to_test\"]][\n                PyFunceble.INTERN[\"to_test\"]\n            ][\"expiration_date\"]\n\n            if result:\n                # The expiration date from the database is not empty nor\n                # equal to None.\n\n                # We return it.\n                return result\n\n        # We return None, there is no data to work with.\n        return None", "language": "python", "code": "def get_expiration_date(self):\n        \"\"\"\n        Get the expiration date from the database.\n\n        :return: The expiration date from the database.\n        :rtype: str|None\n        \"\"\"\n\n        if self._authorization() and self.is_in_database() and not self.is_time_older():\n            # * We are authorized to work.\n            # and\n            # * The element we are testing is in the database.\n            # and\n            # * The expiration date is in the future.\n\n            # We get the expiration date from the database.\n            result = PyFunceble.INTERN[\"whois_db\"][PyFunceble.INTERN[\"file_to_test\"]][\n                PyFunceble.INTERN[\"to_test\"]\n            ][\"expiration_date\"]\n\n            if result:\n                # The expiration date from the database is not empty nor\n                # equal to None.\n\n                # We return it.\n                return result\n\n        # We return None, there is no data to work with.\n        return None", "code_tokens": ["def", "get_expiration_date", "(", "self", ")", ":", "if", "self", ".", "_authorization", "(", ")", "and", "self", ".", "is_in_database", "(", ")", "and", "not", "self", ".", "is_time_older", "(", ")", ":", "# * We are authorized to work.", "# and", "# * The element we are testing is in the database.", "# and", "# * The expiration date is in the future.", "# We get the expiration date from the database.", "result", "=", "PyFunceble", ".", "INTERN", "[", "\"whois_db\"", "]", "[", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", "]", "[", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", "]", "[", "\"expiration_date\"", "]", "if", "result", ":", "# The expiration date from the database is not empty nor", "# equal to None.", "# We return it.", "return", "result", "# We return None, there is no data to work with.", "return", "None"], "docstring": "", "docstring_tokens": [], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/database.py#L816-L844", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/displaypub.py", "func_name": "publish_display_data", "original_string": "def publish_display_data(source, data, metadata=None):\n    \"\"\"Publish data and metadata to all frontends.\n\n    See the ``display_data`` message in the messaging documentation for\n    more details about this message type.\n\n    The following MIME types are currently implemented:\n\n    * text/plain\n    * text/html\n    * text/latex\n    * application/json\n    * application/javascript\n    * image/png\n    * image/jpeg\n    * image/svg+xml\n\n    Parameters\n    ----------\n    source : str\n        A string that give the function or method that created the data,\n        such as 'IPython.core.page'.\n    data : dict\n        A dictionary having keys that are valid MIME types (like\n        'text/plain' or 'image/svg+xml') and values that are the data for\n        that MIME type. The data itself must be a JSON'able data\n        structure. Minimally all data should have the 'text/plain' data,\n        which can be displayed by all frontends. If more than the plain\n        text is given, it is up to the frontend to decide which\n        representation to use.\n    metadata : dict\n        A dictionary for metadata related to the data. This can contain\n        arbitrary key, value pairs that frontends can use to interpret\n        the data.\n    \"\"\"\n    from IPython.core.interactiveshell import InteractiveShell\n    InteractiveShell.instance().display_pub.publish(\n        source,\n        data,\n        metadata\n    )", "language": "python", "code": "def publish_display_data(source, data, metadata=None):\n    \"\"\"Publish data and metadata to all frontends.\n\n    See the ``display_data`` message in the messaging documentation for\n    more details about this message type.\n\n    The following MIME types are currently implemented:\n\n    * text/plain\n    * text/html\n    * text/latex\n    * application/json\n    * application/javascript\n    * image/png\n    * image/jpeg\n    * image/svg+xml\n\n    Parameters\n    ----------\n    source : str\n        A string that give the function or method that created the data,\n        such as 'IPython.core.page'.\n    data : dict\n        A dictionary having keys that are valid MIME types (like\n        'text/plain' or 'image/svg+xml') and values that are the data for\n        that MIME type. The data itself must be a JSON'able data\n        structure. Minimally all data should have the 'text/plain' data,\n        which can be displayed by all frontends. If more than the plain\n        text is given, it is up to the frontend to decide which\n        representation to use.\n    metadata : dict\n        A dictionary for metadata related to the data. This can contain\n        arbitrary key, value pairs that frontends can use to interpret\n        the data.\n    \"\"\"\n    from IPython.core.interactiveshell import InteractiveShell\n    InteractiveShell.instance().display_pub.publish(\n        source,\n        data,\n        metadata\n    )", "code_tokens": ["def", "publish_display_data", "(", "source", ",", "data", ",", "metadata", "=", "None", ")", ":", "from", "IPython", ".", "core", ".", "interactiveshell", "import", "InteractiveShell", "InteractiveShell", ".", "instance", "(", ")", ".", "display_pub", ".", "publish", "(", "source", ",", "data", ",", "metadata", ")"], "docstring": "", "docstring_tokens": [], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/displaypub.py#L119-L159", "partition": "test"}
{"repo": "PiotrDabkowski/Js2Py", "path": "js2py/legecy_translators/constants.py", "func_name": "unify_string_literals", "original_string": "def unify_string_literals(js_string):\n    \"\"\"this function parses the string just like javascript\n       for example literal '\\d' in JavaScript would be interpreted\n       as 'd' - backslash would be ignored and in Pyhon this\n       would be interpreted as '\\\\d' This function fixes this problem.\"\"\"\n    n = 0\n    res = ''\n    limit = len(js_string)\n    while n < limit:\n        char = js_string[n]\n        if char == '\\\\':\n            new, n = do_escape(js_string, n)\n            res += new\n        else:\n            res += char\n            n += 1\n    return res", "language": "python", "code": "def unify_string_literals(js_string):\n    \"\"\"this function parses the string just like javascript\n       for example literal '\\d' in JavaScript would be interpreted\n       as 'd' - backslash would be ignored and in Pyhon this\n       would be interpreted as '\\\\d' This function fixes this problem.\"\"\"\n    n = 0\n    res = ''\n    limit = len(js_string)\n    while n < limit:\n        char = js_string[n]\n        if char == '\\\\':\n            new, n = do_escape(js_string, n)\n            res += new\n        else:\n            res += char\n            n += 1\n    return res", "code_tokens": ["def", "unify_string_literals", "(", "js_string", ")", ":", "n", "=", "0", "res", "=", "''", "limit", "=", "len", "(", "js_string", ")", "while", "n", "<", "limit", ":", "char", "=", "js_string", "[", "n", "]", "if", "char", "==", "'\\\\'", ":", "new", ",", "n", "=", "do_escape", "(", "js_string", ",", "n", ")", "res", "+=", "new", "else", ":", "res", "+=", "char", "n", "+=", "1", "return", "res"], "docstring": "", "docstring_tokens": [], "sha": "c0fa43f5679cf91ca8986c5747fcb07a433dc584", "url": "https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/legecy_translators/constants.py#L238-L254", "partition": "valid"}
{"repo": "bakwc/PySyncObj", "path": "pysyncobj/transport.py", "func_name": "TCPTransport._onNewIncomingConnection", "original_string": "def _onNewIncomingConnection(self, conn):\n        \"\"\"\n        Callback for connections initiated by the other side\n\n        :param conn: connection object\n        :type conn: TcpConnection\n        \"\"\"\n\n        self._unknownConnections.add(conn)\n        encryptor = self._syncObj.encryptor\n        if encryptor:\n            conn.encryptor = encryptor\n        conn.setOnMessageReceivedCallback(functools.partial(self._onIncomingMessageReceived, conn))\n        conn.setOnDisconnectedCallback(functools.partial(self._onDisconnected, conn))", "language": "python", "code": "def _onNewIncomingConnection(self, conn):\n        \"\"\"\n        Callback for connections initiated by the other side\n\n        :param conn: connection object\n        :type conn: TcpConnection\n        \"\"\"\n\n        self._unknownConnections.add(conn)\n        encryptor = self._syncObj.encryptor\n        if encryptor:\n            conn.encryptor = encryptor\n        conn.setOnMessageReceivedCallback(functools.partial(self._onIncomingMessageReceived, conn))\n        conn.setOnDisconnectedCallback(functools.partial(self._onDisconnected, conn))", "code_tokens": ["def", "_onNewIncomingConnection", "(", "self", ",", "conn", ")", ":", "self", ".", "_unknownConnections", ".", "add", "(", "conn", ")", "encryptor", "=", "self", ".", "_syncObj", ".", "encryptor", "if", "encryptor", ":", "conn", ".", "encryptor", "=", "encryptor", "conn", ".", "setOnMessageReceivedCallback", "(", "functools", ".", "partial", "(", "self", ".", "_onIncomingMessageReceived", ",", "conn", ")", ")", "conn", ".", "setOnDisconnectedCallback", "(", "functools", ".", "partial", "(", "self", ".", "_onDisconnected", ",", "conn", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "be3b0aaa932d5156f5df140c23c962430f51b7b8", "url": "https://github.com/bakwc/PySyncObj/blob/be3b0aaa932d5156f5df140c23c962430f51b7b8/pysyncobj/transport.py#L287-L300", "partition": "test"}
{"repo": "kmmbvnr/django-any", "path": "django_any/forms.py", "func_name": "decimal_field_data", "original_string": "def decimal_field_data(field, **kwargs):\n    \"\"\"\n    Return random value for DecimalField\n\n    >>> result = any_form_field(forms.DecimalField(max_value=100, min_value=11, max_digits=4, decimal_places = 2))\n    >>> type(result)\n    <type 'str'>\n    >>> from decimal import Decimal\n    >>> Decimal(result) >= 11, Decimal(result) <= Decimal('99.99')\n    (True, True)\n    \"\"\"\n    min_value = 0\n    max_value = 10\n    from django.core.validators import MinValueValidator, MaxValueValidator \n    for elem in field.validators:\n        if isinstance(elem, MinValueValidator):\n            min_value = elem.limit_value\n        if isinstance(elem, MaxValueValidator):\n            max_value = elem.limit_value\n    if (field.max_digits and field.decimal_places):\n        from decimal import Decimal\n        max_value = min(max_value,\n                        Decimal('%s.%s' % ('9'*(field.max_digits-field.decimal_places),\n                                           '9'*field.decimal_places)))\n\n    min_value = kwargs.get('min_value') or min_value\n    max_value = kwargs.get('max_value') or max_value\n\n    return str(xunit.any_decimal(min_value=min_value,\n                             max_value=max_value,\n                             decimal_places = field.decimal_places or 2))", "language": "python", "code": "def decimal_field_data(field, **kwargs):\n    \"\"\"\n    Return random value for DecimalField\n\n    >>> result = any_form_field(forms.DecimalField(max_value=100, min_value=11, max_digits=4, decimal_places = 2))\n    >>> type(result)\n    <type 'str'>\n    >>> from decimal import Decimal\n    >>> Decimal(result) >= 11, Decimal(result) <= Decimal('99.99')\n    (True, True)\n    \"\"\"\n    min_value = 0\n    max_value = 10\n    from django.core.validators import MinValueValidator, MaxValueValidator \n    for elem in field.validators:\n        if isinstance(elem, MinValueValidator):\n            min_value = elem.limit_value\n        if isinstance(elem, MaxValueValidator):\n            max_value = elem.limit_value\n    if (field.max_digits and field.decimal_places):\n        from decimal import Decimal\n        max_value = min(max_value,\n                        Decimal('%s.%s' % ('9'*(field.max_digits-field.decimal_places),\n                                           '9'*field.decimal_places)))\n\n    min_value = kwargs.get('min_value') or min_value\n    max_value = kwargs.get('max_value') or max_value\n\n    return str(xunit.any_decimal(min_value=min_value,\n                             max_value=max_value,\n                             decimal_places = field.decimal_places or 2))", "code_tokens": ["def", "decimal_field_data", "(", "field", ",", "*", "*", "kwargs", ")", ":", "min_value", "=", "0", "max_value", "=", "10", "from", "django", ".", "core", ".", "validators", "import", "MinValueValidator", ",", "MaxValueValidator", "for", "elem", "in", "field", ".", "validators", ":", "if", "isinstance", "(", "elem", ",", "MinValueValidator", ")", ":", "min_value", "=", "elem", ".", "limit_value", "if", "isinstance", "(", "elem", ",", "MaxValueValidator", ")", ":", "max_value", "=", "elem", ".", "limit_value", "if", "(", "field", ".", "max_digits", "and", "field", ".", "decimal_places", ")", ":", "from", "decimal", "import", "Decimal", "max_value", "=", "min", "(", "max_value", ",", "Decimal", "(", "'%s.%s'", "%", "(", "'9'", "*", "(", "field", ".", "max_digits", "-", "field", ".", "decimal_places", ")", ",", "'9'", "*", "field", ".", "decimal_places", ")", ")", ")", "min_value", "=", "kwargs", ".", "get", "(", "'min_value'", ")", "or", "min_value", "max_value", "=", "kwargs", ".", "get", "(", "'max_value'", ")", "or", "max_value", "return", "str", "(", "xunit", ".", "any_decimal", "(", "min_value", "=", "min_value", ",", "max_value", "=", "max_value", ",", "decimal_places", "=", "field", ".", "decimal_places", "or", "2", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "6f64ebd05476e2149e2e71deeefbb10f8edfc412", "url": "https://github.com/kmmbvnr/django-any/blob/6f64ebd05476e2149e2e71deeefbb10f8edfc412/django_any/forms.py#L94-L124", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/ultratb.py", "func_name": "TBTools.color_toggle", "original_string": "def color_toggle(self):\n        \"\"\"Toggle between the currently active color scheme and NoColor.\"\"\"\n\n        if self.color_scheme_table.active_scheme_name == 'NoColor':\n            self.color_scheme_table.set_active_scheme(self.old_scheme)\n            self.Colors = self.color_scheme_table.active_colors\n        else:\n            self.old_scheme = self.color_scheme_table.active_scheme_name\n            self.color_scheme_table.set_active_scheme('NoColor')\n            self.Colors = self.color_scheme_table.active_colors", "language": "python", "code": "def color_toggle(self):\n        \"\"\"Toggle between the currently active color scheme and NoColor.\"\"\"\n\n        if self.color_scheme_table.active_scheme_name == 'NoColor':\n            self.color_scheme_table.set_active_scheme(self.old_scheme)\n            self.Colors = self.color_scheme_table.active_colors\n        else:\n            self.old_scheme = self.color_scheme_table.active_scheme_name\n            self.color_scheme_table.set_active_scheme('NoColor')\n            self.Colors = self.color_scheme_table.active_colors", "code_tokens": ["def", "color_toggle", "(", "self", ")", ":", "if", "self", ".", "color_scheme_table", ".", "active_scheme_name", "==", "'NoColor'", ":", "self", ".", "color_scheme_table", ".", "set_active_scheme", "(", "self", ".", "old_scheme", ")", "self", ".", "Colors", "=", "self", ".", "color_scheme_table", ".", "active_colors", "else", ":", "self", ".", "old_scheme", "=", "self", ".", "color_scheme_table", ".", "active_scheme_name", "self", ".", "color_scheme_table", ".", "set_active_scheme", "(", "'NoColor'", ")", "self", ".", "Colors", "=", "self", ".", "color_scheme_table", ".", "active_colors"], "docstring": "", "docstring_tokens": [], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/ultratb.py#L391-L400", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/models/tokenizers/utils.py", "func_name": "ngramize", "original_string": "def ngramize(items: List[str], ngram_range=(1, 1)) -> Generator[List[str], Any, None]:\n    \"\"\"\n    Make ngrams from a list of tokens/lemmas\n    :param items: list of tokens, lemmas or other strings to form ngrams\n    :param ngram_range: range for producing ngrams, ex. for unigrams + bigrams should be set to\n    (1, 2), for bigrams only should be set to (2, 2)\n    :return: ngrams (as strings) generator\n    \"\"\"\n\n    ngrams = []\n    ranges = [(0, i) for i in range(ngram_range[0], ngram_range[1] + 1)]\n    for r in ranges:\n        ngrams += list(zip(*[items[j:] for j in range(*r)]))\n\n    formatted_ngrams = [' '.join(item) for item in ngrams]\n\n    yield formatted_ngrams", "language": "python", "code": "def ngramize(items: List[str], ngram_range=(1, 1)) -> Generator[List[str], Any, None]:\n    \"\"\"\n    Make ngrams from a list of tokens/lemmas\n    :param items: list of tokens, lemmas or other strings to form ngrams\n    :param ngram_range: range for producing ngrams, ex. for unigrams + bigrams should be set to\n    (1, 2), for bigrams only should be set to (2, 2)\n    :return: ngrams (as strings) generator\n    \"\"\"\n\n    ngrams = []\n    ranges = [(0, i) for i in range(ngram_range[0], ngram_range[1] + 1)]\n    for r in ranges:\n        ngrams += list(zip(*[items[j:] for j in range(*r)]))\n\n    formatted_ngrams = [' '.join(item) for item in ngrams]\n\n    yield formatted_ngrams", "code_tokens": ["def", "ngramize", "(", "items", ":", "List", "[", "str", "]", ",", "ngram_range", "=", "(", "1", ",", "1", ")", ")", "->", "Generator", "[", "List", "[", "str", "]", ",", "Any", ",", "None", "]", ":", "ngrams", "=", "[", "]", "ranges", "=", "[", "(", "0", ",", "i", ")", "for", "i", "in", "range", "(", "ngram_range", "[", "0", "]", ",", "ngram_range", "[", "1", "]", "+", "1", ")", "]", "for", "r", "in", "ranges", ":", "ngrams", "+=", "list", "(", "zip", "(", "*", "[", "items", "[", "j", ":", "]", "for", "j", "in", "range", "(", "*", "r", ")", "]", ")", ")", "formatted_ngrams", "=", "[", "' '", ".", "join", "(", "item", ")", "for", "item", "in", "ngrams", "]", "yield", "formatted_ngrams"], "docstring": "", "docstring_tokens": [], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/models/tokenizers/utils.py#L38-L54", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/mlx.py", "func_name": "run", "original_string": "def run(script='TEMP3D_default.mlx', log=None, ml_log=None,\n        mlp_in=None, mlp_out=None, overwrite=False, file_in=None,\n        file_out=None, output_mask=None, cmd=None, ml_version=ML_VERSION,\n        print_meshlabserver_output=True):\n    \"\"\"Run meshlabserver in a subprocess.\n\n    Args:\n        log (str): filename of the log file for meshlabxml. If not\n            None, all meshlabserver stdout and stderr messages\n            will be appended to this file.\n        ml_log (str): filename of the log file output directly by\n            meshlabserver.\n        mlp_in (str or list): input meshlab project file. Can be a\n            single filename or a list of filenames. Filenames will\n            be loaded in the order given. All project files will\n            be loaded before individual input files. If you want\n            to load project and input files in a different order\n            then you should use a custom cmd.\n        mlp_out (str): output meshlab project file. Specify a\n            single filename (meshlabserver accepts multiple output\n            project filenames, however they will all be identical,\n            so there is little use). When this option is used all\n            layers will be saved as ply files.\n        overwrite (bool): when specifying mlp_out, this determines\n            whether any existing files will be overwritten (if\n            True) or new filenames created (if False). If a new\n            project file is created meshes will have '_out' added\n            to their name.\n        file_in (str or list): input mesh filename. Can be a single\n            filename or a list of filenames. Filenames will be\n            loaded in the order given. All project files will be\n            loaded before individual input files. If you want to\n            load project and input files in a different order then\n            you should use a custom cmd.\n        file_out (str or list): output mesh filename. Can be a\n            single filename or a list of filenames. The current\n            layer will be saved to this filename or filenames.\n            Multiple filenames are useful for saving to multiple\n            formats at the same time. Currently there is no way to\n            output multiple layers except for saving a mlp project\n            file.\n        output_mask (str or list): output mask options for the\n            output file. Values must include the flag, i.e. -m or\n            -output_mask. If this is not provided for an output\n            file then function \"default_output_mask\" is used to\n            determine default values.\n        script (str): the mlx filter script filename to execute.\n        cmd (str): a full meshlabserver command line, such as\n            \"meshlabserver -input file.stl\". If not None, this\n            will override all other arguements except for log.\n        print_meshlabserver_output (bool): Pass meshlabserver's output to stdout; useful for debugging.\n                                           Only used if log is None.\n\n    Notes:\n        Meshlabserver can't handle spaces in paths or filenames (on Windows at least; haven't tested on other platforms). Enclosing the name in quotes or escaping the space has no effect.\n\n    Returns:\n        return code of meshlabserver process; 0 if successful\n    \"\"\"\n    if cmd is None:\n        cmd = 'meshlabserver'\n        if ml_log is not None:\n            # Initialize ml_log\n            ml_log_file = open(ml_log, 'w')\n            ml_log_file.close()\n            cmd += ' -l %s' % ml_log\n        if mlp_in is not None:\n            # make a list if it isn't already\n            mlp_in = util.make_list(mlp_in)\n            for val in mlp_in:\n                cmd += ' -p \"%s\"' % val\n        if mlp_out is not None:\n            cmd += ' -w %s' % mlp_out\n            if overwrite:\n                cmd += ' -v'\n        if (mlp_in is None) and (file_in is None):\n\t\t\t# If no input files are provided use the default created by begin().\n\t\t\t# This works around the fact that meshlabserver will\n\t\t\t# not run without an input file.\n            file_in = ['TEMP3D.xyz']\n        if file_in is not None:\n            # make a list if it isn't already\n            file_in = util.make_list(file_in)\n            for val in file_in:\n                if val == 'bunny':\n                    cmd += ' -i \"%s\"' % os.path.join(THIS_MODULEPATH, os.pardir,\n                                                     'models', 'bunny_flat(1Z).ply')\n                elif val == 'bunny_raw':\n                    cmd += ' -i \"%s\"' % os.path.join(THIS_MODULEPATH, os.pardir,\n                                                     'models', 'bunny_raw(-1250Y).ply')\n                else:\n                    cmd += ' -i \"%s\"' % val\n        if file_out is not None:\n            # make a list if it isn't already\n            file_out = util.make_list(file_out)\n            if output_mask is not None:\n                output_mask = util.make_list(output_mask)\n            else:\n                output_mask = []\n            for index, val in enumerate(file_out):\n                cmd += ' -o \"%s\"' % val\n                try:\n                    cmd += ' %s' % output_mask[index]\n                except IndexError:  # If output_mask can't be found use defaults\n                    cmd += ' %s' % default_output_mask(val, ml_version=ml_version)\n        if script is not None:\n            cmd += ' -s \"%s\"' % script\n    if log is not None:\n        log_file = open(log, 'a')\n        log_file.write('meshlabserver cmd = %s\\n' % cmd)\n        log_file.write('***START OF MESHLAB STDOUT & STDERR***\\n')\n        log_file.close()\n        log_file = open(log, 'a')\n    else:\n        if print_meshlabserver_output:\n            log_file = None\n            print('meshlabserver cmd = %s' % cmd)\n            print('***START OF MESHLAB STDOUT & STDERR***')\n        else:\n            log_file = open(os.devnull, 'w')\n    while True:\n        # TODO: test if shell=True is really needed\n        return_code = subprocess.call(cmd, shell=True,\n                                      stdout=log_file, stderr=log_file,\n                                      universal_newlines=True)\n        if log is not None:\n            log_file.close()\n        if (return_code == 0) or handle_error(program_name='MeshLab', cmd=cmd, log=log):\n            break\n    if log is not None:\n        log_file = open(log, 'a')\n        log_file.write('***END OF MESHLAB STDOUT & STDERR***\\n')\n        log_file.write('meshlabserver return code = %s\\n\\n' % return_code)\n        log_file.close()\n    return return_code", "language": "python", "code": "def run(script='TEMP3D_default.mlx', log=None, ml_log=None,\n        mlp_in=None, mlp_out=None, overwrite=False, file_in=None,\n        file_out=None, output_mask=None, cmd=None, ml_version=ML_VERSION,\n        print_meshlabserver_output=True):\n    \"\"\"Run meshlabserver in a subprocess.\n\n    Args:\n        log (str): filename of the log file for meshlabxml. If not\n            None, all meshlabserver stdout and stderr messages\n            will be appended to this file.\n        ml_log (str): filename of the log file output directly by\n            meshlabserver.\n        mlp_in (str or list): input meshlab project file. Can be a\n            single filename or a list of filenames. Filenames will\n            be loaded in the order given. All project files will\n            be loaded before individual input files. If you want\n            to load project and input files in a different order\n            then you should use a custom cmd.\n        mlp_out (str): output meshlab project file. Specify a\n            single filename (meshlabserver accepts multiple output\n            project filenames, however they will all be identical,\n            so there is little use). When this option is used all\n            layers will be saved as ply files.\n        overwrite (bool): when specifying mlp_out, this determines\n            whether any existing files will be overwritten (if\n            True) or new filenames created (if False). If a new\n            project file is created meshes will have '_out' added\n            to their name.\n        file_in (str or list): input mesh filename. Can be a single\n            filename or a list of filenames. Filenames will be\n            loaded in the order given. All project files will be\n            loaded before individual input files. If you want to\n            load project and input files in a different order then\n            you should use a custom cmd.\n        file_out (str or list): output mesh filename. Can be a\n            single filename or a list of filenames. The current\n            layer will be saved to this filename or filenames.\n            Multiple filenames are useful for saving to multiple\n            formats at the same time. Currently there is no way to\n            output multiple layers except for saving a mlp project\n            file.\n        output_mask (str or list): output mask options for the\n            output file. Values must include the flag, i.e. -m or\n            -output_mask. If this is not provided for an output\n            file then function \"default_output_mask\" is used to\n            determine default values.\n        script (str): the mlx filter script filename to execute.\n        cmd (str): a full meshlabserver command line, such as\n            \"meshlabserver -input file.stl\". If not None, this\n            will override all other arguements except for log.\n        print_meshlabserver_output (bool): Pass meshlabserver's output to stdout; useful for debugging.\n                                           Only used if log is None.\n\n    Notes:\n        Meshlabserver can't handle spaces in paths or filenames (on Windows at least; haven't tested on other platforms). Enclosing the name in quotes or escaping the space has no effect.\n\n    Returns:\n        return code of meshlabserver process; 0 if successful\n    \"\"\"\n    if cmd is None:\n        cmd = 'meshlabserver'\n        if ml_log is not None:\n            # Initialize ml_log\n            ml_log_file = open(ml_log, 'w')\n            ml_log_file.close()\n            cmd += ' -l %s' % ml_log\n        if mlp_in is not None:\n            # make a list if it isn't already\n            mlp_in = util.make_list(mlp_in)\n            for val in mlp_in:\n                cmd += ' -p \"%s\"' % val\n        if mlp_out is not None:\n            cmd += ' -w %s' % mlp_out\n            if overwrite:\n                cmd += ' -v'\n        if (mlp_in is None) and (file_in is None):\n\t\t\t# If no input files are provided use the default created by begin().\n\t\t\t# This works around the fact that meshlabserver will\n\t\t\t# not run without an input file.\n            file_in = ['TEMP3D.xyz']\n        if file_in is not None:\n            # make a list if it isn't already\n            file_in = util.make_list(file_in)\n            for val in file_in:\n                if val == 'bunny':\n                    cmd += ' -i \"%s\"' % os.path.join(THIS_MODULEPATH, os.pardir,\n                                                     'models', 'bunny_flat(1Z).ply')\n                elif val == 'bunny_raw':\n                    cmd += ' -i \"%s\"' % os.path.join(THIS_MODULEPATH, os.pardir,\n                                                     'models', 'bunny_raw(-1250Y).ply')\n                else:\n                    cmd += ' -i \"%s\"' % val\n        if file_out is not None:\n            # make a list if it isn't already\n            file_out = util.make_list(file_out)\n            if output_mask is not None:\n                output_mask = util.make_list(output_mask)\n            else:\n                output_mask = []\n            for index, val in enumerate(file_out):\n                cmd += ' -o \"%s\"' % val\n                try:\n                    cmd += ' %s' % output_mask[index]\n                except IndexError:  # If output_mask can't be found use defaults\n                    cmd += ' %s' % default_output_mask(val, ml_version=ml_version)\n        if script is not None:\n            cmd += ' -s \"%s\"' % script\n    if log is not None:\n        log_file = open(log, 'a')\n        log_file.write('meshlabserver cmd = %s\\n' % cmd)\n        log_file.write('***START OF MESHLAB STDOUT & STDERR***\\n')\n        log_file.close()\n        log_file = open(log, 'a')\n    else:\n        if print_meshlabserver_output:\n            log_file = None\n            print('meshlabserver cmd = %s' % cmd)\n            print('***START OF MESHLAB STDOUT & STDERR***')\n        else:\n            log_file = open(os.devnull, 'w')\n    while True:\n        # TODO: test if shell=True is really needed\n        return_code = subprocess.call(cmd, shell=True,\n                                      stdout=log_file, stderr=log_file,\n                                      universal_newlines=True)\n        if log is not None:\n            log_file.close()\n        if (return_code == 0) or handle_error(program_name='MeshLab', cmd=cmd, log=log):\n            break\n    if log is not None:\n        log_file = open(log, 'a')\n        log_file.write('***END OF MESHLAB STDOUT & STDERR***\\n')\n        log_file.write('meshlabserver return code = %s\\n\\n' % return_code)\n        log_file.close()\n    return return_code", "code_tokens": ["def", "run", "(", "script", "=", "'TEMP3D_default.mlx'", ",", "log", "=", "None", ",", "ml_log", "=", "None", ",", "mlp_in", "=", "None", ",", "mlp_out", "=", "None", ",", "overwrite", "=", "False", ",", "file_in", "=", "None", ",", "file_out", "=", "None", ",", "output_mask", "=", "None", ",", "cmd", "=", "None", ",", "ml_version", "=", "ML_VERSION", ",", "print_meshlabserver_output", "=", "True", ")", ":", "if", "cmd", "is", "None", ":", "cmd", "=", "'meshlabserver'", "if", "ml_log", "is", "not", "None", ":", "# Initialize ml_log", "ml_log_file", "=", "open", "(", "ml_log", ",", "'w'", ")", "ml_log_file", ".", "close", "(", ")", "cmd", "+=", "' -l %s'", "%", "ml_log", "if", "mlp_in", "is", "not", "None", ":", "# make a list if it isn't already", "mlp_in", "=", "util", ".", "make_list", "(", "mlp_in", ")", "for", "val", "in", "mlp_in", ":", "cmd", "+=", "' -p \"%s\"'", "%", "val", "if", "mlp_out", "is", "not", "None", ":", "cmd", "+=", "' -w %s'", "%", "mlp_out", "if", "overwrite", ":", "cmd", "+=", "' -v'", "if", "(", "mlp_in", "is", "None", ")", "and", "(", "file_in", "is", "None", ")", ":", "# If no input files are provided use the default created by begin().", "# This works around the fact that meshlabserver will", "# not run without an input file.", "file_in", "=", "[", "'TEMP3D.xyz'", "]", "if", "file_in", "is", "not", "None", ":", "# make a list if it isn't already", "file_in", "=", "util", ".", "make_list", "(", "file_in", ")", "for", "val", "in", "file_in", ":", "if", "val", "==", "'bunny'", ":", "cmd", "+=", "' -i \"%s\"'", "%", "os", ".", "path", ".", "join", "(", "THIS_MODULEPATH", ",", "os", ".", "pardir", ",", "'models'", ",", "'bunny_flat(1Z).ply'", ")", "elif", "val", "==", "'bunny_raw'", ":", "cmd", "+=", "' -i \"%s\"'", "%", "os", ".", "path", ".", "join", "(", "THIS_MODULEPATH", ",", "os", ".", "pardir", ",", "'models'", ",", "'bunny_raw(-1250Y).ply'", ")", "else", ":", "cmd", "+=", "' -i \"%s\"'", "%", "val", "if", "file_out", "is", "not", "None", ":", "# make a list if it isn't already", "file_out", "=", "util", ".", "make_list", "(", "file_out", ")", "if", "output_mask", "is", "not", "None", ":", "output_mask", "=", "util", ".", "make_list", "(", "output_mask", ")", "else", ":", "output_mask", "=", "[", "]", "for", "index", ",", "val", "in", "enumerate", "(", "file_out", ")", ":", "cmd", "+=", "' -o \"%s\"'", "%", "val", "try", ":", "cmd", "+=", "' %s'", "%", "output_mask", "[", "index", "]", "except", "IndexError", ":", "# If output_mask can't be found use defaults", "cmd", "+=", "' %s'", "%", "default_output_mask", "(", "val", ",", "ml_version", "=", "ml_version", ")", "if", "script", "is", "not", "None", ":", "cmd", "+=", "' -s \"%s\"'", "%", "script", "if", "log", "is", "not", "None", ":", "log_file", "=", "open", "(", "log", ",", "'a'", ")", "log_file", ".", "write", "(", "'meshlabserver cmd = %s\\n'", "%", "cmd", ")", "log_file", ".", "write", "(", "'***START OF MESHLAB STDOUT & STDERR***\\n'", ")", "log_file", ".", "close", "(", ")", "log_file", "=", "open", "(", "log", ",", "'a'", ")", "else", ":", "if", "print_meshlabserver_output", ":", "log_file", "=", "None", "print", "(", "'meshlabserver cmd = %s'", "%", "cmd", ")", "print", "(", "'***START OF MESHLAB STDOUT & STDERR***'", ")", "else", ":", "log_file", "=", "open", "(", "os", ".", "devnull", ",", "'w'", ")", "while", "True", ":", "# TODO: test if shell=True is really needed", "return_code", "=", "subprocess", ".", "call", "(", "cmd", ",", "shell", "=", "True", ",", "stdout", "=", "log_file", ",", "stderr", "=", "log_file", ",", "universal_newlines", "=", "True", ")", "if", "log", "is", "not", "None", ":", "log_file", ".", "close", "(", ")", "if", "(", "return_code", "==", "0", ")", "or", "handle_error", "(", "program_name", "=", "'MeshLab'", ",", "cmd", "=", "cmd", ",", "log", "=", "log", ")", ":", "break", "if", "log", "is", "not", "None", ":", "log_file", "=", "open", "(", "log", ",", "'a'", ")", "log_file", ".", "write", "(", "'***END OF MESHLAB STDOUT & STDERR***\\n'", ")", "log_file", ".", "write", "(", "'meshlabserver return code = %s\\n\\n'", "%", "return_code", ")", "log_file", ".", "close", "(", ")", "return", "return_code"], "docstring": "", "docstring_tokens": [], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/mlx.py#L319-L453", "partition": "test"}
{"repo": "padelt/temper-python", "path": "temperusb/temper.py", "func_name": "TemperDevice.get_temperatures", "original_string": "def get_temperatures(self, sensors=None):\n        \"\"\"\n        Get device temperature reading.\n\n        Params:\n        - sensors: optional list of sensors to get a reading for, examples:\n          [0,] - get reading for sensor 0\n          [0, 1,] - get reading for sensors 0 and 1\n          None - get readings for all sensors\n        \"\"\"\n        _sensors = sensors\n        if _sensors is None:\n            _sensors = list(range(0, self._sensor_count))\n\n        if not set(_sensors).issubset(list(range(0, self._sensor_count))):\n            raise ValueError(\n                'Some or all of the sensors in the list %s are out of range '\n                'given a sensor_count of %d.  Valid range: %s' % (\n                    _sensors,\n                    self._sensor_count,\n                    list(range(0, self._sensor_count)),\n                )\n            )\n\n        data = self.get_data()\n        data = data['temp_data']\n\n        results = {}\n\n        # Interpret device response\n        for sensor in _sensors:\n            offset = self.lookup_offset(sensor)\n            celsius = struct.unpack_from('>h', data, offset)[0] / 256.0\n            # Apply scaling and offset (if any)\n            celsius = celsius * self._scale + self._offset\n            results[sensor] = {\n                'ports': self.get_ports(),\n                'bus': self.get_bus(),\n                'sensor': sensor,\n                'temperature_f': celsius * 1.8 + 32.0,\n                'temperature_c': celsius,\n                'temperature_mc': celsius * 1000,\n                'temperature_k': celsius + 273.15,\n            }\n\n        return results", "language": "python", "code": "def get_temperatures(self, sensors=None):\n        \"\"\"\n        Get device temperature reading.\n\n        Params:\n        - sensors: optional list of sensors to get a reading for, examples:\n          [0,] - get reading for sensor 0\n          [0, 1,] - get reading for sensors 0 and 1\n          None - get readings for all sensors\n        \"\"\"\n        _sensors = sensors\n        if _sensors is None:\n            _sensors = list(range(0, self._sensor_count))\n\n        if not set(_sensors).issubset(list(range(0, self._sensor_count))):\n            raise ValueError(\n                'Some or all of the sensors in the list %s are out of range '\n                'given a sensor_count of %d.  Valid range: %s' % (\n                    _sensors,\n                    self._sensor_count,\n                    list(range(0, self._sensor_count)),\n                )\n            )\n\n        data = self.get_data()\n        data = data['temp_data']\n\n        results = {}\n\n        # Interpret device response\n        for sensor in _sensors:\n            offset = self.lookup_offset(sensor)\n            celsius = struct.unpack_from('>h', data, offset)[0] / 256.0\n            # Apply scaling and offset (if any)\n            celsius = celsius * self._scale + self._offset\n            results[sensor] = {\n                'ports': self.get_ports(),\n                'bus': self.get_bus(),\n                'sensor': sensor,\n                'temperature_f': celsius * 1.8 + 32.0,\n                'temperature_c': celsius,\n                'temperature_mc': celsius * 1000,\n                'temperature_k': celsius + 273.15,\n            }\n\n        return results", "code_tokens": ["def", "get_temperatures", "(", "self", ",", "sensors", "=", "None", ")", ":", "_sensors", "=", "sensors", "if", "_sensors", "is", "None", ":", "_sensors", "=", "list", "(", "range", "(", "0", ",", "self", ".", "_sensor_count", ")", ")", "if", "not", "set", "(", "_sensors", ")", ".", "issubset", "(", "list", "(", "range", "(", "0", ",", "self", ".", "_sensor_count", ")", ")", ")", ":", "raise", "ValueError", "(", "'Some or all of the sensors in the list %s are out of range '", "'given a sensor_count of %d.  Valid range: %s'", "%", "(", "_sensors", ",", "self", ".", "_sensor_count", ",", "list", "(", "range", "(", "0", ",", "self", ".", "_sensor_count", ")", ")", ",", ")", ")", "data", "=", "self", ".", "get_data", "(", ")", "data", "=", "data", "[", "'temp_data'", "]", "results", "=", "{", "}", "# Interpret device response", "for", "sensor", "in", "_sensors", ":", "offset", "=", "self", ".", "lookup_offset", "(", "sensor", ")", "celsius", "=", "struct", ".", "unpack_from", "(", "'>h'", ",", "data", ",", "offset", ")", "[", "0", "]", "/", "256.0", "# Apply scaling and offset (if any)", "celsius", "=", "celsius", "*", "self", ".", "_scale", "+", "self", ".", "_offset", "results", "[", "sensor", "]", "=", "{", "'ports'", ":", "self", ".", "get_ports", "(", ")", ",", "'bus'", ":", "self", ".", "get_bus", "(", ")", ",", "'sensor'", ":", "sensor", ",", "'temperature_f'", ":", "celsius", "*", "1.8", "+", "32.0", ",", "'temperature_c'", ":", "celsius", ",", "'temperature_mc'", ":", "celsius", "*", "1000", ",", "'temperature_k'", ":", "celsius", "+", "273.15", ",", "}", "return", "results"], "docstring": "", "docstring_tokens": [], "sha": "cbdbace7e6755b1d91a2603ab63c9cb778078f79", "url": "https://github.com/padelt/temper-python/blob/cbdbace7e6755b1d91a2603ab63c9cb778078f79/temperusb/temper.py#L298-L343", "partition": "valid"}
{"repo": "CloverHealth/temple", "path": "temple/setup.py", "func_name": "_patched_run_hook", "original_string": "def _patched_run_hook(hook_name, project_dir, context):\n    \"\"\"Used to patch cookiecutter's ``run_hook`` function.\n\n    This patched version ensures that the temple.yaml file is created before\n    any cookiecutter hooks are executed\n    \"\"\"\n    if hook_name == 'post_gen_project':\n        with temple.utils.cd(project_dir):\n            temple.utils.write_temple_config(context['cookiecutter'],\n                                             context['template'],\n                                             context['version'])\n    return cc_hooks.run_hook(hook_name, project_dir, context)", "language": "python", "code": "def _patched_run_hook(hook_name, project_dir, context):\n    \"\"\"Used to patch cookiecutter's ``run_hook`` function.\n\n    This patched version ensures that the temple.yaml file is created before\n    any cookiecutter hooks are executed\n    \"\"\"\n    if hook_name == 'post_gen_project':\n        with temple.utils.cd(project_dir):\n            temple.utils.write_temple_config(context['cookiecutter'],\n                                             context['template'],\n                                             context['version'])\n    return cc_hooks.run_hook(hook_name, project_dir, context)", "code_tokens": ["def", "_patched_run_hook", "(", "hook_name", ",", "project_dir", ",", "context", ")", ":", "if", "hook_name", "==", "'post_gen_project'", ":", "with", "temple", ".", "utils", ".", "cd", "(", "project_dir", ")", ":", "temple", ".", "utils", ".", "write_temple_config", "(", "context", "[", "'cookiecutter'", "]", ",", "context", "[", "'template'", "]", ",", "context", "[", "'version'", "]", ")", "return", "cc_hooks", ".", "run_hook", "(", "hook_name", ",", "project_dir", ",", "context", ")"], "docstring": "", "docstring_tokens": [], "sha": "d7b75da2459f72ba74d6f3b6e1ab95c3d1b92ccd", "url": "https://github.com/CloverHealth/temple/blob/d7b75da2459f72ba74d6f3b6e1ab95c3d1b92ccd/temple/setup.py#L18-L29", "partition": "valid"}
{"repo": "EmbodiedCognition/pagoda", "path": "pagoda/physics.py", "func_name": "Joint.cfms", "original_string": "def cfms(self, cfms):\n        '''Set the CFM values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        '''\n        _set_params(self.ode_obj, 'CFM', cfms, self.ADOF + self.LDOF)", "language": "python", "code": "def cfms(self, cfms):\n        '''Set the CFM values for this object's degrees of freedom.\n\n        Parameters\n        ----------\n        cfms : float or sequence of float\n            A CFM value to set on all degrees of freedom, or a list\n            containing one such value for each degree of freedom.\n        '''\n        _set_params(self.ode_obj, 'CFM', cfms, self.ADOF + self.LDOF)", "code_tokens": ["def", "cfms", "(", "self", ",", "cfms", ")", ":", "_set_params", "(", "self", ".", "ode_obj", ",", "'CFM'", ",", "cfms", ",", "self", ".", "ADOF", "+", "self", ".", "LDOF", ")"], "docstring": "", "docstring_tokens": [], "sha": "8892f847026d98aba8646ecbc4589397e6dec7bd", "url": "https://github.com/EmbodiedCognition/pagoda/blob/8892f847026d98aba8646ecbc4589397e6dec7bd/pagoda/physics.py#L665-L674", "partition": "valid"}
{"repo": "refenv/cijoe", "path": "modules/cij/reporter.py", "func_name": "rehome", "original_string": "def rehome(old, new, struct):\n    \"\"\"\n    Replace all absolute paths to \"re-home\" it\n    \"\"\"\n\n    if old == new:\n        return\n\n    if isinstance(struct, list):\n        for item in struct:\n            rehome(old, new, item)\n    elif isinstance(struct, dict):\n        for key, val in struct.iteritems():\n            if isinstance(val, (dict, list)):\n                rehome(old, new, val)\n            elif \"conf\" in key:\n                continue\n            elif \"orig\" in key:\n                continue\n            elif \"root\" in key or \"path\" in key:\n                struct[key] = struct[key].replace(old, new)", "language": "python", "code": "def rehome(old, new, struct):\n    \"\"\"\n    Replace all absolute paths to \"re-home\" it\n    \"\"\"\n\n    if old == new:\n        return\n\n    if isinstance(struct, list):\n        for item in struct:\n            rehome(old, new, item)\n    elif isinstance(struct, dict):\n        for key, val in struct.iteritems():\n            if isinstance(val, (dict, list)):\n                rehome(old, new, val)\n            elif \"conf\" in key:\n                continue\n            elif \"orig\" in key:\n                continue\n            elif \"root\" in key or \"path\" in key:\n                struct[key] = struct[key].replace(old, new)", "code_tokens": ["def", "rehome", "(", "old", ",", "new", ",", "struct", ")", ":", "if", "old", "==", "new", ":", "return", "if", "isinstance", "(", "struct", ",", "list", ")", ":", "for", "item", "in", "struct", ":", "rehome", "(", "old", ",", "new", ",", "item", ")", "elif", "isinstance", "(", "struct", ",", "dict", ")", ":", "for", "key", ",", "val", "in", "struct", ".", "iteritems", "(", ")", ":", "if", "isinstance", "(", "val", ",", "(", "dict", ",", "list", ")", ")", ":", "rehome", "(", "old", ",", "new", ",", "val", ")", "elif", "\"conf\"", "in", "key", ":", "continue", "elif", "\"orig\"", "in", "key", ":", "continue", "elif", "\"root\"", "in", "key", "or", "\"path\"", "in", "key", ":", "struct", "[", "key", "]", "=", "struct", "[", "key", "]", ".", "replace", "(", "old", ",", "new", ")"], "docstring": "", "docstring_tokens": [], "sha": "21d7b2ed4ff68e0a1457e7df2db27f6334f1a379", "url": "https://github.com/refenv/cijoe/blob/21d7b2ed4ff68e0a1457e7df2db27f6334f1a379/modules/cij/reporter.py#L252-L272", "partition": "valid"}
{"repo": "numenta/nupic", "path": "src/nupic/frameworks/opf/experiment_runner.py", "func_name": "_reportCommandLineUsageErrorAndExit", "original_string": "def _reportCommandLineUsageErrorAndExit(parser, message):\n  \"\"\"Report usage error and exit program with error indication.\"\"\"\n  print parser.get_usage()\n  print message\n  sys.exit(1)", "language": "python", "code": "def _reportCommandLineUsageErrorAndExit(parser, message):\n  \"\"\"Report usage error and exit program with error indication.\"\"\"\n  print parser.get_usage()\n  print message\n  sys.exit(1)", "code_tokens": ["def", "_reportCommandLineUsageErrorAndExit", "(", "parser", ",", "message", ")", ":", "print", "parser", ".", "get_usage", "(", ")", "print", "message", "sys", ".", "exit", "(", "1", ")"], "docstring": "", "docstring_tokens": [], "sha": "5922fafffdccc8812e72b3324965ad2f7d4bbdad", "url": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/frameworks/opf/experiment_runner.py#L379-L383", "partition": "valid"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/transpiler/passes/optimize_1q_gates.py", "func_name": "Optimize1qGates.yzy_to_zyz", "original_string": "def yzy_to_zyz(xi, theta1, theta2, eps=1e-9):  # pylint: disable=invalid-name\n        \"\"\"Express a Y.Z.Y single qubit gate as a Z.Y.Z gate.\n\n        Solve the equation\n\n        .. math::\n\n        Ry(theta1).Rz(xi).Ry(theta2) = Rz(phi).Ry(theta).Rz(lambda)\n\n        for theta, phi, and lambda.\n\n        Return a solution theta, phi, and lambda.\n        \"\"\"\n        quaternion_yzy = quaternion_from_euler([theta1, xi, theta2], 'yzy')\n        euler = quaternion_yzy.to_zyz()\n        quaternion_zyz = quaternion_from_euler(euler, 'zyz')\n        # output order different than rotation order\n        out_angles = (euler[1], euler[0], euler[2])\n        abs_inner = abs(quaternion_zyz.data.dot(quaternion_yzy.data))\n        if not np.allclose(abs_inner, 1, eps):\n            raise TranspilerError('YZY and ZYZ angles do not give same rotation matrix.')\n        out_angles = tuple(0 if np.abs(angle) < _CHOP_THRESHOLD else angle\n                           for angle in out_angles)\n        return out_angles", "language": "python", "code": "def yzy_to_zyz(xi, theta1, theta2, eps=1e-9):  # pylint: disable=invalid-name\n        \"\"\"Express a Y.Z.Y single qubit gate as a Z.Y.Z gate.\n\n        Solve the equation\n\n        .. math::\n\n        Ry(theta1).Rz(xi).Ry(theta2) = Rz(phi).Ry(theta).Rz(lambda)\n\n        for theta, phi, and lambda.\n\n        Return a solution theta, phi, and lambda.\n        \"\"\"\n        quaternion_yzy = quaternion_from_euler([theta1, xi, theta2], 'yzy')\n        euler = quaternion_yzy.to_zyz()\n        quaternion_zyz = quaternion_from_euler(euler, 'zyz')\n        # output order different than rotation order\n        out_angles = (euler[1], euler[0], euler[2])\n        abs_inner = abs(quaternion_zyz.data.dot(quaternion_yzy.data))\n        if not np.allclose(abs_inner, 1, eps):\n            raise TranspilerError('YZY and ZYZ angles do not give same rotation matrix.')\n        out_angles = tuple(0 if np.abs(angle) < _CHOP_THRESHOLD else angle\n                           for angle in out_angles)\n        return out_angles", "code_tokens": ["def", "yzy_to_zyz", "(", "xi", ",", "theta1", ",", "theta2", ",", "eps", "=", "1e-9", ")", ":", "# pylint: disable=invalid-name", "quaternion_yzy", "=", "quaternion_from_euler", "(", "[", "theta1", ",", "xi", ",", "theta2", "]", ",", "'yzy'", ")", "euler", "=", "quaternion_yzy", ".", "to_zyz", "(", ")", "quaternion_zyz", "=", "quaternion_from_euler", "(", "euler", ",", "'zyz'", ")", "# output order different than rotation order", "out_angles", "=", "(", "euler", "[", "1", "]", ",", "euler", "[", "0", "]", ",", "euler", "[", "2", "]", ")", "abs_inner", "=", "abs", "(", "quaternion_zyz", ".", "data", ".", "dot", "(", "quaternion_yzy", ".", "data", ")", ")", "if", "not", "np", ".", "allclose", "(", "abs_inner", ",", "1", ",", "eps", ")", ":", "raise", "TranspilerError", "(", "'YZY and ZYZ angles do not give same rotation matrix.'", ")", "out_angles", "=", "tuple", "(", "0", "if", "np", ".", "abs", "(", "angle", ")", "<", "_CHOP_THRESHOLD", "else", "angle", "for", "angle", "in", "out_angles", ")", "return", "out_angles"], "docstring": "", "docstring_tokens": [], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/transpiler/passes/optimize_1q_gates.py#L212-L235", "partition": "test"}
{"repo": "tensorlayer/tensorlayer", "path": "tensorlayer/layers/utils.py", "func_name": "merge_networks", "original_string": "def merge_networks(layers=None):\n    \"\"\"Merge all parameters, layers and dropout probabilities to a :class:`Layer`.\n    The output of return network is the first network in the list.\n\n    Parameters\n    ----------\n    layers : list of :class:`Layer`\n        Merge all parameters, layers and dropout probabilities to the first layer in the list.\n\n    Returns\n    --------\n    :class:`Layer`\n        The network after merging all parameters, layers and dropout probabilities to the first network in the list.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> n1 = ...\n    >>> n2 = ...\n    >>> n1 = tl.layers.merge_networks([n1, n2])\n\n    \"\"\"\n    if layers is None:\n        raise Exception(\"layers should be a list of TensorLayer's Layers.\")\n    layer = layers[0]\n\n    all_params = []\n    all_layers = []\n    all_drop = {}\n\n    for l in layers:\n        all_params.extend(l.all_params)\n        all_layers.extend(l.all_layers)\n        all_drop.update(l.all_drop)\n\n    layer.all_params = list(all_params)\n    layer.all_layers = list(all_layers)\n    layer.all_drop = dict(all_drop)\n\n    layer.all_layers = list_remove_repeat(layer.all_layers)\n    layer.all_params = list_remove_repeat(layer.all_params)\n\n    return layer", "language": "python", "code": "def merge_networks(layers=None):\n    \"\"\"Merge all parameters, layers and dropout probabilities to a :class:`Layer`.\n    The output of return network is the first network in the list.\n\n    Parameters\n    ----------\n    layers : list of :class:`Layer`\n        Merge all parameters, layers and dropout probabilities to the first layer in the list.\n\n    Returns\n    --------\n    :class:`Layer`\n        The network after merging all parameters, layers and dropout probabilities to the first network in the list.\n\n    Examples\n    ---------\n    >>> import tensorlayer as tl\n    >>> n1 = ...\n    >>> n2 = ...\n    >>> n1 = tl.layers.merge_networks([n1, n2])\n\n    \"\"\"\n    if layers is None:\n        raise Exception(\"layers should be a list of TensorLayer's Layers.\")\n    layer = layers[0]\n\n    all_params = []\n    all_layers = []\n    all_drop = {}\n\n    for l in layers:\n        all_params.extend(l.all_params)\n        all_layers.extend(l.all_layers)\n        all_drop.update(l.all_drop)\n\n    layer.all_params = list(all_params)\n    layer.all_layers = list(all_layers)\n    layer.all_drop = dict(all_drop)\n\n    layer.all_layers = list_remove_repeat(layer.all_layers)\n    layer.all_params = list_remove_repeat(layer.all_params)\n\n    return layer", "code_tokens": ["def", "merge_networks", "(", "layers", "=", "None", ")", ":", "if", "layers", "is", "None", ":", "raise", "Exception", "(", "\"layers should be a list of TensorLayer's Layers.\"", ")", "layer", "=", "layers", "[", "0", "]", "all_params", "=", "[", "]", "all_layers", "=", "[", "]", "all_drop", "=", "{", "}", "for", "l", "in", "layers", ":", "all_params", ".", "extend", "(", "l", ".", "all_params", ")", "all_layers", ".", "extend", "(", "l", ".", "all_layers", ")", "all_drop", ".", "update", "(", "l", ".", "all_drop", ")", "layer", ".", "all_params", "=", "list", "(", "all_params", ")", "layer", ".", "all_layers", "=", "list", "(", "all_layers", ")", "layer", ".", "all_drop", "=", "dict", "(", "all_drop", ")", "layer", ".", "all_layers", "=", "list_remove_repeat", "(", "layer", ".", "all_layers", ")", "layer", ".", "all_params", "=", "list_remove_repeat", "(", "layer", ".", "all_params", ")", "return", "layer"], "docstring": "", "docstring_tokens": [], "sha": "aa9e52e36c7058a7e6fd81d36563ca6850b21956", "url": "https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/layers/utils.py#L271-L313", "partition": "valid"}
{"repo": "tensorforce/tensorforce", "path": "tensorforce/models/q_demo_model.py", "func_name": "QDemoModel.get_variables", "original_string": "def get_variables(self, include_submodules=False, include_nontrainable=False):\n        \"\"\"\n        Returns the TensorFlow variables used by the model.\n\n        Returns:\n            List of variables.\n        \"\"\"\n        model_variables = super(QDemoModel, self).get_variables(\n            include_submodules=include_submodules,\n            include_nontrainable=include_nontrainable\n        )\n\n        if include_nontrainable:\n            demo_memory_variables = self.demo_memory.get_variables()\n            model_variables += demo_memory_variables\n\n        return model_variables", "language": "python", "code": "def get_variables(self, include_submodules=False, include_nontrainable=False):\n        \"\"\"\n        Returns the TensorFlow variables used by the model.\n\n        Returns:\n            List of variables.\n        \"\"\"\n        model_variables = super(QDemoModel, self).get_variables(\n            include_submodules=include_submodules,\n            include_nontrainable=include_nontrainable\n        )\n\n        if include_nontrainable:\n            demo_memory_variables = self.demo_memory.get_variables()\n            model_variables += demo_memory_variables\n\n        return model_variables", "code_tokens": ["def", "get_variables", "(", "self", ",", "include_submodules", "=", "False", ",", "include_nontrainable", "=", "False", ")", ":", "model_variables", "=", "super", "(", "QDemoModel", ",", "self", ")", ".", "get_variables", "(", "include_submodules", "=", "include_submodules", ",", "include_nontrainable", "=", "include_nontrainable", ")", "if", "include_nontrainable", ":", "demo_memory_variables", "=", "self", ".", "demo_memory", ".", "get_variables", "(", ")", "model_variables", "+=", "demo_memory_variables", "return", "model_variables"], "docstring": "", "docstring_tokens": [], "sha": "520a8d992230e382f08e315ede5fc477f5e26bfb", "url": "https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/tensorforce/models/q_demo_model.py#L299-L315", "partition": "valid"}
{"repo": "Ex-Mente/auxi.0", "path": "auxi/modelling/process/materials/thermo.py", "func_name": "MaterialPackage.get_element_mass_dictionary", "original_string": "def get_element_mass_dictionary(self):\n        \"\"\"\n        Determine the masses of elements in the package and return as a\n        dictionary.\n\n        :returns: Dictionary of element symbols and masses. [kg]\n        \"\"\"\n\n        element_symbols = self.material.elements\n        element_masses = self.get_element_masses()\n\n        return {s: m for s, m in zip(element_symbols, element_masses)}", "language": "python", "code": "def get_element_mass_dictionary(self):\n        \"\"\"\n        Determine the masses of elements in the package and return as a\n        dictionary.\n\n        :returns: Dictionary of element symbols and masses. [kg]\n        \"\"\"\n\n        element_symbols = self.material.elements\n        element_masses = self.get_element_masses()\n\n        return {s: m for s, m in zip(element_symbols, element_masses)}", "code_tokens": ["def", "get_element_mass_dictionary", "(", "self", ")", ":", "element_symbols", "=", "self", ".", "material", ".", "elements", "element_masses", "=", "self", ".", "get_element_masses", "(", ")", "return", "{", "s", ":", "m", "for", "s", ",", "m", "in", "zip", "(", "element_symbols", ",", "element_masses", ")", "}"], "docstring": "", "docstring_tokens": [], "sha": "2dcdae74154f136f8ca58289fe5b20772f215046", "url": "https://github.com/Ex-Mente/auxi.0/blob/2dcdae74154f136f8ca58289fe5b20772f215046/auxi/modelling/process/materials/thermo.py#L949-L960", "partition": "valid"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._prm_write_shared_table", "original_string": "def _prm_write_shared_table(self, key, hdf5_group, fullname, **kwargs):\n        \"\"\"Creates a new empty table\"\"\"\n        first_row = None\n        description = None\n        if 'first_row' in kwargs:\n            first_row = kwargs.pop('first_row')\n            if not 'description' in kwargs:\n                description = {}\n                for colname in first_row:\n                    data = first_row[colname]\n                    column = self._all_get_table_col(key, [data], fullname)\n                    description[colname] = column\n\n        if 'description' in kwargs:\n            description = kwargs.pop('description')\n\n        if 'filters' in kwargs:\n            filters = kwargs.pop('filters')\n        else:\n            filters = self._all_get_filters(kwargs)\n\n        table = self._hdf5file.create_table(where=hdf5_group, name=key,\n                                          description=description,\n                                          filters=filters,\n                                          **kwargs)\n        table.flush()\n\n        if first_row is not None:\n            row = table.row\n            for key in description:\n                row[key] = first_row[key]\n\n            row.append()\n            table.flush()", "language": "python", "code": "def _prm_write_shared_table(self, key, hdf5_group, fullname, **kwargs):\n        \"\"\"Creates a new empty table\"\"\"\n        first_row = None\n        description = None\n        if 'first_row' in kwargs:\n            first_row = kwargs.pop('first_row')\n            if not 'description' in kwargs:\n                description = {}\n                for colname in first_row:\n                    data = first_row[colname]\n                    column = self._all_get_table_col(key, [data], fullname)\n                    description[colname] = column\n\n        if 'description' in kwargs:\n            description = kwargs.pop('description')\n\n        if 'filters' in kwargs:\n            filters = kwargs.pop('filters')\n        else:\n            filters = self._all_get_filters(kwargs)\n\n        table = self._hdf5file.create_table(where=hdf5_group, name=key,\n                                          description=description,\n                                          filters=filters,\n                                          **kwargs)\n        table.flush()\n\n        if first_row is not None:\n            row = table.row\n            for key in description:\n                row[key] = first_row[key]\n\n            row.append()\n            table.flush()", "code_tokens": ["def", "_prm_write_shared_table", "(", "self", ",", "key", ",", "hdf5_group", ",", "fullname", ",", "*", "*", "kwargs", ")", ":", "first_row", "=", "None", "description", "=", "None", "if", "'first_row'", "in", "kwargs", ":", "first_row", "=", "kwargs", ".", "pop", "(", "'first_row'", ")", "if", "not", "'description'", "in", "kwargs", ":", "description", "=", "{", "}", "for", "colname", "in", "first_row", ":", "data", "=", "first_row", "[", "colname", "]", "column", "=", "self", ".", "_all_get_table_col", "(", "key", ",", "[", "data", "]", ",", "fullname", ")", "description", "[", "colname", "]", "=", "column", "if", "'description'", "in", "kwargs", ":", "description", "=", "kwargs", ".", "pop", "(", "'description'", ")", "if", "'filters'", "in", "kwargs", ":", "filters", "=", "kwargs", ".", "pop", "(", "'filters'", ")", "else", ":", "filters", "=", "self", ".", "_all_get_filters", "(", "kwargs", ")", "table", "=", "self", ".", "_hdf5file", ".", "create_table", "(", "where", "=", "hdf5_group", ",", "name", "=", "key", ",", "description", "=", "description", ",", "filters", "=", "filters", ",", "*", "*", "kwargs", ")", "table", ".", "flush", "(", ")", "if", "first_row", "is", "not", "None", ":", "row", "=", "table", ".", "row", "for", "key", "in", "description", ":", "row", "[", "key", "]", "=", "first_row", "[", "key", "]", "row", ".", "append", "(", ")", "table", ".", "flush", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L4075-L4108", "partition": "test"}
{"repo": "rainwoodman/sharedmem", "path": "sharedmem/parallel.py", "func_name": "Barrier.abort", "original_string": "def abort(self):\n        \"\"\" ensure the master exit from Barrier \"\"\"\n        self.mutex.release()\n        self.turnstile.release()\n        self.mutex.release()\n        self.turnstile2.release()", "language": "python", "code": "def abort(self):\n        \"\"\" ensure the master exit from Barrier \"\"\"\n        self.mutex.release()\n        self.turnstile.release()\n        self.mutex.release()\n        self.turnstile2.release()", "code_tokens": ["def", "abort", "(", "self", ")", ":", "self", ".", "mutex", ".", "release", "(", ")", "self", ".", "turnstile", ".", "release", "(", ")", "self", ".", "mutex", ".", "release", "(", ")", "self", ".", "turnstile2", ".", "release", "(", ")"], "docstring": "", "docstring_tokens": [], "sha": "b23e59c1ed0e28f7b6c96c17a04d55c700e06e3a", "url": "https://github.com/rainwoodman/sharedmem/blob/b23e59c1ed0e28f7b6c96c17a04d55c700e06e3a/sharedmem/parallel.py#L387-L392", "partition": "valid"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-mgmt-hdinsight/azure/mgmt/hdinsight/operations/clusters_operations.py", "func_name": "ClustersOperations.execute_script_actions", "original_string": "def execute_script_actions(\n            self, resource_group_name, cluster_name, persist_on_success, script_actions=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Executes script actions on the specified HDInsight cluster.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param cluster_name: The name of the cluster.\n        :type cluster_name: str\n        :param persist_on_success: Gets or sets if the scripts needs to be\n         persisted.\n        :type persist_on_success: bool\n        :param script_actions: The list of run time script actions.\n        :type script_actions:\n         list[~azure.mgmt.hdinsight.models.RuntimeScriptAction]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.hdinsight.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._execute_script_actions_initial(\n            resource_group_name=resource_group_name,\n            cluster_name=cluster_name,\n            persist_on_success=persist_on_success,\n            script_actions=script_actions,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)", "language": "python", "code": "def execute_script_actions(\n            self, resource_group_name, cluster_name, persist_on_success, script_actions=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Executes script actions on the specified HDInsight cluster.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param cluster_name: The name of the cluster.\n        :type cluster_name: str\n        :param persist_on_success: Gets or sets if the scripts needs to be\n         persisted.\n        :type persist_on_success: bool\n        :param script_actions: The list of run time script actions.\n        :type script_actions:\n         list[~azure.mgmt.hdinsight.models.RuntimeScriptAction]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.hdinsight.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._execute_script_actions_initial(\n            resource_group_name=resource_group_name,\n            cluster_name=cluster_name,\n            persist_on_success=persist_on_success,\n            script_actions=script_actions,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)", "code_tokens": ["def", "execute_script_actions", "(", "self", ",", "resource_group_name", ",", "cluster_name", ",", "persist_on_success", ",", "script_actions", "=", "None", ",", "custom_headers", "=", "None", ",", "raw", "=", "False", ",", "polling", "=", "True", ",", "*", "*", "operation_config", ")", ":", "raw_result", "=", "self", ".", "_execute_script_actions_initial", "(", "resource_group_name", "=", "resource_group_name", ",", "cluster_name", "=", "cluster_name", ",", "persist_on_success", "=", "persist_on_success", ",", "script_actions", "=", "script_actions", ",", "custom_headers", "=", "custom_headers", ",", "raw", "=", "True", ",", "*", "*", "operation_config", ")", "def", "get_long_running_output", "(", "response", ")", ":", "if", "raw", ":", "client_raw_response", "=", "ClientRawResponse", "(", "None", ",", "response", ")", "return", "client_raw_response", "lro_delay", "=", "operation_config", ".", "get", "(", "'long_running_operation_timeout'", ",", "self", ".", "config", ".", "long_running_operation_timeout", ")", "if", "polling", "is", "True", ":", "polling_method", "=", "ARMPolling", "(", "lro_delay", ",", "*", "*", "operation_config", ")", "elif", "polling", "is", "False", ":", "polling_method", "=", "NoPolling", "(", ")", "else", ":", "polling_method", "=", "polling", "return", "LROPoller", "(", "self", ".", "_client", ",", "raw_result", ",", "get_long_running_output", ",", "polling_method", ")"], "docstring": "", "docstring_tokens": [], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-hdinsight/azure/mgmt/hdinsight/operations/clusters_operations.py#L844-L891", "partition": "test"}
{"repo": "gunyarakun/python-shogi", "path": "shogi/__init__.py", "func_name": "Board.sfen", "original_string": "def sfen(self):\n        '''\n        Gets an SFEN representation of the current position.\n        '''\n        sfen = []\n        empty = 0\n\n        # Position part.\n        for square in SQUARES:\n            piece = self.piece_at(square)\n\n            if not piece:\n                empty += 1\n            else:\n                if empty:\n                    sfen.append(str(empty))\n                    empty = 0\n                sfen.append(piece.symbol())\n\n            if BB_SQUARES[square] & BB_FILE_1:\n                if empty:\n                    sfen.append(str(empty))\n                    empty = 0\n\n                if square != I1:\n                    sfen.append('/')\n\n        sfen.append(' ')\n\n        # Side to move.\n        if self.turn == WHITE:\n            sfen.append('w')\n        else:\n            sfen.append('b')\n\n        sfen.append(' ')\n\n        # Pieces in hand\n        pih_len = 0\n        for color in COLORS:\n            p = self.pieces_in_hand[color]\n            pih_len += len(p)\n            for piece_type in sorted(p.keys(), reverse=True):\n                if p[piece_type] >= 1:\n                    if p[piece_type] > 1:\n                        sfen.append(str(p[piece_type]))\n                    piece = Piece(piece_type, color)\n                    sfen.append(piece.symbol())\n        if pih_len == 0:\n            sfen.append('-')\n\n        sfen.append(' ')\n\n        # Move count\n        sfen.append(str(self.move_number))\n\n        return ''.join(sfen)", "language": "python", "code": "def sfen(self):\n        '''\n        Gets an SFEN representation of the current position.\n        '''\n        sfen = []\n        empty = 0\n\n        # Position part.\n        for square in SQUARES:\n            piece = self.piece_at(square)\n\n            if not piece:\n                empty += 1\n            else:\n                if empty:\n                    sfen.append(str(empty))\n                    empty = 0\n                sfen.append(piece.symbol())\n\n            if BB_SQUARES[square] & BB_FILE_1:\n                if empty:\n                    sfen.append(str(empty))\n                    empty = 0\n\n                if square != I1:\n                    sfen.append('/')\n\n        sfen.append(' ')\n\n        # Side to move.\n        if self.turn == WHITE:\n            sfen.append('w')\n        else:\n            sfen.append('b')\n\n        sfen.append(' ')\n\n        # Pieces in hand\n        pih_len = 0\n        for color in COLORS:\n            p = self.pieces_in_hand[color]\n            pih_len += len(p)\n            for piece_type in sorted(p.keys(), reverse=True):\n                if p[piece_type] >= 1:\n                    if p[piece_type] > 1:\n                        sfen.append(str(p[piece_type]))\n                    piece = Piece(piece_type, color)\n                    sfen.append(piece.symbol())\n        if pih_len == 0:\n            sfen.append('-')\n\n        sfen.append(' ')\n\n        # Move count\n        sfen.append(str(self.move_number))\n\n        return ''.join(sfen)", "code_tokens": ["def", "sfen", "(", "self", ")", ":", "sfen", "=", "[", "]", "empty", "=", "0", "# Position part.", "for", "square", "in", "SQUARES", ":", "piece", "=", "self", ".", "piece_at", "(", "square", ")", "if", "not", "piece", ":", "empty", "+=", "1", "else", ":", "if", "empty", ":", "sfen", ".", "append", "(", "str", "(", "empty", ")", ")", "empty", "=", "0", "sfen", ".", "append", "(", "piece", ".", "symbol", "(", ")", ")", "if", "BB_SQUARES", "[", "square", "]", "&", "BB_FILE_1", ":", "if", "empty", ":", "sfen", ".", "append", "(", "str", "(", "empty", ")", ")", "empty", "=", "0", "if", "square", "!=", "I1", ":", "sfen", ".", "append", "(", "'/'", ")", "sfen", ".", "append", "(", "' '", ")", "# Side to move.", "if", "self", ".", "turn", "==", "WHITE", ":", "sfen", ".", "append", "(", "'w'", ")", "else", ":", "sfen", ".", "append", "(", "'b'", ")", "sfen", ".", "append", "(", "' '", ")", "# Pieces in hand", "pih_len", "=", "0", "for", "color", "in", "COLORS", ":", "p", "=", "self", ".", "pieces_in_hand", "[", "color", "]", "pih_len", "+=", "len", "(", "p", ")", "for", "piece_type", "in", "sorted", "(", "p", ".", "keys", "(", ")", ",", "reverse", "=", "True", ")", ":", "if", "p", "[", "piece_type", "]", ">=", "1", ":", "if", "p", "[", "piece_type", "]", ">", "1", ":", "sfen", ".", "append", "(", "str", "(", "p", "[", "piece_type", "]", ")", ")", "piece", "=", "Piece", "(", "piece_type", ",", "color", ")", "sfen", ".", "append", "(", "piece", ".", "symbol", "(", ")", ")", "if", "pih_len", "==", "0", ":", "sfen", ".", "append", "(", "'-'", ")", "sfen", ".", "append", "(", "' '", ")", "# Move count", "sfen", ".", "append", "(", "str", "(", "self", ".", "move_number", ")", ")", "return", "''", ".", "join", "(", "sfen", ")"], "docstring": "", "docstring_tokens": [], "sha": "137fe5f5e72251e8a97a1dba4a9b44b7c3c79914", "url": "https://github.com/gunyarakun/python-shogi/blob/137fe5f5e72251e8a97a1dba4a9b44b7c3c79914/shogi/__init__.py#L1069-L1125", "partition": "test"}
{"repo": "fossasia/knittingpattern", "path": "knittingpattern/Dumper/json.py", "func_name": "JSONDumper.knitting_pattern", "original_string": "def knitting_pattern(self, specification=None):\n        \"\"\"loads a :class:`knitting pattern\n        <knittingpattern.KnittingPattern.KnittingPattern>` from the dumped\n        content\n\n        :param specification: a\n          :class:`~knittingpattern.ParsingSpecification.ParsingSpecification`\n          or :obj:`None` to use the default specification\"\"\"\n        from ..ParsingSpecification import new_knitting_pattern_set_loader\n        if specification is None:\n            loader = new_knitting_pattern_set_loader()\n        else:\n            loader = new_knitting_pattern_set_loader(specification)\n        return loader.object(self.object())", "language": "python", "code": "def knitting_pattern(self, specification=None):\n        \"\"\"loads a :class:`knitting pattern\n        <knittingpattern.KnittingPattern.KnittingPattern>` from the dumped\n        content\n\n        :param specification: a\n          :class:`~knittingpattern.ParsingSpecification.ParsingSpecification`\n          or :obj:`None` to use the default specification\"\"\"\n        from ..ParsingSpecification import new_knitting_pattern_set_loader\n        if specification is None:\n            loader = new_knitting_pattern_set_loader()\n        else:\n            loader = new_knitting_pattern_set_loader(specification)\n        return loader.object(self.object())", "code_tokens": ["def", "knitting_pattern", "(", "self", ",", "specification", "=", "None", ")", ":", "from", ".", ".", "ParsingSpecification", "import", "new_knitting_pattern_set_loader", "if", "specification", "is", "None", ":", "loader", "=", "new_knitting_pattern_set_loader", "(", ")", "else", ":", "loader", "=", "new_knitting_pattern_set_loader", "(", "specification", ")", "return", "loader", ".", "object", "(", "self", ".", "object", "(", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027", "url": "https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Dumper/json.py#L26-L39", "partition": "valid"}
{"repo": "ambv/retype", "path": "retype.py", "func_name": "main", "original_string": "def main(src, pyi_dir, target_dir, incremental, quiet, replace_any, hg, traceback):\n    \"\"\"Re-apply type annotations from .pyi stubs to your codebase.\"\"\"\n    Config.incremental = incremental\n    Config.replace_any = replace_any\n    returncode = 0\n    for src_entry in src:\n        for file, error, exc_type, tb in retype_path(\n            Path(src_entry),\n            pyi_dir=Path(pyi_dir),\n            targets=Path(target_dir),\n            src_explicitly_given=True,\n            quiet=quiet,\n            hg=hg,\n        ):\n            print(f'error: {file}: {error}', file=sys.stderr)\n            if traceback:\n                print('Traceback (most recent call last):', file=sys.stderr)\n                for line in tb:\n                    print(line, file=sys.stderr, end='')\n                print(f'{exc_type.__name__}: {error}', file=sys.stderr)\n            returncode += 1\n    if not src and not quiet:\n        print('warning: no sources given', file=sys.stderr)\n\n    # According to http://tldp.org/LDP/abs/html/index.html starting with 126\n    # we have special returncodes.\n    sys.exit(min(returncode, 125))", "language": "python", "code": "def main(src, pyi_dir, target_dir, incremental, quiet, replace_any, hg, traceback):\n    \"\"\"Re-apply type annotations from .pyi stubs to your codebase.\"\"\"\n    Config.incremental = incremental\n    Config.replace_any = replace_any\n    returncode = 0\n    for src_entry in src:\n        for file, error, exc_type, tb in retype_path(\n            Path(src_entry),\n            pyi_dir=Path(pyi_dir),\n            targets=Path(target_dir),\n            src_explicitly_given=True,\n            quiet=quiet,\n            hg=hg,\n        ):\n            print(f'error: {file}: {error}', file=sys.stderr)\n            if traceback:\n                print('Traceback (most recent call last):', file=sys.stderr)\n                for line in tb:\n                    print(line, file=sys.stderr, end='')\n                print(f'{exc_type.__name__}: {error}', file=sys.stderr)\n            returncode += 1\n    if not src and not quiet:\n        print('warning: no sources given', file=sys.stderr)\n\n    # According to http://tldp.org/LDP/abs/html/index.html starting with 126\n    # we have special returncodes.\n    sys.exit(min(returncode, 125))", "code_tokens": ["def", "main", "(", "src", ",", "pyi_dir", ",", "target_dir", ",", "incremental", ",", "quiet", ",", "replace_any", ",", "hg", ",", "traceback", ")", ":", "Config", ".", "incremental", "=", "incremental", "Config", ".", "replace_any", "=", "replace_any", "returncode", "=", "0", "for", "src_entry", "in", "src", ":", "for", "file", ",", "error", ",", "exc_type", ",", "tb", "in", "retype_path", "(", "Path", "(", "src_entry", ")", ",", "pyi_dir", "=", "Path", "(", "pyi_dir", ")", ",", "targets", "=", "Path", "(", "target_dir", ")", ",", "src_explicitly_given", "=", "True", ",", "quiet", "=", "quiet", ",", "hg", "=", "hg", ",", ")", ":", "print", "(", "f'error: {file}: {error}'", ",", "file", "=", "sys", ".", "stderr", ")", "if", "traceback", ":", "print", "(", "'Traceback (most recent call last):'", ",", "file", "=", "sys", ".", "stderr", ")", "for", "line", "in", "tb", ":", "print", "(", "line", ",", "file", "=", "sys", ".", "stderr", ",", "end", "=", "''", ")", "print", "(", "f'{exc_type.__name__}: {error}'", ",", "file", "=", "sys", ".", "stderr", ")", "returncode", "+=", "1", "if", "not", "src", "and", "not", "quiet", ":", "print", "(", "'warning: no sources given'", ",", "file", "=", "sys", ".", "stderr", ")", "# According to http://tldp.org/LDP/abs/html/index.html starting with 126", "# we have special returncodes.", "sys", ".", "exit", "(", "min", "(", "returncode", ",", "125", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "03137abd4d9c9845f3cced1006190b5cca64d879", "url": "https://github.com/ambv/retype/blob/03137abd4d9c9845f3cced1006190b5cca64d879/retype.py#L87-L113", "partition": "valid"}
{"repo": "nosedjango/nosedjango", "path": "nosedjango/nosedjango.py", "func_name": "get_settings_path", "original_string": "def get_settings_path(settings_module):\n    '''\n    Hunt down the settings.py module by going up the FS path\n    '''\n    cwd = os.getcwd()\n    settings_filename = '%s.py' % (\n        settings_module.split('.')[-1]\n    )\n    while cwd:\n        if settings_filename in os.listdir(cwd):\n            break\n        cwd = os.path.split(cwd)[0]\n        if os.name == 'nt' and NT_ROOT.match(cwd):\n            return None\n        elif cwd == '/':\n            return None\n    return cwd", "language": "python", "code": "def get_settings_path(settings_module):\n    '''\n    Hunt down the settings.py module by going up the FS path\n    '''\n    cwd = os.getcwd()\n    settings_filename = '%s.py' % (\n        settings_module.split('.')[-1]\n    )\n    while cwd:\n        if settings_filename in os.listdir(cwd):\n            break\n        cwd = os.path.split(cwd)[0]\n        if os.name == 'nt' and NT_ROOT.match(cwd):\n            return None\n        elif cwd == '/':\n            return None\n    return cwd", "code_tokens": ["def", "get_settings_path", "(", "settings_module", ")", ":", "cwd", "=", "os", ".", "getcwd", "(", ")", "settings_filename", "=", "'%s.py'", "%", "(", "settings_module", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "while", "cwd", ":", "if", "settings_filename", "in", "os", ".", "listdir", "(", "cwd", ")", ":", "break", "cwd", "=", "os", ".", "path", ".", "split", "(", "cwd", ")", "[", "0", "]", "if", "os", ".", "name", "==", "'nt'", "and", "NT_ROOT", ".", "match", "(", "cwd", ")", ":", "return", "None", "elif", "cwd", "==", "'/'", ":", "return", "None", "return", "cwd"], "docstring": "", "docstring_tokens": [], "sha": "cd4d06857c88291769bc38e5c9573f43b7ffcd6a", "url": "https://github.com/nosedjango/nosedjango/blob/cd4d06857c88291769bc38e5c9573f43b7ffcd6a/nosedjango/nosedjango.py#L29-L45", "partition": "valid"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py", "func_name": "ServiceManagementService.shutdown_roles", "original_string": "def shutdown_roles(self, service_name, deployment_name, role_names,\n                       post_shutdown_action='Stopped'):\n        '''\n        Shuts down the specified virtual machines.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_names:\n            The names of the roles, as an enumerable of strings.\n        post_shutdown_action:\n            Specifies how the Virtual Machine should be shut down. Values are:\n                Stopped\n                    Shuts down the Virtual Machine but retains the compute\n                    resources. You will continue to be billed for the resources\n                    that the stopped machine uses.\n                StoppedDeallocated\n                    Shuts down the Virtual Machine and releases the compute\n                    resources. You are not billed for the compute resources that\n                    this Virtual Machine uses. If a static Virtual Network IP\n                    address is assigned to the Virtual Machine, it is reserved.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.shutdown_roles_operation_to_xml(\n                role_names, post_shutdown_action),\n            as_async=True)", "language": "python", "code": "def shutdown_roles(self, service_name, deployment_name, role_names,\n                       post_shutdown_action='Stopped'):\n        '''\n        Shuts down the specified virtual machines.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_names:\n            The names of the roles, as an enumerable of strings.\n        post_shutdown_action:\n            Specifies how the Virtual Machine should be shut down. Values are:\n                Stopped\n                    Shuts down the Virtual Machine but retains the compute\n                    resources. You will continue to be billed for the resources\n                    that the stopped machine uses.\n                StoppedDeallocated\n                    Shuts down the Virtual Machine and releases the compute\n                    resources. You are not billed for the compute resources that\n                    this Virtual Machine uses. If a static Virtual Network IP\n                    address is assigned to the Virtual Machine, it is reserved.\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.shutdown_roles_operation_to_xml(\n                role_names, post_shutdown_action),\n            as_async=True)", "code_tokens": ["def", "shutdown_roles", "(", "self", ",", "service_name", ",", "deployment_name", ",", "role_names", ",", "post_shutdown_action", "=", "'Stopped'", ")", ":", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "_validate_not_none", "(", "'deployment_name'", ",", "deployment_name", ")", "_validate_not_none", "(", "'role_names'", ",", "role_names", ")", "_validate_not_none", "(", "'post_shutdown_action'", ",", "post_shutdown_action", ")", "return", "self", ".", "_perform_post", "(", "self", ".", "_get_roles_operations_path", "(", "service_name", ",", "deployment_name", ")", ",", "_XmlSerializer", ".", "shutdown_roles_operation_to_xml", "(", "role_names", ",", "post_shutdown_action", ")", ",", "as_async", "=", "True", ")"], "docstring": "", "docstring_tokens": [], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementservice.py#L1758-L1789", "partition": "test"}
{"repo": "ui/django-thumbnails", "path": "thumbnails/images.py", "func_name": "create", "original_string": "def create(source_name, size, metadata_backend=None, storage_backend=None):\n    \"\"\"\n    Creates a thumbnail file and its relevant metadata. Returns a\n    Thumbnail instance.\n    \"\"\"\n\n    if storage_backend is None:\n        storage_backend = backends.storage.get_backend()\n    if metadata_backend is None:\n        metadata_backend = backends.metadata.get_backend()\n\n    thumbnail_file = processors.process(storage_backend.open(source_name), size)\n    thumbnail_file = post_processors.process(thumbnail_file, size)\n    name = get_thumbnail_name(source_name, size)\n    name = storage_backend.save(name, thumbnail_file)\n\n    metadata = metadata_backend.add_thumbnail(source_name, size, name)\n    return Thumbnail(metadata=metadata, storage=storage_backend)", "language": "python", "code": "def create(source_name, size, metadata_backend=None, storage_backend=None):\n    \"\"\"\n    Creates a thumbnail file and its relevant metadata. Returns a\n    Thumbnail instance.\n    \"\"\"\n\n    if storage_backend is None:\n        storage_backend = backends.storage.get_backend()\n    if metadata_backend is None:\n        metadata_backend = backends.metadata.get_backend()\n\n    thumbnail_file = processors.process(storage_backend.open(source_name), size)\n    thumbnail_file = post_processors.process(thumbnail_file, size)\n    name = get_thumbnail_name(source_name, size)\n    name = storage_backend.save(name, thumbnail_file)\n\n    metadata = metadata_backend.add_thumbnail(source_name, size, name)\n    return Thumbnail(metadata=metadata, storage=storage_backend)", "code_tokens": ["def", "create", "(", "source_name", ",", "size", ",", "metadata_backend", "=", "None", ",", "storage_backend", "=", "None", ")", ":", "if", "storage_backend", "is", "None", ":", "storage_backend", "=", "backends", ".", "storage", ".", "get_backend", "(", ")", "if", "metadata_backend", "is", "None", ":", "metadata_backend", "=", "backends", ".", "metadata", ".", "get_backend", "(", ")", "thumbnail_file", "=", "processors", ".", "process", "(", "storage_backend", ".", "open", "(", "source_name", ")", ",", "size", ")", "thumbnail_file", "=", "post_processors", ".", "process", "(", "thumbnail_file", ",", "size", ")", "name", "=", "get_thumbnail_name", "(", "source_name", ",", "size", ")", "name", "=", "storage_backend", ".", "save", "(", "name", ",", "thumbnail_file", ")", "metadata", "=", "metadata_backend", ".", "add_thumbnail", "(", "source_name", ",", "size", ",", "name", ")", "return", "Thumbnail", "(", "metadata", "=", "metadata", ",", "storage", "=", "storage_backend", ")"], "docstring": "", "docstring_tokens": [], "sha": "5cef55e7f167060458709ed760dd43981124796a", "url": "https://github.com/ui/django-thumbnails/blob/5cef55e7f167060458709ed760dd43981124796a/thumbnails/images.py#L68-L85", "partition": "test"}
{"repo": "waqasbhatti/astrobase", "path": "astrobase/lcdb.py", "func_name": "LCDB.open_default", "original_string": "def open_default(self):\n        '''\n        This opens the database connection using the default database parameters\n        given in the ~/.astrobase/astrobase.conf file.\n\n        '''\n\n        if HAVECONF:\n            self.open(DBDATA, DBUSER, DBPASS, DBHOST)\n        else:\n            LOGERROR(\"no default DB connection config found in lcdb.conf, \"\n                     \"this function won't work otherwise\")", "language": "python", "code": "def open_default(self):\n        '''\n        This opens the database connection using the default database parameters\n        given in the ~/.astrobase/astrobase.conf file.\n\n        '''\n\n        if HAVECONF:\n            self.open(DBDATA, DBUSER, DBPASS, DBHOST)\n        else:\n            LOGERROR(\"no default DB connection config found in lcdb.conf, \"\n                     \"this function won't work otherwise\")", "code_tokens": ["def", "open_default", "(", "self", ")", ":", "if", "HAVECONF", ":", "self", ".", "open", "(", "DBDATA", ",", "DBUSER", ",", "DBPASS", ",", "DBHOST", ")", "else", ":", "LOGERROR", "(", "\"no default DB connection config found in lcdb.conf, \"", "\"this function won't work otherwise\"", ")"], "docstring": "", "docstring_tokens": [], "sha": "2922a14619d183fb28005fa7d02027ac436f2265", "url": "https://github.com/waqasbhatti/astrobase/blob/2922a14619d183fb28005fa7d02027ac436f2265/astrobase/lcdb.py#L265-L276", "partition": "valid"}
{"repo": "chrisspen/burlap", "path": "burlap/deb.py", "func_name": "uninstall", "original_string": "def uninstall(packages, purge=False, options=None):\n    \"\"\"\n    Remove one or more packages.\n\n    If *purge* is ``True``, the package configuration files will be\n    removed from the system.\n\n    Extra *options* may be passed to ``apt-get`` if necessary.\n    \"\"\"\n    manager = MANAGER\n    command = \"purge\" if purge else \"remove\"\n    if options is None:\n        options = []\n    if not isinstance(packages, six.string_types):\n        packages = \" \".join(packages)\n    options.append(\"--assume-yes\")\n    options = \" \".join(options)\n    cmd = '%(manager)s %(command)s %(options)s %(packages)s' % locals()\n    run_as_root(cmd, pty=False)", "language": "python", "code": "def uninstall(packages, purge=False, options=None):\n    \"\"\"\n    Remove one or more packages.\n\n    If *purge* is ``True``, the package configuration files will be\n    removed from the system.\n\n    Extra *options* may be passed to ``apt-get`` if necessary.\n    \"\"\"\n    manager = MANAGER\n    command = \"purge\" if purge else \"remove\"\n    if options is None:\n        options = []\n    if not isinstance(packages, six.string_types):\n        packages = \" \".join(packages)\n    options.append(\"--assume-yes\")\n    options = \" \".join(options)\n    cmd = '%(manager)s %(command)s %(options)s %(packages)s' % locals()\n    run_as_root(cmd, pty=False)", "code_tokens": ["def", "uninstall", "(", "packages", ",", "purge", "=", "False", ",", "options", "=", "None", ")", ":", "manager", "=", "MANAGER", "command", "=", "\"purge\"", "if", "purge", "else", "\"remove\"", "if", "options", "is", "None", ":", "options", "=", "[", "]", "if", "not", "isinstance", "(", "packages", ",", "six", ".", "string_types", ")", ":", "packages", "=", "\" \"", ".", "join", "(", "packages", ")", "options", ".", "append", "(", "\"--assume-yes\"", ")", "options", "=", "\" \"", ".", "join", "(", "options", ")", "cmd", "=", "'%(manager)s %(command)s %(options)s %(packages)s'", "%", "locals", "(", ")", "run_as_root", "(", "cmd", ",", "pty", "=", "False", ")"], "docstring": "", "docstring_tokens": [], "sha": "a92b0a8e5206850bb777c74af8421ea8b33779bd", "url": "https://github.com/chrisspen/burlap/blob/a92b0a8e5206850bb777c74af8421ea8b33779bd/burlap/deb.py#L103-L121", "partition": "valid"}
{"repo": "HydraChain/hydrachain", "path": "hydrachain/consensus/synchronizer.py", "func_name": "Synchronizer.request", "original_string": "def request(self):\n        \"\"\"\n        sync the missing blocks between:\n            head\n            highest height with signing lockset\n\n        we get these locksets by collecting votes on all heights\n        \"\"\"\n        missing = self.missing\n        self.cm.log('sync.request', missing=len(missing), requested=len(self.requested),\n                    received=len(self.received))\n        if self.requested:\n            self.cm.log('waiting for requested')\n            return\n        if len(self.received) + self.max_getproposals_count >= self.max_queued:\n            self.cm.log('queue is full')\n            return\n        if not missing:\n            self.cm.log('insync')\n            return\n        if self.last_active_protocol is None:  # FIXME, check if it is active\n            self.cm.log('no active protocol', last_active_protocol=self.last_active_protocol)\n            return\n        self.cm.log('collecting')\n        blocknumbers = []\n        for h in missing:\n            if h not in self.received and h not in self.requested:\n                blocknumbers.append(h)\n                self.requested.add(h)\n                if len(blocknumbers) == self.max_getproposals_count:\n                    break\n        self.cm.log('collected', num=len(blocknumbers))\n        if not blocknumbers:\n            return\n        self.cm.log('requesting', num=len(blocknumbers),\n                    requesting_range=(blocknumbers[0], blocknumbers[-1]))\n        self.last_active_protocol.send_getblockproposals(*blocknumbers)\n        # setup alarm\n        self.cm.chainservice.setup_alarm(self.timeout, self.on_alarm, blocknumbers)", "language": "python", "code": "def request(self):\n        \"\"\"\n        sync the missing blocks between:\n            head\n            highest height with signing lockset\n\n        we get these locksets by collecting votes on all heights\n        \"\"\"\n        missing = self.missing\n        self.cm.log('sync.request', missing=len(missing), requested=len(self.requested),\n                    received=len(self.received))\n        if self.requested:\n            self.cm.log('waiting for requested')\n            return\n        if len(self.received) + self.max_getproposals_count >= self.max_queued:\n            self.cm.log('queue is full')\n            return\n        if not missing:\n            self.cm.log('insync')\n            return\n        if self.last_active_protocol is None:  # FIXME, check if it is active\n            self.cm.log('no active protocol', last_active_protocol=self.last_active_protocol)\n            return\n        self.cm.log('collecting')\n        blocknumbers = []\n        for h in missing:\n            if h not in self.received and h not in self.requested:\n                blocknumbers.append(h)\n                self.requested.add(h)\n                if len(blocknumbers) == self.max_getproposals_count:\n                    break\n        self.cm.log('collected', num=len(blocknumbers))\n        if not blocknumbers:\n            return\n        self.cm.log('requesting', num=len(blocknumbers),\n                    requesting_range=(blocknumbers[0], blocknumbers[-1]))\n        self.last_active_protocol.send_getblockproposals(*blocknumbers)\n        # setup alarm\n        self.cm.chainservice.setup_alarm(self.timeout, self.on_alarm, blocknumbers)", "code_tokens": ["def", "request", "(", "self", ")", ":", "missing", "=", "self", ".", "missing", "self", ".", "cm", ".", "log", "(", "'sync.request'", ",", "missing", "=", "len", "(", "missing", ")", ",", "requested", "=", "len", "(", "self", ".", "requested", ")", ",", "received", "=", "len", "(", "self", ".", "received", ")", ")", "if", "self", ".", "requested", ":", "self", ".", "cm", ".", "log", "(", "'waiting for requested'", ")", "return", "if", "len", "(", "self", ".", "received", ")", "+", "self", ".", "max_getproposals_count", ">=", "self", ".", "max_queued", ":", "self", ".", "cm", ".", "log", "(", "'queue is full'", ")", "return", "if", "not", "missing", ":", "self", ".", "cm", ".", "log", "(", "'insync'", ")", "return", "if", "self", ".", "last_active_protocol", "is", "None", ":", "# FIXME, check if it is active", "self", ".", "cm", ".", "log", "(", "'no active protocol'", ",", "last_active_protocol", "=", "self", ".", "last_active_protocol", ")", "return", "self", ".", "cm", ".", "log", "(", "'collecting'", ")", "blocknumbers", "=", "[", "]", "for", "h", "in", "missing", ":", "if", "h", "not", "in", "self", ".", "received", "and", "h", "not", "in", "self", ".", "requested", ":", "blocknumbers", ".", "append", "(", "h", ")", "self", ".", "requested", ".", "add", "(", "h", ")", "if", "len", "(", "blocknumbers", ")", "==", "self", ".", "max_getproposals_count", ":", "break", "self", ".", "cm", ".", "log", "(", "'collected'", ",", "num", "=", "len", "(", "blocknumbers", ")", ")", "if", "not", "blocknumbers", ":", "return", "self", ".", "cm", ".", "log", "(", "'requesting'", ",", "num", "=", "len", "(", "blocknumbers", ")", ",", "requesting_range", "=", "(", "blocknumbers", "[", "0", "]", ",", "blocknumbers", "[", "-", "1", "]", ")", ")", "self", ".", "last_active_protocol", ".", "send_getblockproposals", "(", "*", "blocknumbers", ")", "# setup alarm", "self", ".", "cm", ".", "chainservice", ".", "setup_alarm", "(", "self", ".", "timeout", ",", "self", ".", "on_alarm", ",", "blocknumbers", ")"], "docstring": "", "docstring_tokens": [], "sha": "6c0919b0575dc8aa481f3a8c703e1a7f0575ecc3", "url": "https://github.com/HydraChain/hydrachain/blob/6c0919b0575dc8aa481f3a8c703e1a7f0575ecc3/hydrachain/consensus/synchronizer.py#L38-L76", "partition": "test"}
{"repo": "tmontaigu/pylas", "path": "pylas/lasreader.py", "func_name": "LasReader._read_compressed_points_data", "original_string": "def _read_compressed_points_data(self, laszip_vlr, point_format):\n        \"\"\" reads the compressed point record\n        \"\"\"\n        offset_to_chunk_table = struct.unpack(\"<q\", self.stream.read(8))[0]\n        size_of_point_data = offset_to_chunk_table - self.stream.tell()\n\n        if offset_to_chunk_table <= 0:\n            logger.warning(\n                \"Strange offset to chunk table: {}, ignoring it..\".format(\n                    offset_to_chunk_table\n                )\n            )\n            size_of_point_data = -1  # Read everything\n\n        points = record.PackedPointRecord.from_compressed_buffer(\n            self.stream.read(size_of_point_data),\n            point_format,\n            self.header.point_count,\n            laszip_vlr,\n        )\n        return points", "language": "python", "code": "def _read_compressed_points_data(self, laszip_vlr, point_format):\n        \"\"\" reads the compressed point record\n        \"\"\"\n        offset_to_chunk_table = struct.unpack(\"<q\", self.stream.read(8))[0]\n        size_of_point_data = offset_to_chunk_table - self.stream.tell()\n\n        if offset_to_chunk_table <= 0:\n            logger.warning(\n                \"Strange offset to chunk table: {}, ignoring it..\".format(\n                    offset_to_chunk_table\n                )\n            )\n            size_of_point_data = -1  # Read everything\n\n        points = record.PackedPointRecord.from_compressed_buffer(\n            self.stream.read(size_of_point_data),\n            point_format,\n            self.header.point_count,\n            laszip_vlr,\n        )\n        return points", "code_tokens": ["def", "_read_compressed_points_data", "(", "self", ",", "laszip_vlr", ",", "point_format", ")", ":", "offset_to_chunk_table", "=", "struct", ".", "unpack", "(", "\"<q\"", ",", "self", ".", "stream", ".", "read", "(", "8", ")", ")", "[", "0", "]", "size_of_point_data", "=", "offset_to_chunk_table", "-", "self", ".", "stream", ".", "tell", "(", ")", "if", "offset_to_chunk_table", "<=", "0", ":", "logger", ".", "warning", "(", "\"Strange offset to chunk table: {}, ignoring it..\"", ".", "format", "(", "offset_to_chunk_table", ")", ")", "size_of_point_data", "=", "-", "1", "# Read everything", "points", "=", "record", ".", "PackedPointRecord", ".", "from_compressed_buffer", "(", "self", ".", "stream", ".", "read", "(", "size_of_point_data", ")", ",", "point_format", ",", "self", ".", "header", ".", "point_count", ",", "laszip_vlr", ",", ")", "return", "points"], "docstring": "", "docstring_tokens": [], "sha": "8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06", "url": "https://github.com/tmontaigu/pylas/blob/8335a1a7d7677f0e4bc391bb6fa3c75b42ed5b06/pylas/lasreader.py#L119-L139", "partition": "test"}
{"repo": "pytorch/vision", "path": "torchvision/datasets/utils.py", "func_name": "list_files", "original_string": "def list_files(root, suffix, prefix=False):\n    \"\"\"List all files ending with a suffix at a given root\n\n    Args:\n        root (str): Path to directory whose folders need to be listed\n        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n            It uses the Python \"str.endswith\" method and is passed directly\n        prefix (bool, optional): If true, prepends the path to each result, otherwise\n            only returns the name of the files found\n    \"\"\"\n    root = os.path.expanduser(root)\n    files = list(\n        filter(\n            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n            os.listdir(root)\n        )\n    )\n\n    if prefix is True:\n        files = [os.path.join(root, d) for d in files]\n\n    return files", "language": "python", "code": "def list_files(root, suffix, prefix=False):\n    \"\"\"List all files ending with a suffix at a given root\n\n    Args:\n        root (str): Path to directory whose folders need to be listed\n        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n            It uses the Python \"str.endswith\" method and is passed directly\n        prefix (bool, optional): If true, prepends the path to each result, otherwise\n            only returns the name of the files found\n    \"\"\"\n    root = os.path.expanduser(root)\n    files = list(\n        filter(\n            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n            os.listdir(root)\n        )\n    )\n\n    if prefix is True:\n        files = [os.path.join(root, d) for d in files]\n\n    return files", "code_tokens": ["def", "list_files", "(", "root", ",", "suffix", ",", "prefix", "=", "False", ")", ":", "root", "=", "os", ".", "path", ".", "expanduser", "(", "root", ")", "files", "=", "list", "(", "filter", "(", "lambda", "p", ":", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "root", ",", "p", ")", ")", "and", "p", ".", "endswith", "(", "suffix", ")", ",", "os", ".", "listdir", "(", "root", ")", ")", ")", "if", "prefix", "is", "True", ":", "files", "=", "[", "os", ".", "path", ".", "join", "(", "root", ",", "d", ")", "for", "d", "in", "files", "]", "return", "files"], "docstring": "", "docstring_tokens": [], "sha": "3afcf3cd49661c466c75ea536b0b2a7ff57f9a05", "url": "https://github.com/pytorch/vision/blob/3afcf3cd49661c466c75ea536b0b2a7ff57f9a05/torchvision/datasets/utils.py#L115-L136", "partition": "test"}
{"repo": "drager/django-simple-blog", "path": "simpleblog/views.py", "func_name": "BlogDetailView.render_to_response", "original_string": "def render_to_response(self, context, **response_kwargs):\n        \"\"\"\n        Returns a response with a template depending if the request is ajax \n        or not and it renders with the given context.\n        \"\"\"\n        if self.request.is_ajax():\n            template = self.page_template\n        else:\n            template = self.get_template_names()\n        return self.response_class(\n            request=self.request,\n            template=template,\n            context=context,\n            **response_kwargs\n        )", "language": "python", "code": "def render_to_response(self, context, **response_kwargs):\n        \"\"\"\n        Returns a response with a template depending if the request is ajax \n        or not and it renders with the given context.\n        \"\"\"\n        if self.request.is_ajax():\n            template = self.page_template\n        else:\n            template = self.get_template_names()\n        return self.response_class(\n            request=self.request,\n            template=template,\n            context=context,\n            **response_kwargs\n        )", "code_tokens": ["def", "render_to_response", "(", "self", ",", "context", ",", "*", "*", "response_kwargs", ")", ":", "if", "self", ".", "request", ".", "is_ajax", "(", ")", ":", "template", "=", "self", ".", "page_template", "else", ":", "template", "=", "self", ".", "get_template_names", "(", ")", "return", "self", ".", "response_class", "(", "request", "=", "self", ".", "request", ",", "template", "=", "template", ",", "context", "=", "context", ",", "*", "*", "response_kwargs", ")"], "docstring": "", "docstring_tokens": [], "sha": "8f6575c485dc316bc908431fc8bddcae7624e050", "url": "https://github.com/drager/django-simple-blog/blob/8f6575c485dc316bc908431fc8bddcae7624e050/simpleblog/views.py#L73-L87", "partition": "valid"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/session.py", "func_name": "_Session._create_placeholder_objects", "original_string": "def _create_placeholder_objects(self):\r\n        \"\"\" PDF objects #1 through #3 are typically saved for the\r\n            Zeroth, Catalog, and Pages Objects. This program will save\r\n            the numbers, but outputs the individual Page and Content objects\r\n            first. The actual Catalog and Pages objects are calculated after.\r\n\r\n        \"\"\"\r\n        self.objects.append(\"Zeroth\")\r\n        self.objects.append(\"Catalog\")\r\n        self.objects.append(\"Pages\")", "language": "python", "code": "def _create_placeholder_objects(self):\r\n        \"\"\" PDF objects #1 through #3 are typically saved for the\r\n            Zeroth, Catalog, and Pages Objects. This program will save\r\n            the numbers, but outputs the individual Page and Content objects\r\n            first. The actual Catalog and Pages objects are calculated after.\r\n\r\n        \"\"\"\r\n        self.objects.append(\"Zeroth\")\r\n        self.objects.append(\"Catalog\")\r\n        self.objects.append(\"Pages\")", "code_tokens": ["def", "_create_placeholder_objects", "(", "self", ")", ":", "self", ".", "objects", ".", "append", "(", "\"Zeroth\"", ")", "self", ".", "objects", ".", "append", "(", "\"Catalog\"", ")", "self", ".", "objects", ".", "append", "(", "\"Pages\"", ")"], "docstring": "", "docstring_tokens": [], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/session.py#L53-L62", "partition": "test"}
{"repo": "CitrineInformatics/python-citrination-client", "path": "citrination_client/models/client.py", "func_name": "ModelsClient.get_data_view", "original_string": "def get_data_view(self, data_view_id):\n        \"\"\"\n        Retrieves a summary of information for a given data view\n            - view id\n            - name\n            - description\n            - columns\n\n        :param data_view_id: The ID number of the data view to which the\n            run belongs, as a string\n        :type data_view_id: str\n        \"\"\"\n\n        url = routes.get_data_view(data_view_id)\n\n        response = self._get(url).json()\n\n        result = response[\"data\"][\"data_view\"]\n\n        datasets_list = []\n        for dataset in result[\"datasets\"]:\n            datasets_list.append(Dataset(\n                name=dataset[\"name\"],\n                id=dataset[\"id\"],\n                description=dataset[\"description\"]\n            ))\n\n        columns_list = []\n        for column in result[\"columns\"]:\n            columns_list.append(ColumnFactory.from_dict(column))\n\n        return DataView(\n            view_id=data_view_id,\n            name=result[\"name\"],\n            description=result[\"description\"],\n            datasets=datasets_list,\n            columns=columns_list,\n        )", "language": "python", "code": "def get_data_view(self, data_view_id):\n        \"\"\"\n        Retrieves a summary of information for a given data view\n            - view id\n            - name\n            - description\n            - columns\n\n        :param data_view_id: The ID number of the data view to which the\n            run belongs, as a string\n        :type data_view_id: str\n        \"\"\"\n\n        url = routes.get_data_view(data_view_id)\n\n        response = self._get(url).json()\n\n        result = response[\"data\"][\"data_view\"]\n\n        datasets_list = []\n        for dataset in result[\"datasets\"]:\n            datasets_list.append(Dataset(\n                name=dataset[\"name\"],\n                id=dataset[\"id\"],\n                description=dataset[\"description\"]\n            ))\n\n        columns_list = []\n        for column in result[\"columns\"]:\n            columns_list.append(ColumnFactory.from_dict(column))\n\n        return DataView(\n            view_id=data_view_id,\n            name=result[\"name\"],\n            description=result[\"description\"],\n            datasets=datasets_list,\n            columns=columns_list,\n        )", "code_tokens": ["def", "get_data_view", "(", "self", ",", "data_view_id", ")", ":", "url", "=", "routes", ".", "get_data_view", "(", "data_view_id", ")", "response", "=", "self", ".", "_get", "(", "url", ")", ".", "json", "(", ")", "result", "=", "response", "[", "\"data\"", "]", "[", "\"data_view\"", "]", "datasets_list", "=", "[", "]", "for", "dataset", "in", "result", "[", "\"datasets\"", "]", ":", "datasets_list", ".", "append", "(", "Dataset", "(", "name", "=", "dataset", "[", "\"name\"", "]", ",", "id", "=", "dataset", "[", "\"id\"", "]", ",", "description", "=", "dataset", "[", "\"description\"", "]", ")", ")", "columns_list", "=", "[", "]", "for", "column", "in", "result", "[", "\"columns\"", "]", ":", "columns_list", ".", "append", "(", "ColumnFactory", ".", "from_dict", "(", "column", ")", ")", "return", "DataView", "(", "view_id", "=", "data_view_id", ",", "name", "=", "result", "[", "\"name\"", "]", ",", "description", "=", "result", "[", "\"description\"", "]", ",", "datasets", "=", "datasets_list", ",", "columns", "=", "columns_list", ",", ")"], "docstring": "", "docstring_tokens": [], "sha": "409984fc65ce101a620f069263f155303492465c", "url": "https://github.com/CitrineInformatics/python-citrination-client/blob/409984fc65ce101a620f069263f155303492465c/citrination_client/models/client.py#L281-L318", "partition": "valid"}
{"repo": "iotile/typedargs", "path": "typedargs/metadata.py", "func_name": "AnnotatedMetadata.format_returnvalue", "original_string": "def format_returnvalue(self, value):\n        \"\"\"Format the return value of this function as a string.\n\n        Args:\n            value (object): The return value that we are supposed to format.\n\n        Returns:\n            str: The formatted return value, or None if this function indicates\n                that it does not return data\n        \"\"\"\n\n        self._ensure_loaded()\n\n        if not self.return_info.is_data:\n            return None\n\n        # If the return value is typed, use the type_system to format it\n        if self.return_info.type_name is not None:\n            return typeinfo.type_system.format_value(value, self.return_info.type_name, self.return_info.formatter)\n\n        # Otherwise we expect a callable function to convert this value to a string\n        return self.return_info.formatter(value)", "language": "python", "code": "def format_returnvalue(self, value):\n        \"\"\"Format the return value of this function as a string.\n\n        Args:\n            value (object): The return value that we are supposed to format.\n\n        Returns:\n            str: The formatted return value, or None if this function indicates\n                that it does not return data\n        \"\"\"\n\n        self._ensure_loaded()\n\n        if not self.return_info.is_data:\n            return None\n\n        # If the return value is typed, use the type_system to format it\n        if self.return_info.type_name is not None:\n            return typeinfo.type_system.format_value(value, self.return_info.type_name, self.return_info.formatter)\n\n        # Otherwise we expect a callable function to convert this value to a string\n        return self.return_info.formatter(value)", "code_tokens": ["def", "format_returnvalue", "(", "self", ",", "value", ")", ":", "self", ".", "_ensure_loaded", "(", ")", "if", "not", "self", ".", "return_info", ".", "is_data", ":", "return", "None", "# If the return value is typed, use the type_system to format it", "if", "self", ".", "return_info", ".", "type_name", "is", "not", "None", ":", "return", "typeinfo", ".", "type_system", ".", "format_value", "(", "value", ",", "self", ".", "return_info", ".", "type_name", ",", "self", ".", "return_info", ".", "formatter", ")", "# Otherwise we expect a callable function to convert this value to a string", "return", "self", ".", "return_info", ".", "formatter", "(", "value", ")"], "docstring": "", "docstring_tokens": [], "sha": "0a5091a664b9b4d836e091e9ba583e944f438fd8", "url": "https://github.com/iotile/typedargs/blob/0a5091a664b9b4d836e091e9ba583e944f438fd8/typedargs/metadata.py#L260-L281", "partition": "test"}
{"repo": "nerox8664/pytorch2keras", "path": "pytorch2keras/reshape_layers.py", "func_name": "convert_unsqueeze", "original_string": "def convert_unsqueeze(params, w_name, scope_name, inputs, layers, weights, names):\n    \"\"\"\n    Convert unsqueeze operation.\n\n    Args:\n        params: dictionary with layer parameters\n        w_name: name prefix in state_dict\n        scope_name: pytorch scope name\n        inputs: pytorch node inputs\n        layers: dictionary with keras tensors\n        weights: pytorch state_dict\n        names: use short names for keras layers\n    \"\"\"\n    print('Converting unsqueeze ...')\n\n    if names == 'short':\n        tf_name = 'UNSQ' + random_string(4)\n    elif names == 'keep':\n        tf_name = w_name\n    else:\n        tf_name = w_name + str(random.random())\n\n    def target_layer(x):\n        import keras\n        return keras.backend.expand_dims(x)\n\n    lambda_layer = keras.layers.Lambda(target_layer, name=tf_name + 'E')\n    layers[scope_name] = lambda_layer(layers[inputs[0]])", "language": "python", "code": "def convert_unsqueeze(params, w_name, scope_name, inputs, layers, weights, names):\n    \"\"\"\n    Convert unsqueeze operation.\n\n    Args:\n        params: dictionary with layer parameters\n        w_name: name prefix in state_dict\n        scope_name: pytorch scope name\n        inputs: pytorch node inputs\n        layers: dictionary with keras tensors\n        weights: pytorch state_dict\n        names: use short names for keras layers\n    \"\"\"\n    print('Converting unsqueeze ...')\n\n    if names == 'short':\n        tf_name = 'UNSQ' + random_string(4)\n    elif names == 'keep':\n        tf_name = w_name\n    else:\n        tf_name = w_name + str(random.random())\n\n    def target_layer(x):\n        import keras\n        return keras.backend.expand_dims(x)\n\n    lambda_layer = keras.layers.Lambda(target_layer, name=tf_name + 'E')\n    layers[scope_name] = lambda_layer(layers[inputs[0]])", "code_tokens": ["def", "convert_unsqueeze", "(", "params", ",", "w_name", ",", "scope_name", ",", "inputs", ",", "layers", ",", "weights", ",", "names", ")", ":", "print", "(", "'Converting unsqueeze ...'", ")", "if", "names", "==", "'short'", ":", "tf_name", "=", "'UNSQ'", "+", "random_string", "(", "4", ")", "elif", "names", "==", "'keep'", ":", "tf_name", "=", "w_name", "else", ":", "tf_name", "=", "w_name", "+", "str", "(", "random", ".", "random", "(", ")", ")", "def", "target_layer", "(", "x", ")", ":", "import", "keras", "return", "keras", ".", "backend", ".", "expand_dims", "(", "x", ")", "lambda_layer", "=", "keras", ".", "layers", ".", "Lambda", "(", "target_layer", ",", "name", "=", "tf_name", "+", "'E'", ")", "layers", "[", "scope_name", "]", "=", "lambda_layer", "(", "layers", "[", "inputs", "[", "0", "]", "]", ")"], "docstring": "", "docstring_tokens": [], "sha": "750eaf747323580e6732d0c5ba9f2f39cb096764", "url": "https://github.com/nerox8664/pytorch2keras/blob/750eaf747323580e6732d0c5ba9f2f39cb096764/pytorch2keras/reshape_layers.py#L124-L151", "partition": "valid"}
{"repo": "tensorforce/tensorforce", "path": "docs/mistune.py", "func_name": "Renderer.autolink", "original_string": "def autolink(self, link, is_email=False):\n        \"\"\"Rendering a given link or email address.\n\n        :param link: link content or email address.\n        :param is_email: whether this is an email or not.\n        \"\"\"\n        text = link = escape(link)\n        if is_email:\n            link = 'mailto:%s' % link\n        return '<a href=\"%s\">%s</a>' % (link, text)", "language": "python", "code": "def autolink(self, link, is_email=False):\n        \"\"\"Rendering a given link or email address.\n\n        :param link: link content or email address.\n        :param is_email: whether this is an email or not.\n        \"\"\"\n        text = link = escape(link)\n        if is_email:\n            link = 'mailto:%s' % link\n        return '<a href=\"%s\">%s</a>' % (link, text)", "code_tokens": ["def", "autolink", "(", "self", ",", "link", ",", "is_email", "=", "False", ")", ":", "text", "=", "link", "=", "escape", "(", "link", ")", "if", "is_email", ":", "link", "=", "'mailto:%s'", "%", "link", "return", "'<a href=\"%s\">%s</a>'", "%", "(", "link", ",", "text", ")"], "docstring": "", "docstring_tokens": [], "sha": "520a8d992230e382f08e315ede5fc477f5e26bfb", "url": "https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/docs/mistune.py#L854-L863", "partition": "valid"}
{"repo": "opencobra/cobrapy", "path": "cobra/flux_analysis/variability.py", "func_name": "flux_variability_analysis", "original_string": "def flux_variability_analysis(model, reaction_list=None, loopless=False,\n                              fraction_of_optimum=1.0, pfba_factor=None,\n                              processes=None):\n    \"\"\"\n    Determine the minimum and maximum possible flux value for each reaction.\n\n    Parameters\n    ----------\n    model : cobra.Model\n        The model for which to run the analysis. It will *not* be modified.\n    reaction_list : list of cobra.Reaction or str, optional\n        The reactions for which to obtain min/max fluxes. If None will use\n        all reactions in the model (default).\n    loopless : boolean, optional\n        Whether to return only loopless solutions. This is significantly\n        slower. Please also refer to the notes.\n    fraction_of_optimum : float, optional\n        Must be <= 1.0. Requires that the objective value is at least the\n        fraction times maximum objective value. A value of 0.85 for instance\n        means that the objective has to be at least at 85% percent of its\n        maximum.\n    pfba_factor : float, optional\n        Add an additional constraint to the model that requires the total sum\n        of absolute fluxes must not be larger than this value times the\n        smallest possible sum of absolute fluxes, i.e., by setting the value\n        to 1.1 the total sum of absolute fluxes must not be more than\n        10% larger than the pFBA solution. Since the pFBA solution is the\n        one that optimally minimizes the total flux sum, the ``pfba_factor``\n        should, if set, be larger than one. Setting this value may lead to\n        more realistic predictions of the effective flux bounds.\n    processes : int, optional\n        The number of parallel processes to run. If not explicitly passed,\n        will be set from the global configuration singleton.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A data frame with reaction identifiers as the index and two columns:\n        - maximum: indicating the highest possible flux\n        - minimum: indicating the lowest possible flux\n\n    Notes\n    -----\n    This implements the fast version as described in [1]_. Please note that\n    the flux distribution containing all minimal/maximal fluxes does not have\n    to be a feasible solution for the model. Fluxes are minimized/maximized\n    individually and a single minimal flux might require all others to be\n    suboptimal.\n\n    Using the loopless option will lead to a significant increase in\n    computation time (about a factor of 100 for large models). However, the\n    algorithm used here (see [2]_) is still more than 1000x faster than the\n    \"naive\" version using ``add_loopless(model)``. Also note that if you have\n    included constraints that force a loop (for instance by setting all fluxes\n    in a loop to be non-zero) this loop will be included in the solution.\n\n    References\n    ----------\n    .. [1] Computationally efficient flux variability analysis.\n       Gudmundsson S, Thiele I.\n       BMC Bioinformatics. 2010 Sep 29;11:489.\n       doi: 10.1186/1471-2105-11-489, PMID: 20920235\n\n    .. [2] CycleFreeFlux: efficient removal of thermodynamically infeasible\n       loops from flux distributions.\n       Desouki AA, Jarre F, Gelius-Dietrich G, Lercher MJ.\n       Bioinformatics. 2015 Jul 1;31(13):2159-65.\n       doi: 10.1093/bioinformatics/btv096.\n    \"\"\"\n    if reaction_list is None:\n        reaction_ids = [r.id for r in model.reactions]\n    else:\n        reaction_ids = [r.id\n                        for r in model.reactions.get_by_any(reaction_list)]\n\n    if processes is None:\n        processes = CONFIGURATION.processes\n    num_reactions = len(reaction_ids)\n    processes = min(processes, num_reactions)\n\n    fva_result = DataFrame({\n        \"minimum\": zeros(num_reactions, dtype=float),\n        \"maximum\": zeros(num_reactions, dtype=float)\n    }, index=reaction_ids)\n    prob = model.problem\n    with model:\n        # Safety check before setting up FVA.\n        model.slim_optimize(error_value=None,\n                            message=\"There is no optimal solution for the \"\n                                    \"chosen objective!\")\n        # Add the previous objective as a variable to the model then set it to\n        # zero. This also uses the fraction to create the lower/upper bound for\n        # the old objective.\n        # TODO: Use utility function here (fix_objective_as_constraint)?\n        if model.solver.objective.direction == \"max\":\n            fva_old_objective = prob.Variable(\n                \"fva_old_objective\",\n                lb=fraction_of_optimum * model.solver.objective.value)\n        else:\n            fva_old_objective = prob.Variable(\n                \"fva_old_objective\",\n                ub=fraction_of_optimum * model.solver.objective.value)\n        fva_old_obj_constraint = prob.Constraint(\n            model.solver.objective.expression - fva_old_objective, lb=0, ub=0,\n            name=\"fva_old_objective_constraint\")\n        model.add_cons_vars([fva_old_objective, fva_old_obj_constraint])\n\n        if pfba_factor is not None:\n            if pfba_factor < 1.:\n                warn(\"The 'pfba_factor' should be larger or equal to 1.\",\n                     UserWarning)\n            with model:\n                add_pfba(model, fraction_of_optimum=0)\n                ub = model.slim_optimize(error_value=None)\n                flux_sum = prob.Variable(\"flux_sum\", ub=pfba_factor * ub)\n                flux_sum_constraint = prob.Constraint(\n                    model.solver.objective.expression - flux_sum, lb=0, ub=0,\n                    name=\"flux_sum_constraint\")\n            model.add_cons_vars([flux_sum, flux_sum_constraint])\n\n        model.objective = Zero  # This will trigger the reset as well\n        for what in (\"minimum\", \"maximum\"):\n            if processes > 1:\n                # We create and destroy a new pool here in order to set the\n                # objective direction for all reactions. This creates a\n                # slight overhead but seems the most clean.\n                chunk_size = len(reaction_ids) // processes\n                pool = multiprocessing.Pool(\n                    processes,\n                    initializer=_init_worker,\n                    initargs=(model, loopless, what[:3])\n                )\n                for rxn_id, value in pool.imap_unordered(_fva_step,\n                                                         reaction_ids,\n                                                         chunksize=chunk_size):\n                    fva_result.at[rxn_id, what] = value\n                pool.close()\n                pool.join()\n            else:\n                _init_worker(model, loopless, what[:3])\n                for rxn_id, value in map(_fva_step, reaction_ids):\n                    fva_result.at[rxn_id, what] = value\n\n    return fva_result[[\"minimum\", \"maximum\"]]", "language": "python", "code": "def flux_variability_analysis(model, reaction_list=None, loopless=False,\n                              fraction_of_optimum=1.0, pfba_factor=None,\n                              processes=None):\n    \"\"\"\n    Determine the minimum and maximum possible flux value for each reaction.\n\n    Parameters\n    ----------\n    model : cobra.Model\n        The model for which to run the analysis. It will *not* be modified.\n    reaction_list : list of cobra.Reaction or str, optional\n        The reactions for which to obtain min/max fluxes. If None will use\n        all reactions in the model (default).\n    loopless : boolean, optional\n        Whether to return only loopless solutions. This is significantly\n        slower. Please also refer to the notes.\n    fraction_of_optimum : float, optional\n        Must be <= 1.0. Requires that the objective value is at least the\n        fraction times maximum objective value. A value of 0.85 for instance\n        means that the objective has to be at least at 85% percent of its\n        maximum.\n    pfba_factor : float, optional\n        Add an additional constraint to the model that requires the total sum\n        of absolute fluxes must not be larger than this value times the\n        smallest possible sum of absolute fluxes, i.e., by setting the value\n        to 1.1 the total sum of absolute fluxes must not be more than\n        10% larger than the pFBA solution. Since the pFBA solution is the\n        one that optimally minimizes the total flux sum, the ``pfba_factor``\n        should, if set, be larger than one. Setting this value may lead to\n        more realistic predictions of the effective flux bounds.\n    processes : int, optional\n        The number of parallel processes to run. If not explicitly passed,\n        will be set from the global configuration singleton.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A data frame with reaction identifiers as the index and two columns:\n        - maximum: indicating the highest possible flux\n        - minimum: indicating the lowest possible flux\n\n    Notes\n    -----\n    This implements the fast version as described in [1]_. Please note that\n    the flux distribution containing all minimal/maximal fluxes does not have\n    to be a feasible solution for the model. Fluxes are minimized/maximized\n    individually and a single minimal flux might require all others to be\n    suboptimal.\n\n    Using the loopless option will lead to a significant increase in\n    computation time (about a factor of 100 for large models). However, the\n    algorithm used here (see [2]_) is still more than 1000x faster than the\n    \"naive\" version using ``add_loopless(model)``. Also note that if you have\n    included constraints that force a loop (for instance by setting all fluxes\n    in a loop to be non-zero) this loop will be included in the solution.\n\n    References\n    ----------\n    .. [1] Computationally efficient flux variability analysis.\n       Gudmundsson S, Thiele I.\n       BMC Bioinformatics. 2010 Sep 29;11:489.\n       doi: 10.1186/1471-2105-11-489, PMID: 20920235\n\n    .. [2] CycleFreeFlux: efficient removal of thermodynamically infeasible\n       loops from flux distributions.\n       Desouki AA, Jarre F, Gelius-Dietrich G, Lercher MJ.\n       Bioinformatics. 2015 Jul 1;31(13):2159-65.\n       doi: 10.1093/bioinformatics/btv096.\n    \"\"\"\n    if reaction_list is None:\n        reaction_ids = [r.id for r in model.reactions]\n    else:\n        reaction_ids = [r.id\n                        for r in model.reactions.get_by_any(reaction_list)]\n\n    if processes is None:\n        processes = CONFIGURATION.processes\n    num_reactions = len(reaction_ids)\n    processes = min(processes, num_reactions)\n\n    fva_result = DataFrame({\n        \"minimum\": zeros(num_reactions, dtype=float),\n        \"maximum\": zeros(num_reactions, dtype=float)\n    }, index=reaction_ids)\n    prob = model.problem\n    with model:\n        # Safety check before setting up FVA.\n        model.slim_optimize(error_value=None,\n                            message=\"There is no optimal solution for the \"\n                                    \"chosen objective!\")\n        # Add the previous objective as a variable to the model then set it to\n        # zero. This also uses the fraction to create the lower/upper bound for\n        # the old objective.\n        # TODO: Use utility function here (fix_objective_as_constraint)?\n        if model.solver.objective.direction == \"max\":\n            fva_old_objective = prob.Variable(\n                \"fva_old_objective\",\n                lb=fraction_of_optimum * model.solver.objective.value)\n        else:\n            fva_old_objective = prob.Variable(\n                \"fva_old_objective\",\n                ub=fraction_of_optimum * model.solver.objective.value)\n        fva_old_obj_constraint = prob.Constraint(\n            model.solver.objective.expression - fva_old_objective, lb=0, ub=0,\n            name=\"fva_old_objective_constraint\")\n        model.add_cons_vars([fva_old_objective, fva_old_obj_constraint])\n\n        if pfba_factor is not None:\n            if pfba_factor < 1.:\n                warn(\"The 'pfba_factor' should be larger or equal to 1.\",\n                     UserWarning)\n            with model:\n                add_pfba(model, fraction_of_optimum=0)\n                ub = model.slim_optimize(error_value=None)\n                flux_sum = prob.Variable(\"flux_sum\", ub=pfba_factor * ub)\n                flux_sum_constraint = prob.Constraint(\n                    model.solver.objective.expression - flux_sum, lb=0, ub=0,\n                    name=\"flux_sum_constraint\")\n            model.add_cons_vars([flux_sum, flux_sum_constraint])\n\n        model.objective = Zero  # This will trigger the reset as well\n        for what in (\"minimum\", \"maximum\"):\n            if processes > 1:\n                # We create and destroy a new pool here in order to set the\n                # objective direction for all reactions. This creates a\n                # slight overhead but seems the most clean.\n                chunk_size = len(reaction_ids) // processes\n                pool = multiprocessing.Pool(\n                    processes,\n                    initializer=_init_worker,\n                    initargs=(model, loopless, what[:3])\n                )\n                for rxn_id, value in pool.imap_unordered(_fva_step,\n                                                         reaction_ids,\n                                                         chunksize=chunk_size):\n                    fva_result.at[rxn_id, what] = value\n                pool.close()\n                pool.join()\n            else:\n                _init_worker(model, loopless, what[:3])\n                for rxn_id, value in map(_fva_step, reaction_ids):\n                    fva_result.at[rxn_id, what] = value\n\n    return fva_result[[\"minimum\", \"maximum\"]]", "code_tokens": ["def", "flux_variability_analysis", "(", "model", ",", "reaction_list", "=", "None", ",", "loopless", "=", "False", ",", "fraction_of_optimum", "=", "1.0", ",", "pfba_factor", "=", "None", ",", "processes", "=", "None", ")", ":", "if", "reaction_list", "is", "None", ":", "reaction_ids", "=", "[", "r", ".", "id", "for", "r", "in", "model", ".", "reactions", "]", "else", ":", "reaction_ids", "=", "[", "r", ".", "id", "for", "r", "in", "model", ".", "reactions", ".", "get_by_any", "(", "reaction_list", ")", "]", "if", "processes", "is", "None", ":", "processes", "=", "CONFIGURATION", ".", "processes", "num_reactions", "=", "len", "(", "reaction_ids", ")", "processes", "=", "min", "(", "processes", ",", "num_reactions", ")", "fva_result", "=", "DataFrame", "(", "{", "\"minimum\"", ":", "zeros", "(", "num_reactions", ",", "dtype", "=", "float", ")", ",", "\"maximum\"", ":", "zeros", "(", "num_reactions", ",", "dtype", "=", "float", ")", "}", ",", "index", "=", "reaction_ids", ")", "prob", "=", "model", ".", "problem", "with", "model", ":", "# Safety check before setting up FVA.", "model", ".", "slim_optimize", "(", "error_value", "=", "None", ",", "message", "=", "\"There is no optimal solution for the \"", "\"chosen objective!\"", ")", "# Add the previous objective as a variable to the model then set it to", "# zero. This also uses the fraction to create the lower/upper bound for", "# the old objective.", "# TODO: Use utility function here (fix_objective_as_constraint)?", "if", "model", ".", "solver", ".", "objective", ".", "direction", "==", "\"max\"", ":", "fva_old_objective", "=", "prob", ".", "Variable", "(", "\"fva_old_objective\"", ",", "lb", "=", "fraction_of_optimum", "*", "model", ".", "solver", ".", "objective", ".", "value", ")", "else", ":", "fva_old_objective", "=", "prob", ".", "Variable", "(", "\"fva_old_objective\"", ",", "ub", "=", "fraction_of_optimum", "*", "model", ".", "solver", ".", "objective", ".", "value", ")", "fva_old_obj_constraint", "=", "prob", ".", "Constraint", "(", "model", ".", "solver", ".", "objective", ".", "expression", "-", "fva_old_objective", ",", "lb", "=", "0", ",", "ub", "=", "0", ",", "name", "=", "\"fva_old_objective_constraint\"", ")", "model", ".", "add_cons_vars", "(", "[", "fva_old_objective", ",", "fva_old_obj_constraint", "]", ")", "if", "pfba_factor", "is", "not", "None", ":", "if", "pfba_factor", "<", "1.", ":", "warn", "(", "\"The 'pfba_factor' should be larger or equal to 1.\"", ",", "UserWarning", ")", "with", "model", ":", "add_pfba", "(", "model", ",", "fraction_of_optimum", "=", "0", ")", "ub", "=", "model", ".", "slim_optimize", "(", "error_value", "=", "None", ")", "flux_sum", "=", "prob", ".", "Variable", "(", "\"flux_sum\"", ",", "ub", "=", "pfba_factor", "*", "ub", ")", "flux_sum_constraint", "=", "prob", ".", "Constraint", "(", "model", ".", "solver", ".", "objective", ".", "expression", "-", "flux_sum", ",", "lb", "=", "0", ",", "ub", "=", "0", ",", "name", "=", "\"flux_sum_constraint\"", ")", "model", ".", "add_cons_vars", "(", "[", "flux_sum", ",", "flux_sum_constraint", "]", ")", "model", ".", "objective", "=", "Zero", "# This will trigger the reset as well", "for", "what", "in", "(", "\"minimum\"", ",", "\"maximum\"", ")", ":", "if", "processes", ">", "1", ":", "# We create and destroy a new pool here in order to set the", "# objective direction for all reactions. This creates a", "# slight overhead but seems the most clean.", "chunk_size", "=", "len", "(", "reaction_ids", ")", "//", "processes", "pool", "=", "multiprocessing", ".", "Pool", "(", "processes", ",", "initializer", "=", "_init_worker", ",", "initargs", "=", "(", "model", ",", "loopless", ",", "what", "[", ":", "3", "]", ")", ")", "for", "rxn_id", ",", "value", "in", "pool", ".", "imap_unordered", "(", "_fva_step", ",", "reaction_ids", ",", "chunksize", "=", "chunk_size", ")", ":", "fva_result", ".", "at", "[", "rxn_id", ",", "what", "]", "=", "value", "pool", ".", "close", "(", ")", "pool", ".", "join", "(", ")", "else", ":", "_init_worker", "(", "model", ",", "loopless", ",", "what", "[", ":", "3", "]", ")", "for", "rxn_id", ",", "value", "in", "map", "(", "_fva_step", ",", "reaction_ids", ")", ":", "fva_result", ".", "at", "[", "rxn_id", ",", "what", "]", "=", "value", "return", "fva_result", "[", "[", "\"minimum\"", ",", "\"maximum\"", "]", "]"], "docstring": "", "docstring_tokens": [], "sha": "9d1987cdb3a395cf4125a3439c3b002ff2be2009", "url": "https://github.com/opencobra/cobrapy/blob/9d1987cdb3a395cf4125a3439c3b002ff2be2009/cobra/flux_analysis/variability.py#L57-L200", "partition": "valid"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/remo.py", "func_name": "ReMo.metadata_updated_on", "original_string": "def metadata_updated_on(item):\n        \"\"\"Extracts the update time from a ReMo item.\n\n        The timestamp is extracted from 'end' field.\n        This date is converted to a perceval format using a float value.\n\n        :param item: item generated by the backend\n\n        :returns: a UNIX timestamp\n        \"\"\"\n        if 'end' in item:\n            # events updated field\n            updated = item['end']\n        elif 'date_joined_program' in item:\n            # users updated field that always appear\n            updated = item['date_joined_program']\n        elif 'report_date' in item:\n            # activities updated field\n            updated = item['report_date']\n        else:\n            raise ValueError(\"Can't find updated field for item \" + str(item))\n\n        return float(str_to_datetime(updated).timestamp())", "language": "python", "code": "def metadata_updated_on(item):\n        \"\"\"Extracts the update time from a ReMo item.\n\n        The timestamp is extracted from 'end' field.\n        This date is converted to a perceval format using a float value.\n\n        :param item: item generated by the backend\n\n        :returns: a UNIX timestamp\n        \"\"\"\n        if 'end' in item:\n            # events updated field\n            updated = item['end']\n        elif 'date_joined_program' in item:\n            # users updated field that always appear\n            updated = item['date_joined_program']\n        elif 'report_date' in item:\n            # activities updated field\n            updated = item['report_date']\n        else:\n            raise ValueError(\"Can't find updated field for item \" + str(item))\n\n        return float(str_to_datetime(updated).timestamp())", "code_tokens": ["def", "metadata_updated_on", "(", "item", ")", ":", "if", "'end'", "in", "item", ":", "# events updated field", "updated", "=", "item", "[", "'end'", "]", "elif", "'date_joined_program'", "in", "item", ":", "# users updated field that always appear", "updated", "=", "item", "[", "'date_joined_program'", "]", "elif", "'report_date'", "in", "item", ":", "# activities updated field", "updated", "=", "item", "[", "'report_date'", "]", "else", ":", "raise", "ValueError", "(", "\"Can't find updated field for item \"", "+", "str", "(", "item", ")", ")", "return", "float", "(", "str_to_datetime", "(", "updated", ")", ".", "timestamp", "(", ")", ")"], "docstring": "", "docstring_tokens": [], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/remo.py#L173-L195", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/interface.py", "func_name": "TrepanInterface.msg", "original_string": "def msg(self, msg):\n        \"\"\" used to write to a debugger that is connected to this\n        server; `str' written will have a newline added to it\n        \"\"\"\n        if hasattr(self.output, 'writeline'):\n            self.output.writeline(msg)\n        elif hasattr(self.output, 'writelines'):\n            self.output.writelines(msg + \"\\n\")\n            pass\n        return", "language": "python", "code": "def msg(self, msg):\n        \"\"\" used to write to a debugger that is connected to this\n        server; `str' written will have a newline added to it\n        \"\"\"\n        if hasattr(self.output, 'writeline'):\n            self.output.writeline(msg)\n        elif hasattr(self.output, 'writelines'):\n            self.output.writelines(msg + \"\\n\")\n            pass\n        return", "code_tokens": ["def", "msg", "(", "self", ",", "msg", ")", ":", "if", "hasattr", "(", "self", ".", "output", ",", "'writeline'", ")", ":", "self", ".", "output", ".", "writeline", "(", "msg", ")", "elif", "hasattr", "(", "self", ".", "output", ",", "'writelines'", ")", ":", "self", ".", "output", ".", "writelines", "(", "msg", "+", "\"\\n\"", ")", "pass", "return"], "docstring": "", "docstring_tokens": [], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/interface.py#L57-L66", "partition": "test"}
{"repo": "NarrativeScience/lsi", "path": "src/lsi/utils/hosts.py", "func_name": "get_entries", "original_string": "def get_entries(latest, filters, exclude, limit=None):\n    \"\"\"\n    Lists all available instances.\n\n    :param latest: If true, ignores the cache and grabs the latest list.\n    :type latest: ``bool``\n    :param filters: Filters to apply to results. A result will only be shown\n                    if it includes all text in all filters.\n    :type filters: [``str``]\n    :param exclude: The opposite of filters. Results will be rejected if they\n                    include any of these strings.\n    :type exclude: [``str``]\n    :param limit: Maximum number of entries to show (default no maximum).\n    :type limit: ``int`` or ``NoneType``\n\n    :return: A list of host entries.\n    :rtype: ``list`` of :py:class:`HostEntry`\n    \"\"\"\n    entry_list = _list_all_latest() if latest is True or not _is_valid_cache()\\\n                 else _list_all_cached()\n    filtered = filter_entries(entry_list, filters, exclude)\n    if limit is not None:\n        return filtered[:limit]\n    else:\n        return filtered", "language": "python", "code": "def get_entries(latest, filters, exclude, limit=None):\n    \"\"\"\n    Lists all available instances.\n\n    :param latest: If true, ignores the cache and grabs the latest list.\n    :type latest: ``bool``\n    :param filters: Filters to apply to results. A result will only be shown\n                    if it includes all text in all filters.\n    :type filters: [``str``]\n    :param exclude: The opposite of filters. Results will be rejected if they\n                    include any of these strings.\n    :type exclude: [``str``]\n    :param limit: Maximum number of entries to show (default no maximum).\n    :type limit: ``int`` or ``NoneType``\n\n    :return: A list of host entries.\n    :rtype: ``list`` of :py:class:`HostEntry`\n    \"\"\"\n    entry_list = _list_all_latest() if latest is True or not _is_valid_cache()\\\n                 else _list_all_cached()\n    filtered = filter_entries(entry_list, filters, exclude)\n    if limit is not None:\n        return filtered[:limit]\n    else:\n        return filtered", "code_tokens": ["def", "get_entries", "(", "latest", ",", "filters", ",", "exclude", ",", "limit", "=", "None", ")", ":", "entry_list", "=", "_list_all_latest", "(", ")", "if", "latest", "is", "True", "or", "not", "_is_valid_cache", "(", ")", "else", "_list_all_cached", "(", ")", "filtered", "=", "filter_entries", "(", "entry_list", ",", "filters", ",", "exclude", ")", "if", "limit", "is", "not", "None", ":", "return", "filtered", "[", ":", "limit", "]", "else", ":", "return", "filtered"], "docstring": "", "docstring_tokens": [], "sha": "7d901b03fdb1a34ef795e5412bfe9685d948e32d", "url": "https://github.com/NarrativeScience/lsi/blob/7d901b03fdb1a34ef795e5412bfe9685d948e32d/src/lsi/utils/hosts.py#L432-L456", "partition": "test"}
{"repo": "jaraco/jaraco.path", "path": "jaraco/path.py", "func_name": "ensure_dir_exists", "original_string": "def ensure_dir_exists(func):\r\n\t\"wrap a function that returns a dir, making sure it exists\"\r\n\t@functools.wraps(func)\r\n\tdef make_if_not_present():\r\n\t\tdir = func()\r\n\t\tif not os.path.isdir(dir):\r\n\t\t\tos.makedirs(dir)\r\n\t\treturn dir\r\n\treturn make_if_not_present", "language": "python", "code": "def ensure_dir_exists(func):\r\n\t\"wrap a function that returns a dir, making sure it exists\"\r\n\t@functools.wraps(func)\r\n\tdef make_if_not_present():\r\n\t\tdir = func()\r\n\t\tif not os.path.isdir(dir):\r\n\t\t\tos.makedirs(dir)\r\n\t\treturn dir\r\n\treturn make_if_not_present", "code_tokens": ["def", "ensure_dir_exists", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "make_if_not_present", "(", ")", ":", "dir", "=", "func", "(", ")", "if", "not", "os", ".", "path", ".", "isdir", "(", "dir", ")", ":", "os", ".", "makedirs", "(", "dir", ")", "return", "dir", "return", "make_if_not_present"], "docstring": "", "docstring_tokens": [], "sha": "39e4da09f325382e21b0917b1b5cd027edce8728", "url": "https://github.com/jaraco/jaraco.path/blob/39e4da09f325382e21b0917b1b5cd027edce8728/jaraco/path.py#L229-L237", "partition": "valid"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backend.py", "func_name": "Backend.fetch", "original_string": "def fetch(self, category, filter_classified=False, **kwargs):\n        \"\"\"Fetch items from the repository.\n\n        The method retrieves items from a repository.\n\n        To removed classified fields from the resulting items, set\n        the parameter `filter_classified`. Take into account this\n        parameter is incompatible with archiving items. Raw client\n        data are archived before any other process. Therefore,\n        classified data  are stored within the archive. To prevent\n        from possible data leaks or security issues when users do\n        not need these fields, archiving and filtering are not\n        compatible.\n\n        :param category: the category of the items fetched\n        :param filter_classified: remove classified fields from the resulting items\n        :param kwargs: a list of other parameters (e.g., from_date, offset, etc.\n        specific for each backend)\n\n        :returns: a generator of items\n\n        :raises BackendError: either when the category is not valid or\n            'filter_classified' and 'archive' are active at the same time.\n        \"\"\"\n        if category not in self.categories:\n            cause = \"%s category not valid for %s\" % (category, self.__class__.__name__)\n            raise BackendError(cause=cause)\n\n        if filter_classified and self.archive:\n            cause = \"classified fields filtering is not compatible with archiving items\"\n            raise BackendError(cause=cause)\n\n        if self.archive:\n            self.archive.init_metadata(self.origin, self.__class__.__name__, self.version, category,\n                                       kwargs)\n\n        self.client = self._init_client()\n\n        for item in self.fetch_items(category, **kwargs):\n            if filter_classified:\n                item = self.filter_classified_data(item)\n\n            yield self.metadata(item, filter_classified=filter_classified)", "language": "python", "code": "def fetch(self, category, filter_classified=False, **kwargs):\n        \"\"\"Fetch items from the repository.\n\n        The method retrieves items from a repository.\n\n        To removed classified fields from the resulting items, set\n        the parameter `filter_classified`. Take into account this\n        parameter is incompatible with archiving items. Raw client\n        data are archived before any other process. Therefore,\n        classified data  are stored within the archive. To prevent\n        from possible data leaks or security issues when users do\n        not need these fields, archiving and filtering are not\n        compatible.\n\n        :param category: the category of the items fetched\n        :param filter_classified: remove classified fields from the resulting items\n        :param kwargs: a list of other parameters (e.g., from_date, offset, etc.\n        specific for each backend)\n\n        :returns: a generator of items\n\n        :raises BackendError: either when the category is not valid or\n            'filter_classified' and 'archive' are active at the same time.\n        \"\"\"\n        if category not in self.categories:\n            cause = \"%s category not valid for %s\" % (category, self.__class__.__name__)\n            raise BackendError(cause=cause)\n\n        if filter_classified and self.archive:\n            cause = \"classified fields filtering is not compatible with archiving items\"\n            raise BackendError(cause=cause)\n\n        if self.archive:\n            self.archive.init_metadata(self.origin, self.__class__.__name__, self.version, category,\n                                       kwargs)\n\n        self.client = self._init_client()\n\n        for item in self.fetch_items(category, **kwargs):\n            if filter_classified:\n                item = self.filter_classified_data(item)\n\n            yield self.metadata(item, filter_classified=filter_classified)", "code_tokens": ["def", "fetch", "(", "self", ",", "category", ",", "filter_classified", "=", "False", ",", "*", "*", "kwargs", ")", ":", "if", "category", "not", "in", "self", ".", "categories", ":", "cause", "=", "\"%s category not valid for %s\"", "%", "(", "category", ",", "self", ".", "__class__", ".", "__name__", ")", "raise", "BackendError", "(", "cause", "=", "cause", ")", "if", "filter_classified", "and", "self", ".", "archive", ":", "cause", "=", "\"classified fields filtering is not compatible with archiving items\"", "raise", "BackendError", "(", "cause", "=", "cause", ")", "if", "self", ".", "archive", ":", "self", ".", "archive", ".", "init_metadata", "(", "self", ".", "origin", ",", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "version", ",", "category", ",", "kwargs", ")", "self", ".", "client", "=", "self", ".", "_init_client", "(", ")", "for", "item", "in", "self", ".", "fetch_items", "(", "category", ",", "*", "*", "kwargs", ")", ":", "if", "filter_classified", ":", "item", "=", "self", ".", "filter_classified_data", "(", "item", ")", "yield", "self", ".", "metadata", "(", "item", ",", "filter_classified", "=", "filter_classified", ")"], "docstring": "", "docstring_tokens": [], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backend.py#L124-L166", "partition": "test"}
{"repo": "bipsandbytes/django-api", "path": "django_api/decorators.py", "func_name": "validate_json_request", "original_string": "def validate_json_request(required_fields):\n    \"\"\"\n    Return a decorator that ensures that the request passed to the view\n    function/method has a valid JSON request body with the given required\n    fields.  The dict parsed from the JSON is then passed as the second\n    argument to the decorated function/method.  For example:\n\n    @json_request({'name', 'date'})\n    def view_func(request, request_dict):\n        ...\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapped_func(request, *args, **kwargs):\n            try:\n                request_dict = json.loads(request.raw_post_data)\n            except ValueError as e:\n                return JsonResponseBadRequest('invalid POST JSON: %s' % e)\n\n            for k in required_fields:\n                if k not in request_dict:\n                    return JsonResponseBadRequest(\n                        'POST JSON must contain property \\'%s\\'' % k)\n\n            return func(request, request_dict, *args, **kwargs)\n        return wrapped_func\n    return decorator", "language": "python", "code": "def validate_json_request(required_fields):\n    \"\"\"\n    Return a decorator that ensures that the request passed to the view\n    function/method has a valid JSON request body with the given required\n    fields.  The dict parsed from the JSON is then passed as the second\n    argument to the decorated function/method.  For example:\n\n    @json_request({'name', 'date'})\n    def view_func(request, request_dict):\n        ...\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapped_func(request, *args, **kwargs):\n            try:\n                request_dict = json.loads(request.raw_post_data)\n            except ValueError as e:\n                return JsonResponseBadRequest('invalid POST JSON: %s' % e)\n\n            for k in required_fields:\n                if k not in request_dict:\n                    return JsonResponseBadRequest(\n                        'POST JSON must contain property \\'%s\\'' % k)\n\n            return func(request, request_dict, *args, **kwargs)\n        return wrapped_func\n    return decorator", "code_tokens": ["def", "validate_json_request", "(", "required_fields", ")", ":", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapped_func", "(", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "try", ":", "request_dict", "=", "json", ".", "loads", "(", "request", ".", "raw_post_data", ")", "except", "ValueError", "as", "e", ":", "return", "JsonResponseBadRequest", "(", "'invalid POST JSON: %s'", "%", "e", ")", "for", "k", "in", "required_fields", ":", "if", "k", "not", "in", "request_dict", ":", "return", "JsonResponseBadRequest", "(", "'POST JSON must contain property \\'%s\\''", "%", "k", ")", "return", "func", "(", "request", ",", "request_dict", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapped_func", "return", "decorator"], "docstring": "", "docstring_tokens": [], "sha": "df99f4ccbb0c5128bd06da83f60881a85f6dbfe1", "url": "https://github.com/bipsandbytes/django-api/blob/df99f4ccbb0c5128bd06da83f60881a85f6dbfe1/django_api/decorators.py#L221-L247", "partition": "test"}
