{"repo": "openai/baselines", "path": "baselines/deepq/deepq.py", "func_name": "ActWrapper.save_act", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Save model to a pickle located at `path`", "docstring_tokens": ["Save", "model", "to", "a", "pickle", "located", "at", "path"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/deepq/deepq.py#L55-L72", "partition": "valid", "up_fun_num": 5, "context": "import os\nimport tempfile\n\nimport tensorflow as tf\nimport zipfile\nimport cloudpickle\nimport numpy as np\n\nimport baselines.common.tf_util as U\nfrom baselines.common.tf_util import load_variables, save_variables\nfrom baselines import logger\nfrom baselines.common.schedules import LinearSchedule\nfrom baselines.common import set_global_seeds\n\nfrom baselines import deepq\nfrom baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\nfrom baselines.deepq.utils import ObservationInput\n\nfrom baselines.common.tf_util import get_session\nfrom baselines.deepq.models import build_q_func\n\n\nclass ActWrapper(object):\n    def __init__(self, act, act_params):\n        self._act = act\n        self._act_params = act_params\n        self.initial_state = None\n\n    @staticmethod\n    def load_act(path):\n        with open(path, \"rb\") as f:\n            model_data, act_params = cloudpickle.load(f)\n        act = deepq.build_act(**act_params)\n        sess = tf.Session()\n        sess.__enter__()\n        with tempfile.TemporaryDirectory() as td:\n            arc_path = os.path.join(td, \"packed.zip\")\n            with open(arc_path, \"wb\") as f:\n                f.write(model_data)\n\n            zipfile.ZipFile(arc_path, \"r\", zipfile.ZIP_DEFLATED).extractall(td)\n            load_variables(os.path.join(td, \"model\"))\n\n        return ActWrapper(act, act_params)\n\n    def __call__(self, *args, **kwargs):\n        return self._act(*args, **kwargs)\n\n    def step(self, observation, **kwargs):\n        # DQN doesn't use RNNs so we ignore states and masks\n        kwargs.pop(\"S\", None)\n        kwargs.pop(\"M\", None)\n        return self._act([observation], **kwargs), None, None, None\n\n    def save(self, path):\n        save_variables(path)\n\n\ndef load_act(path):\n    \"\"\"Load act function that was returned by learn function.\n\n    Parameters\n    ----------\n    path: str\n        path to the act function pickle\n\n    Returns\n    -------\n    act: ActWrapper\n        function that takes a batch of observations\n        and returns actions.\n    \"\"\"\n    return ActWrapper.load_act(path)\n\n\ndef learn(\n    env,\n    network,\n    seed=None,\n    lr=5e-4,\n    total_timesteps=100000,\n    buffer_size=50000,\n    exploration_fraction=0.1,\n    exploration_final_eps=0.02,\n    train_freq=1,\n    batch_size=32,\n    print_freq=100,\n    checkpoint_freq=10000,\n    checkpoint_path=None,\n    learning_starts=1000,\n    gamma=1.0,\n    target_network_update_freq=500,\n    prioritized_replay=False,\n    prioritized_replay_alpha=0.6,\n    prioritized_replay_beta0=0.4,\n    prioritized_replay_beta_iters=None,\n    prioritized_replay_eps=1e-6,\n    param_noise=False,\n    callback=None,\n    load_path=None,\n    **network_kwargs\n):\n    \"\"\"Train a deepq model.\n\n    Parameters\n    -------\n    env: gym.Env\n        environment to train on\n    network: string or a function\n        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n    seed: int or None\n        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n    lr: float\n        learning rate for adam optimizer\n    total_timesteps: int\n        number of env steps to optimizer for\n    buffer_size: int\n        size of the replay buffer\n    exploration_fraction: float\n        fraction of entire training period over which the exploration rate is annealed\n    exploration_final_eps: float\n        final value of random action probability\n    train_freq: int\n        update the model every `train_freq` steps.\n        set to None to disable printing\n    batch_size: int\n        size of a batched sampled from replay buffer for training\n    print_freq: int\n        how often to print out training progress\n        set to None to disable printing\n    checkpoint_freq: int\n        how often to save the model. This is so that the best version is restored\n        at the end of the training. If you do not wish to restore the best version at\n        the end of the training set this variable to None.\n    learning_starts: int\n        how many steps of the model to collect transitions for before learning starts\n    gamma: float\n        discount factor\n    target_network_update_freq: int\n        update the target network every `target_network_update_freq` steps.\n    prioritized_replay: True\n        if True prioritized replay buffer will be used.\n    prioritized_replay_alpha: float\n        alpha parameter for prioritized replay buffer\n    prioritized_replay_beta0: float\n        initial value of beta for prioritized replay buffer\n    prioritized_replay_beta_iters: int\n        number of iterations over which beta will be annealed from initial value\n        to 1.0. If set to None equals to total_timesteps.\n    prioritized_replay_eps: float\n        epsilon to add to the TD errors when updating priorities.\n    param_noise: bool\n        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n    callback: (locals, globals) -> None\n        function called at every steps with state of the algorithm.\n        If callback returns true training stops.\n    load_path: str\n        path to load the model from. (default: None)\n    **network_kwargs\n        additional keyword arguments to pass to the network builder.\n\n    Returns\n    -------\n    act: ActWrapper\n        Wrapper over act function. Adds ability to save it and load it.\n        See header of baselines/deepq/categorical.py for details on the act function.\n    \"\"\"\n    # Create all the functions necessary to train the model\n\n    sess = get_session()\n    set_global_seeds(seed)\n\n    q_func = build_q_func(network, **network_kwargs)\n\n    # capture the shape outside the closure so that the env object is not serialized\n    # by cloudpickle when serializing make_obs_ph\n\n    observation_space = env.observation_space\n\n    def make_obs_ph(name):\n        return ObservationInput(observation_space, name=name)\n\n    act, train, update_target, debug = deepq.build_train(\n        make_obs_ph=make_obs_ph,\n        q_func=q_func,\n        num_actions=env.action_space.n,\n        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n        gamma=gamma,\n        grad_norm_clipping=10,\n        param_noise=param_noise,\n    )\n\n    act_params = {\n        \"make_obs_ph\": make_obs_ph,\n        \"q_func\": q_func,\n        \"num_actions\": env.action_space.n,\n    }\n\n    act = ActWrapper(act, act_params)\n\n    # Create the replay buffer\n    if prioritized_replay:\n        replay_buffer = PrioritizedReplayBuffer(\n            buffer_size, alpha=prioritized_replay_alpha\n        )\n        if prioritized_replay_beta_iters is None:\n            prioritized_replay_beta_iters = total_timesteps\n        beta_schedule = LinearSchedule(\n            prioritized_replay_beta_iters,\n            initial_p=prioritized_replay_beta0,\n            final_p=1.0,\n        )\n    else:\n        replay_buffer = ReplayBuffer(buffer_size)\n        beta_schedule = None\n    # Create the schedule for exploration starting from 1.\n    exploration = LinearSchedule(\n        schedule_timesteps=int(exploration_fraction * total_timesteps),\n        initial_p=1.0,\n        final_p=exploration_final_eps,\n    )\n\n    # Initialize the parameters and copy them to the target network.\n    U.initialize()\n    update_target()\n\n    episode_rewards = [0.0]\n    saved_mean_reward = None\n    obs = env.reset()\n    reset = True\n\n    with tempfile.TemporaryDirectory() as td:\n        td = checkpoint_path or td\n\n        model_file = os.path.join(td, \"model\")\n        model_saved = False\n\n        if tf.train.latest_checkpoint(td) is not None:\n            load_variables(model_file)\n            logger.log(\"Loaded model from {}\".format(model_file))\n            model_saved = True\n        elif load_path is not None:\n            load_variables(load_path)\n            logger.log(\"Loaded model from {}\".format(load_path))\n\n        for t in range(total_timesteps):\n            if callback is not None:\n                if callback(locals(), globals()):\n                    break\n            # Take action and update exploration to the newest value\n            kwargs = {}\n            if not param_noise:\n                update_eps = exploration.value(t)\n                update_param_noise_threshold = 0.0\n            else:\n                update_eps = 0.0\n                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n                # for detailed explanation.\n                update_param_noise_threshold = -np.log(\n                    1.0\n                    - exploration.value(t)\n                    + exploration.value(t) / float(env.action_space.n)\n                )\n                kwargs[\"reset\"] = reset\n                kwargs[\"update_param_noise_threshold\"] = update_param_noise_threshold\n                kwargs[\"update_param_noise_scale\"] = True\n            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n            env_action = action\n            reset = False\n            new_obs, rew, done, _ = env.step(env_action)\n            # Store transition in the replay buffer.\n            replay_buffer.add(obs, action, rew, new_obs, float(done))\n            obs = new_obs\n\n            episode_rewards[-1] += rew\n            if done:\n                obs = env.reset()\n                episode_rewards.append(0.0)\n                reset = True\n\n            if t > learning_starts and t % train_freq == 0:\n                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n                if prioritized_replay:\n                    experience = replay_buffer.sample(\n                        batch_size, beta=beta_schedule.value(t)\n                    )\n                    (\n                        obses_t,\n                        actions,\n                        rewards,\n                        obses_tp1,\n                        dones,\n                        weights,\n                        batch_idxes,\n                    ) = experience\n                else:\n                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(\n                        batch_size\n                    )\n                    weights, batch_idxes = np.ones_like(rewards), None\n                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n                if prioritized_replay:\n                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n\n            if t > learning_starts and t % target_network_update_freq == 0:\n                # Update target network periodically.\n                update_target()\n\n            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n            num_episodes = len(episode_rewards)\n            if (\n                done\n                and print_freq is not None\n                and len(episode_rewards) % print_freq == 0\n            ):\n                logger.record_tabular(\"steps\", t)\n                logger.record_tabular(\"episodes\", num_episodes)\n                logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n                logger.record_tabular(\n                    \"% time spent exploring\", int(100 * exploration.value(t))\n                )\n                logger.dump_tabular()\n\n            if (\n                checkpoint_freq is not None\n                and t > learning_starts\n                and num_episodes > 100\n                and t % checkpoint_freq == 0\n            ):\n                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:\n                    if print_freq is not None:\n                        logger.log(\n                            \"Saving model due to mean reward increase: {} -> {}\".format(\n                                saved_mean_reward, mean_100ep_reward\n                            )\n                        )\n                    save_variables(model_file)\n                    model_saved = True\n                    saved_mean_reward = mean_100ep_reward\n        if model_saved:\n            if print_freq is not None:\n                logger.log(\n                    \"Restored model with mean reward: {}\".format(saved_mean_reward)\n                )\n            load_variables(model_file)\n\n    return act\n", "levels": [0, 1, 1, 1, 1, 1, 0, 1], "package": ["import os", "import tempfile", "import tensorflow as tf", "import zipfile", "import cloudpickle", "import numpy as np", "import baselines.common.tf_util as U", "from baselines.common.tf_util import load_variables, save_variables", "from baselines import logger", "from baselines.common.schedules import LinearSchedule", "from baselines.common import set_global_seeds", "from baselines import deepq", "from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer", "from baselines.deepq.utils import ObservationInput", "from baselines.common.tf_util import get_session", "from baselines.deepq.models import build_q_func"], "function": ["class ActWrapper(object):\n", "    def __init__(self, act, act_params):\n", "    def load_act(path):\n", "    def __call__(self, *args, **kwargs):\n", "    def step(self, observation, **kwargs):\n", "    def save(self, path):\n", "def load_act(path):\n", "    def make_obs_ph(name):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/models.py", "func_name": "nature_cnn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "CNN from Nature paper.", "docstring_tokens": ["CNN", "from", "Nature", "paper", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/models.py#L16-L27", "partition": "valid", "up_fun_num": 2, "context": "import numpy as np\nimport tensorflow as tf\nfrom baselines.a2c import utils\nfrom baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch\nfrom baselines.common.mpi_running_mean_std import RunningMeanStd\nimport tensorflow.contrib.layers as layers\n\nmapping = {}\n\n\ndef register(name):\n    def _thunk(func):\n        mapping[name] = func\n        return func\n\n    return _thunk\n\n\n@register(\"mlp\")\ndef mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n    \"\"\"\n    Stack of fully-connected layers to be used in a policy / q-function approximator\n\n    Parameters:\n    ----------\n\n    num_layers: int                 number of fully-connected layers (default: 2)\n\n    num_hidden: int                 size of fully-connected layers (default: 64)\n\n    activation:                     activation function (default: tf.tanh)\n\n    Returns:\n    -------\n\n    function that builds fully connected network with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X):\n        h = tf.layers.flatten(X)\n        for i in range(num_layers):\n            h = fc(h, \"mlp_fc{}\".format(i), nh=num_hidden, init_scale=np.sqrt(2))\n            if layer_norm:\n                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n            h = activation(h)\n\n        return h\n\n    return network_fn\n\n\n@register(\"cnn\")\ndef cnn(**conv_kwargs):\n    def network_fn(X):\n        return nature_cnn(X, **conv_kwargs)\n\n    return network_fn\n\n\n@register(\"cnn_small\")\ndef cnn_small(**conv_kwargs):\n    def network_fn(X):\n        h = tf.cast(X, tf.float32) / 255.0\n\n        activ = tf.nn.relu\n        h = activ(\n            conv(h, \"c1\", nf=8, rf=8, stride=4, init_scale=np.sqrt(2), **conv_kwargs)\n        )\n        h = activ(\n            conv(h, \"c2\", nf=16, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs)\n        )\n        h = conv_to_fc(h)\n        h = activ(fc(h, \"fc1\", nh=128, init_scale=np.sqrt(2)))\n        return h\n\n    return network_fn\n\n\n@register(\"lstm\")\ndef lstm(nlstm=128, layer_norm=False):\n    \"\"\"\n    Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters:\n    ----------\n\n    nlstm: int          LSTM hidden state size\n\n    layer_norm: bool    if True, layer-normalized version of LSTM is used\n\n    Returns:\n    -------\n\n    function that builds LSTM with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X, nenv=1):\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = tf.layers.flatten(X)\n\n        M = tf.placeholder(tf.float32, [nbatch])  # mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2 * nlstm])  # states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        if layer_norm:\n            h5, snew = utils.lnlstm(xs, ms, S, scope=\"lnlstm\", nh=nlstm)\n        else:\n            h5, snew = utils.lstm(xs, ms, S, scope=\"lstm\", nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {\"S\": S, \"M\": M, \"state\": snew, \"initial_state\": initial_state}\n\n    return network_fn\n\n\n@register(\"cnn_lstm\")\ndef cnn_lstm(nlstm=128, layer_norm=False, **conv_kwargs):\n    def network_fn(X, nenv=1):\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = nature_cnn(X, **conv_kwargs)\n\n        M = tf.placeholder(tf.float32, [nbatch])  # mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2 * nlstm])  # states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        if layer_norm:\n            h5, snew = utils.lnlstm(xs, ms, S, scope=\"lnlstm\", nh=nlstm)\n        else:\n            h5, snew = utils.lstm(xs, ms, S, scope=\"lstm\", nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {\"S\": S, \"M\": M, \"state\": snew, \"initial_state\": initial_state}\n\n    return network_fn\n\n\n@register(\"cnn_lnlstm\")\ndef cnn_lnlstm(nlstm=128, **conv_kwargs):\n    return cnn_lstm(nlstm, layer_norm=True, **conv_kwargs)\n\n\n@register(\"conv_only\")\ndef conv_only(convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)], **conv_kwargs):\n    \"\"\"\n    convolutions-only net\n\n    Parameters:\n    ----------\n\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\n\n    Returns:\n\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer\n\n    \"\"\"\n\n    def network_fn(X):\n        out = tf.cast(X, tf.float32) / 255.0\n        with tf.variable_scope(\"convnet\"):\n            for num_outputs, kernel_size, stride in convs:\n                out = layers.convolution2d(\n                    out,\n                    num_outputs=num_outputs,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    activation_fn=tf.nn.relu,\n                    **conv_kwargs\n                )\n\n        return out\n\n    return network_fn\n\n\ndef _normalize_clip_observation(x, clip_range=[-5.0, 5.0]):\n    rms = RunningMeanStd(shape=x.shape[1:])\n    norm_x = tf.clip_by_value(\n        (x - rms.mean) / rms.std, min(clip_range), max(clip_range)\n    )\n    return norm_x, rms\n\n\ndef get_network_builder(name):\n    \"\"\"\n    If you want to register your own network outside models.py, you just need:\n\n    Usage Example:\n    -------------\n    from baselines.common.models import register\n    @register(\"your_network_name\")\n    def your_network_define(**net_kwargs):\n        ...\n        return network_fn\n\n    \"\"\"\n    if callable(name):\n        return name\n    elif name in mapping:\n        return mapping[name]\n    else:\n        raise ValueError(\"Unknown network type: {}\".format(name))\n", "levels": [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0], "package": ["import numpy as np", "import tensorflow as tf", "from baselines.a2c import utils", "from baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch", "from baselines.common.mpi_running_mean_std import RunningMeanStd", "import tensorflow.contrib.layers as layers"], "function": ["def register(name):\n", "    def _thunk(func):\n", "def mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n", "    def network_fn(X):\n", "def cnn(**conv_kwargs):\n", "    def network_fn(X):\n", "def cnn_small(**conv_kwargs):\n", "    def network_fn(X):\n", "def lstm(nlstm=128, layer_norm=False):\n", "    def network_fn(X, nenv=1):\n", "def cnn_lstm(nlstm=128, layer_norm=False, **conv_kwargs):\n", "    def network_fn(X, nenv=1):\n", "def cnn_lnlstm(nlstm=128, **conv_kwargs):\n", "def conv_only(convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)], **conv_kwargs):\n", "    def network_fn(X):\n", "def _normalize_clip_observation(x, clip_range=[-5.0, 5.0]):\n", "def get_network_builder(name):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/models.py", "func_name": "conv_only", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "convolutions-only net\n\n    Parameters:\n    ----------\n\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\n\n    Returns:\n\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer", "docstring_tokens": ["convolutions", "-", "only", "net"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/models.py#L171-L198", "partition": "valid", "up_fun_num": 14, "context": "import numpy as np\nimport tensorflow as tf\nfrom baselines.a2c import utils\nfrom baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch\nfrom baselines.common.mpi_running_mean_std import RunningMeanStd\nimport tensorflow.contrib.layers as layers\n\nmapping = {}\n\n\ndef register(name):\n    def _thunk(func):\n        mapping[name] = func\n        return func\n\n    return _thunk\n\n\ndef nature_cnn(unscaled_images, **conv_kwargs):\n    \"\"\"\n    CNN from Nature paper.\n    \"\"\"\n    scaled_images = tf.cast(unscaled_images, tf.float32) / 255.0\n    activ = tf.nn.relu\n    h = activ(\n        conv(\n            scaled_images,\n            \"c1\",\n            nf=32,\n            rf=8,\n            stride=4,\n            init_scale=np.sqrt(2),\n            **conv_kwargs\n        )\n    )\n    h2 = activ(\n        conv(h, \"c2\", nf=64, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs)\n    )\n    h3 = activ(\n        conv(h2, \"c3\", nf=64, rf=3, stride=1, init_scale=np.sqrt(2), **conv_kwargs)\n    )\n    h3 = conv_to_fc(h3)\n    return activ(fc(h3, \"fc1\", nh=512, init_scale=np.sqrt(2)))\n\n\n@register(\"mlp\")\ndef mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n    \"\"\"\n    Stack of fully-connected layers to be used in a policy / q-function approximator\n\n    Parameters:\n    ----------\n\n    num_layers: int                 number of fully-connected layers (default: 2)\n\n    num_hidden: int                 size of fully-connected layers (default: 64)\n\n    activation:                     activation function (default: tf.tanh)\n\n    Returns:\n    -------\n\n    function that builds fully connected network with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X):\n        h = tf.layers.flatten(X)\n        for i in range(num_layers):\n            h = fc(h, \"mlp_fc{}\".format(i), nh=num_hidden, init_scale=np.sqrt(2))\n            if layer_norm:\n                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n            h = activation(h)\n\n        return h\n\n    return network_fn\n\n\n@register(\"cnn\")\ndef cnn(**conv_kwargs):\n    def network_fn(X):\n        return nature_cnn(X, **conv_kwargs)\n\n    return network_fn\n\n\n@register(\"cnn_small\")\ndef cnn_small(**conv_kwargs):\n    def network_fn(X):\n        h = tf.cast(X, tf.float32) / 255.0\n\n        activ = tf.nn.relu\n        h = activ(\n            conv(h, \"c1\", nf=8, rf=8, stride=4, init_scale=np.sqrt(2), **conv_kwargs)\n        )\n        h = activ(\n            conv(h, \"c2\", nf=16, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs)\n        )\n        h = conv_to_fc(h)\n        h = activ(fc(h, \"fc1\", nh=128, init_scale=np.sqrt(2)))\n        return h\n\n    return network_fn\n\n\n@register(\"lstm\")\ndef lstm(nlstm=128, layer_norm=False):\n    \"\"\"\n    Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters:\n    ----------\n\n    nlstm: int          LSTM hidden state size\n\n    layer_norm: bool    if True, layer-normalized version of LSTM is used\n\n    Returns:\n    -------\n\n    function that builds LSTM with a given input tensor / placeholder\n    \"\"\"\n\n    def network_fn(X, nenv=1):\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = tf.layers.flatten(X)\n\n        M = tf.placeholder(tf.float32, [nbatch])  # mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2 * nlstm])  # states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        if layer_norm:\n            h5, snew = utils.lnlstm(xs, ms, S, scope=\"lnlstm\", nh=nlstm)\n        else:\n            h5, snew = utils.lstm(xs, ms, S, scope=\"lstm\", nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {\"S\": S, \"M\": M, \"state\": snew, \"initial_state\": initial_state}\n\n    return network_fn\n\n\n@register(\"cnn_lstm\")\ndef cnn_lstm(nlstm=128, layer_norm=False, **conv_kwargs):\n    def network_fn(X, nenv=1):\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = nature_cnn(X, **conv_kwargs)\n\n        M = tf.placeholder(tf.float32, [nbatch])  # mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2 * nlstm])  # states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        if layer_norm:\n            h5, snew = utils.lnlstm(xs, ms, S, scope=\"lnlstm\", nh=nlstm)\n        else:\n            h5, snew = utils.lstm(xs, ms, S, scope=\"lstm\", nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {\"S\": S, \"M\": M, \"state\": snew, \"initial_state\": initial_state}\n\n    return network_fn\n\n\n@register(\"cnn_lnlstm\")\ndef cnn_lnlstm(nlstm=128, **conv_kwargs):\n    return cnn_lstm(nlstm, layer_norm=True, **conv_kwargs)\n\n\n@register(\"conv_only\")\ndef _normalize_clip_observation(x, clip_range=[-5.0, 5.0]):\n    rms = RunningMeanStd(shape=x.shape[1:])\n    norm_x = tf.clip_by_value(\n        (x - rms.mean) / rms.std, min(clip_range), max(clip_range)\n    )\n    return norm_x, rms\n\n\ndef get_network_builder(name):\n    \"\"\"\n    If you want to register your own network outside models.py, you just need:\n\n    Usage Example:\n    -------------\n    from baselines.common.models import register\n    @register(\"your_network_name\")\n    def your_network_define(**net_kwargs):\n        ...\n        return network_fn\n\n    \"\"\"\n    if callable(name):\n        return name\n    elif name in mapping:\n        return mapping[name]\n    else:\n        raise ValueError(\"Unknown network type: {}\".format(name))\n", "levels": [0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], "package": ["import numpy as np", "import tensorflow as tf", "from baselines.a2c import utils", "from baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch", "from baselines.common.mpi_running_mean_std import RunningMeanStd", "import tensorflow.contrib.layers as layers"], "function": ["def register(name):\n", "    def _thunk(func):\n", "def nature_cnn(unscaled_images, **conv_kwargs):\n", "def mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n", "    def network_fn(X):\n", "def cnn(**conv_kwargs):\n", "    def network_fn(X):\n", "def cnn_small(**conv_kwargs):\n", "    def network_fn(X):\n", "def lstm(nlstm=128, layer_norm=False):\n", "    def network_fn(X, nenv=1):\n", "def cnn_lstm(nlstm=128, layer_norm=False, **conv_kwargs):\n", "    def network_fn(X, nenv=1):\n", "def cnn_lnlstm(nlstm=128, **conv_kwargs):\n", "def _normalize_clip_observation(x, clip_range=[-5.0, 5.0]):\n", "def get_network_builder(name):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/cmd_util.py", "func_name": "make_vec_env", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.", "docstring_tokens": ["Create", "a", "wrapped", "monitored", "SubprocVecEnv", "for", "Atari", "and", "MuJoCo", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/cmd_util.py#L21-L52", "partition": "valid", "up_fun_num": 0, "context": "\"\"\"\nHelpers for scripts like run_atari.py.\n\"\"\"\n\nimport os\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\nimport gym\nfrom gym.wrappers import FlattenDictWrapper\nfrom baselines import logger\nfrom baselines.bench import Monitor\nfrom baselines.common import set_global_seeds\nfrom baselines.common.atari_wrappers import make_atari, wrap_deepmind\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom baselines.common import retro_wrappers\n\n\ndef make_env(\n    env_id,\n    env_type,\n    mpi_rank=0,\n    subrank=0,\n    seed=None,\n    reward_scale=1.0,\n    gamestate=None,\n    flatten_dict_observations=True,\n    wrapper_kwargs=None,\n    logger_dir=None,\n):\n    wrapper_kwargs = wrapper_kwargs or {}\n    if env_type == \"atari\":\n        env = make_atari(env_id)\n    elif env_type == \"retro\":\n        import retro\n\n        gamestate = gamestate or retro.State.DEFAULT\n        env = retro_wrappers.make_retro(\n            game=env_id,\n            max_episode_steps=10000,\n            use_restricted_actions=retro.Actions.DISCRETE,\n            state=gamestate,\n        )\n    else:\n        env = gym.make(env_id)\n\n    if flatten_dict_observations and isinstance(env.observation_space, gym.spaces.Dict):\n        keys = env.observation_space.spaces.keys()\n        env = gym.wrappers.FlattenDictWrapper(env, dict_keys=list(keys))\n\n    env.seed(seed + subrank if seed is not None else None)\n    env = Monitor(\n        env,\n        logger_dir and os.path.join(logger_dir, str(mpi_rank) + \".\" + str(subrank)),\n        allow_early_resets=True,\n    )\n\n    if env_type == \"atari\":\n        env = wrap_deepmind(env, **wrapper_kwargs)\n    elif env_type == \"retro\":\n        if \"frame_stack\" not in wrapper_kwargs:\n            wrapper_kwargs[\"frame_stack\"] = 1\n        env = retro_wrappers.wrap_deepmind_retro(env, **wrapper_kwargs)\n\n    if reward_scale != 1:\n        env = retro_wrappers.RewardScaler(env, reward_scale)\n\n    return env\n\n\ndef make_mujoco_env(env_id, seed, reward_scale=1.0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    rank = MPI.COMM_WORLD.Get_rank()\n    myseed = seed + 1000 * rank if seed is not None else None\n    set_global_seeds(myseed)\n    env = gym.make(env_id)\n    logger_path = (\n        None if logger.get_dir() is None else os.path.join(logger.get_dir(), str(rank))\n    )\n    env = Monitor(env, logger_path, allow_early_resets=True)\n    env.seed(seed)\n    if reward_scale != 1.0:\n        from baselines.common.retro_wrappers import RewardScaler\n\n        env = RewardScaler(env, reward_scale)\n    return env\n\n\ndef make_robotics_env(env_id, seed, rank=0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    set_global_seeds(seed)\n    env = gym.make(env_id)\n    env = FlattenDictWrapper(env, [\"observation\", \"desired_goal\"])\n    env = Monitor(\n        env,\n        logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n        info_keywords=(\"is_success\",),\n    )\n    env.seed(seed)\n    return env\n\n\ndef arg_parser():\n    \"\"\"\n    Create an empty argparse.ArgumentParser.\n    \"\"\"\n    import argparse\n\n    return argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n\ndef atari_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_atari.py.\n    \"\"\"\n    print(\"Obsolete - use common_arg_parser instead\")\n    return common_arg_parser()\n\n\ndef mujoco_arg_parser():\n    print(\"Obsolete - use common_arg_parser instead\")\n    return common_arg_parser()\n\n\ndef common_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument(\"--env\", help=\"environment ID\", type=str, default=\"Reacher-v2\")\n    parser.add_argument(\n        \"--env_type\",\n        help=\"type of environment, used when the environment type cannot be automatically determined\",\n        type=str,\n    )\n    parser.add_argument(\"--seed\", help=\"RNG seed\", type=int, default=None)\n    parser.add_argument(\"--alg\", help=\"Algorithm\", type=str, default=\"ppo2\")\n    parser.add_argument(\"--num_timesteps\", type=float, default=1e6),\n    parser.add_argument(\n        \"--network\",\n        help=\"network type (mlp, cnn, lstm, cnn_lstm, conv_only)\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--gamestate\",\n        help=\"game state to load (so far only used in retro games)\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--num_env\",\n        help=\"Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco\",\n        default=None,\n        type=int,\n    )\n    parser.add_argument(\n        \"--reward_scale\",\n        help=\"Reward scale factor. Default: 1.0\",\n        default=1.0,\n        type=float,\n    )\n    parser.add_argument(\n        \"--save_path\", help=\"Path to save trained model to\", default=None, type=str\n    )\n    parser.add_argument(\n        \"--save_video_interval\",\n        help=\"Save video every x steps (0 = disabled)\",\n        default=0,\n        type=int,\n    )\n    parser.add_argument(\n        \"--save_video_length\",\n        help=\"Length of recorded video. Default: 200\",\n        default=200,\n        type=int,\n    )\n    parser.add_argument(\"--play\", default=False, action=\"store_true\")\n    return parser\n\n\ndef robotics_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument(\n        \"--env\", help=\"environment ID\", type=str, default=\"FetchReach-v0\"\n    )\n    parser.add_argument(\"--seed\", help=\"RNG seed\", type=int, default=None)\n    parser.add_argument(\"--num-timesteps\", type=int, default=int(1e6))\n    return parser\n\n\ndef parse_unknown_args(args):\n    \"\"\"\n    Parse arguments not consumed by arg parser into a dicitonary\n    \"\"\"\n    retval = {}\n    preceded_by_key = False\n    for arg in args:\n        if arg.startswith(\"--\"):\n            if \"=\" in arg:\n                key = arg.split(\"=\")[0][2:]\n                value = arg.split(\"=\")[1]\n                retval[key] = value\n            else:\n                key = arg[2:]\n                preceded_by_key = True\n        elif preceded_by_key:\n            retval[key] = arg\n            preceded_by_key = False\n\n    return retval\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import os", "from mpi4py import MPI", "import gym", "from gym.wrappers import FlattenDictWrapper", "from baselines import logger", "from baselines.bench import Monitor", "from baselines.common import set_global_seeds", "from baselines.common.atari_wrappers import make_atari, wrap_deepmind", "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv", "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv", "from baselines.common import retro_wrappers", "import retro", "from baselines.common.retro_wrappers import RewardScaler", "import argparse"], "function": ["def make_mujoco_env(env_id, seed, reward_scale=1.0):\n", "def make_robotics_env(env_id, seed, rank=0):\n", "def arg_parser():\n", "def atari_arg_parser():\n", "def mujoco_arg_parser():\n", "def common_arg_parser():\n", "def robotics_arg_parser():\n", "def parse_unknown_args(args):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/cmd_util.py", "func_name": "parse_unknown_args", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Parse arguments not consumed by arg parser into a dicitonary", "docstring_tokens": ["Parse", "arguments", "not", "consumed", "by", "arg", "parser", "into", "a", "dicitonary"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/cmd_util.py#L166-L185", "partition": "valid", "up_fun_num": 9, "context": "\"\"\"\nHelpers for scripts like run_atari.py.\n\"\"\"\n\nimport os\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\nimport gym\nfrom gym.wrappers import FlattenDictWrapper\nfrom baselines import logger\nfrom baselines.bench import Monitor\nfrom baselines.common import set_global_seeds\nfrom baselines.common.atari_wrappers import make_atari, wrap_deepmind\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom baselines.common import retro_wrappers\n\n\ndef make_vec_env(\n    env_id,\n    env_type,\n    num_env,\n    seed,\n    wrapper_kwargs=None,\n    start_index=0,\n    reward_scale=1.0,\n    flatten_dict_observations=True,\n    gamestate=None,\n):\n    \"\"\"\n    Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.\n    \"\"\"\n    wrapper_kwargs = wrapper_kwargs or {}\n    mpi_rank = MPI.COMM_WORLD.Get_rank() if MPI else 0\n    seed = seed + 10000 * mpi_rank if seed is not None else None\n    logger_dir = logger.get_dir()\n\n    def make_thunk(rank):\n        return lambda: make_env(\n            env_id=env_id,\n            env_type=env_type,\n            mpi_rank=mpi_rank,\n            subrank=rank,\n            seed=seed,\n            reward_scale=reward_scale,\n            gamestate=gamestate,\n            flatten_dict_observations=flatten_dict_observations,\n            wrapper_kwargs=wrapper_kwargs,\n            logger_dir=logger_dir,\n        )\n\n    set_global_seeds(seed)\n    if num_env > 1:\n        return SubprocVecEnv([make_thunk(i + start_index) for i in range(num_env)])\n    else:\n        return DummyVecEnv([make_thunk(start_index)])\n\n\ndef make_env(\n    env_id,\n    env_type,\n    mpi_rank=0,\n    subrank=0,\n    seed=None,\n    reward_scale=1.0,\n    gamestate=None,\n    flatten_dict_observations=True,\n    wrapper_kwargs=None,\n    logger_dir=None,\n):\n    wrapper_kwargs = wrapper_kwargs or {}\n    if env_type == \"atari\":\n        env = make_atari(env_id)\n    elif env_type == \"retro\":\n        import retro\n\n        gamestate = gamestate or retro.State.DEFAULT\n        env = retro_wrappers.make_retro(\n            game=env_id,\n            max_episode_steps=10000,\n            use_restricted_actions=retro.Actions.DISCRETE,\n            state=gamestate,\n        )\n    else:\n        env = gym.make(env_id)\n\n    if flatten_dict_observations and isinstance(env.observation_space, gym.spaces.Dict):\n        keys = env.observation_space.spaces.keys()\n        env = gym.wrappers.FlattenDictWrapper(env, dict_keys=list(keys))\n\n    env.seed(seed + subrank if seed is not None else None)\n    env = Monitor(\n        env,\n        logger_dir and os.path.join(logger_dir, str(mpi_rank) + \".\" + str(subrank)),\n        allow_early_resets=True,\n    )\n\n    if env_type == \"atari\":\n        env = wrap_deepmind(env, **wrapper_kwargs)\n    elif env_type == \"retro\":\n        if \"frame_stack\" not in wrapper_kwargs:\n            wrapper_kwargs[\"frame_stack\"] = 1\n        env = retro_wrappers.wrap_deepmind_retro(env, **wrapper_kwargs)\n\n    if reward_scale != 1:\n        env = retro_wrappers.RewardScaler(env, reward_scale)\n\n    return env\n\n\ndef make_mujoco_env(env_id, seed, reward_scale=1.0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    rank = MPI.COMM_WORLD.Get_rank()\n    myseed = seed + 1000 * rank if seed is not None else None\n    set_global_seeds(myseed)\n    env = gym.make(env_id)\n    logger_path = (\n        None if logger.get_dir() is None else os.path.join(logger.get_dir(), str(rank))\n    )\n    env = Monitor(env, logger_path, allow_early_resets=True)\n    env.seed(seed)\n    if reward_scale != 1.0:\n        from baselines.common.retro_wrappers import RewardScaler\n\n        env = RewardScaler(env, reward_scale)\n    return env\n\n\ndef make_robotics_env(env_id, seed, rank=0):\n    \"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    \"\"\"\n    set_global_seeds(seed)\n    env = gym.make(env_id)\n    env = FlattenDictWrapper(env, [\"observation\", \"desired_goal\"])\n    env = Monitor(\n        env,\n        logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n        info_keywords=(\"is_success\",),\n    )\n    env.seed(seed)\n    return env\n\n\ndef arg_parser():\n    \"\"\"\n    Create an empty argparse.ArgumentParser.\n    \"\"\"\n    import argparse\n\n    return argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n\ndef atari_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_atari.py.\n    \"\"\"\n    print(\"Obsolete - use common_arg_parser instead\")\n    return common_arg_parser()\n\n\ndef mujoco_arg_parser():\n    print(\"Obsolete - use common_arg_parser instead\")\n    return common_arg_parser()\n\n\ndef common_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument(\"--env\", help=\"environment ID\", type=str, default=\"Reacher-v2\")\n    parser.add_argument(\n        \"--env_type\",\n        help=\"type of environment, used when the environment type cannot be automatically determined\",\n        type=str,\n    )\n    parser.add_argument(\"--seed\", help=\"RNG seed\", type=int, default=None)\n    parser.add_argument(\"--alg\", help=\"Algorithm\", type=str, default=\"ppo2\")\n    parser.add_argument(\"--num_timesteps\", type=float, default=1e6),\n    parser.add_argument(\n        \"--network\",\n        help=\"network type (mlp, cnn, lstm, cnn_lstm, conv_only)\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--gamestate\",\n        help=\"game state to load (so far only used in retro games)\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--num_env\",\n        help=\"Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco\",\n        default=None,\n        type=int,\n    )\n    parser.add_argument(\n        \"--reward_scale\",\n        help=\"Reward scale factor. Default: 1.0\",\n        default=1.0,\n        type=float,\n    )\n    parser.add_argument(\n        \"--save_path\", help=\"Path to save trained model to\", default=None, type=str\n    )\n    parser.add_argument(\n        \"--save_video_interval\",\n        help=\"Save video every x steps (0 = disabled)\",\n        default=0,\n        type=int,\n    )\n    parser.add_argument(\n        \"--save_video_length\",\n        help=\"Length of recorded video. Default: 200\",\n        default=200,\n        type=int,\n    )\n    parser.add_argument(\"--play\", default=False, action=\"store_true\")\n    return parser\n\n\ndef robotics_arg_parser():\n    \"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    \"\"\"\n    parser = arg_parser()\n    parser.add_argument(\n        \"--env\", help=\"environment ID\", type=str, default=\"FetchReach-v0\"\n    )\n    parser.add_argument(\"--seed\", help=\"RNG seed\", type=int, default=None)\n    parser.add_argument(\"--num-timesteps\", type=int, default=int(1e6))\n    return parser\n", "levels": [0, 0, 0, 0, 0, 0, 0], "package": ["import os", "from mpi4py import MPI", "import gym", "from gym.wrappers import FlattenDictWrapper", "from baselines import logger", "from baselines.bench import Monitor", "from baselines.common import set_global_seeds", "from baselines.common.atari_wrappers import make_atari, wrap_deepmind", "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv", "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv", "from baselines.common import retro_wrappers", "import retro", "from baselines.common.retro_wrappers import RewardScaler", "import argparse"], "function": ["def make_mujoco_env(env_id, seed, reward_scale=1.0):\n", "def make_robotics_env(env_id, seed, rank=0):\n", "def arg_parser():\n", "def atari_arg_parser():\n", "def mujoco_arg_parser():\n", "def common_arg_parser():\n", "def robotics_arg_parser():\n"]}
{"repo": "openai/baselines", "path": "baselines/common/vec_env/vec_env.py", "func_name": "clear_mpi_env_vars", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n    Processes.", "docstring_tokens": ["from", "mpi4py", "import", "MPI", "will", "call", "MPI_Init", "by", "default", ".", "If", "the", "child", "process", "has", "MPI", "environment", "variables", "MPI", "will", "think", "that", "the", "child", "process", "is", "an", "MPI", "process", "just", "like", "the", "parent", "and", "do", "bad", "things", "such", "as", "hang", ".", "This", "context", "manager", "is", "a", "hacky", "way", "to", "clear", "those", "environment", "variables", "temporarily", "such", "as", "when", "we", "are", "starting", "multiprocessing", "Processes", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/vec_env/vec_env.py#L204-L219", "partition": "valid", "up_fun_num": 32, "context": "import contextlib\nimport os\nfrom abc import ABC, abstractmethod\n\nfrom baselines.common.tile_images import tile_images\n\nclass AlreadySteppingError(Exception):\n    \"\"\"\n    Raised when an asynchronous step is running while\n    step_async() is called again.\n    \"\"\"\n\n    def __init__(self):\n        msg = 'already running an async step'\n        Exception.__init__(self, msg)\n\n\nclass NotSteppingError(Exception):\n    \"\"\"\n    Raised when an asynchronous step is not running but\n    step_wait() is called.\n    \"\"\"\n\n    def __init__(self):\n        msg = 'not running an async step'\n        Exception.__init__(self, msg)\n\n\nclass VecEnv(ABC):\n    \"\"\"\n    An abstract asynchronous, vectorized environment.\n    Used to batch data from multiple copies of an environment, so that\n    each observation becomes an batch of observations, and expected action is a batch of actions to\n    be applied per-environment.\n    \"\"\"\n    closed = False\n    viewer = None\n\n    metadata = {\n        'render.modes': ['human', 'rgb_array']\n    }\n\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"\n        Reset all the environments and return an array of\n        observations, or a dict of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step_async(self, actions):\n        \"\"\"\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        \"\"\"\n        Wait for the step taken with step_async().\n\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a dict of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of \"episode done\" booleans\n         - infos: a sequence of info objects\n        \"\"\"\n        pass\n\n    def close_extras(self):\n        \"\"\"\n        Clean up the  extra resources, beyond what's in this base class.\n        Only runs when not self.closed.\n        \"\"\"\n        pass\n\n    def close(self):\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras()\n        self.closed = True\n\n    def step(self, actions):\n        \"\"\"\n        Step the environments synchronously.\n\n        This is available for backwards compatibility.\n        \"\"\"\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self, mode='human'):\n        imgs = self.get_images()\n        bigimg = tile_images(imgs)\n        if mode == 'human':\n            self.get_viewer().imshow(bigimg)\n            return self.get_viewer().isopen\n        elif mode == 'rgb_array':\n            return bigimg\n        else:\n            raise NotImplementedError\n\n    def get_images(self):\n        \"\"\"\n        Return RGB images from each environment\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\n    def get_viewer(self):\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.SimpleImageViewer()\n        return self.viewer\n\nclass VecEnvWrapper(VecEnv):\n    \"\"\"\n    An environment wrapper that applies to an entire batch\n    of environments at once.\n    \"\"\"\n\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        VecEnv.__init__(self,\n                        num_envs=venv.num_envs,\n                        observation_space=observation_space or venv.observation_space,\n                        action_space=action_space or venv.action_space)\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self, mode='human'):\n        return self.venv.render(mode=mode)\n\n    def get_images(self):\n        return self.venv.get_images()\n\nclass VecEnvObservationWrapper(VecEnvWrapper):\n    @abstractmethod\n    def process(self, obs):\n        pass\n\n    def reset(self):\n        obs = self.venv.reset()\n        return self.process(obs)\n\n    def step_wait(self):\n        obs, rews, dones, infos = self.venv.step_wait()\n        return self.process(obs), rews, dones, infos\n\nclass CloudpickleWrapper(object):\n    \"\"\"\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    \"\"\"\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\n\n@contextlib.contextmanager\n", "levels": [0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], "package": ["import contextlib", "import os", "from abc import ABC, abstractmethod", "from baselines.common.tile_images import tile_images"], "function": ["class AlreadySteppingError(Exception):\n", "    def __init__(self):\n", "class NotSteppingError(Exception):\n", "    def __init__(self):\n", "class VecEnv(ABC):\n", "    def __init__(self, num_envs, observation_space, action_space):\n", "    def reset(self):\n", "    def step_async(self, actions):\n", "    def step_wait(self):\n", "    def close_extras(self):\n", "    def close(self):\n", "    def step(self, actions):\n", "    def render(self, mode='human'):\n", "    def get_images(self):\n", "    def unwrapped(self):\n", "    def get_viewer(self):\n", "class VecEnvWrapper(VecEnv):\n", "    def __init__(self, venv, observation_space=None, action_space=None):\n", "    def step_async(self, actions):\n", "    def reset(self):\n", "    def step_wait(self):\n", "    def close(self):\n", "    def render(self, mode='human'):\n", "    def get_images(self):\n", "class VecEnvObservationWrapper(VecEnvWrapper):\n", "    def process(self, obs):\n", "    def reset(self):\n", "    def step_wait(self):\n", "class CloudpickleWrapper(object):\n", "    def __init__(self, x):\n", "    def __getstate__(self):\n", "    def __setstate__(self, ob):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/cg.py", "func_name": "cg", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Demmel p 312", "docstring_tokens": ["Demmel", "p", "312"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/cg.py#L2-L34", "partition": "valid", "up_fun_num": 0, "context": "import numpy as np\n", "levels": [], "package": ["import numpy as np"], "function": []}
{"repo": "openai/baselines", "path": "baselines/common/input.py", "func_name": "observation_placeholder", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space     observation space\n\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\n\n    name: str               name of the placeholder\n\n    Returns:\n    -------\n\n    tensorflow placeholder tensor", "docstring_tokens": ["Create", "placeholder", "to", "feed", "observations", "into", "of", "the", "size", "appropriate", "to", "the", "observation", "space"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/input.py#L5-L31", "partition": "valid", "up_fun_num": 0, "context": "import numpy as np\nimport tensorflow as tf\nfrom gym.spaces import Discrete, Box, MultiDiscrete\n\n\ndef observation_input(ob_space, batch_size=None, name=\"Ob\"):\n    \"\"\"\n    Create placeholder to feed observations into of the size appropriate to the observation space, and add input\n    encoder of the appropriate type.\n    \"\"\"\n\n    placeholder = observation_placeholder(ob_space, batch_size, name)\n    return placeholder, encode_observation(ob_space, placeholder)\n\n\ndef encode_observation(ob_space, placeholder):\n    \"\"\"\n    Encode input in the way that is appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space             observation space\n\n    placeholder: tf.placeholder     observation input placeholder\n    \"\"\"\n    if isinstance(ob_space, Discrete):\n        return tf.to_float(tf.one_hot(placeholder, ob_space.n))\n    elif isinstance(ob_space, Box):\n        return tf.to_float(placeholder)\n    elif isinstance(ob_space, MultiDiscrete):\n        placeholder = tf.cast(placeholder, tf.int32)\n        one_hots = [\n            tf.to_float(tf.one_hot(placeholder[..., i], ob_space.nvec[i]))\n            for i in range(placeholder.shape[-1])\n        ]\n        return tf.concat(one_hots, axis=-1)\n    else:\n        raise NotImplementedError\n", "levels": [0, 0], "package": ["import numpy as np", "import tensorflow as tf", "from gym.spaces import Discrete, Box, MultiDiscrete"], "function": ["def observation_input(ob_space, batch_size=None, name=\"Ob\"):\n", "def encode_observation(ob_space, placeholder):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/input.py", "func_name": "observation_input", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create placeholder to feed observations into of the size appropriate to the observation space, and add input\n    encoder of the appropriate type.", "docstring_tokens": ["Create", "placeholder", "to", "feed", "observations", "into", "of", "the", "size", "appropriate", "to", "the", "observation", "space", "and", "add", "input", "encoder", "of", "the", "appropriate", "type", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/input.py#L34-L41", "partition": "valid", "up_fun_num": 1, "context": "import numpy as np\nimport tensorflow as tf\nfrom gym.spaces import Discrete, Box, MultiDiscrete\n\n\ndef observation_placeholder(ob_space, batch_size=None, name=\"Ob\"):\n    \"\"\"\n    Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space     observation space\n\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\n\n    name: str               name of the placeholder\n\n    Returns:\n    -------\n\n    tensorflow placeholder tensor\n    \"\"\"\n\n    assert (\n        isinstance(ob_space, Discrete)\n        or isinstance(ob_space, Box)\n        or isinstance(ob_space, MultiDiscrete)\n    ), \"Can only deal with Discrete and Box observation spaces for now\"\n\n    dtype = ob_space.dtype\n    if dtype == np.int8:\n        dtype = np.uint8\n\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)\n\n\ndef encode_observation(ob_space, placeholder):\n    \"\"\"\n    Encode input in the way that is appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space             observation space\n\n    placeholder: tf.placeholder     observation input placeholder\n    \"\"\"\n    if isinstance(ob_space, Discrete):\n        return tf.to_float(tf.one_hot(placeholder, ob_space.n))\n    elif isinstance(ob_space, Box):\n        return tf.to_float(placeholder)\n    elif isinstance(ob_space, MultiDiscrete):\n        placeholder = tf.cast(placeholder, tf.int32)\n        one_hots = [\n            tf.to_float(tf.one_hot(placeholder[..., i], ob_space.nvec[i]))\n            for i in range(placeholder.shape[-1])\n        ]\n        return tf.concat(one_hots, axis=-1)\n    else:\n        raise NotImplementedError\n", "levels": [0, 0], "package": ["import numpy as np", "import tensorflow as tf", "from gym.spaces import Discrete, Box, MultiDiscrete"], "function": ["def observation_placeholder(ob_space, batch_size=None, name=\"Ob\"):\n", "def encode_observation(ob_space, placeholder):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/input.py", "func_name": "encode_observation", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Encode input in the way that is appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space             observation space\n\n    placeholder: tf.placeholder     observation input placeholder", "docstring_tokens": ["Encode", "input", "in", "the", "way", "that", "is", "appropriate", "to", "the", "observation", "space"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/input.py#L43-L63", "partition": "valid", "up_fun_num": 2, "context": "import numpy as np\nimport tensorflow as tf\nfrom gym.spaces import Discrete, Box, MultiDiscrete\n\n\ndef observation_placeholder(ob_space, batch_size=None, name=\"Ob\"):\n    \"\"\"\n    Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters:\n    ----------\n\n    ob_space: gym.Space     observation space\n\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\n\n    name: str               name of the placeholder\n\n    Returns:\n    -------\n\n    tensorflow placeholder tensor\n    \"\"\"\n\n    assert (\n        isinstance(ob_space, Discrete)\n        or isinstance(ob_space, Box)\n        or isinstance(ob_space, MultiDiscrete)\n    ), \"Can only deal with Discrete and Box observation spaces for now\"\n\n    dtype = ob_space.dtype\n    if dtype == np.int8:\n        dtype = np.uint8\n\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)\n\n\ndef observation_input(ob_space, batch_size=None, name=\"Ob\"):\n    \"\"\"\n    Create placeholder to feed observations into of the size appropriate to the observation space, and add input\n    encoder of the appropriate type.\n    \"\"\"\n\n    placeholder = observation_placeholder(ob_space, batch_size, name)\n    return placeholder, encode_observation(ob_space, placeholder)\n", "levels": [0, 0], "package": ["import numpy as np", "import tensorflow as tf", "from gym.spaces import Discrete, Box, MultiDiscrete"], "function": ["def observation_placeholder(ob_space, batch_size=None, name=\"Ob\"):\n", "def observation_input(ob_space, batch_size=None, name=\"Ob\"):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/rollout.py", "func_name": "RolloutWorker.save_policy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Pickles the current policy for later inspection.", "docstring_tokens": ["Pickles", "the", "current", "policy", "for", "later", "inspection", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/rollout.py#L151-L155", "partition": "valid", "up_fun_num": 6, "context": "from collections import deque\n\nimport numpy as np\nimport pickle\n\nfrom baselines.her.util import convert_episode_to_batch_major, store_args\n\n\nclass RolloutWorker:\n    @store_args\n    def __init__(\n        self,\n        venv,\n        policy,\n        dims,\n        logger,\n        T,\n        rollout_batch_size=1,\n        exploit=False,\n        use_target_net=False,\n        compute_Q=False,\n        noise_eps=0,\n        random_eps=0,\n        history_len=100,\n        render=False,\n        monitor=False,\n        **kwargs\n    ):\n        \"\"\"Rollout worker generates experience by interacting with one or many environments.\n\n        Args:\n            make_env (function): a factory function that creates a new instance of the environment\n                when called\n            policy (object): the policy that is used to act\n            dims (dict of ints): the dimensions for observations (o), goals (g), and actions (u)\n            logger (object): the logger that is used by the rollout worker\n            rollout_batch_size (int): the number of parallel rollouts that should be used\n            exploit (boolean): whether or not to exploit, i.e. to act optimally according to the\n                current policy without any exploration\n            use_target_net (boolean): whether or not to use the target net for rollouts\n            compute_Q (boolean): whether or not to compute the Q values alongside the actions\n            noise_eps (float): scale of the additive Gaussian noise\n            random_eps (float): probability of selecting a completely random action\n            history_len (int): length of history for statistics smoothing\n            render (boolean): whether or not to render the rollouts\n        \"\"\"\n\n        assert self.T > 0\n\n        self.info_keys = [\n            key.replace(\"info_\", \"\") for key in dims.keys() if key.startswith(\"info_\")\n        ]\n\n        self.success_history = deque(maxlen=history_len)\n        self.Q_history = deque(maxlen=history_len)\n\n        self.n_episodes = 0\n        self.reset_all_rollouts()\n        self.clear_history()\n\n    def reset_all_rollouts(self):\n        self.obs_dict = self.venv.reset()\n        self.initial_o = self.obs_dict[\"observation\"]\n        self.initial_ag = self.obs_dict[\"achieved_goal\"]\n        self.g = self.obs_dict[\"desired_goal\"]\n\n    def generate_rollouts(self):\n        \"\"\"Performs `rollout_batch_size` rollouts in parallel for time horizon `T` with the current\n        policy acting on it accordingly.\n        \"\"\"\n        self.reset_all_rollouts()\n\n        # compute observations\n        o = np.empty(\n            (self.rollout_batch_size, self.dims[\"o\"]), np.float32\n        )  # observations\n        ag = np.empty(\n            (self.rollout_batch_size, self.dims[\"g\"]), np.float32\n        )  # achieved goals\n        o[:] = self.initial_o\n        ag[:] = self.initial_ag\n\n        # generate episodes\n        obs, achieved_goals, acts, goals, successes = [], [], [], [], []\n        dones = []\n        info_values = [\n            np.empty(\n                (self.T - 1, self.rollout_batch_size, self.dims[\"info_\" + key]),\n                np.float32,\n            )\n            for key in self.info_keys\n        ]\n        Qs = []\n        for t in range(self.T):\n            policy_output = self.policy.get_actions(\n                o,\n                ag,\n                self.g,\n                compute_Q=self.compute_Q,\n                noise_eps=self.noise_eps if not self.exploit else 0.0,\n                random_eps=self.random_eps if not self.exploit else 0.0,\n                use_target_net=self.use_target_net,\n            )\n\n            if self.compute_Q:\n                u, Q = policy_output\n                Qs.append(Q)\n            else:\n                u = policy_output\n\n            if u.ndim == 1:\n                # The non-batched case should still have a reasonable shape.\n                u = u.reshape(1, -1)\n\n            o_new = np.empty((self.rollout_batch_size, self.dims[\"o\"]))\n            ag_new = np.empty((self.rollout_batch_size, self.dims[\"g\"]))\n            success = np.zeros(self.rollout_batch_size)\n            # compute new states and observations\n            obs_dict_new, _, done, info = self.venv.step(u)\n            o_new = obs_dict_new[\"observation\"]\n            ag_new = obs_dict_new[\"achieved_goal\"]\n            success = np.array([i.get(\"is_success\", 0.0) for i in info])\n\n            if any(done):\n                # here we assume all environments are done is ~same number of steps, so we terminate rollouts whenever any of the envs returns done\n                # trick with using vecenvs is not to add the obs from the environments that are \"done\", because those are already observations\n                # after a reset\n                break\n\n            for i, info_dict in enumerate(info):\n                for idx, key in enumerate(self.info_keys):\n                    info_values[idx][t, i] = info[i][key]\n\n            if np.isnan(o_new).any():\n                self.logger.warn(\n                    \"NaN caught during rollout generation. Trying again...\"\n                )\n                self.reset_all_rollouts()\n                return self.generate_rollouts()\n\n            dones.append(done)\n            obs.append(o.copy())\n            achieved_goals.append(ag.copy())\n            successes.append(success.copy())\n            acts.append(u.copy())\n            goals.append(self.g.copy())\n            o[...] = o_new\n            ag[...] = ag_new\n        obs.append(o.copy())\n        achieved_goals.append(ag.copy())\n\n        episode = dict(o=obs, u=acts, g=goals, ag=achieved_goals)\n        for key, value in zip(self.info_keys, info_values):\n            episode[\"info_{}\".format(key)] = value\n\n        # stats\n        successful = np.array(successes)[-1, :]\n        assert successful.shape == (self.rollout_batch_size,)\n        success_rate = np.mean(successful)\n        self.success_history.append(success_rate)\n        if self.compute_Q:\n            self.Q_history.append(np.mean(Qs))\n        self.n_episodes += self.rollout_batch_size\n\n        return convert_episode_to_batch_major(episode)\n\n    def clear_history(self):\n        \"\"\"Clears all histories that are used for statistics\"\"\"\n        self.success_history.clear()\n        self.Q_history.clear()\n\n    def current_success_rate(self):\n        return np.mean(self.success_history)\n\n    def current_mean_Q(self):\n        return np.mean(self.Q_history)\n\n    def logs(self, prefix=\"worker\"):\n        \"\"\"Generates a dictionary that contains all collected statistics.\"\"\"\n        logs = []\n        logs += [(\"success_rate\", np.mean(self.success_history))]\n        if self.compute_Q:\n            logs += [(\"mean_Q\", np.mean(self.Q_history))]\n        logs += [(\"episode\", self.n_episodes)]\n\n        if prefix != \"\" and not prefix.endswith(\"/\"):\n            return [(prefix + \"/\" + key, val) for key, val in logs]\n        else:\n            return logs\n", "levels": [0, 1, 1, 1, 1, 1, 1], "package": ["from collections import deque", "import numpy as np", "import pickle", "from baselines.her.util import convert_episode_to_batch_major, store_args"], "function": ["class RolloutWorker:\n", "    def reset_all_rollouts(self):\n", "    def generate_rollouts(self):\n", "    def clear_history(self):\n", "    def current_success_rate(self):\n", "    def current_mean_Q(self):\n", "    def logs(self, prefix=\"worker\"):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/rollout.py", "func_name": "RolloutWorker.logs", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Generates a dictionary that contains all collected statistics.", "docstring_tokens": ["Generates", "a", "dictionary", "that", "contains", "all", "collected", "statistics", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/rollout.py#L157-L169", "partition": "valid", "up_fun_num": 7, "context": "from collections import deque\n\nimport numpy as np\nimport pickle\n\nfrom baselines.her.util import convert_episode_to_batch_major, store_args\n\n\nclass RolloutWorker:\n    @store_args\n    def __init__(\n        self,\n        venv,\n        policy,\n        dims,\n        logger,\n        T,\n        rollout_batch_size=1,\n        exploit=False,\n        use_target_net=False,\n        compute_Q=False,\n        noise_eps=0,\n        random_eps=0,\n        history_len=100,\n        render=False,\n        monitor=False,\n        **kwargs\n    ):\n        \"\"\"Rollout worker generates experience by interacting with one or many environments.\n\n        Args:\n            make_env (function): a factory function that creates a new instance of the environment\n                when called\n            policy (object): the policy that is used to act\n            dims (dict of ints): the dimensions for observations (o), goals (g), and actions (u)\n            logger (object): the logger that is used by the rollout worker\n            rollout_batch_size (int): the number of parallel rollouts that should be used\n            exploit (boolean): whether or not to exploit, i.e. to act optimally according to the\n                current policy without any exploration\n            use_target_net (boolean): whether or not to use the target net for rollouts\n            compute_Q (boolean): whether or not to compute the Q values alongside the actions\n            noise_eps (float): scale of the additive Gaussian noise\n            random_eps (float): probability of selecting a completely random action\n            history_len (int): length of history for statistics smoothing\n            render (boolean): whether or not to render the rollouts\n        \"\"\"\n\n        assert self.T > 0\n\n        self.info_keys = [\n            key.replace(\"info_\", \"\") for key in dims.keys() if key.startswith(\"info_\")\n        ]\n\n        self.success_history = deque(maxlen=history_len)\n        self.Q_history = deque(maxlen=history_len)\n\n        self.n_episodes = 0\n        self.reset_all_rollouts()\n        self.clear_history()\n\n    def reset_all_rollouts(self):\n        self.obs_dict = self.venv.reset()\n        self.initial_o = self.obs_dict[\"observation\"]\n        self.initial_ag = self.obs_dict[\"achieved_goal\"]\n        self.g = self.obs_dict[\"desired_goal\"]\n\n    def generate_rollouts(self):\n        \"\"\"Performs `rollout_batch_size` rollouts in parallel for time horizon `T` with the current\n        policy acting on it accordingly.\n        \"\"\"\n        self.reset_all_rollouts()\n\n        # compute observations\n        o = np.empty(\n            (self.rollout_batch_size, self.dims[\"o\"]), np.float32\n        )  # observations\n        ag = np.empty(\n            (self.rollout_batch_size, self.dims[\"g\"]), np.float32\n        )  # achieved goals\n        o[:] = self.initial_o\n        ag[:] = self.initial_ag\n\n        # generate episodes\n        obs, achieved_goals, acts, goals, successes = [], [], [], [], []\n        dones = []\n        info_values = [\n            np.empty(\n                (self.T - 1, self.rollout_batch_size, self.dims[\"info_\" + key]),\n                np.float32,\n            )\n            for key in self.info_keys\n        ]\n        Qs = []\n        for t in range(self.T):\n            policy_output = self.policy.get_actions(\n                o,\n                ag,\n                self.g,\n                compute_Q=self.compute_Q,\n                noise_eps=self.noise_eps if not self.exploit else 0.0,\n                random_eps=self.random_eps if not self.exploit else 0.0,\n                use_target_net=self.use_target_net,\n            )\n\n            if self.compute_Q:\n                u, Q = policy_output\n                Qs.append(Q)\n            else:\n                u = policy_output\n\n            if u.ndim == 1:\n                # The non-batched case should still have a reasonable shape.\n                u = u.reshape(1, -1)\n\n            o_new = np.empty((self.rollout_batch_size, self.dims[\"o\"]))\n            ag_new = np.empty((self.rollout_batch_size, self.dims[\"g\"]))\n            success = np.zeros(self.rollout_batch_size)\n            # compute new states and observations\n            obs_dict_new, _, done, info = self.venv.step(u)\n            o_new = obs_dict_new[\"observation\"]\n            ag_new = obs_dict_new[\"achieved_goal\"]\n            success = np.array([i.get(\"is_success\", 0.0) for i in info])\n\n            if any(done):\n                # here we assume all environments are done is ~same number of steps, so we terminate rollouts whenever any of the envs returns done\n                # trick with using vecenvs is not to add the obs from the environments that are \"done\", because those are already observations\n                # after a reset\n                break\n\n            for i, info_dict in enumerate(info):\n                for idx, key in enumerate(self.info_keys):\n                    info_values[idx][t, i] = info[i][key]\n\n            if np.isnan(o_new).any():\n                self.logger.warn(\n                    \"NaN caught during rollout generation. Trying again...\"\n                )\n                self.reset_all_rollouts()\n                return self.generate_rollouts()\n\n            dones.append(done)\n            obs.append(o.copy())\n            achieved_goals.append(ag.copy())\n            successes.append(success.copy())\n            acts.append(u.copy())\n            goals.append(self.g.copy())\n            o[...] = o_new\n            ag[...] = ag_new\n        obs.append(o.copy())\n        achieved_goals.append(ag.copy())\n\n        episode = dict(o=obs, u=acts, g=goals, ag=achieved_goals)\n        for key, value in zip(self.info_keys, info_values):\n            episode[\"info_{}\".format(key)] = value\n\n        # stats\n        successful = np.array(successes)[-1, :]\n        assert successful.shape == (self.rollout_batch_size,)\n        success_rate = np.mean(successful)\n        self.success_history.append(success_rate)\n        if self.compute_Q:\n            self.Q_history.append(np.mean(Qs))\n        self.n_episodes += self.rollout_batch_size\n\n        return convert_episode_to_batch_major(episode)\n\n    def clear_history(self):\n        \"\"\"Clears all histories that are used for statistics\"\"\"\n        self.success_history.clear()\n        self.Q_history.clear()\n\n    def current_success_rate(self):\n        return np.mean(self.success_history)\n\n    def current_mean_Q(self):\n        return np.mean(self.Q_history)\n\n    def save_policy(self, path):\n        \"\"\"Pickles the current policy for later inspection.\"\"\"\n        with open(path, \"wb\") as f:\n            pickle.dump(self.policy, f)\n", "levels": [0, 1, 1, 1, 1, 1, 1], "package": ["from collections import deque", "import numpy as np", "import pickle", "from baselines.her.util import convert_episode_to_batch_major, store_args"], "function": ["class RolloutWorker:\n", "    def reset_all_rollouts(self):\n", "    def generate_rollouts(self):\n", "    def clear_history(self):\n", "    def current_success_rate(self):\n", "    def current_mean_Q(self):\n", "    def save_policy(self, path):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/plot_util.py", "func_name": "smooth", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Smooth signal y, where radius is determines the size of the window\n\n    mode='twosided':\n        average over the window [max(index - radius, 0), min(index + radius, len(y)-1)]\n    mode='causal':\n        average over the window [max(index - radius, 0), index]\n\n    valid_only: put nan in entries where the full-sized window is not available", "docstring_tokens": ["Smooth", "signal", "y", "where", "radius", "is", "determines", "the", "size", "of", "the", "window"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/plot_util.py#L11-L37", "partition": "valid", "up_fun_num": 0, "context": "import matplotlib.pyplot as plt\nimport os.path as osp\nimport json\nimport os\nimport numpy as np\nimport pandas\nfrom collections import defaultdict, namedtuple\nfrom baselines.bench import monitor\nfrom baselines.logger import read_json, read_csv\n\n\ndef one_sided_ema(\n    xolds, yolds, low=None, high=None, n=512, decay_steps=1.0, low_counts_threshold=1e-8\n):\n    \"\"\"\n    perform one-sided (causal) EMA (exponential moving average)\n    smoothing and resampling to an even grid with n points.\n    Does not do extrapolation, so we assume\n    xolds[0] <= low && high <= xolds[-1]\n\n    Arguments:\n\n    xolds: array or list  - x values of data. Needs to be sorted in ascending order\n    yolds: array of list  - y values of data. Has to have the same length as xolds\n\n    low: float            - min value of the new x grid. By default equals to xolds[0]\n    high: float           - max value of the new x grid. By default equals to xolds[-1]\n\n    n: int                - number of points in new x grid\n\n    decay_steps: float    - EMA decay factor, expressed in new x grid steps.\n\n    low_counts_threshold: float or int\n                          - y values with counts less than this value will be set to NaN\n\n    Returns:\n        tuple sum_ys, count_ys where\n            xs        - array with new x grid\n            ys        - array of EMA of y at each point of the new x grid\n            count_ys  - array of EMA of y counts at each point of the new x grid\n\n    \"\"\"\n\n    low = xolds[0] if low is None else low\n    high = xolds[-1] if high is None else high\n\n    assert (\n        xolds[0] <= low\n    ), \"low = {} < xolds[0] = {} - extrapolation not permitted!\".format(low, xolds[0])\n    assert (\n        xolds[-1] >= high\n    ), \"high = {} > xolds[-1] = {}  - extrapolation not permitted!\".format(\n        high, xolds[-1]\n    )\n    assert len(xolds) == len(\n        yolds\n    ), \"length of xolds ({}) and yolds ({}) do not match!\".format(\n        len(xolds), len(yolds)\n    )\n\n    xolds = xolds.astype(\"float64\")\n    yolds = yolds.astype(\"float64\")\n\n    luoi = 0  # last unused old index\n    sum_y = 0.0\n    count_y = 0.0\n    xnews = np.linspace(low, high, n)\n    decay_period = (high - low) / (n - 1) * decay_steps\n    interstep_decay = np.exp(-1.0 / decay_steps)\n    sum_ys = np.zeros_like(xnews)\n    count_ys = np.zeros_like(xnews)\n    for i in range(n):\n        xnew = xnews[i]\n        sum_y *= interstep_decay\n        count_y *= interstep_decay\n        while True:\n            xold = xolds[luoi]\n            if xold <= xnew:\n                decay = np.exp(-(xnew - xold) / decay_period)\n                sum_y += decay * yolds[luoi]\n                count_y += decay\n                luoi += 1\n            else:\n                break\n            if luoi >= len(xolds):\n                break\n        sum_ys[i] = sum_y\n        count_ys[i] = count_y\n\n    ys = sum_ys / count_ys\n    ys[count_ys < low_counts_threshold] = np.nan\n\n    return xnews, ys, count_ys\n\n\ndef symmetric_ema(\n    xolds, yolds, low=None, high=None, n=512, decay_steps=1.0, low_counts_threshold=1e-8\n):\n    \"\"\"\n    perform symmetric EMA (exponential moving average)\n    smoothing and resampling to an even grid with n points.\n    Does not do extrapolation, so we assume\n    xolds[0] <= low && high <= xolds[-1]\n\n    Arguments:\n\n    xolds: array or list  - x values of data. Needs to be sorted in ascending order\n    yolds: array of list  - y values of data. Has to have the same length as xolds\n\n    low: float            - min value of the new x grid. By default equals to xolds[0]\n    high: float           - max value of the new x grid. By default equals to xolds[-1]\n\n    n: int                - number of points in new x grid\n\n    decay_steps: float    - EMA decay factor, expressed in new x grid steps.\n\n    low_counts_threshold: float or int\n                          - y values with counts less than this value will be set to NaN\n\n    Returns:\n        tuple sum_ys, count_ys where\n            xs        - array with new x grid\n            ys        - array of EMA of y at each point of the new x grid\n            count_ys  - array of EMA of y counts at each point of the new x grid\n\n    \"\"\"\n    xs, ys1, count_ys1 = one_sided_ema(\n        xolds, yolds, low, high, n, decay_steps, low_counts_threshold=0\n    )\n    _, ys2, count_ys2 = one_sided_ema(\n        -xolds[::-1], yolds[::-1], -high, -low, n, decay_steps, low_counts_threshold=0\n    )\n    ys2 = ys2[::-1]\n    count_ys2 = count_ys2[::-1]\n    count_ys = count_ys1 + count_ys2\n    ys = (ys1 * count_ys1 + ys2 * count_ys2) / count_ys\n    ys[count_ys < low_counts_threshold] = np.nan\n    return xs, ys, count_ys\n\n\nResult = namedtuple(\"Result\", \"monitor progress dirname metadata\")\nResult.__new__.__defaults__ = (None,) * len(Result._fields)\n\n\ndef load_results(\n    root_dir_or_dirs, enable_progress=True, enable_monitor=True, verbose=False\n):\n    \"\"\"\n    load summaries of runs from a list of directories (including subdirectories)\n    Arguments:\n\n    enable_progress: bool - if True, will attempt to load data from progress.csv files (data saved by logger). Default: True\n\n    enable_monitor: bool - if True, will attempt to load data from monitor.csv files (data saved by Monitor environment wrapper). Default: True\n\n    verbose: bool - if True, will print out list of directories from which the data is loaded. Default: False\n\n\n    Returns:\n    List of Result objects with the following fields:\n         - dirname - path to the directory data was loaded from\n         - metadata - run metadata (such as command-line arguments and anything else in metadata.json file\n         - monitor - if enable_monitor is True, this field contains pandas dataframe with loaded monitor.csv file (or aggregate of all *.monitor.csv files in the directory)\n         - progress - if enable_progress is True, this field contains pandas dataframe with loaded progress.csv file\n    \"\"\"\n    import re\n\n    if isinstance(root_dir_or_dirs, str):\n        rootdirs = [osp.expanduser(root_dir_or_dirs)]\n    else:\n        rootdirs = [osp.expanduser(d) for d in root_dir_or_dirs]\n    allresults = []\n    for rootdir in rootdirs:\n        assert osp.exists(rootdir), \"%s doesn't exist\" % rootdir\n        for dirname, dirs, files in os.walk(rootdir):\n            if \"-proc\" in dirname:\n                files[:] = []\n                continue\n            monitor_re = re.compile(r\"(\\d+\\.)?(\\d+\\.)?monitor\\.csv\")\n            if set(\n                [\"metadata.json\", \"monitor.json\", \"progress.json\", \"progress.csv\"]\n            ).intersection(files) or any(\n                [f for f in files if monitor_re.match(f)]\n            ):  # also match monitor files like 0.1.monitor.csv\n                # used to be uncommented, which means do not go deeper than current directory if any of the data files\n                # are found\n                # dirs[:] = []\n                result = {\"dirname\": dirname}\n                if \"metadata.json\" in files:\n                    with open(osp.join(dirname, \"metadata.json\"), \"r\") as fh:\n                        result[\"metadata\"] = json.load(fh)\n                progjson = osp.join(dirname, \"progress.json\")\n                progcsv = osp.join(dirname, \"progress.csv\")\n                if enable_progress:\n                    if osp.exists(progjson):\n                        result[\"progress\"] = pandas.DataFrame(read_json(progjson))\n                    elif osp.exists(progcsv):\n                        try:\n                            result[\"progress\"] = read_csv(progcsv)\n                        except pandas.errors.EmptyDataError:\n                            print(\"skipping progress file in \", dirname, \"empty data\")\n                    else:\n                        if verbose:\n                            print(\"skipping %s: no progress file\" % dirname)\n\n                if enable_monitor:\n                    try:\n                        result[\"monitor\"] = pandas.DataFrame(\n                            monitor.load_results(dirname)\n                        )\n                    except monitor.LoadMonitorResultsError:\n                        print(\"skipping %s: no monitor files\" % dirname)\n                    except Exception as e:\n                        print(\"exception loading monitor file in %s: %s\" % (dirname, e))\n\n                if (\n                    result.get(\"monitor\") is not None\n                    or result.get(\"progress\") is not None\n                ):\n                    allresults.append(Result(**result))\n                    if verbose:\n                        print(\"successfully loaded %s\" % dirname)\n\n    if verbose:\n        print(\"loaded %i results\" % len(allresults))\n    return allresults\n\n\nCOLORS = [\n    \"blue\",\n    \"green\",\n    \"red\",\n    \"cyan\",\n    \"magenta\",\n    \"yellow\",\n    \"black\",\n    \"purple\",\n    \"pink\",\n    \"brown\",\n    \"orange\",\n    \"teal\",\n    \"lightblue\",\n    \"lime\",\n    \"lavender\",\n    \"turquoise\",\n    \"darkgreen\",\n    \"tan\",\n    \"salmon\",\n    \"gold\",\n    \"darkred\",\n    \"darkblue\",\n]\n\n\ndef default_xy_fn(r):\n    x = np.cumsum(r.monitor.l)\n    y = smooth(r.monitor.r, radius=10)\n    return x, y\n\n\ndef default_split_fn(r):\n    import re\n\n    # match name between slash and -<digits> at the end of the string\n    # (slash in the beginning or -<digits> in the end or either may be missing)\n    match = re.search(r\"[^/-]+(?=(-\\d+)?\\Z)\", r.dirname)\n    if match:\n        return match.group(0)\n\n\ndef plot_results(\n    allresults,\n    *,\n    xy_fn=default_xy_fn,\n    split_fn=default_split_fn,\n    group_fn=default_split_fn,\n    average_group=False,\n    shaded_std=True,\n    shaded_err=True,\n    figsize=None,\n    legend_outside=False,\n    resample=0,\n    smooth_step=1.0\n):\n    \"\"\"\n    Plot multiple Results objects\n\n    xy_fn: function Result -> x,y           - function that converts results objects into tuple of x and y values.\n                                              By default, x is cumsum of episode lengths, and y is episode rewards\n\n    split_fn: function Result -> hashable   - function that converts results objects into keys to split curves into sub-panels by.\n                                              That is, the results r for which split_fn(r) is different will be put on different sub-panels.\n                                              By default, the portion of r.dirname between last / and -<digits> is returned. The sub-panels are\n                                              stacked vertically in the figure.\n\n    group_fn: function Result -> hashable   - function that converts results objects into keys to group curves by.\n                                              That is, the results r for which group_fn(r) is the same will be put into the same group.\n                                              Curves in the same group have the same color (if average_group is False), or averaged over\n                                              (if average_group is True). The default value is the same as default value for split_fn\n\n    average_group: bool                     - if True, will average the curves in the same group and plot the mean. Enables resampling\n                                              (if resample = 0, will use 512 steps)\n\n    shaded_std: bool                        - if True (default), the shaded region corresponding to standard deviation of the group of curves will be\n                                              shown (only applicable if average_group = True)\n\n    shaded_err: bool                        - if True (default), the shaded region corresponding to error in mean estimate of the group of curves\n                                              (that is, standard deviation divided by square root of number of curves) will be\n                                              shown (only applicable if average_group = True)\n\n    figsize: tuple or None                  - size of the resulting figure (including sub-panels). By default, width is 6 and height is 6 times number of\n                                              sub-panels.\n\n\n    legend_outside: bool                    - if True, will place the legend outside of the sub-panels.\n\n    resample: int                           - if not zero, size of the uniform grid in x direction to resample onto. Resampling is performed via symmetric\n                                              EMA smoothing (see the docstring for symmetric_ema).\n                                              Default is zero (no resampling). Note that if average_group is True, resampling is necessary; in that case, default\n                                              value is 512.\n\n    smooth_step: float                      - when resampling (i.e. when resample > 0 or average_group is True), use this EMA decay parameter (in units of the new grid step).\n                                              See docstrings for decay_steps in symmetric_ema or one_sided_ema functions.\n\n    \"\"\"\n\n    if split_fn is None:\n        split_fn = lambda _: \"\"\n    if group_fn is None:\n        group_fn = lambda _: \"\"\n    sk2r = defaultdict(list)  # splitkey2results\n    for result in allresults:\n        splitkey = split_fn(result)\n        sk2r[splitkey].append(result)\n    assert len(sk2r) > 0\n    assert isinstance(resample, int), \"0: don't resample. <integer>: that many samples\"\n    nrows = len(sk2r)\n    ncols = 1\n    figsize = figsize or (6, 6 * nrows)\n    f, axarr = plt.subplots(nrows, ncols, sharex=False, squeeze=False, figsize=figsize)\n\n    groups = list(set(group_fn(result) for result in allresults))\n\n    default_samples = 512\n    if average_group:\n        resample = resample or default_samples\n\n    for (isplit, sk) in enumerate(sorted(sk2r.keys())):\n        g2l = {}\n        g2c = defaultdict(int)\n        sresults = sk2r[sk]\n        gresults = defaultdict(list)\n        ax = axarr[isplit][0]\n        for result in sresults:\n            group = group_fn(result)\n            g2c[group] += 1\n            x, y = xy_fn(result)\n            if x is None:\n                x = np.arange(len(y))\n            x, y = map(np.asarray, (x, y))\n            if average_group:\n                gresults[group].append((x, y))\n            else:\n                if resample:\n                    x, y, counts = symmetric_ema(\n                        x, y, x[0], x[-1], resample, decay_steps=smooth_step\n                    )\n                (l,) = ax.plot(x, y, color=COLORS[groups.index(group) % len(COLORS)])\n                g2l[group] = l\n        if average_group:\n            for group in sorted(groups):\n                xys = gresults[group]\n                if not any(xys):\n                    continue\n                color = COLORS[groups.index(group) % len(COLORS)]\n                origxs = [xy[0] for xy in xys]\n                minxlen = min(map(len, origxs))\n\n                def allequal(qs):\n                    return all((q == qs[0]).all() for q in qs[1:])\n\n                if resample:\n                    low = max(x[0] for x in origxs)\n                    high = min(x[-1] for x in origxs)\n                    usex = np.linspace(low, high, resample)\n                    ys = []\n                    for (x, y) in xys:\n                        ys.append(\n                            symmetric_ema(\n                                x, y, low, high, resample, decay_steps=smooth_step\n                            )[1]\n                        )\n                else:\n                    assert allequal(\n                        [x[:minxlen] for x in origxs]\n                    ), \"If you want to average unevenly sampled data, set resample=<number of samples you want>\"\n                    usex = origxs[0]\n                    ys = [xy[1][:minxlen] for xy in xys]\n                ymean = np.mean(ys, axis=0)\n                ystd = np.std(ys, axis=0)\n                ystderr = ystd / np.sqrt(len(ys))\n                (l,) = axarr[isplit][0].plot(usex, ymean, color=color)\n                g2l[group] = l\n                if shaded_err:\n                    ax.fill_between(\n                        usex, ymean - ystderr, ymean + ystderr, color=color, alpha=0.4\n                    )\n                if shaded_std:\n                    ax.fill_between(\n                        usex, ymean - ystd, ymean + ystd, color=color, alpha=0.2\n                    )\n\n        # https://matplotlib.org/users/legend_guide.html\n        plt.tight_layout()\n        if any(g2l.keys()):\n            ax.legend(\n                g2l.values(),\n                [\"%s (%i)\" % (g, g2c[g]) for g in g2l] if average_group else g2l.keys(),\n                loc=2 if legend_outside else None,\n                bbox_to_anchor=(1, 1) if legend_outside else None,\n            )\n        ax.set_title(sk)\n    return f, axarr\n\n\ndef regression_analysis(df):\n    xcols = list(df.columns.copy())\n    xcols.remove(\"score\")\n    ycols = [\"score\"]\n    import statsmodels.api as sm\n\n    mod = sm.OLS(df[ycols], sm.add_constant(df[xcols]), hasconst=False)\n    res = mod.fit()\n    print(res.summary())\n\n\ndef test_smooth():\n    norig = 100\n    nup = 300\n    ndown = 30\n    xs = np.cumsum(np.random.rand(norig) * 10 / norig)\n    yclean = np.sin(xs)\n    ys = yclean + 0.1 * np.random.randn(yclean.size)\n    xup, yup, _ = symmetric_ema(\n        xs, ys, xs.min(), xs.max(), nup, decay_steps=nup / ndown\n    )\n    xdown, ydown, _ = symmetric_ema(\n        xs, ys, xs.min(), xs.max(), ndown, decay_steps=ndown / ndown\n    )\n    xsame, ysame, _ = symmetric_ema(\n        xs, ys, xs.min(), xs.max(), norig, decay_steps=norig / ndown\n    )\n    plt.plot(xs, ys, label=\"orig\", marker=\"x\")\n    plt.plot(xup, yup, label=\"up\", marker=\"x\")\n    plt.plot(xdown, ydown, label=\"down\", marker=\"x\")\n    plt.plot(xsame, ysame, label=\"same\", marker=\"x\")\n    plt.plot(xs, yclean, label=\"clean\", marker=\"x\")\n    plt.legend()\n    plt.show()\n", "levels": [0, 0, 0, 0], "package": ["import matplotlib.pyplot as plt", "import os.path as osp", "import json", "import os", "import numpy as np", "import pandas", "from collections import defaultdict, namedtuple", "from baselines.bench import monitor", "from baselines.logger import read_json, read_csv", "import re", "import re", "import statsmodels.api as sm"], "function": ["def default_xy_fn(r):\n", "def default_split_fn(r):\n", "def regression_analysis(df):\n", "def test_smooth():\n"]}
{"repo": "openai/baselines", "path": "baselines/common/vec_env/util.py", "func_name": "copy_obs_dict", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Deep-copy an observation dict.", "docstring_tokens": ["Deep", "-", "copy", "an", "observation", "dict", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/vec_env/util.py#L11-L15", "partition": "valid", "up_fun_num": 0, "context": "\"\"\"\nHelpers for dealing with vectorized environments.\n\"\"\"\n\nfrom collections import OrderedDict\n\nimport gym\nimport numpy as np\n\n\ndef dict_to_obs(obs_dict):\n    \"\"\"\n    Convert an observation dict into a raw array if the\n    original observation space was not a Dict space.\n    \"\"\"\n    if set(obs_dict.keys()) == {None}:\n        return obs_dict[None]\n    return obs_dict\n\n\ndef obs_space_info(obs_space):\n    \"\"\"\n    Get dict-structured information about a gym.Space.\n\n    Returns:\n      A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.\n    \"\"\"\n    if isinstance(obs_space, gym.spaces.Dict):\n        assert isinstance(obs_space.spaces, OrderedDict)\n        subspaces = obs_space.spaces\n    else:\n        subspaces = {None: obs_space}\n    keys = []\n    shapes = {}\n    dtypes = {}\n    for key, box in subspaces.items():\n        keys.append(key)\n        shapes[key] = box.shape\n        dtypes[key] = box.dtype\n    return keys, shapes, dtypes\n\n\ndef obs_to_dict(obs):\n    \"\"\"\n    Convert an observation into a dict.\n    \"\"\"\n    if isinstance(obs, dict):\n        return obs\n    return {None: obs}\n", "levels": [0, 0, 0], "package": ["from collections import OrderedDict", "import gym", "import numpy as np"], "function": ["def dict_to_obs(obs_dict):\n", "def obs_space_info(obs_space):\n", "def obs_to_dict(obs):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/vec_env/util.py", "func_name": "obs_space_info", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Get dict-structured information about a gym.Space.\n\n    Returns:\n      A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.", "docstring_tokens": ["Get", "dict", "-", "structured", "information", "about", "a", "gym", ".", "Space", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/vec_env/util.py#L28-L50", "partition": "valid", "up_fun_num": 2, "context": "\"\"\"\nHelpers for dealing with vectorized environments.\n\"\"\"\n\nfrom collections import OrderedDict\n\nimport gym\nimport numpy as np\n\n\ndef copy_obs_dict(obs):\n    \"\"\"\n    Deep-copy an observation dict.\n    \"\"\"\n    return {k: np.copy(v) for k, v in obs.items()}\n\n\ndef dict_to_obs(obs_dict):\n    \"\"\"\n    Convert an observation dict into a raw array if the\n    original observation space was not a Dict space.\n    \"\"\"\n    if set(obs_dict.keys()) == {None}:\n        return obs_dict[None]\n    return obs_dict\n\n\ndef obs_to_dict(obs):\n    \"\"\"\n    Convert an observation into a dict.\n    \"\"\"\n    if isinstance(obs, dict):\n        return obs\n    return {None: obs}\n", "levels": [0, 0, 0], "package": ["from collections import OrderedDict", "import gym", "import numpy as np"], "function": ["def copy_obs_dict(obs):\n", "def dict_to_obs(obs_dict):\n", "def obs_to_dict(obs):\n"]}
{"repo": "openai/baselines", "path": "baselines/acer/acer.py", "func_name": "q_retrace", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Calculates q_retrace targets\n\n    :param R: Rewards\n    :param D: Dones\n    :param q_i: Q values for actions taken\n    :param v: V values\n    :param rho_i: Importance weight for each action\n    :return: Q_retrace values", "docstring_tokens": ["Calculates", "q_retrace", "targets"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/acer/acer.py#L25-L51", "partition": "valid", "up_fun_num": 1, "context": "import time\nimport functools\nimport numpy as np\nimport tensorflow as tf\nfrom baselines import logger\n\nfrom baselines.common import set_global_seeds\nfrom baselines.common.policies import build_policy\nfrom baselines.common.tf_util import get_session, save_variables\nfrom baselines.common.vec_env.vec_frame_stack import VecFrameStack\n\nfrom baselines.a2c.utils import batch_to_seq, seq_to_batch\nfrom baselines.a2c.utils import cat_entropy_softmax\nfrom baselines.a2c.utils import Scheduler, find_trainable_variables\nfrom baselines.a2c.utils import EpisodeStats\nfrom baselines.a2c.utils import (\n    get_by_index,\n    check_shape,\n    avg_norm,\n    gradient_add,\n    q_explained_variance,\n)\nfrom baselines.acer.buffer import Buffer\nfrom baselines.acer.runner import Runner\n\n# remove last step\ndef strip(var, nenvs, nsteps, flat=False):\n    vars = batch_to_seq(var, nenvs, nsteps + 1, flat)\n    return seq_to_batch(vars[:-1], flat)\n\n\n# For ACER with PPO clipping instead of trust region\n# def clip(ratio, eps_clip):\n#     # assume 0 <= eps_clip <= 1\n#     return tf.minimum(1 + eps_clip, tf.maximum(1 - eps_clip, ratio))\n\n\nclass Model(object):\n    def __init__(\n        self,\n        policy,\n        ob_space,\n        ac_space,\n        nenvs,\n        nsteps,\n        ent_coef,\n        q_coef,\n        gamma,\n        max_grad_norm,\n        lr,\n        rprop_alpha,\n        rprop_epsilon,\n        total_timesteps,\n        lrschedule,\n        c,\n        trust_region,\n        alpha,\n        delta,\n    ):\n\n        sess = get_session()\n        nact = ac_space.n\n        nbatch = nenvs * nsteps\n\n        A = tf.placeholder(tf.int32, [nbatch])  # actions\n        D = tf.placeholder(tf.float32, [nbatch])  # dones\n        R = tf.placeholder(tf.float32, [nbatch])  # rewards, not returns\n        MU = tf.placeholder(tf.float32, [nbatch, nact])  # mu's\n        LR = tf.placeholder(tf.float32, [])\n        eps = 1e-6\n\n        step_ob_placeholder = tf.placeholder(\n            dtype=ob_space.dtype, shape=(nenvs,) + ob_space.shape\n        )\n        train_ob_placeholder = tf.placeholder(\n            dtype=ob_space.dtype, shape=(nenvs * (nsteps + 1),) + ob_space.shape\n        )\n        with tf.variable_scope(\"acer_model\", reuse=tf.AUTO_REUSE):\n\n            step_model = policy(\n                nbatch=nenvs,\n                nsteps=1,\n                observ_placeholder=step_ob_placeholder,\n                sess=sess,\n            )\n            train_model = policy(\n                nbatch=nbatch,\n                nsteps=nsteps,\n                observ_placeholder=train_ob_placeholder,\n                sess=sess,\n            )\n\n        params = find_trainable_variables(\"acer_model\")\n        print(\"Params {}\".format(len(params)))\n        for var in params:\n            print(var)\n\n        # create polyak averaged model\n        ema = tf.train.ExponentialMovingAverage(alpha)\n        ema_apply_op = ema.apply(params)\n\n        def custom_getter(getter, *args, **kwargs):\n            v = ema.average(getter(*args, **kwargs))\n            print(v.name)\n            return v\n\n        with tf.variable_scope(\"acer_model\", custom_getter=custom_getter, reuse=True):\n            polyak_model = policy(\n                nbatch=nbatch,\n                nsteps=nsteps,\n                observ_placeholder=train_ob_placeholder,\n                sess=sess,\n            )\n\n        # Notation: (var) = batch variable, (var)s = seqeuence variable, (var)_i = variable index by action at step i\n\n        # action probability distributions according to train_model, polyak_model and step_model\n        # poilcy.pi is probability distribution parameters; to obtain distribution that sums to 1 need to take softmax\n        train_model_p = tf.nn.softmax(train_model.pi)\n        polyak_model_p = tf.nn.softmax(polyak_model.pi)\n        step_model_p = tf.nn.softmax(step_model.pi)\n        v = tf.reduce_sum(\n            train_model_p * train_model.q, axis=-1\n        )  # shape is [nenvs * (nsteps + 1)]\n\n        # strip off last step\n        f, f_pol, q = map(\n            lambda var: strip(var, nenvs, nsteps),\n            [train_model_p, polyak_model_p, train_model.q],\n        )\n        # Get pi and q values for actions taken\n        f_i = get_by_index(f, A)\n        q_i = get_by_index(q, A)\n\n        # Compute ratios for importance truncation\n        rho = f / (MU + eps)\n        rho_i = get_by_index(rho, A)\n\n        # Calculate Q_retrace targets\n        qret = q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma)\n\n        # Calculate losses\n        # Entropy\n        # entropy = tf.reduce_mean(strip(train_model.pd.entropy(), nenvs, nsteps))\n        entropy = tf.reduce_mean(cat_entropy_softmax(f))\n\n        # Policy Graident loss, with truncated importance sampling & bias correction\n        v = strip(v, nenvs, nsteps, True)\n        check_shape([qret, v, rho_i, f_i], [[nenvs * nsteps]] * 4)\n        check_shape([rho, f, q], [[nenvs * nsteps, nact]] * 2)\n\n        # Truncated importance sampling\n        adv = qret - v\n        logf = tf.log(f_i + eps)\n        gain_f = logf * tf.stop_gradient(adv * tf.minimum(c, rho_i))  # [nenvs * nsteps]\n        loss_f = -tf.reduce_mean(gain_f)\n\n        # Bias correction for the truncation\n        adv_bc = q - tf.reshape(v, [nenvs * nsteps, 1])  # [nenvs * nsteps, nact]\n        logf_bc = tf.log(f + eps)  # / (f_old + eps)\n        check_shape([adv_bc, logf_bc], [[nenvs * nsteps, nact]] * 2)\n        gain_bc = tf.reduce_sum(\n            logf_bc\n            * tf.stop_gradient(adv_bc * tf.nn.relu(1.0 - (c / (rho + eps))) * f),\n            axis=1,\n        )  # IMP: This is sum, as expectation wrt f\n        loss_bc = -tf.reduce_mean(gain_bc)\n\n        loss_policy = loss_f + loss_bc\n\n        # Value/Q function loss, and explained variance\n        check_shape([qret, q_i], [[nenvs * nsteps]] * 2)\n        ev = q_explained_variance(\n            tf.reshape(q_i, [nenvs, nsteps]), tf.reshape(qret, [nenvs, nsteps])\n        )\n        loss_q = tf.reduce_mean(tf.square(tf.stop_gradient(qret) - q_i) * 0.5)\n\n        # Net loss\n        check_shape([loss_policy, loss_q, entropy], [[]] * 3)\n        loss = loss_policy + q_coef * loss_q - ent_coef * entropy\n\n        if trust_region:\n            g = tf.gradients(\n                -(loss_policy - ent_coef * entropy) * nsteps * nenvs, f\n            )  # [nenvs * nsteps, nact]\n            # k = tf.gradients(KL(f_pol || f), f)\n            k = -f_pol / (\n                f + eps\n            )  # [nenvs * nsteps, nact] # Directly computed gradient of KL divergence wrt f\n            k_dot_g = tf.reduce_sum(k * g, axis=-1)\n            adj = tf.maximum(\n                0.0,\n                (tf.reduce_sum(k * g, axis=-1) - delta)\n                / (tf.reduce_sum(tf.square(k), axis=-1) + eps),\n            )  # [nenvs * nsteps]\n\n            # Calculate stats (before doing adjustment) for logging.\n            avg_norm_k = avg_norm(k)\n            avg_norm_g = avg_norm(g)\n            avg_norm_k_dot_g = tf.reduce_mean(tf.abs(k_dot_g))\n            avg_norm_adj = tf.reduce_mean(tf.abs(adj))\n\n            g = g - tf.reshape(adj, [nenvs * nsteps, 1]) * k\n            grads_f = -g / (\n                nenvs * nsteps\n            )  # These are turst region adjusted gradients wrt f ie statistics of policy pi\n            grads_policy = tf.gradients(f, params, grads_f)\n            grads_q = tf.gradients(loss_q * q_coef, params)\n            grads = [\n                gradient_add(g1, g2, param)\n                for (g1, g2, param) in zip(grads_policy, grads_q, params)\n            ]\n\n            avg_norm_grads_f = avg_norm(grads_f) * (nsteps * nenvs)\n            norm_grads_q = tf.global_norm(grads_q)\n            norm_grads_policy = tf.global_norm(grads_policy)\n        else:\n            grads = tf.gradients(loss, params)\n\n        if max_grad_norm is not None:\n            grads, norm_grads = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads = list(zip(grads, params))\n        trainer = tf.train.RMSPropOptimizer(\n            learning_rate=LR, decay=rprop_alpha, epsilon=rprop_epsilon\n        )\n        _opt_op = trainer.apply_gradients(grads)\n\n        # so when you call _train, you first do the gradient step, then you apply ema\n        with tf.control_dependencies([_opt_op]):\n            _train = tf.group(ema_apply_op)\n\n        lr = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)\n\n        # Ops/Summaries to run, and their names for logging\n        run_ops = [\n            _train,\n            loss,\n            loss_q,\n            entropy,\n            loss_policy,\n            loss_f,\n            loss_bc,\n            ev,\n            norm_grads,\n        ]\n        names_ops = [\n            \"loss\",\n            \"loss_q\",\n            \"entropy\",\n            \"loss_policy\",\n            \"loss_f\",\n            \"loss_bc\",\n            \"explained_variance\",\n            \"norm_grads\",\n        ]\n        if trust_region:\n            run_ops = run_ops + [\n                norm_grads_q,\n                norm_grads_policy,\n                avg_norm_grads_f,\n                avg_norm_k,\n                avg_norm_g,\n                avg_norm_k_dot_g,\n                avg_norm_adj,\n            ]\n            names_ops = names_ops + [\n                \"norm_grads_q\",\n                \"norm_grads_policy\",\n                \"avg_norm_grads_f\",\n                \"avg_norm_k\",\n                \"avg_norm_g\",\n                \"avg_norm_k_dot_g\",\n                \"avg_norm_adj\",\n            ]\n\n        def train(obs, actions, rewards, dones, mus, states, masks, steps):\n            cur_lr = lr.value_steps(steps)\n            td_map = {\n                train_model.X: obs,\n                polyak_model.X: obs,\n                A: actions,\n                R: rewards,\n                D: dones,\n                MU: mus,\n                LR: cur_lr,\n            }\n            if states is not None:\n                td_map[train_model.S] = states\n                td_map[train_model.M] = masks\n                td_map[polyak_model.S] = states\n                td_map[polyak_model.M] = masks\n\n            return names_ops, sess.run(run_ops, td_map)[1:]  # strip off _train\n\n        def _step(observation, **kwargs):\n            return step_model._evaluate(\n                [step_model.action, step_model_p, step_model.state],\n                observation,\n                **kwargs\n            )\n\n        self.train = train\n        self.save = functools.partial(save_variables, sess=sess, variables=params)\n        self.train_model = train_model\n        self.step_model = step_model\n        self._step = _step\n        self.step = self.step_model.step\n\n        self.initial_state = step_model.initial_state\n        tf.global_variables_initializer().run(session=sess)\n\n\nclass Acer:\n    def __init__(self, runner, model, buffer, log_interval):\n        self.runner = runner\n        self.model = model\n        self.buffer = buffer\n        self.log_interval = log_interval\n        self.tstart = None\n        self.episode_stats = EpisodeStats(runner.nsteps, runner.nenv)\n        self.steps = None\n\n    def call(self, on_policy):\n        runner, model, buffer, steps = self.runner, self.model, self.buffer, self.steps\n        if on_policy:\n            enc_obs, obs, actions, rewards, mus, dones, masks = runner.run()\n            self.episode_stats.feed(rewards, dones)\n            if buffer is not None:\n                buffer.put(enc_obs, actions, rewards, mus, dones, masks)\n        else:\n            # get obs, actions, rewards, mus, dones from buffer.\n            obs, actions, rewards, mus, dones, masks = buffer.get()\n\n        # reshape stuff correctly\n        obs = obs.reshape(runner.batch_ob_shape)\n        actions = actions.reshape([runner.nbatch])\n        rewards = rewards.reshape([runner.nbatch])\n        mus = mus.reshape([runner.nbatch, runner.nact])\n        dones = dones.reshape([runner.nbatch])\n        masks = masks.reshape([runner.batch_ob_shape[0]])\n\n        names_ops, values_ops = model.train(\n            obs, actions, rewards, dones, mus, model.initial_state, masks, steps\n        )\n\n        if on_policy and (int(steps / runner.nbatch) % self.log_interval == 0):\n            logger.record_tabular(\"total_timesteps\", steps)\n            logger.record_tabular(\"fps\", int(steps / (time.time() - self.tstart)))\n            # IMP: In EpisodicLife env, during training, we get done=True at each loss of life, not just at the terminal state.\n            # Thus, this is mean until end of life, not end of episode.\n            # For true episode rewards, see the monitor files in the log folder.\n            logger.record_tabular(\n                \"mean_episode_length\", self.episode_stats.mean_length()\n            )\n            logger.record_tabular(\n                \"mean_episode_reward\", self.episode_stats.mean_reward()\n            )\n            for name, val in zip(names_ops, values_ops):\n                logger.record_tabular(name, float(val))\n            logger.dump_tabular()\n\n\ndef learn(\n    network,\n    env,\n    seed=None,\n    nsteps=20,\n    total_timesteps=int(80e6),\n    q_coef=0.5,\n    ent_coef=0.01,\n    max_grad_norm=10,\n    lr=7e-4,\n    lrschedule=\"linear\",\n    rprop_epsilon=1e-5,\n    rprop_alpha=0.99,\n    gamma=0.99,\n    log_interval=100,\n    buffer_size=50000,\n    replay_ratio=4,\n    replay_start=10000,\n    c=10.0,\n    trust_region=True,\n    alpha=0.99,\n    delta=1,\n    load_path=None,\n    **network_kwargs\n):\n\n    \"\"\"\n    Main entrypoint for ACER (Actor-Critic with Experience Replay) algorithm (https://arxiv.org/pdf/1611.01224.pdf)\n    Train an agent with given network architecture on a given environment using ACER.\n\n    Parameters:\n    ----------\n\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n\n    env:                environment. Needs to be vectorized for parallel environment simulation.\n                        The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.\n\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                        nenv is number of environment copies simulated in parallel) (default: 20)\n\n    nstack:             int, size of the frame stack, i.e. number of the frames passed to the step model. Frames are stacked along channel dimension\n                        (last image dimension) (default: 4)\n\n    total_timesteps:    int, number of timesteps (i.e. number of actions taken in the environment) (default: 80M)\n\n    q_coef:             float, value function loss coefficient in the optimization objective (analog of vf_coef for other actor-critic methods)\n\n    ent_coef:           float, policy entropy coefficient in the optimization objective (default: 0.01)\n\n    max_grad_norm:      float, gradient norm clipping coefficient. If set to None, no clipping. (default: 10),\n\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n                        returns fraction of the learning rate (specified as lr) as output\n\n    rprop_epsilon:      float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n\n    rprop_alpha:        float, RMSProp decay parameter (default: 0.99)\n\n    gamma:              float, reward discounting factor (default: 0.99)\n\n    log_interval:       int, number of updates between logging events (default: 100)\n\n    buffer_size:        int, size of the replay buffer (default: 50k)\n\n    replay_ratio:       int, now many (on average) batches of data to sample from the replay buffer take after batch from the environment (default: 4)\n\n    replay_start:       int, the sampling from the replay buffer does not start until replay buffer has at least that many samples (default: 10k)\n\n    c:                  float, importance weight clipping factor (default: 10)\n\n    trust_region        bool, whether or not algorithms estimates the gradient KL divergence between the old and updated policy and uses it to determine step size  (default: True)\n\n    delta:              float, max KL divergence between the old policy and updated policy (default: 1)\n\n    alpha:              float, momentum factor in the Polyak (exponential moving average) averaging of the model parameters (default: 0.99)\n\n    load_path:          str, path to load the model from (default: None)\n\n    **network_kwargs:               keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                                    For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n\n    \"\"\"\n\n    print(\"Running Acer Simple\")\n    print(locals())\n    set_global_seeds(seed)\n    if not isinstance(env, VecFrameStack):\n        env = VecFrameStack(env, 1)\n\n    policy = build_policy(env, network, estimate_q=True, **network_kwargs)\n    nenvs = env.num_envs\n    ob_space = env.observation_space\n    ac_space = env.action_space\n\n    nstack = env.nstack\n    model = Model(\n        policy=policy,\n        ob_space=ob_space,\n        ac_space=ac_space,\n        nenvs=nenvs,\n        nsteps=nsteps,\n        ent_coef=ent_coef,\n        q_coef=q_coef,\n        gamma=gamma,\n        max_grad_norm=max_grad_norm,\n        lr=lr,\n        rprop_alpha=rprop_alpha,\n        rprop_epsilon=rprop_epsilon,\n        total_timesteps=total_timesteps,\n        lrschedule=lrschedule,\n        c=c,\n        trust_region=trust_region,\n        alpha=alpha,\n        delta=delta,\n    )\n\n    runner = Runner(env=env, model=model, nsteps=nsteps)\n    if replay_ratio > 0:\n        buffer = Buffer(env=env, nsteps=nsteps, size=buffer_size)\n    else:\n        buffer = None\n    nbatch = nenvs * nsteps\n    acer = Acer(runner, model, buffer, log_interval)\n    acer.tstart = time.time()\n\n    for acer.steps in range(\n        0, total_timesteps, nbatch\n    ):  # nbatch samples, 1 on_policy call and multiple off-policy calls\n        acer.call(on_policy=True)\n        if replay_ratio > 0 and buffer.has_atleast(replay_start):\n            n = np.random.poisson(replay_ratio)\n            for _ in range(n):\n                acer.call(on_policy=False)  # no simulation steps in this\n\n    return model\n", "levels": [0, 0, 0, 1, 1], "package": ["import time", "import functools", "import numpy as np", "import tensorflow as tf", "from baselines import logger", "from baselines.common import set_global_seeds", "from baselines.common.policies import build_policy", "from baselines.common.tf_util import get_session, save_variables", "from baselines.common.vec_env.vec_frame_stack import VecFrameStack", "from baselines.a2c.utils import batch_to_seq, seq_to_batch", "from baselines.a2c.utils import cat_entropy_softmax", "from baselines.a2c.utils import Scheduler, find_trainable_variables", "from baselines.a2c.utils import EpisodeStats", "from baselines.a2c.utils import (", "from baselines.acer.buffer import Buffer", "from baselines.acer.runner import Runner"], "function": ["def strip(var, nenvs, nsteps, flat=False):\n", "class Model(object):\n", "class Acer:\n", "    def __init__(self, runner, model, buffer, log_interval):\n", "    def call(self, on_policy):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/schedules.py", "func_name": "PiecewiseSchedule.value", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "See Schedule.value", "docstring_tokens": ["See", "Schedule", ".", "value"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/schedules.py#L64-L73", "partition": "valid", "up_fun_num": 8, "context": "\"\"\"This file is used for specifying various schedules that evolve over\ntime throughout the execution of the algorithm, such as:\n - learning rate for the optimizer\n - exploration epsilon for the epsilon greedy exploration strategy\n - beta parameter for beta parameter in prioritized replay\n\nEach schedule has a function `value(t)` which returns the current value\nof the parameter given the timestep t of the optimization procedure.\n\"\"\"\n\n\nclass Schedule(object):\n    def value(self, t):\n        \"\"\"Value of the schedule at time t\"\"\"\n        raise NotImplementedError()\n\n\nclass ConstantSchedule(object):\n    def __init__(self, value):\n        \"\"\"Value remains constant over time.\n\n        Parameters\n        ----------\n        value: float\n            Constant value of the schedule\n        \"\"\"\n        self._v = value\n\n    def value(self, t):\n        \"\"\"See Schedule.value\"\"\"\n        return self._v\n\n\ndef linear_interpolation(l, r, alpha):\n    return l + alpha * (r - l)\n\n\nclass PiecewiseSchedule(object):\n    def __init__(\n        self, endpoints, interpolation=linear_interpolation, outside_value=None\n    ):\n        \"\"\"Piecewise schedule.\n\n        endpoints: [(int, int)]\n            list of pairs `(time, value)` meanining that schedule should output\n            `value` when `t==time`. All the values for time must be sorted in\n            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n            time passed between `time_a` and `time_b` for time `t`.\n        interpolation: lambda float, float, float: float\n            a function that takes value to the left and to the right of t according\n            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n            right endpoint that t has covered. See linear_interpolation for example.\n        outside_value: float\n            if the value is requested outside of all the intervals sepecified in\n            `endpoints` this value is returned. If None then AssertionError is\n            raised when outside value is requested.\n        \"\"\"\n        idxes = [e[0] for e in endpoints]\n        assert idxes == sorted(idxes)\n        self._interpolation = interpolation\n        self._outside_value = outside_value\n        self._endpoints = endpoints\n\n\nclass LinearSchedule(object):\n    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n        \"\"\"Linear interpolation between initial_p and final_p over\n        schedule_timesteps. After this many timesteps pass final_p is\n        returned.\n\n        Parameters\n        ----------\n        schedule_timesteps: int\n            Number of timesteps for which to linearly anneal initial_p\n            to final_p\n        initial_p: float\n            initial output value\n        final_p: float\n            final output value\n        \"\"\"\n        self.schedule_timesteps = schedule_timesteps\n        self.final_p = final_p\n        self.initial_p = initial_p\n\n    def value(self, t):\n        \"\"\"See Schedule.value\"\"\"\n        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n        return self.initial_p + fraction * (self.final_p - self.initial_p)\n", "levels": [0, 1, 0, 1, 1, 0, 0, 0, 1, 1], "package": [], "function": ["class Schedule(object):\n", "    def value(self, t):\n", "class ConstantSchedule(object):\n", "    def __init__(self, value):\n", "    def value(self, t):\n", "def linear_interpolation(l, r, alpha):\n", "class PiecewiseSchedule(object):\n", "class LinearSchedule(object):\n", "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n", "    def value(self, t):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/vec_env/shmem_vec_env.py", "func_name": "_subproc_worker", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Control a single environment instance using IPC and\n    shared memory.", "docstring_tokens": ["Control", "a", "single", "environment", "instance", "using", "IPC", "and", "shared", "memory", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/vec_env/shmem_vec_env.py#L105-L139", "partition": "valid", "up_fun_num": 8, "context": "\"\"\"\nAn interface for asynchronous vectorized environments.\n\"\"\"\n\nimport multiprocessing as mp\nimport numpy as np\nfrom .vec_env import VecEnv, CloudpickleWrapper, clear_mpi_env_vars\nimport ctypes\nfrom baselines import logger\n\nfrom .util import dict_to_obs, obs_space_info, obs_to_dict\n\n_NP_TO_CT = {\n    np.float32: ctypes.c_float,\n    np.int32: ctypes.c_int32,\n    np.int8: ctypes.c_int8,\n    np.uint8: ctypes.c_char,\n    np.bool: ctypes.c_bool,\n}\n\n\nclass ShmemVecEnv(VecEnv):\n    \"\"\"\n    Optimized version of SubprocVecEnv that uses shared variables to communicate observations.\n    \"\"\"\n\n    def __init__(self, env_fns, spaces=None, context=\"spawn\"):\n        \"\"\"\n        If you don't specify observation_space, we'll have to create a dummy\n        environment to get it.\n        \"\"\"\n        ctx = mp.get_context(context)\n        if spaces:\n            observation_space, action_space = spaces\n        else:\n            logger.log(\"Creating dummy env object to get spaces\")\n            with logger.scoped_configure(format_strs=[]):\n                dummy = env_fns[0]()\n                observation_space, action_space = (\n                    dummy.observation_space,\n                    dummy.action_space,\n                )\n                dummy.close()\n                del dummy\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n        self.obs_keys, self.obs_shapes, self.obs_dtypes = obs_space_info(\n            observation_space\n        )\n        self.obs_bufs = [\n            {\n                k: ctx.Array(\n                    _NP_TO_CT[self.obs_dtypes[k].type], int(np.prod(self.obs_shapes[k]))\n                )\n                for k in self.obs_keys\n            }\n            for _ in env_fns\n        ]\n        self.parent_pipes = []\n        self.procs = []\n        with clear_mpi_env_vars():\n            for env_fn, obs_buf in zip(env_fns, self.obs_bufs):\n                wrapped_fn = CloudpickleWrapper(env_fn)\n                parent_pipe, child_pipe = ctx.Pipe()\n                proc = ctx.Process(\n                    target=_subproc_worker,\n                    args=(\n                        child_pipe,\n                        parent_pipe,\n                        wrapped_fn,\n                        obs_buf,\n                        self.obs_shapes,\n                        self.obs_dtypes,\n                        self.obs_keys,\n                    ),\n                )\n                proc.daemon = True\n                self.procs.append(proc)\n                self.parent_pipes.append(parent_pipe)\n                proc.start()\n                child_pipe.close()\n        self.waiting_step = False\n        self.viewer = None\n\n    def reset(self):\n        if self.waiting_step:\n            logger.warn(\"Called reset() while waiting for the step to complete\")\n            self.step_wait()\n        for pipe in self.parent_pipes:\n            pipe.send((\"reset\", None))\n        return self._decode_obses([pipe.recv() for pipe in self.parent_pipes])\n\n    def step_async(self, actions):\n        assert len(actions) == len(self.parent_pipes)\n        for pipe, act in zip(self.parent_pipes, actions):\n            pipe.send((\"step\", act))\n\n    def step_wait(self):\n        outs = [pipe.recv() for pipe in self.parent_pipes]\n        obs, rews, dones, infos = zip(*outs)\n        return self._decode_obses(obs), np.array(rews), np.array(dones), infos\n\n    def close_extras(self):\n        if self.waiting_step:\n            self.step_wait()\n        for pipe in self.parent_pipes:\n            pipe.send((\"close\", None))\n        for pipe in self.parent_pipes:\n            pipe.recv()\n            pipe.close()\n        for proc in self.procs:\n            proc.join()\n\n    def get_images(self, mode=\"human\"):\n        for pipe in self.parent_pipes:\n            pipe.send((\"render\", None))\n        return [pipe.recv() for pipe in self.parent_pipes]\n\n    def _decode_obses(self, obs):\n        result = {}\n        for k in self.obs_keys:\n\n            bufs = [b[k] for b in self.obs_bufs]\n            o = [\n                np.frombuffer(b.get_obj(), dtype=self.obs_dtypes[k]).reshape(\n                    self.obs_shapes[k]\n                )\n                for b in bufs\n            ]\n            result[k] = np.array(o)\n        return dict_to_obs(result)\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1], "package": ["import multiprocessing as mp", "import numpy as np", "from .vec_env import VecEnv, CloudpickleWrapper, clear_mpi_env_vars", "import ctypes", "from baselines import logger", "from .util import dict_to_obs, obs_space_info, obs_to_dict"], "function": ["class ShmemVecEnv(VecEnv):\n", "    def __init__(self, env_fns, spaces=None, context=\"spawn\"):\n", "    def reset(self):\n", "    def step_async(self, actions):\n", "    def step_wait(self):\n", "    def close_extras(self):\n", "    def get_images(self, mode=\"human\"):\n", "    def _decode_obses(self, obs):\n"]}
{"repo": "openai/baselines", "path": "baselines/a2c/a2c.py", "func_name": "learn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Main entrypoint for A2C algorithm. Train a policy with given network architecture on a given environment using a2c algorithm.\n\n    Parameters:\n    -----------\n\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n\n\n    env:                RL environment. Should implement interface similar to VecEnv (baselines.common/vec_env) or be wrapped with DummyVecEnv (baselines.common/vec_env/dummy_vec_env.py)\n\n\n    seed:               seed to make random number sequence in the alorightm reproducible. By default is None which means seed from system noise generator (not reproducible)\n\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n                        nenv is number of environment copies simulated in parallel)\n\n    total_timesteps:    int, total number of timesteps to train on (default: 80M)\n\n    vf_coef:            float, coefficient in front of value function loss in the total loss function (default: 0.5)\n\n    ent_coef:           float, coeffictiant in front of the policy entropy in the total loss function (default: 0.01)\n\n    max_gradient_norm:  float, gradient is clipped to have global L2 norm no more than this value (default: 0.5)\n\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n                        returns fraction of the learning rate (specified as lr) as output\n\n    epsilon:            float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n\n    alpha:              float, RMSProp decay parameter (default: 0.99)\n\n    gamma:              float, reward discounting parameter (default: 0.99)\n\n    log_interval:       int, specifies how frequently the logs are printed out (default: 100)\n\n    **network_kwargs:   keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n                        For instance, 'mlp' network architecture has arguments num_hidden and num_layers.", "docstring_tokens": ["Main", "entrypoint", "for", "A2C", "algorithm", ".", "Train", "a", "policy", "with", "given", "network", "architecture", "on", "a", "given", "environment", "using", "a2c", "algorithm", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/a2c/a2c.py#L119-L231", "partition": "valid", "up_fun_num": 3, "context": "import time\nimport functools\nimport tensorflow as tf\n\nfrom baselines import logger\n\nfrom baselines.common import set_global_seeds, explained_variance\nfrom baselines.common import tf_util\nfrom baselines.common.policies import build_policy\n\n\nfrom baselines.a2c.utils import Scheduler, find_trainable_variables\nfrom baselines.a2c.runner import Runner\nfrom baselines.ppo2.ppo2 import safemean\nfrom collections import deque\n\nfrom tensorflow import losses\n\n\nclass Model(object):\n\n    \"\"\"\n    We use this class to :\n        __init__:\n        - Creates the step_model\n        - Creates the train_model\n\n        train():\n        - Make the training part (feedforward and retropropagation of gradients)\n\n        save/load():\n        - Save load the model\n    \"\"\"\n\n    def __init__(\n        self,\n        policy,\n        env,\n        nsteps,\n        ent_coef=0.01,\n        vf_coef=0.5,\n        max_grad_norm=0.5,\n        lr=7e-4,\n        alpha=0.99,\n        epsilon=1e-5,\n        total_timesteps=int(80e6),\n        lrschedule=\"linear\",\n    ):\n\n        sess = tf_util.get_session()\n        nenvs = env.num_envs\n        nbatch = nenvs * nsteps\n\n        with tf.variable_scope(\"a2c_model\", reuse=tf.AUTO_REUSE):\n            # step_model is used for sampling\n            step_model = policy(nenvs, 1, sess)\n\n            # train_model is used to train our network\n            train_model = policy(nbatch, nsteps, sess)\n\n        A = tf.placeholder(train_model.action.dtype, train_model.action.shape)\n        ADV = tf.placeholder(tf.float32, [nbatch])\n        R = tf.placeholder(tf.float32, [nbatch])\n        LR = tf.placeholder(tf.float32, [])\n\n        # Calculate the loss\n        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n\n        # Policy loss\n        neglogpac = train_model.pd.neglogp(A)\n        # L = A(s,a) * -logpi(a|s)\n        pg_loss = tf.reduce_mean(ADV * neglogpac)\n\n        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n        entropy = tf.reduce_mean(train_model.pd.entropy())\n\n        # Value loss\n        vf_loss = losses.mean_squared_error(tf.squeeze(train_model.vf), R)\n\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n\n        # Update parameters using loss\n        # 1. Get the model parameters\n        params = find_trainable_variables(\"a2c_model\")\n\n        # 2. Calculate the gradients\n        grads = tf.gradients(loss, params)\n        if max_grad_norm is not None:\n            # Clip the gradients (normalize)\n            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads = list(zip(grads, params))\n        # zip aggregate each gradient with parameters associated\n        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n\n        # 3. Make op for one policy and value update step of A2C\n        trainer = tf.train.RMSPropOptimizer(\n            learning_rate=LR, decay=alpha, epsilon=epsilon\n        )\n\n        _train = trainer.apply_gradients(grads)\n\n        lr = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)\n\n        def train(obs, states, rewards, masks, actions, values):\n            # Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n            # rewards = R + yV(s')\n            advs = rewards - values\n            for step in range(len(obs)):\n                cur_lr = lr.value()\n\n            td_map = {train_model.X: obs, A: actions, ADV: advs, R: rewards, LR: cur_lr}\n            if states is not None:\n                td_map[train_model.S] = states\n                td_map[train_model.M] = masks\n            policy_loss, value_loss, policy_entropy, _ = sess.run(\n                [pg_loss, vf_loss, entropy, _train], td_map\n            )\n            return policy_loss, value_loss, policy_entropy\n\n        self.train = train\n        self.train_model = train_model\n        self.step_model = step_model\n        self.step = step_model.step\n        self.value = step_model.value\n        self.initial_state = step_model.initial_state\n        self.save = functools.partial(tf_util.save_variables, sess=sess)\n        self.load = functools.partial(tf_util.load_variables, sess=sess)\n        tf.global_variables_initializer().run(session=sess)\n", "levels": [0], "package": ["import time", "import functools", "import tensorflow as tf", "from baselines import logger", "from baselines.common import set_global_seeds, explained_variance", "from baselines.common import tf_util", "from baselines.common.policies import build_policy", "from baselines.a2c.utils import Scheduler, find_trainable_variables", "from baselines.a2c.runner import Runner", "from baselines.ppo2.ppo2 import safemean", "from collections import deque", "from tensorflow import losses"], "function": ["class Model(object):\n"]}
{"repo": "openai/baselines", "path": "baselines/ppo2/runner.py", "func_name": "sf01", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "swap and then flatten axes 0 and 1", "docstring_tokens": ["swap", "and", "then", "flatten", "axes", "0", "and", "1"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/ppo2/runner.py#L69-L74", "partition": "valid", "up_fun_num": 3, "context": "import numpy as np\nfrom baselines.common.runners import AbstractEnvRunner\n\n\nclass Runner(AbstractEnvRunner):\n    \"\"\"\n    We use this object to make a mini batch of experiences\n    __init__:\n    - Initialize the runner\n\n    run():\n    - Make a mini batch\n    \"\"\"\n\n    def __init__(self, *, env, model, nsteps, gamma, lam):\n        super().__init__(env=env, model=model, nsteps=nsteps)\n        # Lambda used in GAE (General Advantage Estimation)\n        self.lam = lam\n        # Discount rate\n        self.gamma = gamma\n\n    def run(self):\n        # Here, we init the lists that will contain the mb of experiences\n        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones, mb_neglogpacs = (\n            [],\n            [],\n            [],\n            [],\n            [],\n            [],\n        )\n        mb_states = self.states\n        epinfos = []\n        # For n in range number of steps\n        for _ in range(self.nsteps):\n            # Given observations, get action value and neglopacs\n            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n            actions, values, self.states, neglogpacs = self.model.step(\n                self.obs, S=self.states, M=self.dones\n            )\n            mb_obs.append(self.obs.copy())\n            mb_actions.append(actions)\n            mb_values.append(values)\n            mb_neglogpacs.append(neglogpacs)\n            mb_dones.append(self.dones)\n\n            # Take actions in env and look the results\n            # Infos contains a ton of useful informations\n            self.obs[:], rewards, self.dones, infos = self.env.step(actions)\n            for info in infos:\n                maybeepinfo = info.get(\"episode\")\n                if maybeepinfo:\n                    epinfos.append(maybeepinfo)\n            mb_rewards.append(rewards)\n        # batch of steps to batch of rollouts\n        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n        mb_actions = np.asarray(mb_actions)\n        mb_values = np.asarray(mb_values, dtype=np.float32)\n        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n        last_values = self.model.value(self.obs, S=self.states, M=self.dones)\n\n        # discount/bootstrap off value fn\n        mb_returns = np.zeros_like(mb_rewards)\n        mb_advs = np.zeros_like(mb_rewards)\n        lastgaelam = 0\n        for t in reversed(range(self.nsteps)):\n            if t == self.nsteps - 1:\n                nextnonterminal = 1.0 - self.dones\n                nextvalues = last_values\n            else:\n                nextnonterminal = 1.0 - mb_dones[t + 1]\n                nextvalues = mb_values[t + 1]\n            delta = (\n                mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n            )\n            mb_advs[t] = lastgaelam = (\n                delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n            )\n        mb_returns = mb_advs + mb_values\n        return (\n            *map(\n                sf01,\n                (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs),\n            ),\n            mb_states,\n            epinfos,\n        )\n\n\n# obs, returns, masks, actions, values, neglogpacs, states = runner.run()\n", "levels": [0, 1, 1], "package": ["import numpy as np", "from baselines.common.runners import AbstractEnvRunner"], "function": ["class Runner(AbstractEnvRunner):\n", "    def __init__(self, *, env, model, nsteps, gamma, lam):\n", "    def run(self):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/misc_util.py", "func_name": "pretty_eta", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.", "docstring_tokens": ["Print", "the", "number", "of", "seconds", "in", "human", "readable", "format", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/misc_util.py#L65-L104", "partition": "valid", "up_fun_num": 8, "context": "import gym\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport tempfile\nimport zipfile\n\n\ndef zipsame(*seqs):\n    L = len(seqs[0])\n    assert all(len(seq) == L for seq in seqs[1:])\n    return zip(*seqs)\n\n\nclass EzPickle(object):\n    \"\"\"Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=\"bushy\"):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {\n            \"_ezpickle_args\": self._ezpickle_args,\n            \"_ezpickle_kwargs\": self._ezpickle_kwargs,\n        }\n\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)\n\n\ndef set_global_seeds(i):\n    try:\n        import MPI\n\n        rank = MPI.COMM_WORLD.Get_rank()\n    except ImportError:\n        rank = 0\n\n    myseed = i + 1000 * rank if i is not None else None\n    try:\n        import tensorflow as tf\n\n        tf.set_random_seed(myseed)\n    except ImportError:\n        pass\n    np.random.seed(myseed)\n    random.seed(myseed)\n\n\nclass RunningAvg(object):\n    def __init__(self, gamma, init_value=None):\n        \"\"\"Keep a running estimate of a quantity. This is a bit like mean\n        but more sensitive to recent changes.\n\n        Parameters\n        ----------\n        gamma: float\n            Must be between 0 and 1, where 0 is the most sensitive to recent\n            changes.\n        init_value: float or None\n            Initial value of the estimate. If None, it will be set on the first update.\n        \"\"\"\n        self._value = init_value\n        self._gamma = gamma\n\n    def update(self, new_val):\n        \"\"\"Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.\n        \"\"\"\n        if self._value is None:\n            self._value = new_val\n        else:\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val\n\n    def __float__(self):\n        \"\"\"Get the current estimate\"\"\"\n        return self._value\n\n\ndef boolean_flag(parser, name, default=False, help=None):\n    \"\"\"Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag\n    \"\"\"\n    dest = name.replace(\"-\", \"_\")\n    parser.add_argument(\n        \"--\" + name, action=\"store_true\", default=default, dest=dest, help=help\n    )\n    parser.add_argument(\"--no-\" + name, action=\"store_false\", dest=dest)\n\n\ndef get_wrapper_by_name(env, classname):\n    \"\"\"Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname\n    \"\"\"\n    currentenv = env\n    while True:\n        if classname == currentenv.class_name():\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(\"Couldn't find wrapper named %s\" % classname)\n\n\ndef relatively_safe_pickle_dump(obj, path, compression=False):\n    \"\"\"This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    \"\"\"\n    temp_storage = path + \".relatively_safe\"\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(\n                temp_storage, \"w\", compression=zipfile.ZIP_DEFLATED\n            ) as myzip:\n                myzip.write(uncompressed_file.name, \"data\")\n    else:\n        with open(temp_storage, \"wb\") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)\n\n\ndef pickle_load(path, compression=False):\n    \"\"\"Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object\n    \"\"\"\n\n    if compression:\n        with zipfile.ZipFile(path, \"r\", compression=zipfile.ZIP_DEFLATED) as myzip:\n            with myzip.open(\"data\") as f:\n                return pickle.load(f)\n    else:\n        with open(path, \"rb\") as f:\n            return pickle.load(f)\n", "levels": [0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0], "package": ["import gym", "import numpy as np", "import os", "import pickle", "import random", "import tempfile", "import zipfile"], "function": ["def zipsame(*seqs):\n", "class EzPickle(object):\n", "    def __init__(self, *args, **kwargs):\n", "    def __getstate__(self):\n", "    def __setstate__(self, d):\n", "def set_global_seeds(i):\n", "class RunningAvg(object):\n", "    def __init__(self, gamma, init_value=None):\n", "    def update(self, new_val):\n", "    def __float__(self):\n", "def boolean_flag(parser, name, default=False, help=None):\n", "def get_wrapper_by_name(env, classname):\n", "def relatively_safe_pickle_dump(obj, path, compression=False):\n", "def pickle_load(path, compression=False):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/misc_util.py", "func_name": "boolean_flag", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag", "docstring_tokens": ["Add", "a", "boolean", "flag", "to", "argparse", "parser", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/misc_util.py#L140-L156", "partition": "valid", "up_fun_num": 14, "context": "import gym\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport tempfile\nimport zipfile\n\n\ndef zipsame(*seqs):\n    L = len(seqs[0])\n    assert all(len(seq) == L for seq in seqs[1:])\n    return zip(*seqs)\n\n\nclass EzPickle(object):\n    \"\"\"Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=\"bushy\"):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {\n            \"_ezpickle_args\": self._ezpickle_args,\n            \"_ezpickle_kwargs\": self._ezpickle_kwargs,\n        }\n\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)\n\n\ndef set_global_seeds(i):\n    try:\n        import MPI\n\n        rank = MPI.COMM_WORLD.Get_rank()\n    except ImportError:\n        rank = 0\n\n    myseed = i + 1000 * rank if i is not None else None\n    try:\n        import tensorflow as tf\n\n        tf.set_random_seed(myseed)\n    except ImportError:\n        pass\n    np.random.seed(myseed)\n    random.seed(myseed)\n\n\ndef pretty_eta(seconds_left):\n    \"\"\"Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.\n    \"\"\"\n    minutes_left = seconds_left // 60\n    seconds_left %= 60\n    hours_left = minutes_left // 60\n    minutes_left %= 60\n    days_left = hours_left // 24\n    hours_left %= 24\n\n    def helper(cnt, name):\n        return \"{} {}{}\".format(str(cnt), name, (\"s\" if cnt > 1 else \"\"))\n\n    if days_left > 0:\n        msg = helper(days_left, \"day\")\n        if hours_left > 0:\n            msg += \" and \" + helper(hours_left, \"hour\")\n        return msg\n    if hours_left > 0:\n        msg = helper(hours_left, \"hour\")\n        if minutes_left > 0:\n            msg += \" and \" + helper(minutes_left, \"minute\")\n        return msg\n    if minutes_left > 0:\n        return helper(minutes_left, \"minute\")\n    return \"less than a minute\"\n\n\nclass RunningAvg(object):\n    def __init__(self, gamma, init_value=None):\n        \"\"\"Keep a running estimate of a quantity. This is a bit like mean\n        but more sensitive to recent changes.\n\n        Parameters\n        ----------\n        gamma: float\n            Must be between 0 and 1, where 0 is the most sensitive to recent\n            changes.\n        init_value: float or None\n            Initial value of the estimate. If None, it will be set on the first update.\n        \"\"\"\n        self._value = init_value\n        self._gamma = gamma\n\n    def update(self, new_val):\n        \"\"\"Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.\n        \"\"\"\n        if self._value is None:\n            self._value = new_val\n        else:\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val\n\n    def __float__(self):\n        \"\"\"Get the current estimate\"\"\"\n        return self._value\n\n\ndef get_wrapper_by_name(env, classname):\n    \"\"\"Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname\n    \"\"\"\n    currentenv = env\n    while True:\n        if classname == currentenv.class_name():\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(\"Couldn't find wrapper named %s\" % classname)\n\n\ndef relatively_safe_pickle_dump(obj, path, compression=False):\n    \"\"\"This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    \"\"\"\n    temp_storage = path + \".relatively_safe\"\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(\n                temp_storage, \"w\", compression=zipfile.ZIP_DEFLATED\n            ) as myzip:\n                myzip.write(uncompressed_file.name, \"data\")\n    else:\n        with open(temp_storage, \"wb\") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)\n\n\ndef pickle_load(path, compression=False):\n    \"\"\"Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object\n    \"\"\"\n\n    if compression:\n        with zipfile.ZipFile(path, \"r\", compression=zipfile.ZIP_DEFLATED) as myzip:\n            with myzip.open(\"data\") as f:\n                return pickle.load(f)\n    else:\n        with open(path, \"rb\") as f:\n            return pickle.load(f)\n", "levels": [0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0], "package": ["import gym", "import numpy as np", "import os", "import pickle", "import random", "import tempfile", "import zipfile"], "function": ["def zipsame(*seqs):\n", "class EzPickle(object):\n", "    def __init__(self, *args, **kwargs):\n", "    def __getstate__(self):\n", "    def __setstate__(self, d):\n", "def set_global_seeds(i):\n", "def pretty_eta(seconds_left):\n", "    def helper(cnt, name):\n", "class RunningAvg(object):\n", "    def __init__(self, gamma, init_value=None):\n", "    def update(self, new_val):\n", "    def __float__(self):\n", "def get_wrapper_by_name(env, classname):\n", "def relatively_safe_pickle_dump(obj, path, compression=False):\n", "def pickle_load(path, compression=False):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/misc_util.py", "func_name": "get_wrapper_by_name", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname", "docstring_tokens": ["Given", "an", "a", "gym", "environment", "possibly", "wrapped", "multiple", "times", "returns", "a", "wrapper", "of", "class", "named", "classname", "or", "raises", "ValueError", "if", "no", "such", "wrapper", "was", "applied"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/misc_util.py#L159-L182", "partition": "valid", "up_fun_num": 15, "context": "import gym\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport tempfile\nimport zipfile\n\n\ndef zipsame(*seqs):\n    L = len(seqs[0])\n    assert all(len(seq) == L for seq in seqs[1:])\n    return zip(*seqs)\n\n\nclass EzPickle(object):\n    \"\"\"Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=\"bushy\"):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {\n            \"_ezpickle_args\": self._ezpickle_args,\n            \"_ezpickle_kwargs\": self._ezpickle_kwargs,\n        }\n\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)\n\n\ndef set_global_seeds(i):\n    try:\n        import MPI\n\n        rank = MPI.COMM_WORLD.Get_rank()\n    except ImportError:\n        rank = 0\n\n    myseed = i + 1000 * rank if i is not None else None\n    try:\n        import tensorflow as tf\n\n        tf.set_random_seed(myseed)\n    except ImportError:\n        pass\n    np.random.seed(myseed)\n    random.seed(myseed)\n\n\ndef pretty_eta(seconds_left):\n    \"\"\"Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.\n    \"\"\"\n    minutes_left = seconds_left // 60\n    seconds_left %= 60\n    hours_left = minutes_left // 60\n    minutes_left %= 60\n    days_left = hours_left // 24\n    hours_left %= 24\n\n    def helper(cnt, name):\n        return \"{} {}{}\".format(str(cnt), name, (\"s\" if cnt > 1 else \"\"))\n\n    if days_left > 0:\n        msg = helper(days_left, \"day\")\n        if hours_left > 0:\n            msg += \" and \" + helper(hours_left, \"hour\")\n        return msg\n    if hours_left > 0:\n        msg = helper(hours_left, \"hour\")\n        if minutes_left > 0:\n            msg += \" and \" + helper(minutes_left, \"minute\")\n        return msg\n    if minutes_left > 0:\n        return helper(minutes_left, \"minute\")\n    return \"less than a minute\"\n\n\nclass RunningAvg(object):\n    def __init__(self, gamma, init_value=None):\n        \"\"\"Keep a running estimate of a quantity. This is a bit like mean\n        but more sensitive to recent changes.\n\n        Parameters\n        ----------\n        gamma: float\n            Must be between 0 and 1, where 0 is the most sensitive to recent\n            changes.\n        init_value: float or None\n            Initial value of the estimate. If None, it will be set on the first update.\n        \"\"\"\n        self._value = init_value\n        self._gamma = gamma\n\n    def update(self, new_val):\n        \"\"\"Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.\n        \"\"\"\n        if self._value is None:\n            self._value = new_val\n        else:\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val\n\n    def __float__(self):\n        \"\"\"Get the current estimate\"\"\"\n        return self._value\n\n\ndef boolean_flag(parser, name, default=False, help=None):\n    \"\"\"Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag\n    \"\"\"\n    dest = name.replace(\"-\", \"_\")\n    parser.add_argument(\n        \"--\" + name, action=\"store_true\", default=default, dest=dest, help=help\n    )\n    parser.add_argument(\"--no-\" + name, action=\"store_false\", dest=dest)\n\n\ndef relatively_safe_pickle_dump(obj, path, compression=False):\n    \"\"\"This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    \"\"\"\n    temp_storage = path + \".relatively_safe\"\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(\n                temp_storage, \"w\", compression=zipfile.ZIP_DEFLATED\n            ) as myzip:\n                myzip.write(uncompressed_file.name, \"data\")\n    else:\n        with open(temp_storage, \"wb\") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)\n\n\ndef pickle_load(path, compression=False):\n    \"\"\"Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object\n    \"\"\"\n\n    if compression:\n        with zipfile.ZipFile(path, \"r\", compression=zipfile.ZIP_DEFLATED) as myzip:\n            with myzip.open(\"data\") as f:\n                return pickle.load(f)\n    else:\n        with open(path, \"rb\") as f:\n            return pickle.load(f)\n", "levels": [0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0], "package": ["import gym", "import numpy as np", "import os", "import pickle", "import random", "import tempfile", "import zipfile"], "function": ["def zipsame(*seqs):\n", "class EzPickle(object):\n", "    def __init__(self, *args, **kwargs):\n", "    def __getstate__(self):\n", "    def __setstate__(self, d):\n", "def set_global_seeds(i):\n", "def pretty_eta(seconds_left):\n", "    def helper(cnt, name):\n", "class RunningAvg(object):\n", "    def __init__(self, gamma, init_value=None):\n", "    def update(self, new_val):\n", "    def __float__(self):\n", "def boolean_flag(parser, name, default=False, help=None):\n", "def relatively_safe_pickle_dump(obj, path, compression=False):\n", "def pickle_load(path, compression=False):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/misc_util.py", "func_name": "pickle_load", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object", "docstring_tokens": ["Unpickle", "a", "possible", "compressed", "pickle", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/misc_util.py#L221-L243", "partition": "valid", "up_fun_num": 17, "context": "import gym\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport tempfile\nimport zipfile\n\n\ndef zipsame(*seqs):\n    L = len(seqs[0])\n    assert all(len(seq) == L for seq in seqs[1:])\n    return zip(*seqs)\n\n\nclass EzPickle(object):\n    \"\"\"Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=\"bushy\"):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {\n            \"_ezpickle_args\": self._ezpickle_args,\n            \"_ezpickle_kwargs\": self._ezpickle_kwargs,\n        }\n\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)\n\n\ndef set_global_seeds(i):\n    try:\n        import MPI\n\n        rank = MPI.COMM_WORLD.Get_rank()\n    except ImportError:\n        rank = 0\n\n    myseed = i + 1000 * rank if i is not None else None\n    try:\n        import tensorflow as tf\n\n        tf.set_random_seed(myseed)\n    except ImportError:\n        pass\n    np.random.seed(myseed)\n    random.seed(myseed)\n\n\ndef pretty_eta(seconds_left):\n    \"\"\"Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.\n    \"\"\"\n    minutes_left = seconds_left // 60\n    seconds_left %= 60\n    hours_left = minutes_left // 60\n    minutes_left %= 60\n    days_left = hours_left // 24\n    hours_left %= 24\n\n    def helper(cnt, name):\n        return \"{} {}{}\".format(str(cnt), name, (\"s\" if cnt > 1 else \"\"))\n\n    if days_left > 0:\n        msg = helper(days_left, \"day\")\n        if hours_left > 0:\n            msg += \" and \" + helper(hours_left, \"hour\")\n        return msg\n    if hours_left > 0:\n        msg = helper(hours_left, \"hour\")\n        if minutes_left > 0:\n            msg += \" and \" + helper(minutes_left, \"minute\")\n        return msg\n    if minutes_left > 0:\n        return helper(minutes_left, \"minute\")\n    return \"less than a minute\"\n\n\nclass RunningAvg(object):\n    def __init__(self, gamma, init_value=None):\n        \"\"\"Keep a running estimate of a quantity. This is a bit like mean\n        but more sensitive to recent changes.\n\n        Parameters\n        ----------\n        gamma: float\n            Must be between 0 and 1, where 0 is the most sensitive to recent\n            changes.\n        init_value: float or None\n            Initial value of the estimate. If None, it will be set on the first update.\n        \"\"\"\n        self._value = init_value\n        self._gamma = gamma\n\n    def update(self, new_val):\n        \"\"\"Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.\n        \"\"\"\n        if self._value is None:\n            self._value = new_val\n        else:\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val\n\n    def __float__(self):\n        \"\"\"Get the current estimate\"\"\"\n        return self._value\n\n\ndef boolean_flag(parser, name, default=False, help=None):\n    \"\"\"Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag\n    \"\"\"\n    dest = name.replace(\"-\", \"_\")\n    parser.add_argument(\n        \"--\" + name, action=\"store_true\", default=default, dest=dest, help=help\n    )\n    parser.add_argument(\"--no-\" + name, action=\"store_false\", dest=dest)\n\n\ndef get_wrapper_by_name(env, classname):\n    \"\"\"Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname\n    \"\"\"\n    currentenv = env\n    while True:\n        if classname == currentenv.class_name():\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(\"Couldn't find wrapper named %s\" % classname)\n\n\ndef relatively_safe_pickle_dump(obj, path, compression=False):\n    \"\"\"This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    \"\"\"\n    temp_storage = path + \".relatively_safe\"\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(\n                temp_storage, \"w\", compression=zipfile.ZIP_DEFLATED\n            ) as myzip:\n                myzip.write(uncompressed_file.name, \"data\")\n    else:\n        with open(temp_storage, \"wb\") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)\n", "levels": [0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0], "package": ["import gym", "import numpy as np", "import os", "import pickle", "import random", "import tempfile", "import zipfile"], "function": ["def zipsame(*seqs):\n", "class EzPickle(object):\n", "    def __init__(self, *args, **kwargs):\n", "    def __getstate__(self):\n", "    def __setstate__(self, d):\n", "def set_global_seeds(i):\n", "def pretty_eta(seconds_left):\n", "    def helper(cnt, name):\n", "class RunningAvg(object):\n", "    def __init__(self, gamma, init_value=None):\n", "    def update(self, new_val):\n", "    def __float__(self):\n", "def boolean_flag(parser, name, default=False, help=None):\n", "def get_wrapper_by_name(env, classname):\n", "def relatively_safe_pickle_dump(obj, path, compression=False):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/misc_util.py", "func_name": "RunningAvg.update", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.", "docstring_tokens": ["Update", "the", "estimate", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/misc_util.py#L123-L134", "partition": "valid", "up_fun_num": 12, "context": "import gym\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport tempfile\nimport zipfile\n\n\ndef zipsame(*seqs):\n    L = len(seqs[0])\n    assert all(len(seq) == L for seq in seqs[1:])\n    return zip(*seqs)\n\n\nclass EzPickle(object):\n    \"\"\"Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=\"bushy\"):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {\n            \"_ezpickle_args\": self._ezpickle_args,\n            \"_ezpickle_kwargs\": self._ezpickle_kwargs,\n        }\n\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)\n\n\ndef set_global_seeds(i):\n    try:\n        import MPI\n\n        rank = MPI.COMM_WORLD.Get_rank()\n    except ImportError:\n        rank = 0\n\n    myseed = i + 1000 * rank if i is not None else None\n    try:\n        import tensorflow as tf\n\n        tf.set_random_seed(myseed)\n    except ImportError:\n        pass\n    np.random.seed(myseed)\n    random.seed(myseed)\n\n\ndef pretty_eta(seconds_left):\n    \"\"\"Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.\n    \"\"\"\n    minutes_left = seconds_left // 60\n    seconds_left %= 60\n    hours_left = minutes_left // 60\n    minutes_left %= 60\n    days_left = hours_left // 24\n    hours_left %= 24\n\n    def helper(cnt, name):\n        return \"{} {}{}\".format(str(cnt), name, (\"s\" if cnt > 1 else \"\"))\n\n    if days_left > 0:\n        msg = helper(days_left, \"day\")\n        if hours_left > 0:\n            msg += \" and \" + helper(hours_left, \"hour\")\n        return msg\n    if hours_left > 0:\n        msg = helper(hours_left, \"hour\")\n        if minutes_left > 0:\n            msg += \" and \" + helper(minutes_left, \"minute\")\n        return msg\n    if minutes_left > 0:\n        return helper(minutes_left, \"minute\")\n    return \"less than a minute\"\n\n\nclass RunningAvg(object):\n    def __init__(self, gamma, init_value=None):\n        \"\"\"Keep a running estimate of a quantity. This is a bit like mean\n        but more sensitive to recent changes.\n\n        Parameters\n        ----------\n        gamma: float\n            Must be between 0 and 1, where 0 is the most sensitive to recent\n            changes.\n        init_value: float or None\n            Initial value of the estimate. If None, it will be set on the first update.\n        \"\"\"\n        self._value = init_value\n        self._gamma = gamma\n\n    def __float__(self):\n        \"\"\"Get the current estimate\"\"\"\n        return self._value\n\n\ndef boolean_flag(parser, name, default=False, help=None):\n    \"\"\"Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag\n    \"\"\"\n    dest = name.replace(\"-\", \"_\")\n    parser.add_argument(\n        \"--\" + name, action=\"store_true\", default=default, dest=dest, help=help\n    )\n    parser.add_argument(\"--no-\" + name, action=\"store_false\", dest=dest)\n\n\ndef get_wrapper_by_name(env, classname):\n    \"\"\"Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname\n    \"\"\"\n    currentenv = env\n    while True:\n        if classname == currentenv.class_name():\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(\"Couldn't find wrapper named %s\" % classname)\n\n\ndef relatively_safe_pickle_dump(obj, path, compression=False):\n    \"\"\"This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It's never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    \"\"\"\n    temp_storage = path + \".relatively_safe\"\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(\n                temp_storage, \"w\", compression=zipfile.ZIP_DEFLATED\n            ) as myzip:\n                myzip.write(uncompressed_file.name, \"data\")\n    else:\n        with open(temp_storage, \"wb\") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)\n\n\ndef pickle_load(path, compression=False):\n    \"\"\"Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object\n    \"\"\"\n\n    if compression:\n        with zipfile.ZipFile(path, \"r\", compression=zipfile.ZIP_DEFLATED) as myzip:\n            with myzip.open(\"data\") as f:\n                return pickle.load(f)\n    else:\n        with open(path, \"rb\") as f:\n            return pickle.load(f)\n", "levels": [0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0], "package": ["import gym", "import numpy as np", "import os", "import pickle", "import random", "import tempfile", "import zipfile"], "function": ["def zipsame(*seqs):\n", "class EzPickle(object):\n", "    def __init__(self, *args, **kwargs):\n", "    def __getstate__(self):\n", "    def __setstate__(self, d):\n", "def set_global_seeds(i):\n", "def pretty_eta(seconds_left):\n", "    def helper(cnt, name):\n", "class RunningAvg(object):\n", "    def __init__(self, gamma, init_value=None):\n", "    def __float__(self):\n", "def boolean_flag(parser, name, default=False, help=None):\n", "def get_wrapper_by_name(env, classname):\n", "def relatively_safe_pickle_dump(obj, path, compression=False):\n", "def pickle_load(path, compression=False):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/util.py", "func_name": "store_args", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Stores provided method args as instance attributes.", "docstring_tokens": ["Stores", "provided", "method", "args", "as", "instance", "attributes", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/util.py#L14-L38", "partition": "valid", "up_fun_num": 0, "context": "import os\nimport subprocess\nimport sys\nimport importlib\nimport inspect\nimport functools\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom baselines.common import tf_util as U\n\n\ndef import_function(spec):\n    \"\"\"Import a function identified by a string like \"pkg.module:fn_name\".\"\"\"\n    mod_name, fn_name = spec.split(\":\")\n    module = importlib.import_module(mod_name)\n    fn = getattr(module, fn_name)\n    return fn\n\n\ndef flatten_grads(var_list, grads):\n    \"\"\"Flattens a variables and their gradients.\"\"\"\n    return tf.concat(\n        [tf.reshape(grad, [U.numel(v)]) for (v, grad) in zip(var_list, grads)], 0\n    )\n\n\ndef nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n    \"\"\"Creates a simple neural network\"\"\"\n    for i, size in enumerate(layers_sizes):\n        activation = tf.nn.relu if i < len(layers_sizes) - 1 else None\n        input = tf.layers.dense(\n            inputs=input,\n            units=size,\n            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n            reuse=reuse,\n            name=name + \"_\" + str(i),\n        )\n        if activation:\n            input = activation(input)\n    if flatten:\n        assert layers_sizes[-1] == 1\n        input = tf.reshape(input, [-1])\n    return input\n\n\ndef install_mpi_excepthook():\n    import sys\n    from mpi4py import MPI\n\n    old_hook = sys.excepthook\n\n    def new_hook(a, b, c):\n        old_hook(a, b, c)\n        sys.stdout.flush()\n        sys.stderr.flush()\n        MPI.COMM_WORLD.Abort()\n\n    sys.excepthook = new_hook\n\n\ndef mpi_fork(n, extra_mpi_args=[]):\n    \"\"\"Re-launches the current script with workers\n    Returns \"parent\" for original parent, \"child\" for MPI children\n    \"\"\"\n    if n <= 1:\n        return \"child\"\n    if os.getenv(\"IN_MPI\") is None:\n        env = os.environ.copy()\n        env.update(MKL_NUM_THREADS=\"1\", OMP_NUM_THREADS=\"1\", IN_MPI=\"1\")\n        # \"-bind-to core\" is crucial for good performance\n        args = [\"mpirun\", \"-np\", str(n)] + extra_mpi_args + [sys.executable]\n\n        args += sys.argv\n        subprocess.check_call(args, env=env)\n        return \"parent\"\n    else:\n        install_mpi_excepthook()\n        return \"child\"\n\n\ndef convert_episode_to_batch_major(episode):\n    \"\"\"Converts an episode to have the batch dimension in the major (first)\n    dimension.\n    \"\"\"\n    episode_batch = {}\n    for key in episode.keys():\n        val = np.array(episode[key]).copy()\n        # make inputs batch-major instead of time-major\n        episode_batch[key] = val.swapaxes(0, 1)\n\n    return episode_batch\n\n\ndef transitions_in_episode_batch(episode_batch):\n    \"\"\"Number of transitions in a given episode batch.\"\"\"\n    shape = episode_batch[\"u\"].shape\n    return shape[0] * shape[1]\n\n\ndef reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)\n", "levels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "package": ["import os", "import subprocess", "import sys", "import importlib", "import inspect", "import functools", "import tensorflow as tf", "import numpy as np", "from baselines.common import tf_util as U", "import sys", "from mpi4py import MPI"], "function": ["def import_function(spec):\n", "def flatten_grads(var_list, grads):\n", "def nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n", "def install_mpi_excepthook():\n", "    def new_hook(a, b, c):\n", "def mpi_fork(n, extra_mpi_args=[]):\n", "def convert_episode_to_batch_major(episode):\n", "def transitions_in_episode_batch(episode_batch):\n", "def reshape_for_broadcasting(source, target):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/util.py", "func_name": "flatten_grads", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Flattens a variables and their gradients.", "docstring_tokens": ["Flattens", "a", "variables", "and", "their", "gradients", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/util.py#L50-L54", "partition": "valid", "up_fun_num": 3, "context": "import os\nimport subprocess\nimport sys\nimport importlib\nimport inspect\nimport functools\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom baselines.common import tf_util as U\n\n\ndef store_args(method):\n    \"\"\"Stores provided method args as instance attributes.\"\"\"\n    argspec = inspect.getfullargspec(method)\n    defaults = {}\n    if argspec.defaults is not None:\n        defaults = dict(zip(argspec.args[-len(argspec.defaults) :], argspec.defaults))\n    if argspec.kwonlydefaults is not None:\n        defaults.update(argspec.kwonlydefaults)\n    arg_names = argspec.args[1:]\n\n    @functools.wraps(method)\n    def wrapper(*positional_args, **keyword_args):\n        self = positional_args[0]\n        # Get default arg values\n        args = defaults.copy()\n        # Add provided arg values\n        for name, value in zip(arg_names, positional_args[1:]):\n            args[name] = value\n        args.update(keyword_args)\n        self.__dict__.update(args)\n        return method(*positional_args, **keyword_args)\n\n    return wrapper\n\n\ndef import_function(spec):\n    \"\"\"Import a function identified by a string like \"pkg.module:fn_name\".\"\"\"\n    mod_name, fn_name = spec.split(\":\")\n    module = importlib.import_module(mod_name)\n    fn = getattr(module, fn_name)\n    return fn\n\n\ndef nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n    \"\"\"Creates a simple neural network\"\"\"\n    for i, size in enumerate(layers_sizes):\n        activation = tf.nn.relu if i < len(layers_sizes) - 1 else None\n        input = tf.layers.dense(\n            inputs=input,\n            units=size,\n            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n            reuse=reuse,\n            name=name + \"_\" + str(i),\n        )\n        if activation:\n            input = activation(input)\n    if flatten:\n        assert layers_sizes[-1] == 1\n        input = tf.reshape(input, [-1])\n    return input\n\n\ndef install_mpi_excepthook():\n    import sys\n    from mpi4py import MPI\n\n    old_hook = sys.excepthook\n\n    def new_hook(a, b, c):\n        old_hook(a, b, c)\n        sys.stdout.flush()\n        sys.stderr.flush()\n        MPI.COMM_WORLD.Abort()\n\n    sys.excepthook = new_hook\n\n\ndef mpi_fork(n, extra_mpi_args=[]):\n    \"\"\"Re-launches the current script with workers\n    Returns \"parent\" for original parent, \"child\" for MPI children\n    \"\"\"\n    if n <= 1:\n        return \"child\"\n    if os.getenv(\"IN_MPI\") is None:\n        env = os.environ.copy()\n        env.update(MKL_NUM_THREADS=\"1\", OMP_NUM_THREADS=\"1\", IN_MPI=\"1\")\n        # \"-bind-to core\" is crucial for good performance\n        args = [\"mpirun\", \"-np\", str(n)] + extra_mpi_args + [sys.executable]\n\n        args += sys.argv\n        subprocess.check_call(args, env=env)\n        return \"parent\"\n    else:\n        install_mpi_excepthook()\n        return \"child\"\n\n\ndef convert_episode_to_batch_major(episode):\n    \"\"\"Converts an episode to have the batch dimension in the major (first)\n    dimension.\n    \"\"\"\n    episode_batch = {}\n    for key in episode.keys():\n        val = np.array(episode[key]).copy()\n        # make inputs batch-major instead of time-major\n        episode_batch[key] = val.swapaxes(0, 1)\n\n    return episode_batch\n\n\ndef transitions_in_episode_batch(episode_batch):\n    \"\"\"Number of transitions in a given episode batch.\"\"\"\n    shape = episode_batch[\"u\"].shape\n    return shape[0] * shape[1]\n\n\ndef reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)\n", "levels": [0, 1, 0, 0, 0, 1, 0, 0, 0, 0], "package": ["import os", "import subprocess", "import sys", "import importlib", "import inspect", "import functools", "import tensorflow as tf", "import numpy as np", "from baselines.common import tf_util as U", "import sys", "from mpi4py import MPI"], "function": ["def store_args(method):\n", "    def wrapper(*positional_args, **keyword_args):\n", "def import_function(spec):\n", "def nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n", "def install_mpi_excepthook():\n", "    def new_hook(a, b, c):\n", "def mpi_fork(n, extra_mpi_args=[]):\n", "def convert_episode_to_batch_major(episode):\n", "def transitions_in_episode_batch(episode_batch):\n", "def reshape_for_broadcasting(source, target):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/util.py", "func_name": "nn", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a simple neural network", "docstring_tokens": ["Creates", "a", "simple", "neural", "network"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/util.py#L57-L72", "partition": "valid", "up_fun_num": 4, "context": "import os\nimport subprocess\nimport sys\nimport importlib\nimport inspect\nimport functools\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom baselines.common import tf_util as U\n\n\ndef store_args(method):\n    \"\"\"Stores provided method args as instance attributes.\"\"\"\n    argspec = inspect.getfullargspec(method)\n    defaults = {}\n    if argspec.defaults is not None:\n        defaults = dict(zip(argspec.args[-len(argspec.defaults) :], argspec.defaults))\n    if argspec.kwonlydefaults is not None:\n        defaults.update(argspec.kwonlydefaults)\n    arg_names = argspec.args[1:]\n\n    @functools.wraps(method)\n    def wrapper(*positional_args, **keyword_args):\n        self = positional_args[0]\n        # Get default arg values\n        args = defaults.copy()\n        # Add provided arg values\n        for name, value in zip(arg_names, positional_args[1:]):\n            args[name] = value\n        args.update(keyword_args)\n        self.__dict__.update(args)\n        return method(*positional_args, **keyword_args)\n\n    return wrapper\n\n\ndef import_function(spec):\n    \"\"\"Import a function identified by a string like \"pkg.module:fn_name\".\"\"\"\n    mod_name, fn_name = spec.split(\":\")\n    module = importlib.import_module(mod_name)\n    fn = getattr(module, fn_name)\n    return fn\n\n\ndef flatten_grads(var_list, grads):\n    \"\"\"Flattens a variables and their gradients.\"\"\"\n    return tf.concat(\n        [tf.reshape(grad, [U.numel(v)]) for (v, grad) in zip(var_list, grads)], 0\n    )\n\n\ndef install_mpi_excepthook():\n    import sys\n    from mpi4py import MPI\n\n    old_hook = sys.excepthook\n\n    def new_hook(a, b, c):\n        old_hook(a, b, c)\n        sys.stdout.flush()\n        sys.stderr.flush()\n        MPI.COMM_WORLD.Abort()\n\n    sys.excepthook = new_hook\n\n\ndef mpi_fork(n, extra_mpi_args=[]):\n    \"\"\"Re-launches the current script with workers\n    Returns \"parent\" for original parent, \"child\" for MPI children\n    \"\"\"\n    if n <= 1:\n        return \"child\"\n    if os.getenv(\"IN_MPI\") is None:\n        env = os.environ.copy()\n        env.update(MKL_NUM_THREADS=\"1\", OMP_NUM_THREADS=\"1\", IN_MPI=\"1\")\n        # \"-bind-to core\" is crucial for good performance\n        args = [\"mpirun\", \"-np\", str(n)] + extra_mpi_args + [sys.executable]\n\n        args += sys.argv\n        subprocess.check_call(args, env=env)\n        return \"parent\"\n    else:\n        install_mpi_excepthook()\n        return \"child\"\n\n\ndef convert_episode_to_batch_major(episode):\n    \"\"\"Converts an episode to have the batch dimension in the major (first)\n    dimension.\n    \"\"\"\n    episode_batch = {}\n    for key in episode.keys():\n        val = np.array(episode[key]).copy()\n        # make inputs batch-major instead of time-major\n        episode_batch[key] = val.swapaxes(0, 1)\n\n    return episode_batch\n\n\ndef transitions_in_episode_batch(episode_batch):\n    \"\"\"Number of transitions in a given episode batch.\"\"\"\n    shape = episode_batch[\"u\"].shape\n    return shape[0] * shape[1]\n\n\ndef reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)\n", "levels": [0, 1, 0, 0, 0, 1, 0, 0, 0, 0], "package": ["import os", "import subprocess", "import sys", "import importlib", "import inspect", "import functools", "import tensorflow as tf", "import numpy as np", "from baselines.common import tf_util as U", "import sys", "from mpi4py import MPI"], "function": ["def store_args(method):\n", "    def wrapper(*positional_args, **keyword_args):\n", "def import_function(spec):\n", "def flatten_grads(var_list, grads):\n", "def install_mpi_excepthook():\n", "    def new_hook(a, b, c):\n", "def mpi_fork(n, extra_mpi_args=[]):\n", "def convert_episode_to_batch_major(episode):\n", "def transitions_in_episode_batch(episode_batch):\n", "def reshape_for_broadcasting(source, target):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/util.py", "func_name": "mpi_fork", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Re-launches the current script with workers\n    Returns \"parent\" for original parent, \"child\" for MPI children", "docstring_tokens": ["Re", "-", "launches", "the", "current", "script", "with", "workers", "Returns", "parent", "for", "original", "parent", "child", "for", "MPI", "children"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/util.py#L88-L111", "partition": "valid", "up_fun_num": 7, "context": "import os\nimport subprocess\nimport sys\nimport importlib\nimport inspect\nimport functools\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom baselines.common import tf_util as U\n\n\ndef store_args(method):\n    \"\"\"Stores provided method args as instance attributes.\"\"\"\n    argspec = inspect.getfullargspec(method)\n    defaults = {}\n    if argspec.defaults is not None:\n        defaults = dict(zip(argspec.args[-len(argspec.defaults) :], argspec.defaults))\n    if argspec.kwonlydefaults is not None:\n        defaults.update(argspec.kwonlydefaults)\n    arg_names = argspec.args[1:]\n\n    @functools.wraps(method)\n    def wrapper(*positional_args, **keyword_args):\n        self = positional_args[0]\n        # Get default arg values\n        args = defaults.copy()\n        # Add provided arg values\n        for name, value in zip(arg_names, positional_args[1:]):\n            args[name] = value\n        args.update(keyword_args)\n        self.__dict__.update(args)\n        return method(*positional_args, **keyword_args)\n\n    return wrapper\n\n\ndef import_function(spec):\n    \"\"\"Import a function identified by a string like \"pkg.module:fn_name\".\"\"\"\n    mod_name, fn_name = spec.split(\":\")\n    module = importlib.import_module(mod_name)\n    fn = getattr(module, fn_name)\n    return fn\n\n\ndef flatten_grads(var_list, grads):\n    \"\"\"Flattens a variables and their gradients.\"\"\"\n    return tf.concat(\n        [tf.reshape(grad, [U.numel(v)]) for (v, grad) in zip(var_list, grads)], 0\n    )\n\n\ndef nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n    \"\"\"Creates a simple neural network\"\"\"\n    for i, size in enumerate(layers_sizes):\n        activation = tf.nn.relu if i < len(layers_sizes) - 1 else None\n        input = tf.layers.dense(\n            inputs=input,\n            units=size,\n            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n            reuse=reuse,\n            name=name + \"_\" + str(i),\n        )\n        if activation:\n            input = activation(input)\n    if flatten:\n        assert layers_sizes[-1] == 1\n        input = tf.reshape(input, [-1])\n    return input\n\n\ndef install_mpi_excepthook():\n    import sys\n    from mpi4py import MPI\n\n    old_hook = sys.excepthook\n\n    def new_hook(a, b, c):\n        old_hook(a, b, c)\n        sys.stdout.flush()\n        sys.stderr.flush()\n        MPI.COMM_WORLD.Abort()\n\n    sys.excepthook = new_hook\n\n\ndef convert_episode_to_batch_major(episode):\n    \"\"\"Converts an episode to have the batch dimension in the major (first)\n    dimension.\n    \"\"\"\n    episode_batch = {}\n    for key in episode.keys():\n        val = np.array(episode[key]).copy()\n        # make inputs batch-major instead of time-major\n        episode_batch[key] = val.swapaxes(0, 1)\n\n    return episode_batch\n\n\ndef transitions_in_episode_batch(episode_batch):\n    \"\"\"Number of transitions in a given episode batch.\"\"\"\n    shape = episode_batch[\"u\"].shape\n    return shape[0] * shape[1]\n\n\ndef reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)\n", "levels": [0, 1, 0, 0, 0, 0, 1, 0, 0, 0], "package": ["import os", "import subprocess", "import sys", "import importlib", "import inspect", "import functools", "import tensorflow as tf", "import numpy as np", "from baselines.common import tf_util as U", "import sys", "from mpi4py import MPI"], "function": ["def store_args(method):\n", "    def wrapper(*positional_args, **keyword_args):\n", "def import_function(spec):\n", "def flatten_grads(var_list, grads):\n", "def nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\n", "def install_mpi_excepthook():\n", "    def new_hook(a, b, c):\n", "def convert_episode_to_batch_major(episode):\n", "def transitions_in_episode_batch(episode_batch):\n", "def reshape_for_broadcasting(source, target):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/tf_util.py", "func_name": "get_session", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Get default session or create one with a given config", "docstring_tokens": ["Get", "default", "session", "or", "create", "one", "with", "a", "given", "config"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/tf_util.py#L51-L56", "partition": "valid", "up_fun_num": 3, "context": "import numpy as np\nimport tensorflow as tf  # pylint: ignore-module\nimport copy\nimport os\nimport functools\nimport collections\nimport multiprocessing\n\n\ndef switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value (int or bool).\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor.\n        then_expression: TensorFlow operation.\n        else_expression: TensorFlow operation.\n    \"\"\"\n    x_shape = copy.copy(then_expression.get_shape())\n    x = tf.cond(\n        tf.cast(condition, \"bool\"), lambda: then_expression, lambda: else_expression\n    )\n    x.set_shape(x_shape)\n    return x\n\n\n# ================================================================\n# Extras\n# ================================================================\n\n\ndef lrelu(x, leak=0.2):\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * x + f2 * abs(x)\n\n\n# ================================================================\n# Mathematical utils\n# ================================================================\n\n\ndef huber_loss(x, delta=1.0):\n    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n    return tf.where(\n        tf.abs(x) < delta, tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta)\n    )\n\n\n# ================================================================\n# Global session\n# ================================================================\n\n\ndef make_session(config=None, num_cpu=None, make_default=False, graph=None):\n    \"\"\"Returns a session that will use <num_cpu> CPU's only\"\"\"\n    if num_cpu is None:\n        num_cpu = int(os.getenv(\"RCALL_NUM_CPU\", multiprocessing.cpu_count()))\n    if config is None:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=num_cpu,\n            intra_op_parallelism_threads=num_cpu,\n        )\n        config.gpu_options.allow_growth = True\n\n    if make_default:\n        return tf.InteractiveSession(config=config, graph=graph)\n    else:\n        return tf.Session(config=config, graph=graph)\n\n\ndef single_threaded_session():\n    \"\"\"Returns a session which will only use a single CPU\"\"\"\n    return make_session(num_cpu=1)\n\n\ndef in_session(f):\n    @functools.wraps(f)\n    def newfunc(*args, **kwargs):\n        with tf.Session():\n            f(*args, **kwargs)\n\n    return newfunc\n\n\nALREADY_INITIALIZED = set()\n\n\ndef initialize():\n    \"\"\"Initialize all the uninitialized variables in the global scope.\"\"\"\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n    get_session().run(tf.variables_initializer(new_variables))\n    ALREADY_INITIALIZED.update(new_variables)\n\n\n# ================================================================\n# Model components\n# ================================================================\n\n\ndef normc_initializer(std=1.0, axis=0):\n    def _initializer(shape, dtype=None, partition_info=None):  # pylint: disable=W0613\n        out = np.random.randn(*shape).astype(dtype.as_numpy_dtype)\n        out *= std / np.sqrt(np.square(out).sum(axis=axis, keepdims=True))\n        return tf.constant(out)\n\n    return _initializer\n\n\ndef conv2d(\n    x,\n    num_filters,\n    name,\n    filter_size=(3, 3),\n    stride=(1, 1),\n    pad=\"SAME\",\n    dtype=tf.float32,\n    collections=None,\n    summary_tag=None,\n):\n    with tf.variable_scope(name):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [\n            filter_size[0],\n            filter_size[1],\n            int(x.get_shape()[3]),\n            num_filters,\n        ]\n\n        # there are \"num input feature maps * filter height * filter width\"\n        # inputs to each hidden unit\n        fan_in = intprod(filter_shape[:3])\n        # each unit in the lower layer receives a gradient from:\n        # \"num output feature maps * filter height * filter width\" /\n        #   pooling size\n        fan_out = intprod(filter_shape[:2]) * num_filters\n        # initialize weights with random weights\n        w_bound = np.sqrt(6.0 / (fan_in + fan_out))\n\n        w = tf.get_variable(\n            \"W\",\n            filter_shape,\n            dtype,\n            tf.random_uniform_initializer(-w_bound, w_bound),\n            collections=collections,\n        )\n        b = tf.get_variable(\n            \"b\",\n            [1, 1, 1, num_filters],\n            initializer=tf.zeros_initializer(),\n            collections=collections,\n        )\n\n        if summary_tag is not None:\n            tf.summary.image(\n                summary_tag,\n                tf.transpose(\n                    tf.reshape(w, [filter_size[0], filter_size[1], -1, 1]), [2, 0, 1, 3]\n                ),\n                max_images=10,\n            )\n\n        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n\n\n# ================================================================\n# Theano-like Function\n# ================================================================\n\n\ndef function(inputs, outputs, updates=None, givens=None):\n    \"\"\"Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input's placeholders and produces the values of the expressions\n    in outputs.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n        x = tf.placeholder(tf.int32, (), name=\"x\")\n        y = tf.placeholder(tf.int32, (), name=\"y\")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(x=3) == 9\n            assert lin(2, 2) == 10\n            assert lin(x=2, y=3) == 12\n\n    Parameters\n    ----------\n    inputs: [tf.placeholder, tf.constant, or object with make_feed_dict method]\n        list of input arguments\n    outputs: [tf.Variable] or tf.Variable\n        list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    updates: [tf.Operation] or tf.Operation\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored.\n\n    \"\"\"\n    if isinstance(outputs, list):\n        return _Function(inputs, outputs, updates, givens=givens)\n    elif isinstance(outputs, (dict, collections.OrderedDict)):\n        f = _Function(inputs, outputs.values(), updates, givens=givens)\n        return lambda *args, **kwargs: type(outputs)(\n            zip(outputs.keys(), f(*args, **kwargs))\n        )\n    else:\n        f = _Function(inputs, [outputs], updates, givens=givens)\n        return lambda *args, **kwargs: f(*args, **kwargs)[0]\n\n\nclass _Function(object):\n    def __init__(self, inputs, outputs, updates, givens):\n        for inpt in inputs:\n            if not hasattr(inpt, \"make_feed_dict\") and not (\n                type(inpt) is tf.Tensor and len(inpt.op.inputs) == 0\n            ):\n                assert (\n                    False\n                ), \"inputs should all be placeholders, constants, or have a make_feed_dict method\"\n        self.inputs = inputs\n        self.input_names = {\n            inp.name.split(\"/\")[-1].split(\":\")[0]: inp for inp in inputs\n        }\n        updates = updates or []\n        self.update_group = tf.group(*updates)\n        self.outputs_update = list(outputs) + [self.update_group]\n        self.givens = {} if givens is None else givens\n\n    def _feed_input(self, feed_dict, inpt, value):\n        if hasattr(inpt, \"make_feed_dict\"):\n            feed_dict.update(inpt.make_feed_dict(value))\n        else:\n            feed_dict[inpt] = adjust_shape(inpt, value)\n\n    def __call__(self, *args, **kwargs):\n        assert len(args) + len(kwargs) <= len(\n            self.inputs\n        ), \"Too many arguments provided\"\n        feed_dict = {}\n        # Update feed dict with givens.\n        for inpt in self.givens:\n            feed_dict[inpt] = adjust_shape(inpt, feed_dict.get(inpt, self.givens[inpt]))\n        # Update the args\n        for inpt, value in zip(self.inputs, args):\n            self._feed_input(feed_dict, inpt, value)\n        for inpt_name, value in kwargs.items():\n            self._feed_input(feed_dict, self.input_names[inpt_name], value)\n        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n        return results\n\n\n# ================================================================\n# Flat vectors\n# ================================================================\n\n\ndef var_shape(x):\n    out = x.get_shape().as_list()\n    assert all(\n        isinstance(a, int) for a in out\n    ), \"shape function assumes that shape is fully known\"\n    return out\n\n\ndef numel(x):\n    return intprod(var_shape(x))\n\n\ndef intprod(x):\n    return int(np.prod(x))\n\n\ndef flatgrad(loss, var_list, clip_norm=None):\n    grads = tf.gradients(loss, var_list)\n    if clip_norm is not None:\n        grads = [tf.clip_by_norm(grad, clip_norm=clip_norm) for grad in grads]\n    return tf.concat(\n        axis=0,\n        values=[\n            tf.reshape(grad if grad is not None else tf.zeros_like(v), [numel(v)])\n            for (v, grad) in zip(var_list, grads)\n        ],\n    )\n\n\nclass SetFromFlat(object):\n    def __init__(self, var_list, dtype=tf.float32):\n        assigns = []\n        shapes = list(map(var_shape, var_list))\n        total_size = np.sum([intprod(shape) for shape in shapes])\n\n        self.theta = theta = tf.placeholder(dtype, [total_size])\n        start = 0\n        assigns = []\n        for (shape, v) in zip(shapes, var_list):\n            size = intprod(shape)\n            assigns.append(tf.assign(v, tf.reshape(theta[start : start + size], shape)))\n            start += size\n        self.op = tf.group(*assigns)\n\n    def __call__(self, theta):\n        tf.get_default_session().run(self.op, feed_dict={self.theta: theta})\n\n\nclass GetFlat(object):\n    def __init__(self, var_list):\n        self.op = tf.concat(\n            axis=0, values=[tf.reshape(v, [numel(v)]) for v in var_list]\n        )\n\n    def __call__(self):\n        return tf.get_default_session().run(self.op)\n\n\ndef flattenallbut0(x):\n    return tf.reshape(x, [-1, intprod(x.get_shape().as_list()[1:])])\n\n\n# =============================================================\n# TF placeholders management\n# ============================================================\n\n_PLACEHOLDER_CACHE = {}  # name -> (placeholder, dtype, shape)\n\n\ndef get_placeholder(name, dtype, shape):\n    if name in _PLACEHOLDER_CACHE:\n        out, dtype1, shape1 = _PLACEHOLDER_CACHE[name]\n        if out.graph == tf.get_default_graph():\n            assert (\n                dtype1 == dtype and shape1 == shape\n            ), \"Placeholder with name {} has already been registered and has shape {}, different from requested {}\".format(\n                name, shape1, shape\n            )\n            return out\n\n    out = tf.placeholder(dtype=dtype, shape=shape, name=name)\n    _PLACEHOLDER_CACHE[name] = (out, dtype, shape)\n    return out\n\n\ndef get_placeholder_cached(name):\n    return _PLACEHOLDER_CACHE[name][0]\n\n\n# ================================================================\n# Diagnostics\n# ================================================================\n\n\ndef display_var_info(vars):\n    from baselines import logger\n\n    count_params = 0\n    for v in vars:\n        name = v.name\n        if \"/Adam\" in name or \"beta1_power\" in name or \"beta2_power\" in name:\n            continue\n        v_params = np.prod(v.shape.as_list())\n        count_params += v_params\n        if \"/b:\" in name or \"/bias\" in name:\n            continue  # Wx+b, bias is not interesting to look at => count params, but not print\n        logger.info(\n            \"   %s%s %i params %s\"\n            % (name, \" \" * (55 - len(name)), v_params, str(v.shape))\n        )\n\n    logger.info(\"Total model parameters: %0.2f million\" % (count_params * 1e-6))\n\n\ndef get_available_gpus(session_config=None):\n    # based on recipe from https://stackoverflow.com/a/38580201\n\n    # Unless we allocate a session here, subsequent attempts to create one\n    # will ignore our custom config (in particular, allow_growth=True will have\n    # no effect).\n    if session_config is None:\n        session_config = get_session()._config\n\n    from tensorflow.python.client import device_lib\n\n    local_device_protos = device_lib.list_local_devices(session_config)\n    return [x.name for x in local_device_protos if x.device_type == \"GPU\"]\n\n\n# ================================================================\n# Saving variables\n# ================================================================\n\n\ndef load_state(fname, sess=None):\n    from baselines import logger\n\n    logger.warn(\"load_state method is deprecated, please use load_variables instead\")\n    sess = sess or get_session()\n    saver = tf.train.Saver()\n    saver.restore(tf.get_default_session(), fname)\n\n\ndef save_state(fname, sess=None):\n    from baselines import logger\n\n    logger.warn(\"save_state method is deprecated, please use save_variables instead\")\n    sess = sess or get_session()\n    dirname = os.path.dirname(fname)\n    if any(dirname):\n        os.makedirs(dirname, exist_ok=True)\n    saver = tf.train.Saver()\n    saver.save(tf.get_default_session(), fname)\n\n\n# The methods above and below are clearly doing the same thing, and in a rather similar way\n# TODO: ensure there is no subtle differences and remove one\n\n\ndef save_variables(save_path, variables=None, sess=None):\n    import joblib\n\n    sess = sess or get_session()\n    variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    ps = sess.run(variables)\n    save_dict = {v.name: value for v, value in zip(variables, ps)}\n    dirname = os.path.dirname(save_path)\n    if any(dirname):\n        os.makedirs(dirname, exist_ok=True)\n    joblib.dump(save_dict, save_path)\n\n\ndef load_variables(load_path, variables=None, sess=None):\n    import joblib\n\n    sess = sess or get_session()\n    variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    loaded_params = joblib.load(os.path.expanduser(load_path))\n    restores = []\n    if isinstance(loaded_params, list):\n        assert len(loaded_params) == len(\n            variables\n        ), \"number of variables loaded mismatches len(variables)\"\n        for d, v in zip(loaded_params, variables):\n            restores.append(v.assign(d))\n    else:\n        for v in variables:\n            restores.append(v.assign(loaded_params[v.name]))\n\n    sess.run(restores)\n\n\n# ================================================================\n# Shape adjustment for feeding into tf placeholders\n# ================================================================\ndef adjust_shape(placeholder, data):\n    \"\"\"\n    adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters:\n        placeholder     tensorflow input placeholder\n\n        data            input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns:\n        reshaped data\n    \"\"\"\n\n    if not isinstance(data, np.ndarray) and not isinstance(data, list):\n        return data\n    if isinstance(data, list):\n        data = np.array(data)\n\n    placeholder_shape = [x or -1 for x in placeholder.shape.as_list()]\n\n    assert _check_shape(\n        placeholder_shape, data.shape\n    ), \"Shape of data {} is not compatible with shape of the placeholder {}\".format(\n        data.shape, placeholder_shape\n    )\n\n    return np.reshape(data, placeholder_shape)\n\n\ndef _check_shape(placeholder_shape, data_shape):\n    \"\"\"check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)\"\"\"\n\n    return True\n    squeezed_placeholder_shape = _squeeze_shape(placeholder_shape)\n    squeezed_data_shape = _squeeze_shape(data_shape)\n\n    for i, s_data in enumerate(squeezed_data_shape):\n        s_placeholder = squeezed_placeholder_shape[i]\n        if s_placeholder != -1 and s_data != s_placeholder:\n            return False\n\n    return True\n\n\ndef _squeeze_shape(shape):\n    return [x for x in shape if x != 1]\n\n\n# ================================================================\n# Tensorboard interfacing\n# ================================================================\n\n\ndef launch_tensorboard_in_background(log_dir):\n    \"\"\"\n    To log the Tensorflow graph when using rl-algs\n    algorithms, you can run the following code\n    in your main script:\n        import threading, time\n        def start_tensorboard(session):\n            time.sleep(10) # Wait until graph is setup\n            tb_path = osp.join(logger.get_dir(), 'tb')\n            summary_writer = tf.summary.FileWriter(tb_path, graph=session.graph)\n            summary_op = tf.summary.merge_all()\n            launch_tensorboard_in_background(tb_path)\n        session = tf.get_default_session()\n        t = threading.Thread(target=start_tensorboard, args=([session]))\n        t.start()\n    \"\"\"\n    import subprocess\n\n    subprocess.Popen([\"tensorboard\", \"--logdir\", log_dir])\n", "levels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import numpy as np", "import copy", "import os", "import functools", "import collections", "import multiprocessing"], "function": ["def switch(condition, then_expression, else_expression):\n", "def lrelu(x, leak=0.2):\n", "def huber_loss(x, delta=1.0):\n", "def make_session(config=None, num_cpu=None, make_default=False, graph=None):\n", "def single_threaded_session():\n", "def in_session(f):\n", "    def newfunc(*args, **kwargs):\n", "def initialize():\n", "def normc_initializer(std=1.0, axis=0):\n", "def function(inputs, outputs, updates=None, givens=None):\n", "class _Function(object):\n", "    def __init__(self, inputs, outputs, updates, givens):\n", "    def _feed_input(self, feed_dict, inpt, value):\n", "    def __call__(self, *args, **kwargs):\n", "def var_shape(x):\n", "def numel(x):\n", "def intprod(x):\n", "def flatgrad(loss, var_list, clip_norm=None):\n", "class SetFromFlat(object):\n", "    def __init__(self, var_list, dtype=tf.float32):\n", "    def __call__(self, theta):\n", "class GetFlat(object):\n", "    def __init__(self, var_list):\n", "    def __call__(self):\n", "def flattenallbut0(x):\n", "def get_placeholder(name, dtype, shape):\n", "def get_placeholder_cached(name):\n", "def display_var_info(vars):\n", "def get_available_gpus(session_config=None):\n", "def load_state(fname, sess=None):\n", "def save_state(fname, sess=None):\n", "def save_variables(save_path, variables=None, sess=None):\n", "def load_variables(load_path, variables=None, sess=None):\n", "def adjust_shape(placeholder, data):\n", "def _check_shape(placeholder_shape, data_shape):\n", "def _squeeze_shape(shape):\n", "def launch_tensorboard_in_background(log_dir):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/tf_util.py", "func_name": "initialize", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Initialize all the uninitialized variables in the global scope.", "docstring_tokens": ["Initialize", "all", "the", "uninitialized", "variables", "in", "the", "global", "scope", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/tf_util.py#L87-L91", "partition": "valid", "up_fun_num": 8, "context": "import numpy as np\nimport tensorflow as tf  # pylint: ignore-module\nimport copy\nimport os\nimport functools\nimport collections\nimport multiprocessing\n\n\ndef switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value (int or bool).\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor.\n        then_expression: TensorFlow operation.\n        else_expression: TensorFlow operation.\n    \"\"\"\n    x_shape = copy.copy(then_expression.get_shape())\n    x = tf.cond(\n        tf.cast(condition, \"bool\"), lambda: then_expression, lambda: else_expression\n    )\n    x.set_shape(x_shape)\n    return x\n\n\n# ================================================================\n# Extras\n# ================================================================\n\n\ndef lrelu(x, leak=0.2):\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * x + f2 * abs(x)\n\n\n# ================================================================\n# Mathematical utils\n# ================================================================\n\n\ndef huber_loss(x, delta=1.0):\n    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n    return tf.where(\n        tf.abs(x) < delta, tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta)\n    )\n\n\n# ================================================================\n# Global session\n# ================================================================\n\n\ndef get_session(config=None):\n    \"\"\"Get default session or create one with a given config\"\"\"\n    sess = tf.get_default_session()\n    if sess is None:\n        sess = make_session(config=config, make_default=True)\n    return sess\n\n\ndef make_session(config=None, num_cpu=None, make_default=False, graph=None):\n    \"\"\"Returns a session that will use <num_cpu> CPU's only\"\"\"\n    if num_cpu is None:\n        num_cpu = int(os.getenv(\"RCALL_NUM_CPU\", multiprocessing.cpu_count()))\n    if config is None:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=num_cpu,\n            intra_op_parallelism_threads=num_cpu,\n        )\n        config.gpu_options.allow_growth = True\n\n    if make_default:\n        return tf.InteractiveSession(config=config, graph=graph)\n    else:\n        return tf.Session(config=config, graph=graph)\n\n\ndef single_threaded_session():\n    \"\"\"Returns a session which will only use a single CPU\"\"\"\n    return make_session(num_cpu=1)\n\n\ndef in_session(f):\n    @functools.wraps(f)\n    def newfunc(*args, **kwargs):\n        with tf.Session():\n            f(*args, **kwargs)\n\n    return newfunc\n\n\nALREADY_INITIALIZED = set()\n\n\n# ================================================================\n# Model components\n# ================================================================\n\n\ndef normc_initializer(std=1.0, axis=0):\n    def _initializer(shape, dtype=None, partition_info=None):  # pylint: disable=W0613\n        out = np.random.randn(*shape).astype(dtype.as_numpy_dtype)\n        out *= std / np.sqrt(np.square(out).sum(axis=axis, keepdims=True))\n        return tf.constant(out)\n\n    return _initializer\n\n\ndef conv2d(\n    x,\n    num_filters,\n    name,\n    filter_size=(3, 3),\n    stride=(1, 1),\n    pad=\"SAME\",\n    dtype=tf.float32,\n    collections=None,\n    summary_tag=None,\n):\n    with tf.variable_scope(name):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [\n            filter_size[0],\n            filter_size[1],\n            int(x.get_shape()[3]),\n            num_filters,\n        ]\n\n        # there are \"num input feature maps * filter height * filter width\"\n        # inputs to each hidden unit\n        fan_in = intprod(filter_shape[:3])\n        # each unit in the lower layer receives a gradient from:\n        # \"num output feature maps * filter height * filter width\" /\n        #   pooling size\n        fan_out = intprod(filter_shape[:2]) * num_filters\n        # initialize weights with random weights\n        w_bound = np.sqrt(6.0 / (fan_in + fan_out))\n\n        w = tf.get_variable(\n            \"W\",\n            filter_shape,\n            dtype,\n            tf.random_uniform_initializer(-w_bound, w_bound),\n            collections=collections,\n        )\n        b = tf.get_variable(\n            \"b\",\n            [1, 1, 1, num_filters],\n            initializer=tf.zeros_initializer(),\n            collections=collections,\n        )\n\n        if summary_tag is not None:\n            tf.summary.image(\n                summary_tag,\n                tf.transpose(\n                    tf.reshape(w, [filter_size[0], filter_size[1], -1, 1]), [2, 0, 1, 3]\n                ),\n                max_images=10,\n            )\n\n        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n\n\n# ================================================================\n# Theano-like Function\n# ================================================================\n\n\ndef function(inputs, outputs, updates=None, givens=None):\n    \"\"\"Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input's placeholders and produces the values of the expressions\n    in outputs.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n        x = tf.placeholder(tf.int32, (), name=\"x\")\n        y = tf.placeholder(tf.int32, (), name=\"y\")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(x=3) == 9\n            assert lin(2, 2) == 10\n            assert lin(x=2, y=3) == 12\n\n    Parameters\n    ----------\n    inputs: [tf.placeholder, tf.constant, or object with make_feed_dict method]\n        list of input arguments\n    outputs: [tf.Variable] or tf.Variable\n        list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    updates: [tf.Operation] or tf.Operation\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored.\n\n    \"\"\"\n    if isinstance(outputs, list):\n        return _Function(inputs, outputs, updates, givens=givens)\n    elif isinstance(outputs, (dict, collections.OrderedDict)):\n        f = _Function(inputs, outputs.values(), updates, givens=givens)\n        return lambda *args, **kwargs: type(outputs)(\n            zip(outputs.keys(), f(*args, **kwargs))\n        )\n    else:\n        f = _Function(inputs, [outputs], updates, givens=givens)\n        return lambda *args, **kwargs: f(*args, **kwargs)[0]\n\n\nclass _Function(object):\n    def __init__(self, inputs, outputs, updates, givens):\n        for inpt in inputs:\n            if not hasattr(inpt, \"make_feed_dict\") and not (\n                type(inpt) is tf.Tensor and len(inpt.op.inputs) == 0\n            ):\n                assert (\n                    False\n                ), \"inputs should all be placeholders, constants, or have a make_feed_dict method\"\n        self.inputs = inputs\n        self.input_names = {\n            inp.name.split(\"/\")[-1].split(\":\")[0]: inp for inp in inputs\n        }\n        updates = updates or []\n        self.update_group = tf.group(*updates)\n        self.outputs_update = list(outputs) + [self.update_group]\n        self.givens = {} if givens is None else givens\n\n    def _feed_input(self, feed_dict, inpt, value):\n        if hasattr(inpt, \"make_feed_dict\"):\n            feed_dict.update(inpt.make_feed_dict(value))\n        else:\n            feed_dict[inpt] = adjust_shape(inpt, value)\n\n    def __call__(self, *args, **kwargs):\n        assert len(args) + len(kwargs) <= len(\n            self.inputs\n        ), \"Too many arguments provided\"\n        feed_dict = {}\n        # Update feed dict with givens.\n        for inpt in self.givens:\n            feed_dict[inpt] = adjust_shape(inpt, feed_dict.get(inpt, self.givens[inpt]))\n        # Update the args\n        for inpt, value in zip(self.inputs, args):\n            self._feed_input(feed_dict, inpt, value)\n        for inpt_name, value in kwargs.items():\n            self._feed_input(feed_dict, self.input_names[inpt_name], value)\n        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n        return results\n\n\n# ================================================================\n# Flat vectors\n# ================================================================\n\n\ndef var_shape(x):\n    out = x.get_shape().as_list()\n    assert all(\n        isinstance(a, int) for a in out\n    ), \"shape function assumes that shape is fully known\"\n    return out\n\n\ndef numel(x):\n    return intprod(var_shape(x))\n\n\ndef intprod(x):\n    return int(np.prod(x))\n\n\ndef flatgrad(loss, var_list, clip_norm=None):\n    grads = tf.gradients(loss, var_list)\n    if clip_norm is not None:\n        grads = [tf.clip_by_norm(grad, clip_norm=clip_norm) for grad in grads]\n    return tf.concat(\n        axis=0,\n        values=[\n            tf.reshape(grad if grad is not None else tf.zeros_like(v), [numel(v)])\n            for (v, grad) in zip(var_list, grads)\n        ],\n    )\n\n\nclass SetFromFlat(object):\n    def __init__(self, var_list, dtype=tf.float32):\n        assigns = []\n        shapes = list(map(var_shape, var_list))\n        total_size = np.sum([intprod(shape) for shape in shapes])\n\n        self.theta = theta = tf.placeholder(dtype, [total_size])\n        start = 0\n        assigns = []\n        for (shape, v) in zip(shapes, var_list):\n            size = intprod(shape)\n            assigns.append(tf.assign(v, tf.reshape(theta[start : start + size], shape)))\n            start += size\n        self.op = tf.group(*assigns)\n\n    def __call__(self, theta):\n        tf.get_default_session().run(self.op, feed_dict={self.theta: theta})\n\n\nclass GetFlat(object):\n    def __init__(self, var_list):\n        self.op = tf.concat(\n            axis=0, values=[tf.reshape(v, [numel(v)]) for v in var_list]\n        )\n\n    def __call__(self):\n        return tf.get_default_session().run(self.op)\n\n\ndef flattenallbut0(x):\n    return tf.reshape(x, [-1, intprod(x.get_shape().as_list()[1:])])\n\n\n# =============================================================\n# TF placeholders management\n# ============================================================\n\n_PLACEHOLDER_CACHE = {}  # name -> (placeholder, dtype, shape)\n\n\ndef get_placeholder(name, dtype, shape):\n    if name in _PLACEHOLDER_CACHE:\n        out, dtype1, shape1 = _PLACEHOLDER_CACHE[name]\n        if out.graph == tf.get_default_graph():\n            assert (\n                dtype1 == dtype and shape1 == shape\n            ), \"Placeholder with name {} has already been registered and has shape {}, different from requested {}\".format(\n                name, shape1, shape\n            )\n            return out\n\n    out = tf.placeholder(dtype=dtype, shape=shape, name=name)\n    _PLACEHOLDER_CACHE[name] = (out, dtype, shape)\n    return out\n\n\ndef get_placeholder_cached(name):\n    return _PLACEHOLDER_CACHE[name][0]\n\n\n# ================================================================\n# Diagnostics\n# ================================================================\n\n\ndef display_var_info(vars):\n    from baselines import logger\n\n    count_params = 0\n    for v in vars:\n        name = v.name\n        if \"/Adam\" in name or \"beta1_power\" in name or \"beta2_power\" in name:\n            continue\n        v_params = np.prod(v.shape.as_list())\n        count_params += v_params\n        if \"/b:\" in name or \"/bias\" in name:\n            continue  # Wx+b, bias is not interesting to look at => count params, but not print\n        logger.info(\n            \"   %s%s %i params %s\"\n            % (name, \" \" * (55 - len(name)), v_params, str(v.shape))\n        )\n\n    logger.info(\"Total model parameters: %0.2f million\" % (count_params * 1e-6))\n\n\ndef get_available_gpus(session_config=None):\n    # based on recipe from https://stackoverflow.com/a/38580201\n\n    # Unless we allocate a session here, subsequent attempts to create one\n    # will ignore our custom config (in particular, allow_growth=True will have\n    # no effect).\n    if session_config is None:\n        session_config = get_session()._config\n\n    from tensorflow.python.client import device_lib\n\n    local_device_protos = device_lib.list_local_devices(session_config)\n    return [x.name for x in local_device_protos if x.device_type == \"GPU\"]\n\n\n# ================================================================\n# Saving variables\n# ================================================================\n\n\ndef load_state(fname, sess=None):\n    from baselines import logger\n\n    logger.warn(\"load_state method is deprecated, please use load_variables instead\")\n    sess = sess or get_session()\n    saver = tf.train.Saver()\n    saver.restore(tf.get_default_session(), fname)\n\n\ndef save_state(fname, sess=None):\n    from baselines import logger\n\n    logger.warn(\"save_state method is deprecated, please use save_variables instead\")\n    sess = sess or get_session()\n    dirname = os.path.dirname(fname)\n    if any(dirname):\n        os.makedirs(dirname, exist_ok=True)\n    saver = tf.train.Saver()\n    saver.save(tf.get_default_session(), fname)\n\n\n# The methods above and below are clearly doing the same thing, and in a rather similar way\n# TODO: ensure there is no subtle differences and remove one\n\n\ndef save_variables(save_path, variables=None, sess=None):\n    import joblib\n\n    sess = sess or get_session()\n    variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    ps = sess.run(variables)\n    save_dict = {v.name: value for v, value in zip(variables, ps)}\n    dirname = os.path.dirname(save_path)\n    if any(dirname):\n        os.makedirs(dirname, exist_ok=True)\n    joblib.dump(save_dict, save_path)\n\n\ndef load_variables(load_path, variables=None, sess=None):\n    import joblib\n\n    sess = sess or get_session()\n    variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    loaded_params = joblib.load(os.path.expanduser(load_path))\n    restores = []\n    if isinstance(loaded_params, list):\n        assert len(loaded_params) == len(\n            variables\n        ), \"number of variables loaded mismatches len(variables)\"\n        for d, v in zip(loaded_params, variables):\n            restores.append(v.assign(d))\n    else:\n        for v in variables:\n            restores.append(v.assign(loaded_params[v.name]))\n\n    sess.run(restores)\n\n\n# ================================================================\n# Shape adjustment for feeding into tf placeholders\n# ================================================================\ndef adjust_shape(placeholder, data):\n    \"\"\"\n    adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters:\n        placeholder     tensorflow input placeholder\n\n        data            input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns:\n        reshaped data\n    \"\"\"\n\n    if not isinstance(data, np.ndarray) and not isinstance(data, list):\n        return data\n    if isinstance(data, list):\n        data = np.array(data)\n\n    placeholder_shape = [x or -1 for x in placeholder.shape.as_list()]\n\n    assert _check_shape(\n        placeholder_shape, data.shape\n    ), \"Shape of data {} is not compatible with shape of the placeholder {}\".format(\n        data.shape, placeholder_shape\n    )\n\n    return np.reshape(data, placeholder_shape)\n\n\ndef _check_shape(placeholder_shape, data_shape):\n    \"\"\"check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)\"\"\"\n\n    return True\n    squeezed_placeholder_shape = _squeeze_shape(placeholder_shape)\n    squeezed_data_shape = _squeeze_shape(data_shape)\n\n    for i, s_data in enumerate(squeezed_data_shape):\n        s_placeholder = squeezed_placeholder_shape[i]\n        if s_placeholder != -1 and s_data != s_placeholder:\n            return False\n\n    return True\n\n\ndef _squeeze_shape(shape):\n    return [x for x in shape if x != 1]\n\n\n# ================================================================\n# Tensorboard interfacing\n# ================================================================\n\n\ndef launch_tensorboard_in_background(log_dir):\n    \"\"\"\n    To log the Tensorflow graph when using rl-algs\n    algorithms, you can run the following code\n    in your main script:\n        import threading, time\n        def start_tensorboard(session):\n            time.sleep(10) # Wait until graph is setup\n            tb_path = osp.join(logger.get_dir(), 'tb')\n            summary_writer = tf.summary.FileWriter(tb_path, graph=session.graph)\n            summary_op = tf.summary.merge_all()\n            launch_tensorboard_in_background(tb_path)\n        session = tf.get_default_session()\n        t = threading.Thread(target=start_tensorboard, args=([session]))\n        t.start()\n    \"\"\"\n    import subprocess\n\n    subprocess.Popen([\"tensorboard\", \"--logdir\", log_dir])\n", "levels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import numpy as np", "import copy", "import os", "import functools", "import collections", "import multiprocessing"], "function": ["def switch(condition, then_expression, else_expression):\n", "def lrelu(x, leak=0.2):\n", "def huber_loss(x, delta=1.0):\n", "def get_session(config=None):\n", "def make_session(config=None, num_cpu=None, make_default=False, graph=None):\n", "def single_threaded_session():\n", "def in_session(f):\n", "    def newfunc(*args, **kwargs):\n", "def normc_initializer(std=1.0, axis=0):\n", "def function(inputs, outputs, updates=None, givens=None):\n", "class _Function(object):\n", "    def __init__(self, inputs, outputs, updates, givens):\n", "    def _feed_input(self, feed_dict, inpt, value):\n", "    def __call__(self, *args, **kwargs):\n", "def var_shape(x):\n", "def numel(x):\n", "def intprod(x):\n", "def flatgrad(loss, var_list, clip_norm=None):\n", "class SetFromFlat(object):\n", "    def __init__(self, var_list, dtype=tf.float32):\n", "    def __call__(self, theta):\n", "class GetFlat(object):\n", "    def __init__(self, var_list):\n", "    def __call__(self):\n", "def flattenallbut0(x):\n", "def get_placeholder(name, dtype, shape):\n", "def get_placeholder_cached(name):\n", "def display_var_info(vars):\n", "def get_available_gpus(session_config=None):\n", "def load_state(fname, sess=None):\n", "def save_state(fname, sess=None):\n", "def save_variables(save_path, variables=None, sess=None):\n", "def load_variables(load_path, variables=None, sess=None):\n", "def adjust_shape(placeholder, data):\n", "def _check_shape(placeholder_shape, data_shape):\n", "def _squeeze_shape(shape):\n", "def launch_tensorboard_in_background(log_dir):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/tf_util.py", "func_name": "adjust_shape", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters:\n        placeholder     tensorflow input placeholder\n\n        data            input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns:\n        reshaped data", "docstring_tokens": ["adjust", "shape", "of", "the", "data", "to", "the", "shape", "of", "the", "placeholder", "if", "possible", ".", "If", "shape", "is", "incompatible", "AssertionError", "is", "thrown"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/tf_util.py#L377-L401", "partition": "valid", "up_fun_num": 35, "context": "import numpy as np\nimport tensorflow as tf  # pylint: ignore-module\nimport copy\nimport os\nimport functools\nimport collections\nimport multiprocessing\n\n\ndef switch(condition, then_expression, else_expression):\n    \"\"\"Switches between two operations depending on a scalar value (int or bool).\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor.\n        then_expression: TensorFlow operation.\n        else_expression: TensorFlow operation.\n    \"\"\"\n    x_shape = copy.copy(then_expression.get_shape())\n    x = tf.cond(\n        tf.cast(condition, \"bool\"), lambda: then_expression, lambda: else_expression\n    )\n    x.set_shape(x_shape)\n    return x\n\n\n# ================================================================\n# Extras\n# ================================================================\n\n\ndef lrelu(x, leak=0.2):\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * x + f2 * abs(x)\n\n\n# ================================================================\n# Mathematical utils\n# ================================================================\n\n\ndef huber_loss(x, delta=1.0):\n    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n    return tf.where(\n        tf.abs(x) < delta, tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta)\n    )\n\n\n# ================================================================\n# Global session\n# ================================================================\n\n\ndef get_session(config=None):\n    \"\"\"Get default session or create one with a given config\"\"\"\n    sess = tf.get_default_session()\n    if sess is None:\n        sess = make_session(config=config, make_default=True)\n    return sess\n\n\ndef make_session(config=None, num_cpu=None, make_default=False, graph=None):\n    \"\"\"Returns a session that will use <num_cpu> CPU's only\"\"\"\n    if num_cpu is None:\n        num_cpu = int(os.getenv(\"RCALL_NUM_CPU\", multiprocessing.cpu_count()))\n    if config is None:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=num_cpu,\n            intra_op_parallelism_threads=num_cpu,\n        )\n        config.gpu_options.allow_growth = True\n\n    if make_default:\n        return tf.InteractiveSession(config=config, graph=graph)\n    else:\n        return tf.Session(config=config, graph=graph)\n\n\ndef single_threaded_session():\n    \"\"\"Returns a session which will only use a single CPU\"\"\"\n    return make_session(num_cpu=1)\n\n\ndef in_session(f):\n    @functools.wraps(f)\n    def newfunc(*args, **kwargs):\n        with tf.Session():\n            f(*args, **kwargs)\n\n    return newfunc\n\n\nALREADY_INITIALIZED = set()\n\n\ndef initialize():\n    \"\"\"Initialize all the uninitialized variables in the global scope.\"\"\"\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n    get_session().run(tf.variables_initializer(new_variables))\n    ALREADY_INITIALIZED.update(new_variables)\n\n\n# ================================================================\n# Model components\n# ================================================================\n\n\ndef normc_initializer(std=1.0, axis=0):\n    def _initializer(shape, dtype=None, partition_info=None):  # pylint: disable=W0613\n        out = np.random.randn(*shape).astype(dtype.as_numpy_dtype)\n        out *= std / np.sqrt(np.square(out).sum(axis=axis, keepdims=True))\n        return tf.constant(out)\n\n    return _initializer\n\n\ndef conv2d(\n    x,\n    num_filters,\n    name,\n    filter_size=(3, 3),\n    stride=(1, 1),\n    pad=\"SAME\",\n    dtype=tf.float32,\n    collections=None,\n    summary_tag=None,\n):\n    with tf.variable_scope(name):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [\n            filter_size[0],\n            filter_size[1],\n            int(x.get_shape()[3]),\n            num_filters,\n        ]\n\n        # there are \"num input feature maps * filter height * filter width\"\n        # inputs to each hidden unit\n        fan_in = intprod(filter_shape[:3])\n        # each unit in the lower layer receives a gradient from:\n        # \"num output feature maps * filter height * filter width\" /\n        #   pooling size\n        fan_out = intprod(filter_shape[:2]) * num_filters\n        # initialize weights with random weights\n        w_bound = np.sqrt(6.0 / (fan_in + fan_out))\n\n        w = tf.get_variable(\n            \"W\",\n            filter_shape,\n            dtype,\n            tf.random_uniform_initializer(-w_bound, w_bound),\n            collections=collections,\n        )\n        b = tf.get_variable(\n            \"b\",\n            [1, 1, 1, num_filters],\n            initializer=tf.zeros_initializer(),\n            collections=collections,\n        )\n\n        if summary_tag is not None:\n            tf.summary.image(\n                summary_tag,\n                tf.transpose(\n                    tf.reshape(w, [filter_size[0], filter_size[1], -1, 1]), [2, 0, 1, 3]\n                ),\n                max_images=10,\n            )\n\n        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n\n\n# ================================================================\n# Theano-like Function\n# ================================================================\n\n\ndef function(inputs, outputs, updates=None, givens=None):\n    \"\"\"Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input's placeholders and produces the values of the expressions\n    in outputs.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n        x = tf.placeholder(tf.int32, (), name=\"x\")\n        y = tf.placeholder(tf.int32, (), name=\"y\")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(x=3) == 9\n            assert lin(2, 2) == 10\n            assert lin(x=2, y=3) == 12\n\n    Parameters\n    ----------\n    inputs: [tf.placeholder, tf.constant, or object with make_feed_dict method]\n        list of input arguments\n    outputs: [tf.Variable] or tf.Variable\n        list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    updates: [tf.Operation] or tf.Operation\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored.\n\n    \"\"\"\n    if isinstance(outputs, list):\n        return _Function(inputs, outputs, updates, givens=givens)\n    elif isinstance(outputs, (dict, collections.OrderedDict)):\n        f = _Function(inputs, outputs.values(), updates, givens=givens)\n        return lambda *args, **kwargs: type(outputs)(\n            zip(outputs.keys(), f(*args, **kwargs))\n        )\n    else:\n        f = _Function(inputs, [outputs], updates, givens=givens)\n        return lambda *args, **kwargs: f(*args, **kwargs)[0]\n\n\nclass _Function(object):\n    def __init__(self, inputs, outputs, updates, givens):\n        for inpt in inputs:\n            if not hasattr(inpt, \"make_feed_dict\") and not (\n                type(inpt) is tf.Tensor and len(inpt.op.inputs) == 0\n            ):\n                assert (\n                    False\n                ), \"inputs should all be placeholders, constants, or have a make_feed_dict method\"\n        self.inputs = inputs\n        self.input_names = {\n            inp.name.split(\"/\")[-1].split(\":\")[0]: inp for inp in inputs\n        }\n        updates = updates or []\n        self.update_group = tf.group(*updates)\n        self.outputs_update = list(outputs) + [self.update_group]\n        self.givens = {} if givens is None else givens\n\n    def _feed_input(self, feed_dict, inpt, value):\n        if hasattr(inpt, \"make_feed_dict\"):\n            feed_dict.update(inpt.make_feed_dict(value))\n        else:\n            feed_dict[inpt] = adjust_shape(inpt, value)\n\n    def __call__(self, *args, **kwargs):\n        assert len(args) + len(kwargs) <= len(\n            self.inputs\n        ), \"Too many arguments provided\"\n        feed_dict = {}\n        # Update feed dict with givens.\n        for inpt in self.givens:\n            feed_dict[inpt] = adjust_shape(inpt, feed_dict.get(inpt, self.givens[inpt]))\n        # Update the args\n        for inpt, value in zip(self.inputs, args):\n            self._feed_input(feed_dict, inpt, value)\n        for inpt_name, value in kwargs.items():\n            self._feed_input(feed_dict, self.input_names[inpt_name], value)\n        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n        return results\n\n\n# ================================================================\n# Flat vectors\n# ================================================================\n\n\ndef var_shape(x):\n    out = x.get_shape().as_list()\n    assert all(\n        isinstance(a, int) for a in out\n    ), \"shape function assumes that shape is fully known\"\n    return out\n\n\ndef numel(x):\n    return intprod(var_shape(x))\n\n\ndef intprod(x):\n    return int(np.prod(x))\n\n\ndef flatgrad(loss, var_list, clip_norm=None):\n    grads = tf.gradients(loss, var_list)\n    if clip_norm is not None:\n        grads = [tf.clip_by_norm(grad, clip_norm=clip_norm) for grad in grads]\n    return tf.concat(\n        axis=0,\n        values=[\n            tf.reshape(grad if grad is not None else tf.zeros_like(v), [numel(v)])\n            for (v, grad) in zip(var_list, grads)\n        ],\n    )\n\n\nclass SetFromFlat(object):\n    def __init__(self, var_list, dtype=tf.float32):\n        assigns = []\n        shapes = list(map(var_shape, var_list))\n        total_size = np.sum([intprod(shape) for shape in shapes])\n\n        self.theta = theta = tf.placeholder(dtype, [total_size])\n        start = 0\n        assigns = []\n        for (shape, v) in zip(shapes, var_list):\n            size = intprod(shape)\n            assigns.append(tf.assign(v, tf.reshape(theta[start : start + size], shape)))\n            start += size\n        self.op = tf.group(*assigns)\n\n    def __call__(self, theta):\n        tf.get_default_session().run(self.op, feed_dict={self.theta: theta})\n\n\nclass GetFlat(object):\n    def __init__(self, var_list):\n        self.op = tf.concat(\n            axis=0, values=[tf.reshape(v, [numel(v)]) for v in var_list]\n        )\n\n    def __call__(self):\n        return tf.get_default_session().run(self.op)\n\n\ndef flattenallbut0(x):\n    return tf.reshape(x, [-1, intprod(x.get_shape().as_list()[1:])])\n\n\n# =============================================================\n# TF placeholders management\n# ============================================================\n\n_PLACEHOLDER_CACHE = {}  # name -> (placeholder, dtype, shape)\n\n\ndef get_placeholder(name, dtype, shape):\n    if name in _PLACEHOLDER_CACHE:\n        out, dtype1, shape1 = _PLACEHOLDER_CACHE[name]\n        if out.graph == tf.get_default_graph():\n            assert (\n                dtype1 == dtype and shape1 == shape\n            ), \"Placeholder with name {} has already been registered and has shape {}, different from requested {}\".format(\n                name, shape1, shape\n            )\n            return out\n\n    out = tf.placeholder(dtype=dtype, shape=shape, name=name)\n    _PLACEHOLDER_CACHE[name] = (out, dtype, shape)\n    return out\n\n\ndef get_placeholder_cached(name):\n    return _PLACEHOLDER_CACHE[name][0]\n\n\n# ================================================================\n# Diagnostics\n# ================================================================\n\n\ndef display_var_info(vars):\n    from baselines import logger\n\n    count_params = 0\n    for v in vars:\n        name = v.name\n        if \"/Adam\" in name or \"beta1_power\" in name or \"beta2_power\" in name:\n            continue\n        v_params = np.prod(v.shape.as_list())\n        count_params += v_params\n        if \"/b:\" in name or \"/bias\" in name:\n            continue  # Wx+b, bias is not interesting to look at => count params, but not print\n        logger.info(\n            \"   %s%s %i params %s\"\n            % (name, \" \" * (55 - len(name)), v_params, str(v.shape))\n        )\n\n    logger.info(\"Total model parameters: %0.2f million\" % (count_params * 1e-6))\n\n\ndef get_available_gpus(session_config=None):\n    # based on recipe from https://stackoverflow.com/a/38580201\n\n    # Unless we allocate a session here, subsequent attempts to create one\n    # will ignore our custom config (in particular, allow_growth=True will have\n    # no effect).\n    if session_config is None:\n        session_config = get_session()._config\n\n    from tensorflow.python.client import device_lib\n\n    local_device_protos = device_lib.list_local_devices(session_config)\n    return [x.name for x in local_device_protos if x.device_type == \"GPU\"]\n\n\n# ================================================================\n# Saving variables\n# ================================================================\n\n\ndef load_state(fname, sess=None):\n    from baselines import logger\n\n    logger.warn(\"load_state method is deprecated, please use load_variables instead\")\n    sess = sess or get_session()\n    saver = tf.train.Saver()\n    saver.restore(tf.get_default_session(), fname)\n\n\ndef save_state(fname, sess=None):\n    from baselines import logger\n\n    logger.warn(\"save_state method is deprecated, please use save_variables instead\")\n    sess = sess or get_session()\n    dirname = os.path.dirname(fname)\n    if any(dirname):\n        os.makedirs(dirname, exist_ok=True)\n    saver = tf.train.Saver()\n    saver.save(tf.get_default_session(), fname)\n\n\n# The methods above and below are clearly doing the same thing, and in a rather similar way\n# TODO: ensure there is no subtle differences and remove one\n\n\ndef save_variables(save_path, variables=None, sess=None):\n    import joblib\n\n    sess = sess or get_session()\n    variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    ps = sess.run(variables)\n    save_dict = {v.name: value for v, value in zip(variables, ps)}\n    dirname = os.path.dirname(save_path)\n    if any(dirname):\n        os.makedirs(dirname, exist_ok=True)\n    joblib.dump(save_dict, save_path)\n\n\ndef load_variables(load_path, variables=None, sess=None):\n    import joblib\n\n    sess = sess or get_session()\n    variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    loaded_params = joblib.load(os.path.expanduser(load_path))\n    restores = []\n    if isinstance(loaded_params, list):\n        assert len(loaded_params) == len(\n            variables\n        ), \"number of variables loaded mismatches len(variables)\"\n        for d, v in zip(loaded_params, variables):\n            restores.append(v.assign(d))\n    else:\n        for v in variables:\n            restores.append(v.assign(loaded_params[v.name]))\n\n    sess.run(restores)\n\n\n# ================================================================\n# Shape adjustment for feeding into tf placeholders\n# ================================================================\n\n\ndef _check_shape(placeholder_shape, data_shape):\n    \"\"\"check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)\"\"\"\n\n    return True\n    squeezed_placeholder_shape = _squeeze_shape(placeholder_shape)\n    squeezed_data_shape = _squeeze_shape(data_shape)\n\n    for i, s_data in enumerate(squeezed_data_shape):\n        s_placeholder = squeezed_placeholder_shape[i]\n        if s_placeholder != -1 and s_data != s_placeholder:\n            return False\n\n    return True\n\n\ndef _squeeze_shape(shape):\n    return [x for x in shape if x != 1]\n\n\n# ================================================================\n# Tensorboard interfacing\n# ================================================================\n\n\ndef launch_tensorboard_in_background(log_dir):\n    \"\"\"\n    To log the Tensorflow graph when using rl-algs\n    algorithms, you can run the following code\n    in your main script:\n        import threading, time\n        def start_tensorboard(session):\n            time.sleep(10) # Wait until graph is setup\n            tb_path = osp.join(logger.get_dir(), 'tb')\n            summary_writer = tf.summary.FileWriter(tb_path, graph=session.graph)\n            summary_op = tf.summary.merge_all()\n            launch_tensorboard_in_background(tb_path)\n        session = tf.get_default_session()\n        t = threading.Thread(target=start_tensorboard, args=([session]))\n        t.start()\n    \"\"\"\n    import subprocess\n\n    subprocess.Popen([\"tensorboard\", \"--logdir\", log_dir])\n", "levels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "package": ["import numpy as np", "import copy", "import os", "import functools", "import collections", "import multiprocessing"], "function": ["def switch(condition, then_expression, else_expression):\n", "def lrelu(x, leak=0.2):\n", "def huber_loss(x, delta=1.0):\n", "def get_session(config=None):\n", "def make_session(config=None, num_cpu=None, make_default=False, graph=None):\n", "def single_threaded_session():\n", "def in_session(f):\n", "    def newfunc(*args, **kwargs):\n", "def initialize():\n", "def normc_initializer(std=1.0, axis=0):\n", "def function(inputs, outputs, updates=None, givens=None):\n", "class _Function(object):\n", "    def __init__(self, inputs, outputs, updates, givens):\n", "    def _feed_input(self, feed_dict, inpt, value):\n", "    def __call__(self, *args, **kwargs):\n", "def var_shape(x):\n", "def numel(x):\n", "def intprod(x):\n", "def flatgrad(loss, var_list, clip_norm=None):\n", "class SetFromFlat(object):\n", "    def __init__(self, var_list, dtype=tf.float32):\n", "    def __call__(self, theta):\n", "class GetFlat(object):\n", "    def __init__(self, var_list):\n", "    def __call__(self):\n", "def flattenallbut0(x):\n", "def get_placeholder(name, dtype, shape):\n", "def get_placeholder_cached(name):\n", "def display_var_info(vars):\n", "def get_available_gpus(session_config=None):\n", "def load_state(fname, sess=None):\n", "def save_state(fname, sess=None):\n", "def save_variables(save_path, variables=None, sess=None):\n", "def load_variables(load_path, variables=None, sess=None):\n", "def _check_shape(placeholder_shape, data_shape):\n", "def _squeeze_shape(shape):\n", "def launch_tensorboard_in_background(log_dir):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/atari_wrappers.py", "func_name": "wrap_deepmind", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Configure environment for DeepMind-style Atari.", "docstring_tokens": ["Configure", "environment", "for", "DeepMind", "-", "style", "Atari", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/atari_wrappers.py#L235-L249", "partition": "valid", "up_fun_num": 37, "context": "import numpy as np\nimport os\n\nos.environ.setdefault(\"PATH\", \"\")\nfrom collections import deque\nimport gym\nfrom gym import spaces\nimport cv2\n\ncv2.ocl.setUseOpenCL(False)\nfrom .wrappers import TimeLimit\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n\n    def reset(self, **kwargs):\n        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(\n                1, self.noop_max + 1\n            )  # pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it's important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        if self.grayscale:\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n            )\n        else:\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(self.height, self.width, 3), dtype=np.uint8\n            )\n\n    def observation(self, frame):\n        if self.grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(\n            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n        )\n        if self.grayscale:\n            frame = np.expand_dims(frame, -1)\n        return frame\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        \"\"\"Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(\n            low=0,\n            high=255,\n            shape=(shp[:-1] + (shp[-1] * k,)),\n            dtype=env.observation_space.dtype,\n        )\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=1, shape=env.observation_space.shape, dtype=np.float32\n        )\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        \"\"\"This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You'd not believe how complex the previous solution was.\"\"\"\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[..., i]\n\n\ndef make_atari(env_id, max_episode_steps=None):\n    env = gym.make(env_id)\n    assert \"NoFrameskip\" in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env\n", "levels": [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0], "package": ["import numpy as np", "import os", "from collections import deque", "import gym", "from gym import spaces", "import cv2", "from .wrappers import TimeLimit"], "function": ["class NoopResetEnv(gym.Wrapper):\n", "    def __init__(self, env, noop_max=30):\n", "    def reset(self, **kwargs):\n", "    def step(self, ac):\n", "class FireResetEnv(gym.Wrapper):\n", "    def __init__(self, env):\n", "    def reset(self, **kwargs):\n", "    def step(self, ac):\n", "class EpisodicLifeEnv(gym.Wrapper):\n", "    def __init__(self, env):\n", "    def step(self, action):\n", "    def reset(self, **kwargs):\n", "class MaxAndSkipEnv(gym.Wrapper):\n", "    def __init__(self, env, skip=4):\n", "    def step(self, action):\n", "    def reset(self, **kwargs):\n", "class ClipRewardEnv(gym.RewardWrapper):\n", "    def __init__(self, env):\n", "    def reward(self, reward):\n", "class WarpFrame(gym.ObservationWrapper):\n", "    def __init__(self, env, width=84, height=84, grayscale=True):\n", "    def observation(self, frame):\n", "class FrameStack(gym.Wrapper):\n", "    def __init__(self, env, k):\n", "    def reset(self):\n", "    def step(self, action):\n", "    def _get_ob(self):\n", "class ScaledFloatFrame(gym.ObservationWrapper):\n", "    def __init__(self, env):\n", "    def observation(self, observation):\n", "class LazyFrames(object):\n", "    def __init__(self, frames):\n", "    def _force(self):\n", "    def __array__(self, dtype=None):\n", "    def __len__(self):\n", "    def __getitem__(self, i):\n", "def make_atari(env_id, max_episode_steps=None):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/atari_wrappers.py", "func_name": "EpisodicLifeEnv.reset", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.", "docstring_tokens": ["Reset", "only", "when", "lives", "are", "exhausted", ".", "This", "way", "all", "states", "are", "still", "reachable", "even", "though", "lives", "are", "episodic", "and", "the", "learner", "need", "not", "know", "about", "any", "of", "this", "behind", "-", "the", "-", "scenes", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/atari_wrappers.py#L84-L95", "partition": "valid", "up_fun_num": 11, "context": "import numpy as np\nimport os\n\nos.environ.setdefault(\"PATH\", \"\")\nfrom collections import deque\nimport gym\nfrom gym import spaces\nimport cv2\n\ncv2.ocl.setUseOpenCL(False)\nfrom .wrappers import TimeLimit\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n\n    def reset(self, **kwargs):\n        \"\"\"Do no-op action for a number of steps in [1, noop_max].\"\"\"\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(\n                1, self.noop_max + 1\n            )  # pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it's important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        if self.grayscale:\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n            )\n        else:\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(self.height, self.width, 3), dtype=np.uint8\n            )\n\n    def observation(self, frame):\n        if self.grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(\n            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n        )\n        if self.grayscale:\n            frame = np.expand_dims(frame, -1)\n        return frame\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        \"\"\"Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(\n            low=0,\n            high=255,\n            shape=(shp[:-1] + (shp[-1] * k,)),\n            dtype=env.observation_space.dtype,\n        )\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=1, shape=env.observation_space.shape, dtype=np.float32\n        )\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        \"\"\"This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You'd not believe how complex the previous solution was.\"\"\"\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[..., i]\n\n\ndef make_atari(env_id, max_episode_steps=None):\n    env = gym.make(env_id)\n    assert \"NoFrameskip\" in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env\n\n\ndef wrap_deepmind(\n    env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False\n):\n    \"\"\"Configure environment for DeepMind-style Atari.\"\"\"\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if \"FIRE\" in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env\n", "levels": [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0], "package": ["import numpy as np", "import os", "from collections import deque", "import gym", "from gym import spaces", "import cv2", "from .wrappers import TimeLimit"], "function": ["class NoopResetEnv(gym.Wrapper):\n", "    def __init__(self, env, noop_max=30):\n", "    def reset(self, **kwargs):\n", "    def step(self, ac):\n", "class FireResetEnv(gym.Wrapper):\n", "    def __init__(self, env):\n", "    def reset(self, **kwargs):\n", "    def step(self, ac):\n", "class EpisodicLifeEnv(gym.Wrapper):\n", "    def __init__(self, env):\n", "    def step(self, action):\n", "class MaxAndSkipEnv(gym.Wrapper):\n", "    def __init__(self, env, skip=4):\n", "    def step(self, action):\n", "    def reset(self, **kwargs):\n", "class ClipRewardEnv(gym.RewardWrapper):\n", "    def __init__(self, env):\n", "    def reward(self, reward):\n", "class WarpFrame(gym.ObservationWrapper):\n", "    def __init__(self, env, width=84, height=84, grayscale=True):\n", "    def observation(self, frame):\n", "class FrameStack(gym.Wrapper):\n", "    def __init__(self, env, k):\n", "    def reset(self):\n", "    def step(self, action):\n", "    def _get_ob(self):\n", "class ScaledFloatFrame(gym.ObservationWrapper):\n", "    def __init__(self, env):\n", "    def observation(self, observation):\n", "class LazyFrames(object):\n", "    def __init__(self, frames):\n", "    def _force(self):\n", "    def __array__(self, dtype=None):\n", "    def __len__(self):\n", "    def __getitem__(self, i):\n", "def make_atari(env_id, max_episode_steps=None):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/mpi_util.py", "func_name": "gpu_count", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Count the GPUs on this machine.", "docstring_tokens": ["Count", "the", "GPUs", "on", "this", "machine", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/mpi_util.py#L28-L35", "partition": "valid", "up_fun_num": 1, "context": "from collections import defaultdict\nimport os, numpy as np\nimport platform\nimport shutil\nimport subprocess\nimport warnings\nimport sys\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\n\ndef sync_from_root(sess, variables, comm=None):\n    \"\"\"\n    Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's\n    \"\"\"\n    if comm is None:\n        comm = MPI.COMM_WORLD\n    import tensorflow as tf\n\n    values = comm.bcast(sess.run(variables))\n    sess.run([tf.assign(var, val) for (var, val) in zip(variables, values)])\n\n\ndef setup_mpi_gpus():\n    \"\"\"\n    Set CUDA_VISIBLE_DEVICES to MPI rank if not already set\n    \"\"\"\n    if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n        if sys.platform == \"darwin\":  # This Assumes if you're on OSX you're just\n            ids = []  # doing a smoke test and don't want GPUs\n        else:\n            lrank, _lsize = get_local_rank_size(MPI.COMM_WORLD)\n            ids = [lrank]\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, ids))\n\n\ndef get_local_rank_size(comm):\n    \"\"\"\n    Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine\n    \"\"\"\n    this_node = platform.node()\n    ranks_nodes = comm.allgather((comm.Get_rank(), this_node))\n    node2rankssofar = defaultdict(int)\n    local_rank = None\n    for (rank, node) in ranks_nodes:\n        if rank == comm.Get_rank():\n            local_rank = node2rankssofar[node]\n        node2rankssofar[node] += 1\n    assert local_rank is not None\n    return local_rank, node2rankssofar[this_node]\n\n\ndef share_file(comm, path):\n    \"\"\"\n    Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines\n    \"\"\"\n    localrank, _ = get_local_rank_size(comm)\n    if comm.Get_rank() == 0:\n        with open(path, \"rb\") as fh:\n            data = fh.read()\n        comm.bcast(data)\n    else:\n        data = comm.bcast(None)\n        if localrank == 0:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"wb\") as fh:\n                fh.write(data)\n    comm.Barrier()\n\n\ndef dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n    \"\"\"\n    Perform a reduction operation over dicts\n    \"\"\"\n    if comm is None:\n        return d\n    alldicts = comm.allgather(d)\n    size = comm.size\n    k2li = defaultdict(list)\n    for d in alldicts:\n        for (k, v) in d.items():\n            k2li[k].append(v)\n    result = {}\n    for (k, li) in k2li.items():\n        if assert_all_have_data:\n            assert len(li) == size, \"only %i out of %i MPI workers have sent '%s'\" % (\n                len(li),\n                size,\n                k,\n            )\n        if op == \"mean\":\n            result[k] = np.mean(li, axis=0)\n        elif op == \"sum\":\n            result[k] = np.sum(li, axis=0)\n        else:\n            assert 0, op\n    return result\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["from collections import defaultdict", "import os, numpy as np", "import platform", "import shutil", "import subprocess", "import warnings", "import sys", "from mpi4py import MPI", "import tensorflow as tf"], "function": ["def sync_from_root(sess, variables, comm=None):\n", "def setup_mpi_gpus():\n", "def get_local_rank_size(comm):\n", "def share_file(comm, path):\n", "def dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n", "def mpi_weighted_mean(comm, local_name2valcount):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/mpi_util.py", "func_name": "setup_mpi_gpus", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Set CUDA_VISIBLE_DEVICES to MPI rank if not already set", "docstring_tokens": ["Set", "CUDA_VISIBLE_DEVICES", "to", "MPI", "rank", "if", "not", "already", "set"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/mpi_util.py#L37-L47", "partition": "valid", "up_fun_num": 2, "context": "from collections import defaultdict\nimport os, numpy as np\nimport platform\nimport shutil\nimport subprocess\nimport warnings\nimport sys\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\n\ndef sync_from_root(sess, variables, comm=None):\n    \"\"\"\n    Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's\n    \"\"\"\n    if comm is None:\n        comm = MPI.COMM_WORLD\n    import tensorflow as tf\n\n    values = comm.bcast(sess.run(variables))\n    sess.run([tf.assign(var, val) for (var, val) in zip(variables, values)])\n\n\ndef gpu_count():\n    \"\"\"\n    Count the GPUs on this machine.\n    \"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        return 0\n    output = subprocess.check_output(\n        [\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv\"]\n    )\n    return max(0, len(output.split(b\"\\n\")) - 2)\n\n\ndef get_local_rank_size(comm):\n    \"\"\"\n    Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine\n    \"\"\"\n    this_node = platform.node()\n    ranks_nodes = comm.allgather((comm.Get_rank(), this_node))\n    node2rankssofar = defaultdict(int)\n    local_rank = None\n    for (rank, node) in ranks_nodes:\n        if rank == comm.Get_rank():\n            local_rank = node2rankssofar[node]\n        node2rankssofar[node] += 1\n    assert local_rank is not None\n    return local_rank, node2rankssofar[this_node]\n\n\ndef share_file(comm, path):\n    \"\"\"\n    Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines\n    \"\"\"\n    localrank, _ = get_local_rank_size(comm)\n    if comm.Get_rank() == 0:\n        with open(path, \"rb\") as fh:\n            data = fh.read()\n        comm.bcast(data)\n    else:\n        data = comm.bcast(None)\n        if localrank == 0:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"wb\") as fh:\n                fh.write(data)\n    comm.Barrier()\n\n\ndef dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n    \"\"\"\n    Perform a reduction operation over dicts\n    \"\"\"\n    if comm is None:\n        return d\n    alldicts = comm.allgather(d)\n    size = comm.size\n    k2li = defaultdict(list)\n    for d in alldicts:\n        for (k, v) in d.items():\n            k2li[k].append(v)\n    result = {}\n    for (k, li) in k2li.items():\n        if assert_all_have_data:\n            assert len(li) == size, \"only %i out of %i MPI workers have sent '%s'\" % (\n                len(li),\n                size,\n                k,\n            )\n        if op == \"mean\":\n            result[k] = np.mean(li, axis=0)\n        elif op == \"sum\":\n            result[k] = np.sum(li, axis=0)\n        else:\n            assert 0, op\n    return result\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["from collections import defaultdict", "import os, numpy as np", "import platform", "import shutil", "import subprocess", "import warnings", "import sys", "from mpi4py import MPI", "import tensorflow as tf"], "function": ["def sync_from_root(sess, variables, comm=None):\n", "def gpu_count():\n", "def get_local_rank_size(comm):\n", "def share_file(comm, path):\n", "def dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n", "def mpi_weighted_mean(comm, local_name2valcount):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/mpi_util.py", "func_name": "get_local_rank_size", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine", "docstring_tokens": ["Returns", "the", "rank", "of", "each", "process", "on", "its", "machine", "The", "processes", "on", "a", "given", "machine", "will", "be", "assigned", "ranks", "0", "1", "2", "...", "N", "-", "1", "where", "N", "is", "the", "number", "of", "processes", "on", "this", "machine", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/mpi_util.py#L49-L67", "partition": "valid", "up_fun_num": 3, "context": "from collections import defaultdict\nimport os, numpy as np\nimport platform\nimport shutil\nimport subprocess\nimport warnings\nimport sys\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\n\ndef sync_from_root(sess, variables, comm=None):\n    \"\"\"\n    Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's\n    \"\"\"\n    if comm is None:\n        comm = MPI.COMM_WORLD\n    import tensorflow as tf\n\n    values = comm.bcast(sess.run(variables))\n    sess.run([tf.assign(var, val) for (var, val) in zip(variables, values)])\n\n\ndef gpu_count():\n    \"\"\"\n    Count the GPUs on this machine.\n    \"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        return 0\n    output = subprocess.check_output(\n        [\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv\"]\n    )\n    return max(0, len(output.split(b\"\\n\")) - 2)\n\n\ndef setup_mpi_gpus():\n    \"\"\"\n    Set CUDA_VISIBLE_DEVICES to MPI rank if not already set\n    \"\"\"\n    if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n        if sys.platform == \"darwin\":  # This Assumes if you're on OSX you're just\n            ids = []  # doing a smoke test and don't want GPUs\n        else:\n            lrank, _lsize = get_local_rank_size(MPI.COMM_WORLD)\n            ids = [lrank]\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, ids))\n\n\ndef share_file(comm, path):\n    \"\"\"\n    Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines\n    \"\"\"\n    localrank, _ = get_local_rank_size(comm)\n    if comm.Get_rank() == 0:\n        with open(path, \"rb\") as fh:\n            data = fh.read()\n        comm.bcast(data)\n    else:\n        data = comm.bcast(None)\n        if localrank == 0:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"wb\") as fh:\n                fh.write(data)\n    comm.Barrier()\n\n\ndef dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n    \"\"\"\n    Perform a reduction operation over dicts\n    \"\"\"\n    if comm is None:\n        return d\n    alldicts = comm.allgather(d)\n    size = comm.size\n    k2li = defaultdict(list)\n    for d in alldicts:\n        for (k, v) in d.items():\n            k2li[k].append(v)\n    result = {}\n    for (k, li) in k2li.items():\n        if assert_all_have_data:\n            assert len(li) == size, \"only %i out of %i MPI workers have sent '%s'\" % (\n                len(li),\n                size,\n                k,\n            )\n        if op == \"mean\":\n            result[k] = np.mean(li, axis=0)\n        elif op == \"sum\":\n            result[k] = np.sum(li, axis=0)\n        else:\n            assert 0, op\n    return result\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["from collections import defaultdict", "import os, numpy as np", "import platform", "import shutil", "import subprocess", "import warnings", "import sys", "from mpi4py import MPI", "import tensorflow as tf"], "function": ["def sync_from_root(sess, variables, comm=None):\n", "def gpu_count():\n", "def setup_mpi_gpus():\n", "def share_file(comm, path):\n", "def dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n", "def mpi_weighted_mean(comm, local_name2valcount):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/mpi_util.py", "func_name": "share_file", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines", "docstring_tokens": ["Copies", "the", "file", "from", "rank", "0", "to", "all", "other", "ranks", "Puts", "it", "in", "the", "same", "place", "on", "all", "machines"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/mpi_util.py#L69-L85", "partition": "valid", "up_fun_num": 4, "context": "from collections import defaultdict\nimport os, numpy as np\nimport platform\nimport shutil\nimport subprocess\nimport warnings\nimport sys\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\n\ndef sync_from_root(sess, variables, comm=None):\n    \"\"\"\n    Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's\n    \"\"\"\n    if comm is None:\n        comm = MPI.COMM_WORLD\n    import tensorflow as tf\n\n    values = comm.bcast(sess.run(variables))\n    sess.run([tf.assign(var, val) for (var, val) in zip(variables, values)])\n\n\ndef gpu_count():\n    \"\"\"\n    Count the GPUs on this machine.\n    \"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        return 0\n    output = subprocess.check_output(\n        [\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv\"]\n    )\n    return max(0, len(output.split(b\"\\n\")) - 2)\n\n\ndef setup_mpi_gpus():\n    \"\"\"\n    Set CUDA_VISIBLE_DEVICES to MPI rank if not already set\n    \"\"\"\n    if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n        if sys.platform == \"darwin\":  # This Assumes if you're on OSX you're just\n            ids = []  # doing a smoke test and don't want GPUs\n        else:\n            lrank, _lsize = get_local_rank_size(MPI.COMM_WORLD)\n            ids = [lrank]\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, ids))\n\n\ndef get_local_rank_size(comm):\n    \"\"\"\n    Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine\n    \"\"\"\n    this_node = platform.node()\n    ranks_nodes = comm.allgather((comm.Get_rank(), this_node))\n    node2rankssofar = defaultdict(int)\n    local_rank = None\n    for (rank, node) in ranks_nodes:\n        if rank == comm.Get_rank():\n            local_rank = node2rankssofar[node]\n        node2rankssofar[node] += 1\n    assert local_rank is not None\n    return local_rank, node2rankssofar[this_node]\n\n\ndef dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n    \"\"\"\n    Perform a reduction operation over dicts\n    \"\"\"\n    if comm is None:\n        return d\n    alldicts = comm.allgather(d)\n    size = comm.size\n    k2li = defaultdict(list)\n    for d in alldicts:\n        for (k, v) in d.items():\n            k2li[k].append(v)\n    result = {}\n    for (k, li) in k2li.items():\n        if assert_all_have_data:\n            assert len(li) == size, \"only %i out of %i MPI workers have sent '%s'\" % (\n                len(li),\n                size,\n                k,\n            )\n        if op == \"mean\":\n            result[k] = np.mean(li, axis=0)\n        elif op == \"sum\":\n            result[k] = np.sum(li, axis=0)\n        else:\n            assert 0, op\n    return result\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["from collections import defaultdict", "import os, numpy as np", "import platform", "import shutil", "import subprocess", "import warnings", "import sys", "from mpi4py import MPI", "import tensorflow as tf"], "function": ["def sync_from_root(sess, variables, comm=None):\n", "def gpu_count():\n", "def setup_mpi_gpus():\n", "def get_local_rank_size(comm):\n", "def dict_gather(comm, d, op=\"mean\", assert_all_have_data=True):\n", "def mpi_weighted_mean(comm, local_name2valcount):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/mpi_util.py", "func_name": "dict_gather", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Perform a reduction operation over dicts", "docstring_tokens": ["Perform", "a", "reduction", "operation", "over", "dicts"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/mpi_util.py#L87-L108", "partition": "valid", "up_fun_num": 5, "context": "from collections import defaultdict\nimport os, numpy as np\nimport platform\nimport shutil\nimport subprocess\nimport warnings\nimport sys\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\n\ndef sync_from_root(sess, variables, comm=None):\n    \"\"\"\n    Send the root node's parameters to every worker.\n    Arguments:\n      sess: the TensorFlow session.\n      variables: all parameter variables including optimizer's\n    \"\"\"\n    if comm is None:\n        comm = MPI.COMM_WORLD\n    import tensorflow as tf\n\n    values = comm.bcast(sess.run(variables))\n    sess.run([tf.assign(var, val) for (var, val) in zip(variables, values)])\n\n\ndef gpu_count():\n    \"\"\"\n    Count the GPUs on this machine.\n    \"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        return 0\n    output = subprocess.check_output(\n        [\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv\"]\n    )\n    return max(0, len(output.split(b\"\\n\")) - 2)\n\n\ndef setup_mpi_gpus():\n    \"\"\"\n    Set CUDA_VISIBLE_DEVICES to MPI rank if not already set\n    \"\"\"\n    if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n        if sys.platform == \"darwin\":  # This Assumes if you're on OSX you're just\n            ids = []  # doing a smoke test and don't want GPUs\n        else:\n            lrank, _lsize = get_local_rank_size(MPI.COMM_WORLD)\n            ids = [lrank]\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, ids))\n\n\ndef get_local_rank_size(comm):\n    \"\"\"\n    Returns the rank of each process on its machine\n    The processes on a given machine will be assigned ranks\n        0, 1, 2, ..., N-1,\n    where N is the number of processes on this machine.\n\n    Useful if you want to assign one gpu per machine\n    \"\"\"\n    this_node = platform.node()\n    ranks_nodes = comm.allgather((comm.Get_rank(), this_node))\n    node2rankssofar = defaultdict(int)\n    local_rank = None\n    for (rank, node) in ranks_nodes:\n        if rank == comm.Get_rank():\n            local_rank = node2rankssofar[node]\n        node2rankssofar[node] += 1\n    assert local_rank is not None\n    return local_rank, node2rankssofar[this_node]\n\n\ndef share_file(comm, path):\n    \"\"\"\n    Copies the file from rank 0 to all other ranks\n    Puts it in the same place on all machines\n    \"\"\"\n    localrank, _ = get_local_rank_size(comm)\n    if comm.Get_rank() == 0:\n        with open(path, \"rb\") as fh:\n            data = fh.read()\n        comm.bcast(data)\n    else:\n        data = comm.bcast(None)\n        if localrank == 0:\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"wb\") as fh:\n                fh.write(data)\n    comm.Barrier()\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n", "levels": [0, 0, 0, 0, 0, 0], "package": ["from collections import defaultdict", "import os, numpy as np", "import platform", "import shutil", "import subprocess", "import warnings", "import sys", "from mpi4py import MPI", "import tensorflow as tf"], "function": ["def sync_from_root(sess, variables, comm=None):\n", "def gpu_count():\n", "def setup_mpi_gpus():\n", "def get_local_rank_size(comm):\n", "def share_file(comm, path):\n", "def mpi_weighted_mean(comm, local_name2valcount):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/math_util.py", "func_name": "discount", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "computes discounted sums along 0th dimension of x.\n\n    inputs\n    ------\n    x: ndarray\n    gamma: float\n\n    outputs\n    -------\n    y: ndarray with same shape as x, satisfying\n\n        y[t] = x[t] + gamma*x[t+1] + gamma^2*x[t+2] + ... + gamma^k x[t+k],\n                where k = len(x) - t - 1", "docstring_tokens": ["computes", "discounted", "sums", "along", "0th", "dimension", "of", "x", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/math_util.py#L5-L23", "partition": "valid", "up_fun_num": 0, "context": "import numpy as np\nimport scipy.signal\n\n\ndef explained_variance(ypred, y):\n    \"\"\"\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    \"\"\"\n    assert y.ndim == 1 and ypred.ndim == 1\n    vary = np.var(y)\n    return np.nan if vary == 0 else 1 - np.var(y - ypred) / vary\n\n\ndef explained_variance_2d(ypred, y):\n    assert y.ndim == 2 and ypred.ndim == 2\n    vary = np.var(y, axis=0)\n    out = 1 - np.var(y - ypred) / vary\n    out[vary < 1e-10] = 0\n    return out\n\n\ndef ncc(ypred, y):\n    return np.corrcoef(ypred, y)[1, 0]\n\n\ndef flatten_arrays(arrs):\n    return np.concatenate([arr.flat for arr in arrs])\n\n\ndef unflatten_vector(vec, shapes):\n    i = 0\n    arrs = []\n    for shape in shapes:\n        size = np.prod(shape)\n        arr = vec[i : i + size].reshape(shape)\n        arrs.append(arr)\n        i += size\n    return arrs\n\n\ndef discount_with_boundaries(X, New, gamma):\n    \"\"\"\n    X: 2d array of floats, time x features\n    New: 2d array of bools, indicating when a new episode has started\n    \"\"\"\n    Y = np.zeros_like(X)\n    T = X.shape[0]\n    Y[T - 1] = X[T - 1]\n    for t in range(T - 2, -1, -1):\n        Y[t] = X[t] + gamma * Y[t + 1] * (1 - New[t + 1])\n    return Y\n\n\ndef test_discount_with_boundaries():\n    gamma = 0.9\n    x = np.array([1.0, 2.0, 3.0, 4.0], \"float32\")\n    starts = [1.0, 0.0, 0.0, 1.0]\n    y = discount_with_boundaries(x, starts, gamma)\n    assert np.allclose(y, [1 + gamma * 2 + gamma ** 2 * 3, 2 + gamma * 3, 3, 4])\n", "levels": [0, 0, 0, 0, 0, 0, 0], "package": ["import numpy as np", "import scipy.signal"], "function": ["def explained_variance(ypred, y):\n", "def explained_variance_2d(ypred, y):\n", "def ncc(ypred, y):\n", "def flatten_arrays(arrs):\n", "def unflatten_vector(vec, shapes):\n", "def discount_with_boundaries(X, New, gamma):\n", "def test_discount_with_boundaries():\n"]}
{"repo": "openai/baselines", "path": "baselines/deepq/replay_buffer.py", "func_name": "PrioritizedReplayBuffer.add", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "See ReplayBuffer.store_effect", "docstring_tokens": ["See", "ReplayBuffer", ".", "store_effect"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/deepq/replay_buffer.py#L100-L105", "partition": "valid", "up_fun_num": 8, "context": "import numpy as np\nimport random\n\nfrom baselines.common.segment_tree import SumSegmentTree, MinSegmentTree\n\n\nclass ReplayBuffer(object):\n    def __init__(self, size):\n        \"\"\"Create Replay buffer.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        \"\"\"\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, obs_t, action, reward, obs_tp1, done):\n        data = (obs_t, action, reward, obs_tp1, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, idxes):\n        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n        for i in idxes:\n            data = self._storage[i]\n            obs_t, action, reward, obs_tp1, done = data\n            obses_t.append(np.array(obs_t, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_tp1.append(np.array(obs_tp1, copy=False))\n            dones.append(done)\n        return (\n            np.array(obses_t),\n            np.array(actions),\n            np.array(rewards),\n            np.array(obses_tp1),\n            np.array(dones),\n        )\n\n    def sample(self, batch_size):\n        \"\"\"Sample a batch of experiences.\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        \"\"\"\n        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n        return self._encode_sample(idxes)\n\n\nclass PrioritizedReplayBuffer(ReplayBuffer):\n    def __init__(self, size, alpha):\n        \"\"\"Create Prioritized Replay buffer.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n\n        See Also\n        --------\n        ReplayBuffer.__init__\n        \"\"\"\n        super(PrioritizedReplayBuffer, self).__init__(size)\n        assert alpha >= 0\n        self._alpha = alpha\n\n        it_capacity = 1\n        while it_capacity < size:\n            it_capacity *= 2\n\n        self._it_sum = SumSegmentTree(it_capacity)\n        self._it_min = MinSegmentTree(it_capacity)\n        self._max_priority = 1.0\n\n    def _sample_proportional(self, batch_size):\n        res = []\n        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n        every_range_len = p_total / batch_size\n        for i in range(batch_size):\n            mass = random.random() * every_range_len + i * every_range_len\n            idx = self._it_sum.find_prefixsum_idx(mass)\n            res.append(idx)\n        return res\n\n    def sample(self, batch_size, beta):\n        \"\"\"Sample a batch of experiences.\n\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        \"\"\"\n        assert beta > 0\n\n        idxes = self._sample_proportional(batch_size)\n\n        weights = []\n        p_min = self._it_min.min() / self._it_sum.sum()\n        max_weight = (p_min * len(self._storage)) ** (-beta)\n\n        for idx in idxes:\n            p_sample = self._it_sum[idx] / self._it_sum.sum()\n            weight = (p_sample * len(self._storage)) ** (-beta)\n            weights.append(weight / max_weight)\n        weights = np.array(weights)\n        encoded_sample = self._encode_sample(idxes)\n        return tuple(list(encoded_sample) + [weights, idxes])\n\n    def update_priorities(self, idxes, priorities):\n        \"\"\"Update priorities of sampled transitions.\n\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        \"\"\"\n        assert len(idxes) == len(priorities)\n        for idx, priority in zip(idxes, priorities):\n            assert priority > 0\n            assert 0 <= idx < len(self._storage)\n            self._it_sum[idx] = priority ** self._alpha\n            self._it_min[idx] = priority ** self._alpha\n\n            self._max_priority = max(self._max_priority, priority)\n", "levels": [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["import numpy as np", "import random", "from baselines.common.segment_tree import SumSegmentTree, MinSegmentTree"], "function": ["class ReplayBuffer(object):\n", "    def __init__(self, size):\n", "    def __len__(self):\n", "    def add(self, obs_t, action, reward, obs_tp1, done):\n", "    def _encode_sample(self, idxes):\n", "    def sample(self, batch_size):\n", "class PrioritizedReplayBuffer(ReplayBuffer):\n", "    def __init__(self, size, alpha):\n", "    def _sample_proportional(self, batch_size):\n", "    def sample(self, batch_size, beta):\n", "    def update_priorities(self, idxes, priorities):\n"]}
{"repo": "openai/baselines", "path": "baselines/deepq/replay_buffer.py", "func_name": "PrioritizedReplayBuffer.update_priorities", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Update priorities of sampled transitions.\n\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.", "docstring_tokens": ["Update", "priorities", "of", "sampled", "transitions", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/deepq/replay_buffer.py#L169-L191", "partition": "valid", "up_fun_num": 11, "context": "import numpy as np\nimport random\n\nfrom baselines.common.segment_tree import SumSegmentTree, MinSegmentTree\n\n\nclass ReplayBuffer(object):\n    def __init__(self, size):\n        \"\"\"Create Replay buffer.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        \"\"\"\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, obs_t, action, reward, obs_tp1, done):\n        data = (obs_t, action, reward, obs_tp1, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, idxes):\n        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n        for i in idxes:\n            data = self._storage[i]\n            obs_t, action, reward, obs_tp1, done = data\n            obses_t.append(np.array(obs_t, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_tp1.append(np.array(obs_tp1, copy=False))\n            dones.append(done)\n        return (\n            np.array(obses_t),\n            np.array(actions),\n            np.array(rewards),\n            np.array(obses_tp1),\n            np.array(dones),\n        )\n\n    def sample(self, batch_size):\n        \"\"\"Sample a batch of experiences.\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        \"\"\"\n        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n        return self._encode_sample(idxes)\n\n\nclass PrioritizedReplayBuffer(ReplayBuffer):\n    def __init__(self, size, alpha):\n        \"\"\"Create Prioritized Replay buffer.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n\n        See Also\n        --------\n        ReplayBuffer.__init__\n        \"\"\"\n        super(PrioritizedReplayBuffer, self).__init__(size)\n        assert alpha >= 0\n        self._alpha = alpha\n\n        it_capacity = 1\n        while it_capacity < size:\n            it_capacity *= 2\n\n        self._it_sum = SumSegmentTree(it_capacity)\n        self._it_min = MinSegmentTree(it_capacity)\n        self._max_priority = 1.0\n\n    def add(self, *args, **kwargs):\n        \"\"\"See ReplayBuffer.store_effect\"\"\"\n        idx = self._next_idx\n        super().add(*args, **kwargs)\n        self._it_sum[idx] = self._max_priority ** self._alpha\n        self._it_min[idx] = self._max_priority ** self._alpha\n\n    def _sample_proportional(self, batch_size):\n        res = []\n        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n        every_range_len = p_total / batch_size\n        for i in range(batch_size):\n            mass = random.random() * every_range_len + i * every_range_len\n            idx = self._it_sum.find_prefixsum_idx(mass)\n            res.append(idx)\n        return res\n\n    def sample(self, batch_size, beta):\n        \"\"\"Sample a batch of experiences.\n\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        \"\"\"\n        assert beta > 0\n\n        idxes = self._sample_proportional(batch_size)\n\n        weights = []\n        p_min = self._it_min.min() / self._it_sum.sum()\n        max_weight = (p_min * len(self._storage)) ** (-beta)\n\n        for idx in idxes:\n            p_sample = self._it_sum[idx] / self._it_sum.sum()\n            weight = (p_sample * len(self._storage)) ** (-beta)\n            weights.append(weight / max_weight)\n        weights = np.array(weights)\n        encoded_sample = self._encode_sample(idxes)\n        return tuple(list(encoded_sample) + [weights, idxes])\n", "levels": [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["import numpy as np", "import random", "from baselines.common.segment_tree import SumSegmentTree, MinSegmentTree"], "function": ["class ReplayBuffer(object):\n", "    def __init__(self, size):\n", "    def __len__(self):\n", "    def add(self, obs_t, action, reward, obs_tp1, done):\n", "    def _encode_sample(self, idxes):\n", "    def sample(self, batch_size):\n", "class PrioritizedReplayBuffer(ReplayBuffer):\n", "    def __init__(self, size, alpha):\n", "    def add(self, *args, **kwargs):\n", "    def _sample_proportional(self, batch_size):\n", "    def sample(self, batch_size, beta):\n"]}
{"repo": "openai/baselines", "path": "baselines/common/retro_wrappers.py", "func_name": "wrap_deepmind_retro", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind", "docstring_tokens": ["Configure", "environment", "for", "retro", "games", "using", "config", "similar", "to", "DeepMind", "-", "style", "Atari", "in", "wrap_deepmind"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/retro_wrappers.py#L212-L222", "partition": "valid", "up_fun_num": 30, "context": "from collections import deque\nimport cv2\n\ncv2.ocl.setUseOpenCL(False)\nfrom .atari_wrappers import WarpFrame, ClipRewardEnv, FrameStack, ScaledFloatFrame\nfrom .wrappers import TimeLimit\nimport numpy as np\nimport gym\n\n\nclass StochasticFrameSkip(gym.Wrapper):\n    def __init__(self, env, n, stickprob):\n        gym.Wrapper.__init__(self, env)\n        self.n = n\n        self.stickprob = stickprob\n        self.curac = None\n        self.rng = np.random.RandomState()\n        self.supports_want_render = hasattr(env, \"supports_want_render\")\n\n    def reset(self, **kwargs):\n        self.curac = None\n        return self.env.reset(**kwargs)\n\n    def step(self, ac):\n        done = False\n        totrew = 0\n        for i in range(self.n):\n            # First step after reset, use action\n            if self.curac is None:\n                self.curac = ac\n            # First substep, delay with probability=stickprob\n            elif i == 0:\n                if self.rng.rand() > self.stickprob:\n                    self.curac = ac\n            # Second substep, new action definitely kicks in\n            elif i == 1:\n                self.curac = ac\n            if self.supports_want_render and i < self.n - 1:\n                ob, rew, done, info = self.env.step(self.curac, want_render=False)\n            else:\n                ob, rew, done, info = self.env.step(self.curac)\n            totrew += rew\n            if done:\n                break\n        return ob, totrew, done, info\n\n    def seed(self, s):\n        self.rng.seed(s)\n\n\nclass PartialFrameStack(gym.Wrapper):\n    def __init__(self, env, k, channel=1):\n        \"\"\"\n        Stack one channel (channel keyword) from previous frames\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        shp = env.observation_space.shape\n        self.channel = channel\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(shp[0], shp[1], shp[2] + k - 1),\n            dtype=env.observation_space.dtype,\n        )\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n\n    def reset(self):\n        ob = self.env.reset()\n        assert ob.shape[2] > self.channel\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, ac):\n        ob, reward, done, info = self.env.step(ac)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return np.concatenate(\n            [\n                frame\n                if i == self.k - 1\n                else frame[:, :, self.channel : self.channel + 1]\n                for (i, frame) in enumerate(self.frames)\n            ],\n            axis=2,\n        )\n\n\nclass Downsample(gym.ObservationWrapper):\n    def __init__(self, env, ratio):\n        \"\"\"\n        Downsample images by a factor of ratio\n        \"\"\"\n        gym.ObservationWrapper.__init__(self, env)\n        (oldh, oldw, oldc) = env.observation_space.shape\n        newshape = (oldh // ratio, oldw // ratio, oldc)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255, shape=newshape, dtype=np.uint8\n        )\n\n    def observation(self, frame):\n        height, width, _ = self.observation_space.shape\n        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n        if frame.ndim == 2:\n            frame = frame[:, :, None]\n        return frame\n\n\nclass Rgb2gray(gym.ObservationWrapper):\n    def __init__(self, env):\n        \"\"\"\n        Downsample images by a factor of ratio\n        \"\"\"\n        gym.ObservationWrapper.__init__(self, env)\n        (oldh, oldw, _oldc) = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255, shape=(oldh, oldw, 1), dtype=np.uint8\n        )\n\n    def observation(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        return frame[:, :, None]\n\n\nclass MovieRecord(gym.Wrapper):\n    def __init__(self, env, savedir, k):\n        gym.Wrapper.__init__(self, env)\n        self.savedir = savedir\n        self.k = k\n        self.epcount = 0\n\n    def reset(self):\n        if self.epcount % self.k == 0:\n            self.env.unwrapped.movie_path = self.savedir\n        else:\n            self.env.unwrapped.movie_path = None\n            self.env.unwrapped.movie = None\n        self.epcount += 1\n        return self.env.reset()\n\n\nclass AppendTimeout(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n        self.action_space = env.action_space\n        self.timeout_space = gym.spaces.Box(\n            low=np.array([0.0]), high=np.array([1.0]), dtype=np.float32\n        )\n        self.original_os = env.observation_space\n        if isinstance(self.original_os, gym.spaces.Dict):\n            import copy\n\n            ordered_dict = copy.deepcopy(self.original_os.spaces)\n            ordered_dict[\"value_estimation_timeout\"] = self.timeout_space\n            self.observation_space = gym.spaces.Dict(ordered_dict)\n            self.dict_mode = True\n        else:\n            self.observation_space = gym.spaces.Dict(\n                {\n                    \"original\": self.original_os,\n                    \"value_estimation_timeout\": self.timeout_space,\n                }\n            )\n            self.dict_mode = False\n        self.ac_count = None\n        while 1:\n            if not hasattr(\n                env, \"_max_episode_steps\"\n            ):  # Looking for TimeLimit wrapper that has this field\n                env = env.env\n                continue\n            break\n        self.timeout = env._max_episode_steps\n\n    def step(self, ac):\n        self.ac_count += 1\n        ob, rew, done, info = self.env.step(ac)\n        return self._process(ob), rew, done, info\n\n    def reset(self):\n        self.ac_count = 0\n        return self._process(self.env.reset())\n\n    def _process(self, ob):\n        fracmissing = 1 - self.ac_count / self.timeout\n        if self.dict_mode:\n            ob[\"value_estimation_timeout\"] = fracmissing\n        else:\n            return {\"original\": ob, \"value_estimation_timeout\": fracmissing}\n\n\nclass StartDoingRandomActionsWrapper(gym.Wrapper):\n    \"\"\"\n    Warning: can eat info dicts, not good if you depend on them\n    \"\"\"\n\n    def __init__(self, env, max_random_steps, on_startup=True, every_episode=False):\n        gym.Wrapper.__init__(self, env)\n        self.on_startup = on_startup\n        self.every_episode = every_episode\n        self.random_steps = max_random_steps\n        self.last_obs = None\n        if on_startup:\n            self.some_random_steps()\n\n    def some_random_steps(self):\n        self.last_obs = self.env.reset()\n        n = np.random.randint(self.random_steps)\n        # print(\"running for random %i frames\" % n)\n        for _ in range(n):\n            self.last_obs, _, done, _ = self.env.step(self.env.action_space.sample())\n            if done:\n                self.last_obs = self.env.reset()\n\n    def reset(self):\n        return self.last_obs\n\n    def step(self, a):\n        self.last_obs, rew, done, info = self.env.step(a)\n        if done:\n            self.last_obs = self.env.reset()\n            if self.every_episode:\n                self.some_random_steps()\n        return self.last_obs, rew, done, info\n\n\ndef make_retro(*, game, state=None, max_episode_steps=4500, **kwargs):\n    import retro\n\n    if state is None:\n        state = retro.State.DEFAULT\n    env = retro.make(game, state, **kwargs)\n    env = StochasticFrameSkip(env, n=4, stickprob=0.25)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env\n\n\nclass SonicDiscretizer(gym.ActionWrapper):\n    \"\"\"\n    Wrap a gym-retro environment and make it use discrete\n    actions for the Sonic game.\n    \"\"\"\n\n    def __init__(self, env):\n        super(SonicDiscretizer, self).__init__(env)\n        buttons = [\n            \"B\",\n            \"A\",\n            \"MODE\",\n            \"START\",\n            \"UP\",\n            \"DOWN\",\n            \"LEFT\",\n            \"RIGHT\",\n            \"C\",\n            \"Y\",\n            \"X\",\n            \"Z\",\n        ]\n        actions = [\n            [\"LEFT\"],\n            [\"RIGHT\"],\n            [\"LEFT\", \"DOWN\"],\n            [\"RIGHT\", \"DOWN\"],\n            [\"DOWN\"],\n            [\"DOWN\", \"B\"],\n            [\"B\"],\n        ]\n        self._actions = []\n        for action in actions:\n            arr = np.array([False] * 12)\n            for button in action:\n                arr[buttons.index(button)] = True\n            self._actions.append(arr)\n        self.action_space = gym.spaces.Discrete(len(self._actions))\n\n    def action(self, a):  # pylint: disable=W0221\n        return self._actions[a].copy()\n\n\nclass RewardScaler(gym.RewardWrapper):\n    \"\"\"\n    Bring rewards to a reasonable scale for PPO.\n    This is incredibly important and effects performance\n    drastically.\n    \"\"\"\n\n    def __init__(self, env, scale=0.01):\n        super(RewardScaler, self).__init__(env)\n        self.scale = scale\n\n    def reward(self, reward):\n        return reward * self.scale\n\n\nclass AllowBacktracking(gym.Wrapper):\n    \"\"\"\n    Use deltas in max(X) as the reward, rather than deltas\n    in X. This way, agents are not discouraged too heavily\n    from exploring backwards if there is no way to advance\n    head-on in the level.\n    \"\"\"\n\n    def __init__(self, env):\n        super(AllowBacktracking, self).__init__(env)\n        self._cur_x = 0\n        self._max_x = 0\n\n    def reset(self, **kwargs):  # pylint: disable=E0202\n        self._cur_x = 0\n        self._max_x = 0\n        return self.env.reset(**kwargs)\n\n    def step(self, action):  # pylint: disable=E0202\n        obs, rew, done, info = self.env.step(action)\n        self._cur_x += rew\n        rew = max(0, self._cur_x - self._max_x)\n        self._max_x = max(self._max_x, self._cur_x)\n        return obs, rew, done, info\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1], "package": ["from collections import deque", "import cv2", "from .atari_wrappers import WarpFrame, ClipRewardEnv, FrameStack, ScaledFloatFrame", "from .wrappers import TimeLimit", "import numpy as np", "import gym"], "function": ["class StochasticFrameSkip(gym.Wrapper):\n", "    def __init__(self, env, n, stickprob):\n", "    def reset(self, **kwargs):\n", "    def step(self, ac):\n", "    def seed(self, s):\n", "class PartialFrameStack(gym.Wrapper):\n", "    def __init__(self, env, k, channel=1):\n", "    def reset(self):\n", "    def step(self, ac):\n", "    def _get_ob(self):\n", "class Downsample(gym.ObservationWrapper):\n", "    def __init__(self, env, ratio):\n", "    def observation(self, frame):\n", "class Rgb2gray(gym.ObservationWrapper):\n", "    def __init__(self, env):\n", "    def observation(self, frame):\n", "class MovieRecord(gym.Wrapper):\n", "    def __init__(self, env, savedir, k):\n", "    def reset(self):\n", "class AppendTimeout(gym.Wrapper):\n", "    def __init__(self, env):\n", "    def step(self, ac):\n", "    def reset(self):\n", "    def _process(self, ob):\n", "class StartDoingRandomActionsWrapper(gym.Wrapper):\n", "    def __init__(self, env, max_random_steps, on_startup=True, every_episode=False):\n", "    def some_random_steps(self):\n", "    def reset(self):\n", "    def step(self, a):\n", "def make_retro(*, game, state=None, max_episode_steps=4500, **kwargs):\n", "class SonicDiscretizer(gym.ActionWrapper):\n", "    def __init__(self, env):\n", "class RewardScaler(gym.RewardWrapper):\n", "    def __init__(self, env, scale=0.01):\n", "    def reward(self, reward):\n", "class AllowBacktracking(gym.Wrapper):\n", "    def __init__(self, env):\n"]}
{"repo": "openai/baselines", "path": "baselines/her/her_sampler.py", "func_name": "make_sample_her_transitions", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a sample function that can be used for HER experience replay.\n\n    Args:\n        replay_strategy (in ['future', 'none']): the HER replay strategy; if set to 'none',\n            regular DDPG experience replay is used\n        replay_k (int): the ratio between HER replays and regular replays (e.g. k = 4 -> 4 times\n            as many HER replays as regular replays are used)\n        reward_fun (function): function to re-compute the reward with substituted goals", "docstring_tokens": ["Creates", "a", "sample", "function", "that", "can", "be", "used", "for", "HER", "experience", "replay", "."], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/her/her_sampler.py#L4-L63", "partition": "valid", "up_fun_num": 0, "context": "import numpy as np\n", "levels": [], "package": ["import numpy as np"], "function": []}
{"repo": "openai/baselines", "path": "baselines/run.py", "func_name": "parse_cmdline_kwargs", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "convert a list of '='-spaced command-line arguments to a dictionary, evaluating python objects when possible", "docstring_tokens": ["convert", "a", "list", "of", "=", "-", "spaced", "command", "-", "line", "arguments", "to", "a", "dictionary", "evaluating", "python", "objects", "when", "possible"], "sha": "3301089b48c42b87b396e246ea3f56fa4bfc9678", "url": "https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/run.py#L180-L192", "partition": "valid", "up_fun_num": 7, "context": "import sys\nimport re\nimport multiprocessing\nimport os.path as osp\nimport gym\nfrom collections import defaultdict\nimport tensorflow as tf\nimport numpy as np\n\nfrom baselines.common.vec_env import VecFrameStack, VecNormalize, VecEnv\nfrom baselines.common.vec_env.vec_video_recorder import VecVideoRecorder\nfrom baselines.common.cmd_util import (\n    common_arg_parser,\n    parse_unknown_args,\n    make_vec_env,\n    make_env,\n)\nfrom baselines.common.tf_util import get_session\nfrom baselines import logger\nfrom importlib import import_module\n\ntry:\n    from mpi4py import MPI\nexcept ImportError:\n    MPI = None\n\ntry:\n    import pybullet_envs\nexcept ImportError:\n    pybullet_envs = None\n\ntry:\n    import roboschool\nexcept ImportError:\n    roboschool = None\n\n_game_envs = defaultdict(set)\nfor env in gym.envs.registry.all():\n    # TODO: solve this with regexes\n    env_type = env._entry_point.split(\":\")[0].split(\".\")[-1]\n    _game_envs[env_type].add(env.id)\n\n# reading benchmark names directly from retro requires\n# importing retro here, and for some reason that crashes tensorflow\n# in ubuntu\n_game_envs[\"retro\"] = {\n    \"BubbleBobble-Nes\",\n    \"SuperMarioBros-Nes\",\n    \"TwinBee3PokoPokoDaimaou-Nes\",\n    \"SpaceHarrier-Nes\",\n    \"SonicTheHedgehog-Genesis\",\n    \"Vectorman-Genesis\",\n    \"FinalFight-Snes\",\n    \"SpaceInvaders-Snes\",\n}\n\n\ndef train(args, extra_args):\n    env_type, env_id = get_env_type(args)\n    print(\"env_type: {}\".format(env_type))\n\n    total_timesteps = int(args.num_timesteps)\n    seed = args.seed\n\n    learn = get_learn_function(args.alg)\n    alg_kwargs = get_learn_function_defaults(args.alg, env_type)\n    alg_kwargs.update(extra_args)\n\n    env = build_env(args)\n    if args.save_video_interval != 0:\n        env = VecVideoRecorder(\n            env,\n            osp.join(logger.get_dir(), \"videos\"),\n            record_video_trigger=lambda x: x % args.save_video_interval == 0,\n            video_length=args.save_video_length,\n        )\n\n    if args.network:\n        alg_kwargs[\"network\"] = args.network\n    else:\n        if alg_kwargs.get(\"network\") is None:\n            alg_kwargs[\"network\"] = get_default_network(env_type)\n\n    print(\n        \"Training {} on {}:{} with arguments \\n{}\".format(\n            args.alg, env_type, env_id, alg_kwargs\n        )\n    )\n\n    model = learn(env=env, seed=seed, total_timesteps=total_timesteps, **alg_kwargs)\n\n    return model, env\n\n\ndef build_env(args):\n    ncpu = multiprocessing.cpu_count()\n    if sys.platform == \"darwin\":\n        ncpu //= 2\n    nenv = args.num_env or ncpu\n    alg = args.alg\n    seed = args.seed\n\n    env_type, env_id = get_env_type(args)\n\n    if env_type in {\"atari\", \"retro\"}:\n        if alg == \"deepq\":\n            env = make_env(\n                env_id, env_type, seed=seed, wrapper_kwargs={\"frame_stack\": True}\n            )\n        elif alg == \"trpo_mpi\":\n            env = make_env(env_id, env_type, seed=seed)\n        else:\n            frame_stack_size = 4\n            env = make_vec_env(\n                env_id,\n                env_type,\n                nenv,\n                seed,\n                gamestate=args.gamestate,\n                reward_scale=args.reward_scale,\n            )\n            env = VecFrameStack(env, frame_stack_size)\n\n    else:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            intra_op_parallelism_threads=1,\n            inter_op_parallelism_threads=1,\n        )\n        config.gpu_options.allow_growth = True\n        get_session(config=config)\n\n        flatten_dict_observations = alg not in {\"her\"}\n        env = make_vec_env(\n            env_id,\n            env_type,\n            args.num_env or 1,\n            seed,\n            reward_scale=args.reward_scale,\n            flatten_dict_observations=flatten_dict_observations,\n        )\n\n        if env_type == \"mujoco\":\n            env = VecNormalize(env, use_tf=True)\n\n    return env\n\n\ndef get_env_type(args):\n    env_id = args.env\n\n    if args.env_type is not None:\n        return args.env_type, env_id\n\n    # Re-parse the gym registry, since we could have new envs since last time.\n    for env in gym.envs.registry.all():\n        env_type = env._entry_point.split(\":\")[0].split(\".\")[-1]\n        _game_envs[env_type].add(env.id)  # This is a set so add is idempotent\n\n    if env_id in _game_envs.keys():\n        env_type = env_id\n        env_id = [g for g in _game_envs[env_type]][0]\n    else:\n        env_type = None\n        for g, e in _game_envs.items():\n            if env_id in e:\n                env_type = g\n                break\n        if \":\" in env_id:\n            env_type = re.sub(r\":.*\", \"\", env_id)\n        assert env_type is not None, \"env_id {} is not recognized in env types\".format(\n            env_id, _game_envs.keys()\n        )\n\n    return env_type, env_id\n\n\ndef get_default_network(env_type):\n    if env_type in {\"atari\", \"retro\"}:\n        return \"cnn\"\n    else:\n        return \"mlp\"\n\n\ndef get_alg_module(alg, submodule=None):\n    submodule = submodule or alg\n    try:\n        # first try to import the alg module from baselines\n        alg_module = import_module(\".\".join([\"baselines\", alg, submodule]))\n    except ImportError:\n        # then from rl_algs\n        alg_module = import_module(\".\".join([\"rl_\" + \"algs\", alg, submodule]))\n\n    return alg_module\n\n\ndef get_learn_function(alg):\n    return get_alg_module(alg).learn\n\n\ndef get_learn_function_defaults(alg, env_type):\n    try:\n        alg_defaults = get_alg_module(alg, \"defaults\")\n        kwargs = getattr(alg_defaults, env_type)()\n    except (ImportError, AttributeError):\n        kwargs = {}\n    return kwargs\n\n\ndef main(args):\n    # configure logger, disable logging in child MPI processes (with rank > 0)\n\n    arg_parser = common_arg_parser()\n    args, unknown_args = arg_parser.parse_known_args(args)\n    extra_args = parse_cmdline_kwargs(unknown_args)\n\n    if MPI is None or MPI.COMM_WORLD.Get_rank() == 0:\n        rank = 0\n        logger.configure()\n    else:\n        logger.configure(format_strs=[])\n        rank = MPI.COMM_WORLD.Get_rank()\n\n    model, env = train(args, extra_args)\n\n    if args.save_path is not None and rank == 0:\n        save_path = osp.expanduser(args.save_path)\n        model.save(save_path)\n\n    if args.play:\n        logger.log(\"Running trained model\")\n        obs = env.reset()\n\n        state = model.initial_state if hasattr(model, \"initial_state\") else None\n        dones = np.zeros((1,))\n\n        episode_rew = 0\n        while True:\n            if state is not None:\n                actions, _, state, _ = model.step(obs, S=state, M=dones)\n            else:\n                actions, _, _, _ = model.step(obs)\n\n            obs, rew, done, _ = env.step(actions)\n            episode_rew += rew[0] if isinstance(env, VecEnv) else rew\n            env.render()\n            done = done.any() if isinstance(done, np.ndarray) else done\n            if done:\n                print(\"episode_rew={}\".format(episode_rew))\n                episode_rew = 0\n                obs = env.reset()\n\n    env.close()\n\n    return model\n\n\nif __name__ == \"__main__\":\n    main(sys.argv)\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0], "package": ["import sys", "import re", "import multiprocessing", "import os.path as osp", "import gym", "from collections import defaultdict", "import tensorflow as tf", "import numpy as np", "from baselines.common.vec_env import VecFrameStack, VecNormalize, VecEnv", "from baselines.common.vec_env.vec_video_recorder import VecVideoRecorder", "from baselines.common.cmd_util import (", "from baselines.common.tf_util import get_session", "from baselines import logger", "from importlib import import_module", "from mpi4py import MPI", "import pybullet_envs", "import roboschool"], "function": ["def train(args, extra_args):\n", "def build_env(args):\n", "def get_env_type(args):\n", "def get_default_network(env_type):\n", "def get_alg_module(alg, submodule=None):\n", "def get_learn_function(alg):\n", "def get_learn_function_defaults(alg, env_type):\n", "def main(args):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "compute_geometric_median", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.", "docstring_tokens": ["Estimate", "the", "geometric", "median", "of", "points", "in", "2D", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L13-L58", "partition": "valid", "up_fun_num": 0, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "Keypoint.project", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.", "docstring_tokens": ["Project", "the", "keypoint", "onto", "a", "new", "position", "on", "a", "new", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L105-L131", "partition": "valid", "up_fun_num": 5, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "Keypoint.shift", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.", "docstring_tokens": ["Move", "the", "keypoint", "around", "on", "an", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L133-L151", "partition": "valid", "up_fun_num": 6, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "Keypoint.draw_on_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.", "docstring_tokens": ["Draw", "the", "keypoint", "onto", "a", "given", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L153-L250", "partition": "valid", "up_fun_num": 7, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "Keypoint.generate_similar_points_manhattan", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.", "docstring_tokens": ["Generate", "nearby", "points", "to", "this", "keypoint", "based", "on", "manhattan", "distance", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L252-L315", "partition": "valid", "up_fun_num": 7, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "Keypoint.copy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.", "docstring_tokens": ["Create", "a", "shallow", "copy", "of", "the", "Keypoint", "object", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L317-L337", "partition": "valid", "up_fun_num": 8, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "Keypoint.deepcopy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.", "docstring_tokens": ["Create", "a", "deep", "copy", "of", "the", "Keypoint", "object", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L339-L361", "partition": "valid", "up_fun_num": 9, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "KeypointsOnImage.on", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.", "docstring_tokens": ["Project", "keypoints", "from", "one", "image", "to", "a", "new", "one", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L414-L435", "partition": "valid", "up_fun_num": 17, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "KeypointsOnImage.draw_on_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.", "docstring_tokens": ["Draw", "all", "keypoints", "onto", "a", "given", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L437-L480", "partition": "valid", "up_fun_num": 18, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "KeypointsOnImage.shift", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.", "docstring_tokens": ["Move", "the", "keypoints", "around", "on", "an", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L482-L501", "partition": "valid", "up_fun_num": 18, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "KeypointsOnImage.copy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.", "docstring_tokens": ["Create", "a", "shallow", "copy", "of", "the", "KeypointsOnImage", "object", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L825-L850", "partition": "valid", "up_fun_num": 27, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def deepcopy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.\n\n        \"\"\"\n        # for some reason deepcopy is way slower here than manual copy\n        if keypoints is None:\n            keypoints = [kp.deepcopy() for kp in self.keypoints]\n        if shape is None:\n            shape = tuple(self.shape)\n        return KeypointsOnImage(keypoints, shape)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def deepcopy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/kps.py", "func_name": "KeypointsOnImage.deepcopy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a deep copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Deep copy.", "docstring_tokens": ["Create", "a", "deep", "copy", "of", "the", "KeypointsOnImage", "object", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/kps.py#L852-L877", "partition": "valid", "up_fun_num": 28, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport scipy.spatial.distance\nimport six.moves as sm\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\ndef compute_geometric_median(X, eps=1e-5):\n    \"\"\"\n    Estimate the geometric median of points in 2D.\n\n    Code from https://stackoverflow.com/a/30305181\n\n    Parameters\n    ----------\n    X : (N,2) ndarray\n        Points in 2D. Second axis must be given in xy-form.\n\n    eps : float, optional\n        Distance threshold when to return the median.\n\n    Returns\n    -------\n    (2,) ndarray\n        Geometric median as xy-coordinate.\n\n    \"\"\"\n    y = np.mean(X, 0)\n\n    while True:\n        D = scipy.spatial.distance.cdist(X, [y])\n        nonzeros = (D != 0)[:, 0]\n\n        Dinv = 1 / D[nonzeros]\n        Dinvs = np.sum(Dinv)\n        W = Dinv / Dinvs\n        T = np.sum(W * X[nonzeros], 0)\n\n        num_zeros = len(X) - np.sum(nonzeros)\n        if num_zeros == 0:\n            y1 = T\n        elif num_zeros == len(X):\n            return y\n        else:\n            R = (T - y) * Dinvs\n            r = np.linalg.norm(R)\n            rinv = 0 if r == 0 else num_zeros / r\n            y1 = max(0, 1 - rinv) * T + min(1, rinv) * y\n\n        if scipy.spatial.distance.euclidean(y, y1) < eps:\n            return y1\n\n        y = y1\n\n\nclass Keypoint(object):\n    \"\"\"\n    A single keypoint (aka landmark) on an image.\n\n    Parameters\n    ----------\n    x : number\n        Coordinate of the keypoint on the x axis.\n\n    y : number\n        Coordinate of the keypoint on the y axis.\n\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x_int(self):\n        \"\"\"\n        Return the keypoint's x-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's x-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.x))\n\n    @property\n    def y_int(self):\n        \"\"\"\n        Return the keypoint's y-coordinate, rounded to the closest integer.\n\n        Returns\n        -------\n        result : int\n            Keypoint's y-coordinate, rounded to the closest integer.\n\n        \"\"\"\n        return int(np.round(self.y))\n\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the keypoint onto a new position on a new image.\n\n        E.g. if the keypoint is on its original image at x=(10 of 100 pixels)\n        and y=(20 of 100 pixels) and is projected onto a new image with\n        size (width=200, height=200), its new position will be (20, 40).\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        xy_proj = project_coords([(self.x, self.y)], from_shape, to_shape)\n        return self.deepcopy(x=xy_proj[0][0], y=xy_proj[0][1])\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoint around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move by this value on the x axis.\n\n        y : number, optional\n            Move by this value on the y axis.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Keypoint object with new coordinates.\n\n        \"\"\"\n        return self.deepcopy(self.x + x, self.y + y)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw the keypoint onto a given image.\n\n        The keypoint is drawn as a square.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoint.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of the keypoint. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of the keypoint. If set to ``S``, each square will have\n            size ``S x S``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the keypoint.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if the keypoint is outside of the\n            image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoint.\n\n        \"\"\"\n        if copy:\n            image = np.copy(image)\n\n        if image.ndim == 2:\n            assert ia.is_single_number(color), (\n                \"Got a 2D image. Expected then 'color' to be a single number, \"\n                \"but got %s.\" % (str(color),)\n            )\n        elif image.ndim == 3 and ia.is_single_number(color):\n            color = [color] * image.shape[-1]\n\n        input_dtype = image.dtype\n        alpha_color = color\n        if alpha < 0.01:\n            # keypoint invisible, nothing to do\n            return image\n        elif alpha > 0.99:\n            alpha = 1\n        else:\n            image = image.astype(np.float32, copy=False)\n            alpha_color = alpha * np.array(color)\n\n        height, width = image.shape[0:2]\n\n        y, x = self.y_int, self.x_int\n\n        x1 = max(x - size // 2, 0)\n        x2 = min(x + 1 + size // 2, width)\n        y1 = max(y - size // 2, 0)\n        y2 = min(y + 1 + size // 2, height)\n\n        x1_clipped, x2_clipped = np.clip([x1, x2], 0, width)\n        y1_clipped, y2_clipped = np.clip([y1, y2], 0, height)\n\n        x1_clipped_ooi = x1_clipped < 0 or x1_clipped >= width\n        x2_clipped_ooi = x2_clipped < 0 or x2_clipped >= width + 1\n        y1_clipped_ooi = y1_clipped < 0 or y1_clipped >= height\n        y2_clipped_ooi = y2_clipped < 0 or y2_clipped >= height + 1\n        x_ooi = x1_clipped_ooi and x2_clipped_ooi\n        y_ooi = y1_clipped_ooi and y2_clipped_ooi\n        x_zero_size = (x2_clipped - x1_clipped) < 1  # min size is 1px\n        y_zero_size = (y2_clipped - y1_clipped) < 1\n        if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size:\n            if alpha == 1:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = color\n            else:\n                image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] = (\n                    1 - alpha\n                ) * image[y1_clipped:y2_clipped, x1_clipped:x2_clipped] + alpha_color\n        else:\n            if raise_if_out_of_image:\n                raise Exception(\n                    \"Cannot draw keypoint x=%.8f, y=%.8f on image with \"\n                    \"shape %s.\" % (y, x, image.shape)\n                )\n\n        if image.dtype.name != input_dtype.name:\n            if input_dtype.name == \"uint8\":\n                image = np.clip(image, 0, 255, out=image)\n            image = image.astype(input_dtype, copy=False)\n        return image\n\n    def generate_similar_points_manhattan(\n        self, nb_steps, step_size, return_array=False\n    ):\n        \"\"\"\n        Generate nearby points to this keypoint based on manhattan distance.\n\n        To generate the first neighbouring points, a distance of S (step size) is moved from the\n        center point (this keypoint) to the top, right, bottom and left, resulting in four new\n        points. From these new points, the pattern is repeated. Overlapping points are ignored.\n\n        The resulting points have a shape similar to a square rotated by 45 degrees.\n\n        Parameters\n        ----------\n        nb_steps : int\n            The number of steps to move from the center point. nb_steps=1 results in a total of\n            5 output points (1 center point + 4 neighbours).\n\n        step_size : number\n            The step size to move from every point to its neighbours.\n\n        return_array : bool, optional\n            Whether to return the generated points as a list of keypoints or an array\n            of shape ``(N,2)``, where ``N`` is the number of generated points and the second axis contains\n            the x- (first value) and y- (second value) coordinates.\n\n        Returns\n        -------\n        points : list of imgaug.Keypoint or (N,2) ndarray\n            If return_array was False, then a list of Keypoint.\n            Otherwise a numpy array of shape ``(N,2)``, where ``N`` is the number of generated points and\n            the second axis contains the x- (first value) and y- (second value) coordinates.\n            The center keypoint (the one on which this function was called) is always included.\n\n        \"\"\"\n        # TODO add test\n        # Points generates in manhattan style with S steps have a shape similar to a 45deg rotated\n        # square. The center line with the origin point has S+1+S = 1+2*S points (S to the left,\n        # S to the right). The lines above contain (S+1+S)-2 + (S+1+S)-2-2 + ... + 1 points. E.g.\n        # for S=2 it would be 3+1=4 and for S=3 it would be 5+3+1=9. Same for the lines below the\n        # center. Hence the total number of points is S+1+S + 2*(S^2).\n        points = np.zeros(\n            (nb_steps + 1 + nb_steps + 2 * (nb_steps ** 2), 2), dtype=np.float32\n        )\n\n        # we start at the bottom-most line and move towards the top-most line\n        yy = np.linspace(\n            self.y - nb_steps * step_size,\n            self.y + nb_steps * step_size,\n            nb_steps + 1 + nb_steps,\n        )\n\n        # bottom-most line contains only one point\n        width = 1\n\n        nth_point = 0\n        for i_y, y in enumerate(yy):\n            if width == 1:\n                xx = [self.x]\n            else:\n                xx = np.linspace(\n                    self.x - (width - 1) // 2 * step_size,\n                    self.x + (width - 1) // 2 * step_size,\n                    width,\n                )\n            for x in xx:\n                points[nth_point] = [x, y]\n                nth_point += 1\n            if i_y < nb_steps:\n                width += 2\n            else:\n                width -= 2\n\n        if return_array:\n            return points\n        return [\n            self.deepcopy(x=points[i, 0], y=points[i, 1])\n            for i in sm.xrange(points.shape[0])\n        ]\n\n    def copy(self, x=None, y=None):\n        \"\"\"\n        Create a shallow copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Shallow copy.\n\n        \"\"\"\n        return self.deepcopy(x=x, y=y)\n\n    def deepcopy(self, x=None, y=None):\n        \"\"\"\n        Create a deep copy of the Keypoint object.\n\n        Parameters\n        ----------\n        x : None or number, optional\n            Coordinate of the keypoint on the x axis.\n            If ``None``, the instance's value will be copied.\n\n        y : None or number, optional\n            Coordinate of the keypoint on the y axis.\n            If ``None``, the instance's value will be copied.\n\n        Returns\n        -------\n        imgaug.Keypoint\n            Deep copy.\n\n        \"\"\"\n        x = self.x if x is None else x\n        y = self.y if y is None else y\n        return Keypoint(x=x, y=y)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"Keypoint(x=%.8f, y=%.8f)\" % (self.x, self.y)\n\n\nclass KeypointsOnImage(object):\n    \"\"\"\n    Object that represents all keypoints on a single image.\n\n    Parameters\n    ----------\n    keypoints : list of imgaug.Keypoint\n        List of keypoints on the image.\n\n    shape : tuple of int\n        The shape of the image on which the keypoints are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((70, 70))\n    >>> kps = [Keypoint(x=10, y=20), Keypoint(x=34, y=60)]\n    >>> kps_oi = KeypointsOnImage(kps, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, keypoints, shape):\n        self.keypoints = keypoints\n        self.shape = normalize_shape(shape)\n\n    @property\n    def height(self):\n        return self.shape[0]\n\n    @property\n    def width(self):\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero keypoints.\n\n        Returns\n        -------\n        result : bool\n            True if this object contains zero keypoints.\n\n        \"\"\"\n        return len(self.keypoints) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project keypoints from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the keypoints are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        keypoints : imgaug.KeypointsOnImage\n            Object containing all projected keypoints.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        else:\n            keypoints = [kp.project(self.shape, shape) for kp in self.keypoints]\n            return self.deepcopy(keypoints, shape)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=3,\n        copy=True,\n        raise_if_out_of_image=False,\n    ):\n        \"\"\"\n        Draw all keypoints onto a given image.\n\n        Each keypoint is marked by a square of a chosen color and size.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the keypoints.\n            This image should usually have the same shape as\n            set in KeypointsOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all keypoints. If a single int ``C``, then that is\n            equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            The opacity of the drawn keypoint, where ``1.0`` denotes a fully\n            visible keypoint and ``0.0`` an invisible one.\n\n        size : int, optional\n            The size of each point. If set to ``C``, each square will have\n            size ``C x C``.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the points.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any keypoint is outside of the image.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn keypoints.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n        for keypoint in self.keypoints:\n            image = keypoint.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n            )\n        return image\n\n    def shift(self, x=0, y=0):\n        \"\"\"\n        Move the keypoints around on an image.\n\n        Parameters\n        ----------\n        x : number, optional\n            Move each keypoint by this value on the x axis.\n\n        y : number, optional\n            Move each keypoint by this value on the y axis.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            Keypoints after moving them.\n\n        \"\"\"\n        keypoints = [keypoint.shift(x=x, y=y) for keypoint in self.keypoints]\n        return self.deepcopy(keypoints)\n\n    @ia.deprecated(alt_func=\"KeypointsOnImage.to_xy_array()\")\n    def get_coords_array(self):\n        \"\"\"\n        Convert the coordinates of all keypoints in this object to an array of shape (N,2).\n\n        Returns\n        -------\n        result : (N, 2) ndarray\n            Where N is the number of keypoints. Each first value is the\n            x coordinate, each second value is the y coordinate.\n\n        \"\"\"\n        return self.to_xy_array()\n\n    def to_xy_array(self):\n        \"\"\"\n        Convert keypoint coordinates to ``(N,2)`` array.\n\n        Returns\n        -------\n        (N, 2) ndarray\n            Array containing the coordinates of all keypoints.\n            Shape is ``(N,2)`` with coordinates in xy-form.\n\n        \"\"\"\n        result = np.zeros((len(self.keypoints), 2), dtype=np.float32)\n        for i, keypoint in enumerate(self.keypoints):\n            result[i, 0] = keypoint.x\n            result[i, 1] = keypoint.y\n        return result\n\n    @staticmethod\n    @ia.deprecated(alt_func=\"KeypointsOnImage.from_xy_array()\")\n    def from_coords_array(coords, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        coords : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image.\n            Each first entry ``coords[i, 0]`` is expected to be the x coordinate.\n            Each second entry ``coords[i, 1]`` is expected to be the y coordinate.\n\n        shape : tuple\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        return KeypointsOnImage.from_xy_array(coords, shape)\n\n    @classmethod\n    def from_xy_array(cls, xy, shape):\n        \"\"\"\n        Convert an array (N,2) with a given image shape to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        xy : (N, 2) ndarray\n            Coordinates of ``N`` keypoints on the original image, given\n            as ``(N,2)`` array of xy-coordinates.\n\n        shape : tuple of int or ndarray\n            Shape tuple of the image on which the keypoints are placed.\n\n        Returns\n        -------\n        KeypointsOnImage\n            KeypointsOnImage object that contains all keypoints from the array.\n\n        \"\"\"\n        keypoints = [Keypoint(x=coord[0], y=coord[1]) for coord in xy]\n        return KeypointsOnImage(keypoints, shape)\n\n    # TODO add to_gaussian_heatmaps(), from_gaussian_heatmaps()\n    def to_keypoint_image(self, size=1):\n        \"\"\"\n        Draws a new black image of shape ``(H,W,N)`` in which all keypoint coordinates are set to 255.\n        (H=shape height, W=shape width, N=number of keypoints)\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports the\n        augmentation of images.\n\n        Parameters\n        -------\n        size : int\n            Size of each (squared) point.\n\n        Returns\n        -------\n        image : (H,W,N) ndarray\n            Image in which the keypoints are marked. H is the height,\n            defined in KeypointsOnImage.shape[0] (analogous W). N is the\n            number of keypoints.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        image = np.zeros((height, width, len(self.keypoints)), dtype=np.uint8)\n        ia.do_assert(size % 2 != 0)\n        sizeh = max(0, (size - 1) // 2)\n        for i, keypoint in enumerate(self.keypoints):\n            # TODO for float values spread activation over several cells\n            # here and do voting at the end\n            y = keypoint.y_int\n            x = keypoint.x_int\n\n            x1 = np.clip(x - sizeh, 0, width - 1)\n            x2 = np.clip(x + sizeh + 1, 0, width)\n            y1 = np.clip(y - sizeh, 0, height - 1)\n            y2 = np.clip(y + sizeh + 1, 0, height)\n\n            if x1 < x2 and y1 < y2:\n                image[y1:y2, x1:x2, i] = 128\n            if 0 <= y < height and 0 <= x < width:\n                image[y, x, i] = 255\n        return image\n\n    @staticmethod\n    def from_keypoint_image(\n        image, if_not_found_coords={\"x\": -1, \"y\": -1}, threshold=1, nb_channels=None\n    ):  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        \"\"\"\n        Converts an image generated by ``to_keypoint_image()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        image : (H,W,N) ndarray\n            The keypoints image. N is the number of keypoints.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in `image`.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y`` with\n            each containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : int, optional\n            The search for keypoints works by searching for the argmax in\n            each channel. This parameters contains the minimum value that\n            the max must have in order to be viewed as a keypoint.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        out : KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(image.shape) == 3)\n        height, width, nb_keypoints = image.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            maxidx_flat = np.argmax(image[..., i])\n            maxidx_ndim = np.unravel_index(maxidx_flat, (height, width))\n            found = image[maxidx_ndim[0], maxidx_ndim[1], i] >= threshold\n            if found:\n                keypoints.append(Keypoint(x=maxidx_ndim[1], y=maxidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def to_distance_maps(self, inverted=False):\n        \"\"\"\n        Generates a ``(H,W,K)`` output containing ``K`` distance maps for ``K`` keypoints.\n\n        The k-th distance map contains at every location ``(y, x)`` the euclidean distance to the k-th keypoint.\n\n        This function can be used as a helper when augmenting keypoints with a method that only supports\n        the augmentation of images.\n\n        Parameters\n        -------\n        inverted : bool, optional\n            If True, inverted distance maps are returned where each distance value d is replaced\n            by ``d/(d+1)``, i.e. the distance maps have values in the range ``(0.0, 1.0]`` with 1.0\n            denoting exactly the position of the respective keypoint.\n\n        Returns\n        -------\n        distance_maps : (H,W,K) ndarray\n            A ``float32`` array containing ``K`` distance maps for ``K`` keypoints. Each location\n            ``(y, x, k)`` in the array denotes the euclidean distance at ``(y, x)`` to the ``k``-th keypoint.\n            In inverted mode the distance ``d`` is replaced by ``d/(d+1)``. The height and width\n            of the array match the height and width in ``KeypointsOnImage.shape``.\n\n        \"\"\"\n        ia.do_assert(len(self.keypoints) > 0)\n        height, width = self.shape[0:2]\n        distance_maps = np.zeros((height, width, len(self.keypoints)), dtype=np.float32)\n\n        yy = np.arange(0, height)\n        xx = np.arange(0, width)\n        grid_xx, grid_yy = np.meshgrid(xx, yy)\n\n        for i, keypoint in enumerate(self.keypoints):\n            y, x = keypoint.y, keypoint.x\n            distance_maps[:, :, i] = (grid_xx - x) ** 2 + (grid_yy - y) ** 2\n        distance_maps = np.sqrt(distance_maps)\n        if inverted:\n            return 1 / (distance_maps + 1)\n        return distance_maps\n\n    # TODO add option to if_not_found_coords to reuse old keypoint coords\n    @staticmethod\n    def from_distance_maps(\n        distance_maps,\n        inverted=False,\n        if_not_found_coords={\"x\": -1, \"y\": -1},\n        threshold=None,  # pylint: disable=locally-disabled, dangerous-default-value, line-too-long\n        nb_channels=None,\n    ):\n        \"\"\"\n        Converts maps generated by ``to_distance_maps()`` back to a KeypointsOnImage object.\n\n        Parameters\n        ----------\n        distance_maps : (H,W,N) ndarray\n            The distance maps. N is the number of keypoints.\n\n        inverted : bool, optional\n            Whether the given distance maps were generated in inverted or normal mode.\n\n        if_not_found_coords : tuple or list or dict or None, optional\n            Coordinates to use for keypoints that cannot be found in ``distance_maps``.\n            If this is a list/tuple, it must have two integer values.\n            If it is a dictionary, it must have the keys ``x`` and ``y``, with each\n            containing one integer value.\n            If this is None, then the keypoint will not be added to the final\n            KeypointsOnImage object.\n\n        threshold : float, optional\n            The search for keypoints works by searching for the argmin (non-inverted) or\n            argmax (inverted) in each channel. This parameters contains the maximum (non-inverted)\n            or minimum (inverted) value to accept in order to view a hit as a keypoint.\n            Use None to use no min/max.\n\n        nb_channels : None or int, optional\n            Number of channels of the image on which the keypoints are placed.\n            Some keypoint augmenters require that information.\n            If set to None, the keypoint's shape will be set\n            to ``(height, width)``, otherwise ``(height, width, nb_channels)``.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            The extracted keypoints.\n\n        \"\"\"\n        ia.do_assert(len(distance_maps.shape) == 3)\n        height, width, nb_keypoints = distance_maps.shape\n\n        drop_if_not_found = False\n        if if_not_found_coords is None:\n            drop_if_not_found = True\n            if_not_found_x = -1\n            if_not_found_y = -1\n        elif isinstance(if_not_found_coords, (tuple, list)):\n            ia.do_assert(len(if_not_found_coords) == 2)\n            if_not_found_x = if_not_found_coords[0]\n            if_not_found_y = if_not_found_coords[1]\n        elif isinstance(if_not_found_coords, dict):\n            if_not_found_x = if_not_found_coords[\"x\"]\n            if_not_found_y = if_not_found_coords[\"y\"]\n        else:\n            raise Exception(\n                \"Expected if_not_found_coords to be None or tuple or list or dict, got %s.\"\n                % (type(if_not_found_coords),)\n            )\n\n        keypoints = []\n        for i in sm.xrange(nb_keypoints):\n            # TODO introduce voting here among all distance values that have min/max values\n            if inverted:\n                hitidx_flat = np.argmax(distance_maps[..., i])\n            else:\n                hitidx_flat = np.argmin(distance_maps[..., i])\n            hitidx_ndim = np.unravel_index(hitidx_flat, (height, width))\n            if not inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] < threshold\n            elif inverted and threshold is not None:\n                found = distance_maps[hitidx_ndim[0], hitidx_ndim[1], i] >= threshold\n            else:\n                found = True\n            if found:\n                keypoints.append(Keypoint(x=hitidx_ndim[1], y=hitidx_ndim[0]))\n            else:\n                if drop_if_not_found:\n                    pass  # dont add the keypoint to the result list, i.e. drop it\n                else:\n                    keypoints.append(Keypoint(x=if_not_found_x, y=if_not_found_y))\n\n        out_shape = (height, width)\n        if nb_channels is not None:\n            out_shape += (nb_channels,)\n        return KeypointsOnImage(keypoints, shape=out_shape)\n\n    def copy(self, keypoints=None, shape=None):\n        \"\"\"\n        Create a shallow copy of the KeypointsOnImage object.\n\n        Parameters\n        ----------\n        keypoints : None or list of imgaug.Keypoint, optional\n            List of keypoints on the image. If ``None``, the instance's\n            keypoints will be copied.\n\n        shape : tuple of int, optional\n            The shape of the image on which the keypoints are placed.\n            If ``None``, the instance's shape will be copied.\n\n        Returns\n        -------\n        imgaug.KeypointsOnImage\n            Shallow copy.\n\n        \"\"\"\n        result = copy.copy(self)\n        if keypoints is not None:\n            result.keypoints = keypoints\n        if shape is not None:\n            result.shape = shape\n        return result\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"KeypointsOnImage(%s, shape=%s)\" % (str(self.keypoints), self.shape)\n", "levels": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import scipy.spatial.distance", "import six.moves as sm", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["def compute_geometric_median(X, eps=1e-5):\n", "class Keypoint(object):\n", "    def __init__(self, x, y):\n", "    def x_int(self):\n", "    def y_int(self):\n", "    def project(self, from_shape, to_shape):\n", "    def shift(self, x=0, y=0):\n", "    def copy(self, x=None, y=None):\n", "    def deepcopy(self, x=None, y=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class KeypointsOnImage(object):\n", "    def __init__(self, keypoints, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def shift(self, x=0, y=0):\n", "    def get_coords_array(self):\n", "    def to_xy_array(self):\n", "    def from_coords_array(coords, shape):\n", "    def from_xy_array(cls, xy, shape):\n", "    def to_keypoint_image(self, size=1):\n", "    def to_distance_maps(self, inverted=False):\n", "    def copy(self, keypoints=None, shape=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.project", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.", "docstring_tokens": ["Project", "the", "bounding", "box", "onto", "a", "differently", "shaped", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L198-L231", "partition": "valid", "up_fun_num": 12, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.extend", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.", "docstring_tokens": ["Extend", "the", "size", "of", "the", "bounding", "box", "along", "its", "sides", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L233-L265", "partition": "valid", "up_fun_num": 13, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.intersection", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.", "docstring_tokens": ["Compute", "the", "intersection", "bounding", "box", "of", "this", "bounding", "box", "and", "another", "one", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L267-L296", "partition": "valid", "up_fun_num": 14, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.union", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.", "docstring_tokens": ["Compute", "the", "union", "bounding", "box", "of", "this", "bounding", "box", "and", "another", "one", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L298-L321", "partition": "valid", "up_fun_num": 15, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.iou", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.", "docstring_tokens": ["Compute", "the", "IoU", "of", "this", "bounding", "box", "with", "another", "one", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L323-L348", "partition": "valid", "up_fun_num": 16, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.is_fully_within_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.", "docstring_tokens": ["Estimate", "whether", "the", "bounding", "box", "is", "fully", "inside", "the", "image", "area", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L350-L370", "partition": "valid", "up_fun_num": 17, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.is_partly_within_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.", "docstring_tokens": ["Estimate", "whether", "the", "bounding", "box", "is", "at", "least", "partially", "inside", "the", "image", "area", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L372-L394", "partition": "valid", "up_fun_num": 18, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.is_out_of_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.", "docstring_tokens": ["Estimate", "whether", "the", "bounding", "box", "is", "partially", "or", "fully", "outside", "of", "the", "image", "area", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L396-L425", "partition": "valid", "up_fun_num": 19, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.clip_out_of_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.", "docstring_tokens": ["Clip", "off", "all", "parts", "of", "the", "bounding", "box", "that", "are", "outside", "of", "the", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L433-L468", "partition": "valid", "up_fun_num": 21, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.draw_on_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.", "docstring_tokens": ["Draw", "the", "bounding", "box", "on", "an", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L507-L591", "partition": "valid", "up_fun_num": 23, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.extract_from_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.", "docstring_tokens": ["Extract", "the", "image", "pixels", "within", "the", "bounding", "box", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L594-L714", "partition": "valid", "up_fun_num": 23, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBox.copy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.", "docstring_tokens": ["Create", "a", "shallow", "copy", "of", "the", "BoundingBox", "object", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L738-L771", "partition": "valid", "up_fun_num": 24, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBoxesOnImage.draw_on_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.", "docstring_tokens": ["Draw", "all", "bounding", "boxes", "onto", "a", "given", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L954-L1005", "partition": "valid", "up_fun_num": 36, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBoxesOnImage.remove_out_of_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.", "docstring_tokens": ["Remove", "all", "bounding", "boxes", "that", "are", "fully", "or", "partially", "outside", "of", "the", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L1007-L1028", "partition": "valid", "up_fun_num": 36, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBoxesOnImage.clip_out_of_image", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.", "docstring_tokens": ["Clip", "off", "all", "parts", "from", "all", "bounding", "boxes", "that", "are", "outside", "of", "the", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L1036-L1048", "partition": "valid", "up_fun_num": 38, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def deepcopy(self):\n        \"\"\"\n        Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.\n\n        \"\"\"\n        # Manual copy is far faster than deepcopy for BoundingBoxesOnImage,\n        # so use manual copy here too\n        bbs = [bb.deepcopy() for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bbs, tuple(self.shape))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def deepcopy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/bbs.py", "func_name": "BoundingBoxesOnImage.deepcopy", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create a deep copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Deep copy.", "docstring_tokens": ["Create", "a", "deep", "copy", "of", "the", "BoundingBoxesOnImage", "object", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/bbs.py#L1089-L1102", "partition": "valid", "up_fun_num": 41, "context": "from __future__ import print_function, division, absolute_import\n\nimport copy\n\nimport numpy as np\nimport skimage.draw\nimport skimage.measure\n\nfrom .. import imgaug as ia\nfrom .utils import normalize_shape, project_coords\n\n\n# TODO functions: square(), to_aspect_ratio(), contains_point()\nclass BoundingBox(object):\n    \"\"\"\n    Class representing bounding boxes.\n\n    Each bounding box is parameterized by its top left and bottom right corners. Both are given\n    as x and y-coordinates. The corners are intended to lie inside the bounding box area.\n    As a result, a bounding box that lies completely inside the image but has maximum extensions\n    would have coordinates ``(0.0, 0.0)`` and ``(W - epsilon, H - epsilon)``. Note that coordinates\n    are saved internally as floats.\n\n    Parameters\n    ----------\n    x1 : number\n        X-coordinate of the top left of the bounding box.\n\n    y1 : number\n        Y-coordinate of the top left of the bounding box.\n\n    x2 : number\n        X-coordinate of the bottom right of the bounding box.\n\n    y2 : number\n        Y-coordinate of the bottom right of the bounding box.\n\n    label : None or str, optional\n        Label of the bounding box, e.g. a string representing the class.\n\n    \"\"\"\n\n    def __init__(self, x1, y1, x2, y2, label=None):\n        \"\"\"Create a new BoundingBox instance.\"\"\"\n        if x1 > x2:\n            x2, x1 = x1, x2\n        ia.do_assert(x2 >= x1)\n        if y1 > y2:\n            y2, y1 = y1, y2\n        ia.do_assert(y2 >= y1)\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.label = label\n\n    @property\n    def x1_int(self):\n        \"\"\"\n        Return the x-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y1_int(self):\n        \"\"\"\n        Return the y-coordinate of the top left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the top left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y1)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def x2_int(self):\n        \"\"\"\n        Return the x-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            X-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.x2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def y2_int(self):\n        \"\"\"\n        Return the y-coordinate of the bottom left corner as an integer.\n\n        Returns\n        -------\n        int\n            Y-coordinate of the bottom left corner, rounded to the closest integer.\n\n        \"\"\"\n        return int(\n            np.round(self.y2)\n        )  # use numpy's round to have consistent behaviour between python versions\n\n    @property\n    def height(self):\n        \"\"\"\n        Estimate the height of the bounding box.\n\n        Returns\n        -------\n        number\n            Height of the bounding box.\n\n        \"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def width(self):\n        \"\"\"\n        Estimate the width of the bounding box.\n\n        Returns\n        -------\n        number\n            Width of the bounding box.\n\n        \"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def center_x(self):\n        \"\"\"\n        Estimate the x-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            X-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.x1 + self.width / 2\n\n    @property\n    def center_y(self):\n        \"\"\"\n        Estimate the y-coordinate of the center point of the bounding box.\n\n        Returns\n        -------\n        number\n            Y-coordinate of the center point of the bounding box.\n\n        \"\"\"\n        return self.y1 + self.height / 2\n\n    @property\n    def area(self):\n        \"\"\"\n        Estimate the area of the bounding box.\n\n        Returns\n        -------\n        number\n            Area of the bounding box, i.e. `height * width`.\n\n        \"\"\"\n        return self.height * self.width\n\n    # TODO add test for tuple of number\n    def contains(self, other):\n        \"\"\"\n        Estimate whether the bounding box contains a point.\n\n        Parameters\n        ----------\n        other : tuple of number or imgaug.Keypoint\n            Point to check for.\n\n        Returns\n        -------\n        bool\n            True if the point is contained in the bounding box, False otherwise.\n\n        \"\"\"\n        if isinstance(other, tuple):\n            x, y = other\n        else:\n            x, y = other.x, other.y\n        return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2\n\n    # TODO add tests for ndarray inputs\n    def project(self, from_shape, to_shape):\n        \"\"\"\n        Project the bounding box onto a differently shaped image.\n\n        E.g. if the bounding box is on its original image at\n        x1=(10 of 100 pixels) and y1=(20 of 100 pixels) and is projected onto\n        a new image with size (width=200, height=200), its new position will\n        be (x1=20, y1=40). (Analogous for x2/y2.)\n\n        This is intended for cases where the original image is resized.\n        It cannot be used for more complex changes (e.g. padding, cropping).\n\n        Parameters\n        ----------\n        from_shape : tuple of int or ndarray\n            Shape of the original image. (Before resize.)\n\n        to_shape : tuple of int or ndarray\n            Shape of the new image. (After resize.)\n\n        Returns\n        -------\n        out : imgaug.BoundingBox\n            BoundingBox object with new coordinates.\n\n        \"\"\"\n        coords_proj = project_coords(\n            [(self.x1, self.y1), (self.x2, self.y2)], from_shape, to_shape\n        )\n        return self.copy(\n            x1=coords_proj[0][0],\n            y1=coords_proj[0][1],\n            x2=coords_proj[1][0],\n            y2=coords_proj[1][1],\n            label=self.label,\n        )\n\n    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n        \"\"\"\n        Extend the size of the bounding box along its sides.\n\n        Parameters\n        ----------\n        all_sides : number, optional\n            Value by which to extend the bounding box size along all sides.\n\n        top : number, optional\n            Value by which to extend the bounding box size along its top side.\n\n        right : number, optional\n            Value by which to extend the bounding box size along its right side.\n\n        bottom : number, optional\n            Value by which to extend the bounding box size along its bottom side.\n\n        left : number, optional\n            Value by which to extend the bounding box size along its left side.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Extended bounding box.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 - all_sides - left,\n            x2=self.x2 + all_sides + right,\n            y1=self.y1 - all_sides - top,\n            y2=self.y2 + all_sides + bottom,\n        )\n\n    def intersection(self, other, default=None):\n        \"\"\"\n        Compute the intersection bounding box of this bounding box and another one.\n\n        Note that in extreme cases, the intersection can be a single point, meaning that the intersection bounding box\n        will exist, but then also has a height and width of zero.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the intersection.\n\n        default : any, optional\n            Default value to return if there is no intersection.\n\n        Returns\n        -------\n        imgaug.BoundingBox or any\n            Intersection bounding box of the two bounding boxes if there is an intersection.\n            If there is no intersection, the default value will be returned, which can by anything.\n\n        \"\"\"\n        x1_i = max(self.x1, other.x1)\n        y1_i = max(self.y1, other.y1)\n        x2_i = min(self.x2, other.x2)\n        y2_i = min(self.y2, other.y2)\n        if x1_i > x2_i or y1_i > y2_i:\n            return default\n        else:\n            return BoundingBox(x1=x1_i, y1=y1_i, x2=x2_i, y2=y2_i)\n\n    def union(self, other):\n        \"\"\"\n        Compute the union bounding box of this bounding box and another one.\n\n        This is equivalent to drawing a bounding box around all corners points of both\n        bounding boxes.\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to generate the union.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Union bounding box of the two bounding boxes.\n\n        \"\"\"\n        return BoundingBox(\n            x1=min(self.x1, other.x1),\n            y1=min(self.y1, other.y1),\n            x2=max(self.x2, other.x2),\n            y2=max(self.y2, other.y2),\n        )\n\n    def iou(self, other):\n        \"\"\"\n        Compute the IoU of this bounding box with another one.\n\n        IoU is the intersection over union, defined as::\n\n            ``area(intersection(A, B)) / area(union(A, B))``\n            ``= area(intersection(A, B)) / (area(A) + area(B) - area(intersection(A, B)))``\n\n        Parameters\n        ----------\n        other : imgaug.BoundingBox\n            Other bounding box with which to compare.\n\n        Returns\n        -------\n        float\n            IoU between the two bounding boxes.\n\n        \"\"\"\n        inters = self.intersection(other)\n        if inters is None:\n            return 0.0\n        else:\n            area_union = self.area + other.area - inters.area\n            return inters.area / area_union if area_union > 0 else 0.0\n\n    def is_fully_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is fully inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is fully inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        return self.x1 >= 0 and self.x2 < width and self.y1 >= 0 and self.y2 < height\n\n    def is_partly_within_image(self, image):\n        \"\"\"\n        Estimate whether the bounding box is at least partially inside the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape\n            and must contain at least two integers.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is at least partially inside the image area. False otherwise.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        height, width = shape[0:2]\n        eps = np.finfo(np.float32).eps\n        img_bb = BoundingBox(x1=0, x2=width - eps, y1=0, y2=height - eps)\n        return self.intersection(img_bb) is not None\n\n    def is_out_of_image(self, image, fully=True, partly=False):\n        \"\"\"\n        Estimate whether the bounding box is partially or fully outside of the image area.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use. If an ndarray, its shape will be used. If a tuple, it is\n            assumed to represent the image shape and must contain at least two integers.\n\n        fully : bool, optional\n            Whether to return True if the bounding box is fully outside fo the image area.\n\n        partly : bool, optional\n            Whether to return True if the bounding box is at least partially outside fo the\n            image area.\n\n        Returns\n        -------\n        bool\n            True if the bounding box is partially/fully outside of the image area, depending\n            on defined parameters. False otherwise.\n\n        \"\"\"\n        if self.is_fully_within_image(image):\n            return False\n        elif self.is_partly_within_image(image):\n            return partly\n        else:\n            return fully\n\n    @ia.deprecated(\n        alt_func=\"BoundingBox.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self, *args, **kwargs):\n        return self.clip_out_of_image(*args, **kwargs)\n\n    def clip_out_of_image(self, image):\n        \"\"\"\n        Clip off all parts of the bounding box that are outside of the image.\n\n        Parameters\n        ----------\n        image : (H,W,...) ndarray or tuple of int\n            Image dimensions to use for the clipping of the bounding box.\n            If an ndarray, its shape will be used.\n            If a tuple, it is assumed to represent the image shape and must contain at least two integers.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Bounding box, clipped to fall within the image dimensions.\n\n        \"\"\"\n        shape = normalize_shape(image)\n\n        height, width = shape[0:2]\n        ia.do_assert(height > 0)\n        ia.do_assert(width > 0)\n\n        eps = np.finfo(np.float32).eps\n        x1 = np.clip(self.x1, 0, width - eps)\n        x2 = np.clip(self.x2, 0, width - eps)\n        y1 = np.clip(self.y1, 0, height - eps)\n        y2 = np.clip(self.y2, 0, height - eps)\n\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=self.label)\n\n    # TODO convert this to x/y params?\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift the bounding box from one or more image sides, i.e. move it on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift the bounding box from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift the bounding box from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift the bounding box from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift the bounding box from the left.\n\n        Returns\n        -------\n        result : imgaug.BoundingBox\n            Shifted bounding box.\n\n        \"\"\"\n        top = top if top is not None else 0\n        right = right if right is not None else 0\n        bottom = bottom if bottom is not None else 0\n        left = left if left is not None else 0\n        return self.copy(\n            x1=self.x1 + left - right,\n            x2=self.x2 + left - right,\n            y1=self.y1 + top - bottom,\n            y2=self.y2 + top - bottom,\n        )\n\n    # TODO add explicit test for zero-sized BBs (worked when tested by hand)\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw the bounding box on an image.\n\n        Parameters\n        ----------\n        image : (H,W,C) ndarray(uint8)\n            The image onto which to draw the bounding box.\n\n        color : iterable of int, optional\n            The color to use, corresponding to the channel layout of the image. Usually RGB.\n\n        alpha : float, optional\n            The transparency of the drawn bounding box, where 1.0 denotes no transparency and\n            0.0 is invisible.\n\n        size : int, optional\n            The thickness of the bounding box in pixels. If the value is larger than 1, then\n            additional pixels will be added around the bounding box (i.e. extension towards the\n            outside).\n\n        copy : bool, optional\n            Whether to copy the input image or change it in-place.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an error if the bounding box is fully outside of the\n            image. If set to False, no error will be raised and only the parts inside the image\n            will be drawn.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        result : (H,W,C) ndarray(uint8)\n            Image with bounding box drawn on it.\n\n        \"\"\"\n        if thickness is not None:\n            ia.warn_deprecated(\n                \"Usage of argument 'thickness' in BoundingBox.draw_on_image() \"\n                \"is deprecated. The argument was renamed to 'size'.\"\n            )\n            size = thickness\n\n        if raise_if_out_of_image and self.is_out_of_image(image):\n            raise Exception(\n                \"Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s.\"\n                % (self.x1, self.y1, self.x2, self.y2, image.shape)\n            )\n\n        result = np.copy(image) if copy else image\n\n        if isinstance(color, (tuple, list)):\n            color = np.uint8(color)\n\n        for i in range(size):\n            y1, y2, x1, x2 = self.y1_int, self.y2_int, self.x1_int, self.x2_int\n\n            # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n            # That is technically sensible, but in the case of drawing means that the border lies\n            # just barely outside of the image, making the border disappear, even though the BB\n            # is fully inside the image. Here we correct for that because of beauty reasons.\n            # Same is the case for x coordinates.\n            if self.is_fully_within_image(image):\n                y1 = np.clip(y1, 0, image.shape[0] - 1)\n                y2 = np.clip(y2, 0, image.shape[0] - 1)\n                x1 = np.clip(x1, 0, image.shape[1] - 1)\n                x2 = np.clip(x2, 0, image.shape[1] - 1)\n\n            y = [y1 - i, y1 - i, y2 + i, y2 + i]\n            x = [x1 - i, x2 + i, x2 + i, x1 - i]\n            rr, cc = skimage.draw.polygon_perimeter(y, x, shape=result.shape)\n            if alpha >= 0.99:\n                result[rr, cc, :] = color\n            else:\n                if ia.is_float_array(result):\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255)\n                else:\n                    input_dtype = result.dtype\n                    result = result.astype(np.float32)\n                    result[rr, cc, :] = (1 - alpha) * result[rr, cc, :] + alpha * color\n                    result = np.clip(result, 0, 255).astype(input_dtype)\n\n        return result\n\n    # TODO add tests for pad and pad_max\n    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n        \"\"\"\n        Extract the image pixels within the bounding box.\n\n        This function will zero-pad the image if the bounding box is partially/fully outside of\n        the image.\n\n        Parameters\n        ----------\n        image : (H,W) ndarray or (H,W,C) ndarray\n            The image from which to extract the pixels within the bounding box.\n\n        pad : bool, optional\n            Whether to zero-pad the image if the object is partially/fully\n            outside of it.\n\n        pad_max : None or int, optional\n            The maximum number of pixels that may be zero-paded on any side,\n            i.e. if this has value ``N`` the total maximum of added pixels\n            is ``4*N``.\n            This option exists to prevent extremely large images as a result of\n            single points being moved very far away during augmentation.\n\n        prevent_zero_size : bool, optional\n            Whether to prevent height or width of the extracted image from becoming zero.\n            If this is set to True and height or width of the bounding box is below 1, the height/width will\n            be increased to 1. This can be useful to prevent problems, e.g. with image saving or plotting.\n            If it is set to False, images will be returned as ``(H', W')`` or ``(H', W', 3)`` with ``H`` or\n            ``W`` potentially being 0.\n\n        Returns\n        -------\n        image : (H',W') ndarray or (H',W',C) ndarray\n            Pixels within the bounding box. Zero-padded if the bounding box is partially/fully\n            outside of the image. If prevent_zero_size is activated, it is guarantueed that ``H'>0``\n            and ``W'>0``, otherwise only ``H'>=0`` and ``W'>=0``.\n\n        \"\"\"\n        pad_top = 0\n        pad_right = 0\n        pad_bottom = 0\n        pad_left = 0\n\n        height, width = image.shape[0], image.shape[1]\n        x1, x2, y1, y2 = self.x1_int, self.x2_int, self.y1_int, self.y2_int\n\n        # When y values get into the range (H-0.5, H), the *_int functions round them to H.\n        # That is technically sensible, but in the case of extraction leads to a black border,\n        # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for\n        # that because of beauty reasons.\n        # Same is the case for x coordinates.\n        fully_within = self.is_fully_within_image(image)\n        if fully_within:\n            y1, y2 = np.clip([y1, y2], 0, height - 1)\n            x1, x2 = np.clip([x1, x2], 0, width - 1)\n\n        # TODO add test\n        if prevent_zero_size:\n            if abs(x2 - x1) < 1:\n                x2 = x1 + 1\n            if abs(y2 - y1) < 1:\n                y2 = y1 + 1\n\n        if pad:\n            # if the bb is outside of the image area, the following pads the image\n            # first with black pixels until the bb is inside the image\n            # and only then extracts the image area\n            # TODO probably more efficient to initialize an array of zeros\n            # and copy only the portions of the bb into that array that are\n            # natively inside the image area\n            if x1 < 0:\n                pad_left = abs(x1)\n                x2 = x2 + pad_left\n                width = width + pad_left\n                x1 = 0\n            if y1 < 0:\n                pad_top = abs(y1)\n                y2 = y2 + pad_top\n                height = height + pad_top\n                y1 = 0\n            if x2 >= width:\n                pad_right = x2 - width\n            if y2 >= height:\n                pad_bottom = y2 - height\n\n            paddings = [pad_top, pad_right, pad_bottom, pad_left]\n            any_padded = any([val > 0 for val in paddings])\n            if any_padded:\n                if pad_max is None:\n                    pad_max = max(paddings)\n\n                image = ia.pad(\n                    image,\n                    top=min(pad_top, pad_max),\n                    right=min(pad_right, pad_max),\n                    bottom=min(pad_bottom, pad_max),\n                    left=min(pad_left, pad_max),\n                )\n            return image[y1:y2, x1:x2]\n        else:\n            within_image = (\n                (0, 0, 0, 0) <= (x1, y1, x2, y2) < (width, height, width, height)\n            )\n            out_height, out_width = (y2 - y1), (x2 - x1)\n            nonzero_height = out_height > 0\n            nonzero_width = out_width > 0\n            if within_image and nonzero_height and nonzero_width:\n                return image[y1:y2, x1:x2]\n            if prevent_zero_size:\n                out_height = 1\n                out_width = 1\n            else:\n                out_height = 0\n                out_width = 0\n            if image.ndim == 2:\n                return np.zeros((out_height, out_width), dtype=image.dtype)\n            return np.zeros((out_height, out_width, image.shape[-1]), dtype=image.dtype)\n\n    # TODO also add to_heatmap\n    # TODO add this to BoundingBoxesOnImage\n    def to_keypoints(self):\n        \"\"\"\n        Convert the corners of the bounding box to keypoints (clockwise, starting at top left).\n\n        Returns\n        -------\n        list of imgaug.Keypoint\n            Corners of the bounding box as keypoints.\n\n        \"\"\"\n        # TODO get rid of this deferred import\n        from imgaug.augmentables.kps import Keypoint\n\n        return [\n            Keypoint(x=self.x1, y=self.y1),\n            Keypoint(x=self.x2, y=self.y1),\n            Keypoint(x=self.x2, y=self.y2),\n            Keypoint(x=self.x1, y=self.y2),\n        ]\n\n    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a shallow copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Shallow copy.\n\n        \"\"\"\n        return BoundingBox(\n            x1=self.x1 if x1 is None else x1,\n            x2=self.x2 if x2 is None else x2,\n            y1=self.y1 if y1 is None else y1,\n            y2=self.y2 if y2 is None else y2,\n            label=self.label if label is None else label,\n        )\n\n    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n        \"\"\"\n        Create a deep copy of the BoundingBox object.\n\n        Parameters\n        ----------\n        x1 : None or number\n            If not None, then the x1 coordinate of the copied object will be set to this value.\n\n        y1 : None or number\n            If not None, then the y1 coordinate of the copied object will be set to this value.\n\n        x2 : None or number\n            If not None, then the x2 coordinate of the copied object will be set to this value.\n\n        y2 : None or number\n            If not None, then the y2 coordinate of the copied object will be set to this value.\n\n        label : None or string\n            If not None, then the label of the copied object will be set to this value.\n\n        Returns\n        -------\n        imgaug.BoundingBox\n            Deep copy.\n\n        \"\"\"\n        return self.copy(x1=x1, y1=y1, x2=x2, y2=y2, label=label)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBox(x1=%.4f, y1=%.4f, x2=%.4f, y2=%.4f, label=%s)\" % (\n            self.x1,\n            self.y1,\n            self.x2,\n            self.y2,\n            self.label,\n        )\n\n\nclass BoundingBoxesOnImage(object):\n    \"\"\"\n    Object that represents all bounding boxes on a single image.\n\n    Parameters\n    ----------\n    bounding_boxes : list of imgaug.BoundingBox\n        List of bounding boxes on the image.\n\n    shape : tuple of int\n        The shape of the image on which the bounding boxes are placed.\n\n    Examples\n    --------\n    >>> image = np.zeros((100, 100))\n    >>> bbs = [\n    >>>     BoundingBox(x1=10, y1=20, x2=20, y2=30),\n    >>>     BoundingBox(x1=25, y1=50, x2=30, y2=70)\n    >>> ]\n    >>> bbs_oi = BoundingBoxesOnImage(bbs, shape=image.shape)\n\n    \"\"\"\n\n    def __init__(self, bounding_boxes, shape):\n        self.bounding_boxes = bounding_boxes\n        self.shape = normalize_shape(shape)\n\n    # TODO remove this? here it is image height at BoundingBox it is bounding box height\n    @property\n    def height(self):\n        \"\"\"\n        Get the height of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image height.\n\n        \"\"\"\n        return self.shape[0]\n\n    # TODO remove this? here it is image width at BoundingBox it is bounding box width\n    @property\n    def width(self):\n        \"\"\"\n        Get the width of the image on which the bounding boxes fall.\n\n        Returns\n        -------\n        int\n            Image width.\n\n        \"\"\"\n        return self.shape[1]\n\n    @property\n    def empty(self):\n        \"\"\"\n        Returns whether this object contains zero bounding boxes.\n\n        Returns\n        -------\n        bool\n            True if this object contains zero bounding boxes.\n\n        \"\"\"\n        return len(self.bounding_boxes) == 0\n\n    def on(self, image):\n        \"\"\"\n        Project bounding boxes from one image to a new one.\n\n        Parameters\n        ----------\n        image : ndarray or tuple of int\n            New image onto which the bounding boxes are to be projected.\n            May also simply be that new image's shape tuple.\n\n        Returns\n        -------\n        bounding_boxes : imgaug.BoundingBoxesOnImage\n            Object containing all projected bounding boxes.\n\n        \"\"\"\n        shape = normalize_shape(image)\n        if shape[0:2] == self.shape[0:2]:\n            return self.deepcopy()\n        bounding_boxes = [bb.project(self.shape, shape) for bb in self.bounding_boxes]\n        return BoundingBoxesOnImage(bounding_boxes, shape)\n\n    @classmethod\n    def from_xyxy_array(cls, xyxy, shape):\n        \"\"\"\n        Convert an (N,4) ndarray to a BoundingBoxesOnImage object.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.to_xyxy_array`.\n\n        Parameters\n        ----------\n        xyxy : (N,4) ndarray\n            Array containing the corner coordinates (top-left, bottom-right) of ``N`` bounding boxes\n            in the form ``(x1, y1, x2, y2)``. Should usually be of dtype ``float32``.\n\n        shape : tuple of int\n            Shape of the image on which the bounding boxes are placed.\n            Should usually be ``(H, W, C)`` or ``(H, W)``.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Object containing a list of BoundingBox objects following the provided corner coordinates.\n\n        \"\"\"\n        ia.do_assert(\n            xyxy.shape[1] == 4,\n            \"Expected input array of shape (N, 4), got shape %s.\" % (xyxy.shape,),\n        )\n\n        boxes = [BoundingBox(*row) for row in xyxy]\n\n        return cls(boxes, shape)\n\n    def to_xyxy_array(self, dtype=np.float32):\n        \"\"\"\n        Convert the BoundingBoxesOnImage object to an (N,4) ndarray.\n\n        This is the inverse of :func:`imgaug.BoundingBoxesOnImage.from_xyxy_array`.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype, optional\n            Desired output datatype of the ndarray.\n\n        Returns\n        -------\n        ndarray\n            (N,4) ndarray array, where ``N`` denotes the number of bounding boxes and ``4`` denotes the\n            top-left and bottom-right bounding box corner coordinates in form ``(x1, y1, x2, y2)``.\n\n        \"\"\"\n        xyxy_array = np.zeros((len(self.bounding_boxes), 4), dtype=np.float32)\n\n        for i, box in enumerate(self.bounding_boxes):\n            xyxy_array[i] = [box.x1, box.y1, box.x2, box.y2]\n\n        return xyxy_array.astype(dtype)\n\n    def draw_on_image(\n        self,\n        image,\n        color=(0, 255, 0),\n        alpha=1.0,\n        size=1,\n        copy=True,\n        raise_if_out_of_image=False,\n        thickness=None,\n    ):\n        \"\"\"\n        Draw all bounding boxes onto a given image.\n\n        Parameters\n        ----------\n        image : (H,W,3) ndarray\n            The image onto which to draw the bounding boxes.\n            This image should usually have the same shape as\n            set in BoundingBoxesOnImage.shape.\n\n        color : int or list of int or tuple of int or (3,) ndarray, optional\n            The RGB color of all bounding boxes. If a single int ``C``, then\n            that is equivalent to ``(C,C,C)``.\n\n        alpha : float, optional\n            Alpha/transparency of the bounding box.\n\n        size : int, optional\n            Thickness in pixels.\n\n        copy : bool, optional\n            Whether to copy the image before drawing the bounding boxes.\n\n        raise_if_out_of_image : bool, optional\n            Whether to raise an exception if any bounding box is outside of the\n            image.\n\n        thickness : None or int, optional\n            Deprecated.\n\n        Returns\n        -------\n        image : (H,W,3) ndarray\n            Image with drawn bounding boxes.\n\n        \"\"\"\n        image = np.copy(image) if copy else image\n\n        for bb in self.bounding_boxes:\n            image = bb.draw_on_image(\n                image,\n                color=color,\n                alpha=alpha,\n                size=size,\n                copy=False,\n                raise_if_out_of_image=raise_if_out_of_image,\n                thickness=thickness,\n            )\n\n        return image\n\n    def remove_out_of_image(self, fully=True, partly=False):\n        \"\"\"\n        Remove all bounding boxes that are fully or partially outside of the image.\n\n        Parameters\n        ----------\n        fully : bool, optional\n            Whether to remove bounding boxes that are fully outside of the image.\n\n        partly : bool, optional\n            Whether to remove bounding boxes that are partially outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Reduced set of bounding boxes, with those that were fully/partially outside of\n            the image removed.\n\n        \"\"\"\n        bbs_clean = [\n            bb\n            for bb in self.bounding_boxes\n            if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)\n        ]\n        return BoundingBoxesOnImage(bbs_clean, shape=self.shape)\n\n    @ia.deprecated(\n        alt_func=\"BoundingBoxesOnImage.clip_out_of_image()\",\n        comment=\"clip_out_of_image() has the exactly same \" \"interface.\",\n    )\n    def cut_out_of_image(self):\n        return self.clip_out_of_image()\n\n    def clip_out_of_image(self):\n        \"\"\"\n        Clip off all parts from all bounding boxes that are outside of the image.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Bounding boxes, clipped to fall within the image dimensions.\n\n        \"\"\"\n        bbs_cut = [\n            bb.clip_out_of_image(self.shape)\n            for bb in self.bounding_boxes\n            if bb.is_partly_within_image(self.shape)\n        ]\n        return BoundingBoxesOnImage(bbs_cut, shape=self.shape)\n\n    def shift(self, top=None, right=None, bottom=None, left=None):\n        \"\"\"\n        Shift all bounding boxes from one or more image sides, i.e. move them on the x/y-axis.\n\n        Parameters\n        ----------\n        top : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the top.\n\n        right : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the right.\n\n        bottom : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the bottom.\n\n        left : None or int, optional\n            Amount of pixels by which to shift all bounding boxes from the left.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shifted bounding boxes.\n\n        \"\"\"\n        bbs_new = [\n            bb.shift(top=top, right=right, bottom=bottom, left=left)\n            for bb in self.bounding_boxes\n        ]\n        return BoundingBoxesOnImage(bbs_new, shape=self.shape)\n\n    def copy(self):\n        \"\"\"\n        Create a shallow copy of the BoundingBoxesOnImage object.\n\n        Returns\n        -------\n        imgaug.BoundingBoxesOnImage\n            Shallow copy.\n\n        \"\"\"\n        return copy.copy(self)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        return \"BoundingBoxesOnImage(%s, shape=%s)\" % (\n            str(self.bounding_boxes),\n            self.shape,\n        )\n", "levels": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import copy", "import numpy as np", "import skimage.draw", "import skimage.measure", "from .. import imgaug as ia", "from .utils import normalize_shape, project_coords"], "function": ["class BoundingBox(object):\n", "    def __init__(self, x1, y1, x2, y2, label=None):\n", "    def x1_int(self):\n", "    def y1_int(self):\n", "    def x2_int(self):\n", "    def y2_int(self):\n", "    def height(self):\n", "    def width(self):\n", "    def center_x(self):\n", "    def center_y(self):\n", "    def area(self):\n", "    def contains(self, other):\n", "    def project(self, from_shape, to_shape):\n", "    def extend(self, all_sides=0, top=0, right=0, bottom=0, left=0):\n", "    def intersection(self, other, default=None):\n", "    def union(self, other):\n", "    def iou(self, other):\n", "    def is_fully_within_image(self, image):\n", "    def is_partly_within_image(self, image):\n", "    def is_out_of_image(self, image, fully=True, partly=False):\n", "    def cut_out_of_image(self, *args, **kwargs):\n", "    def clip_out_of_image(self, image):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def extract_from_image(self, image, pad=True, pad_max=None, prevent_zero_size=True):\n", "    def to_keypoints(self):\n", "    def copy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def deepcopy(self, x1=None, y1=None, x2=None, y2=None, label=None):\n", "    def __repr__(self):\n", "    def __str__(self):\n", "class BoundingBoxesOnImage(object):\n", "    def __init__(self, bounding_boxes, shape):\n", "    def height(self):\n", "    def width(self):\n", "    def empty(self):\n", "    def on(self, image):\n", "    def from_xyxy_array(cls, xyxy, shape):\n", "    def to_xyxy_array(self, dtype=np.float32):\n", "    def remove_out_of_image(self, fully=True, partly=False):\n", "    def cut_out_of_image(self):\n", "    def clip_out_of_image(self):\n", "    def shift(self, top=None, right=None, bottom=None, left=None):\n", "    def copy(self):\n", "    def __repr__(self):\n", "    def __str__(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/convolutional.py", "func_name": "Emboss", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Augmenter that embosses images and overlays the result with the original\n    image.\n\n    The embossed version pronounces highlights and shadows,\n    letting the image look as if it was recreated on a metal plate (\"embossed\").\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    strength : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the strength of the embossing.\n        Sane values are somewhere in the range ``(0, 2)`` with 1 being the standard\n        embossing effect. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))\n\n    embosses an image with a variable strength in the range ``0.5 <= x <= 1.5``\n    and overlays the result with a variable alpha in the range ``0.0 <= a <= 1.0``\n    over the old image.", "docstring_tokens": ["Augmenter", "that", "embosses", "images", "and", "overlays", "the", "result", "with", "the", "original", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/convolutional.py#L296-L378", "partition": "valid", "up_fun_num": 9, "context": "\"\"\"\nAugmenters that apply convolutions to images.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead ::\n\n    from imgaug import augmenters as iaa\n\nand then e.g. ::\n\n    seq = iaa.Sequential([\n        iaa.Sharpen((0.0, 1.0)),\n        iaa.Emboss((0.0, 1.0))\n    ])\n\nList of augmenters:\n\n    * Convolve\n    * Sharpen\n    * Emboss\n    * EdgeDetect\n    * DirectedEdgeDetect\n\nFor MotionBlur, see ``blur.py``.\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport types\n\nimport numpy as np\nimport cv2\nimport six.moves as sm\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Convolve(meta.Augmenter):\n    \"\"\"\n    Apply a Convolution to input images.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: no (2)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (4)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (4)\n\n        - (1) rejected by ``cv2.filter2D()``.\n        - (2) causes error: cv2.error: OpenCV(3.4.2) (...)/filter.cpp:4487: error: (-213:The function/feature is\n              not implemented) Unsupported combination of source format (=1), and destination format (=1) in\n              function 'getLinearFilter'.\n        - (3) mapped internally to ``int16``.\n        - (4) mapped internally to ``float32``.\n\n    Parameters\n    ----------\n    matrix : None or (H, W) ndarray or imgaug.parameters.StochasticParameter or callable, optional\n        The weight matrix of the convolution kernel to apply.\n\n            * If None, the input images will not be changed.\n            * If a numpy array, that array will be used for all images and\n              channels as the kernel.\n            * If a callable, the parameter will be called for each image\n              via ``param(image, C, random_state)``. The function must either return\n              a list of ``C`` matrices (i.e. one per channel) or a 2D numpy array\n              (will be used for all channels) or a 3D ``HxWxC`` numpy array.\n              If a list is returned, each entry may be None, which will result\n              in no changes to the respective channel.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> matrix = np.array([[0, -1, 0],\n    >>>                    [-1, 4, -1],\n    >>>                    [0, -1, 0]])\n    >>> aug = iaa.Convolve(matrix=matrix)\n\n    convolves all input images with the kernel shown in the `matrix`\n    variable.\n\n    >>> def gen_matrix(image, nb_channels, random_state):\n    >>>     matrix_A = np.array([[0, -1, 0],\n    >>>                          [-1, 4, -1],\n    >>>                          [0, -1, 0]])\n    >>>     matrix_B = np.array([[0, 1, 0],\n    >>>                          [1, -4, 1],\n    >>>                          [0, 1, 0]])\n    >>>     if image.shape[0] % 2 == 0:\n    >>>         return [matrix_A] * nb_channels\n    >>>     else:\n    >>>         return [matrix_B] * nb_channels\n    >>> aug = iaa.Convolve(matrix=gen_matrix)\n\n    convolves images that have an even height with matrix A and images\n    with an odd height with matrix B.\n\n    \"\"\"\n\n    def __init__(self, matrix=None, name=None, deterministic=False, random_state=None):\n        super(Convolve, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        if matrix is None:\n            self.matrix = None\n            self.matrix_type = \"None\"\n        elif ia.is_np_array(matrix):\n            ia.do_assert(\n                len(matrix.shape) == 2,\n                \"Expected convolution matrix to have 2 axis, got %d (shape %s).\"\n                % (len(matrix.shape), matrix.shape),\n            )\n            self.matrix = matrix\n            self.matrix_type = \"constant\"\n        elif isinstance(matrix, types.FunctionType):\n            self.matrix = matrix\n            self.matrix_type = \"function\"\n        else:\n            raise Exception(\n                \"Expected float, int, tuple/list with 2 entries or StochasticParameter. Got %s.\"\n                % (type(matrix),)\n            )\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        seed = random_state.randint(0, 10 ** 6, 1)[0]\n        for i, image in enumerate(images):\n            _height, _width, nb_channels = images[i].shape\n\n            input_dtype = image.dtype\n            if image.dtype.type in [np.bool_, np.float16]:\n                image = image.astype(np.float32, copy=False)\n            elif image.dtype.type == np.int8:\n                image = image.astype(np.int16, copy=False)\n\n            if self.matrix_type == \"None\":\n                matrices = [None] * nb_channels\n            elif self.matrix_type == \"constant\":\n                matrices = [self.matrix] * nb_channels\n            elif self.matrix_type == \"function\":\n                matrices = self.matrix(\n                    images[i], nb_channels, ia.new_random_state(seed + i)\n                )\n                if ia.is_np_array(matrices) and matrices.ndim == 2:\n                    matrices = np.tile(matrices[..., np.newaxis], (1, 1, nb_channels))\n                ia.do_assert(\n                    (isinstance(matrices, list) and len(matrices) == nb_channels)\n                    or (\n                        ia.is_np_array(matrices)\n                        and matrices.ndim == 3\n                        and matrices.shape[2] == nb_channels\n                    ),\n                    \"Callable provided to Convole must return either a list of 2D matrices (one per image channel) \"\n                    \"or a 2D numpy array \"\n                    \"or a 3D numpy array where the last dimension's size matches the number of image channels. \"\n                    \"Got type %s.\" % (type(matrices),),\n                )\n\n                if ia.is_np_array(matrices):\n                    # Shape of matrices is currently (H, W, C), but in the loop below we need the\n                    # first axis to be the channel index to unify handling of lists of arrays\n                    # and arrays. So we move the channel axis here to the start.\n                    matrices = matrices.transpose((2, 0, 1))\n            else:\n                raise Exception(\"Invalid matrix type\")\n\n            image_aug = image\n            for channel in sm.xrange(nb_channels):\n                if matrices[channel] is not None:\n                    # ndimage.convolve caused problems here\n                    # cv2.filter2D() always returns same output dtype as input dtype\n                    image_aug[..., channel] = cv2.filter2D(\n                        image_aug[..., channel], -1, matrices[channel]\n                    )\n\n            if input_dtype == np.bool_:\n                image_aug = image_aug > 0.5\n            elif input_dtype in [np.int8, np.float16]:\n                image_aug = iadt.restore_dtypes_(image_aug, input_dtype)\n\n            images[i] = image_aug\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        # TODO this can fail for some matrices, e.g. [[0, 0, 1]]\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        # TODO this can fail for some matrices, e.g. [[0, 0, 1]]\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.matrix, self.matrix_type]\n\n\ndef Sharpen(alpha=0, lightness=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sharpens images and overlays the result with the original image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    lightness : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the lightness/brightness of the sharped image.\n        Sane values are somewhere in the range ``(0.5, 2)``.\n        The value 0 results in an edge map. Values higher than 1 create bright\n        images. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Sharpen(alpha=(0.0, 1.0))\n\n    sharpens input images and overlays the sharpened image by a variable\n    amount over the old image.\n\n    >>> aug = Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))\n\n    sharpens input images with a variable lightness in the range\n    ``0.75 <= x <= 2.0`` and with a variable alpha.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    lightness_param = iap.handle_continuous_param(\n        lightness,\n        \"lightness\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        lightness_sample = lightness_param.draw_sample(random_state=random_state_func)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [[-1, -1, -1], [-1, 8 + lightness_sample, -1], [-1, -1, -1]],\n            dtype=np.float32,\n        )\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\n# TODO tests\ndef EdgeDetect(alpha=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that detects all edges in images, marks them in\n    a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = EdgeDetect(alpha=(0.0, 1.0))\n\n    detects edges in an image  and overlays the result with a variable alpha\n    in the range ``0.0 <= a <= 1.0`` over the old image.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    def create_matrices(_image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=np.float32)\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\n# TODO tests\n# TODO merge EdgeDetect and DirectedEdgeDetect?\ndef DirectedEdgeDetect(\n    alpha=0, direction=(0.0, 1.0), name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that detects edges that have certain directions and marks them\n    in a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of edges to pronounce, where 0 represents 0 degrees and 1.0\n        represents 360 degrees (both clockwise, starting at the top).\n        Default value is ``(0.0, 1.0)``, i.e. pick a random angle per image.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=0)\n\n    turns input images into edge images in which edges are detected from\n    top side of the image (i.e. the top sides of horizontal edges are\n    added to the output).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=90/360)\n\n    same as before, but detecting edges from the right (right side of each\n    vertical edge).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=(0.0, 1.0))\n\n    same as before, but detecting edges from a variable direction (anything\n    between 0 and 1.0, i.e. 0 degrees and 360 degrees, starting from the\n    top and moving clockwise).\n\n    >>> aug = DirectedEdgeDetect(alpha=(0.0, 0.3), direction=0)\n\n    generates edge images (edges detected from the top) and overlays them\n    with the input images by a variable amount between 0 and 30 percent\n    (e.g. for 0.3 then ``0.7*old_image + 0.3*edge_image``).\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    direction_param = iap.handle_continuous_param(\n        direction,\n        \"direction\",\n        value_range=None,\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(_image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        direction_sample = direction_param.draw_sample(random_state=random_state_func)\n\n        deg = int(direction_sample * 360) % 360\n        rad = np.deg2rad(deg)\n        x = np.cos(rad - 0.5 * np.pi)\n        y = np.sin(rad - 0.5 * np.pi)\n        direction_vector = np.array([x, y])\n\n        matrix_effect = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=np.float32)\n        for x in [-1, 0, 1]:\n            for y in [-1, 0, 1]:\n                if (x, y) != (0, 0):\n                    cell_vector = np.array([x, y])\n                    distance_deg = np.rad2deg(\n                        ia.angle_between_vectors(cell_vector, direction_vector)\n                    )\n                    distance = distance_deg / 180\n                    similarity = (1 - distance) ** 4\n                    matrix_effect[y + 1, x + 1] = similarity\n        matrix_effect = matrix_effect / np.sum(matrix_effect)\n        matrix_effect = matrix_effect * (-1)\n        matrix_effect[1, 1] = 1\n\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n", "levels": [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import types", "import numpy as np", "import cv2", "import six.moves as sm", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Convolve(meta.Augmenter):\n", "    def __init__(self, matrix=None, name=None, deterministic=False, random_state=None):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Sharpen(alpha=0, lightness=1, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(image, nb_channels, random_state_func):\n", "def EdgeDetect(alpha=0, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(_image, nb_channels, random_state_func):\n", "    def create_matrices(_image, nb_channels, random_state_func):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/convolutional.py", "func_name": "EdgeDetect", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Augmenter that detects all edges in images, marks them in\n    a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = EdgeDetect(alpha=(0.0, 1.0))\n\n    detects edges in an image  and overlays the result with a variable alpha\n    in the range ``0.0 <= a <= 1.0`` over the old image.", "docstring_tokens": ["Augmenter", "that", "detects", "all", "edges", "in", "images", "marks", "them", "in", "a", "black", "and", "white", "image", "and", "then", "overlays", "the", "result", "with", "the", "original", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/convolutional.py#L382-L445", "partition": "valid", "up_fun_num": 11, "context": "\"\"\"\nAugmenters that apply convolutions to images.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead ::\n\n    from imgaug import augmenters as iaa\n\nand then e.g. ::\n\n    seq = iaa.Sequential([\n        iaa.Sharpen((0.0, 1.0)),\n        iaa.Emboss((0.0, 1.0))\n    ])\n\nList of augmenters:\n\n    * Convolve\n    * Sharpen\n    * Emboss\n    * EdgeDetect\n    * DirectedEdgeDetect\n\nFor MotionBlur, see ``blur.py``.\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport types\n\nimport numpy as np\nimport cv2\nimport six.moves as sm\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Convolve(meta.Augmenter):\n    \"\"\"\n    Apply a Convolution to input images.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: no (2)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (4)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (4)\n\n        - (1) rejected by ``cv2.filter2D()``.\n        - (2) causes error: cv2.error: OpenCV(3.4.2) (...)/filter.cpp:4487: error: (-213:The function/feature is\n              not implemented) Unsupported combination of source format (=1), and destination format (=1) in\n              function 'getLinearFilter'.\n        - (3) mapped internally to ``int16``.\n        - (4) mapped internally to ``float32``.\n\n    Parameters\n    ----------\n    matrix : None or (H, W) ndarray or imgaug.parameters.StochasticParameter or callable, optional\n        The weight matrix of the convolution kernel to apply.\n\n            * If None, the input images will not be changed.\n            * If a numpy array, that array will be used for all images and\n              channels as the kernel.\n            * If a callable, the parameter will be called for each image\n              via ``param(image, C, random_state)``. The function must either return\n              a list of ``C`` matrices (i.e. one per channel) or a 2D numpy array\n              (will be used for all channels) or a 3D ``HxWxC`` numpy array.\n              If a list is returned, each entry may be None, which will result\n              in no changes to the respective channel.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> matrix = np.array([[0, -1, 0],\n    >>>                    [-1, 4, -1],\n    >>>                    [0, -1, 0]])\n    >>> aug = iaa.Convolve(matrix=matrix)\n\n    convolves all input images with the kernel shown in the `matrix`\n    variable.\n\n    >>> def gen_matrix(image, nb_channels, random_state):\n    >>>     matrix_A = np.array([[0, -1, 0],\n    >>>                          [-1, 4, -1],\n    >>>                          [0, -1, 0]])\n    >>>     matrix_B = np.array([[0, 1, 0],\n    >>>                          [1, -4, 1],\n    >>>                          [0, 1, 0]])\n    >>>     if image.shape[0] % 2 == 0:\n    >>>         return [matrix_A] * nb_channels\n    >>>     else:\n    >>>         return [matrix_B] * nb_channels\n    >>> aug = iaa.Convolve(matrix=gen_matrix)\n\n    convolves images that have an even height with matrix A and images\n    with an odd height with matrix B.\n\n    \"\"\"\n\n    def __init__(self, matrix=None, name=None, deterministic=False, random_state=None):\n        super(Convolve, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        if matrix is None:\n            self.matrix = None\n            self.matrix_type = \"None\"\n        elif ia.is_np_array(matrix):\n            ia.do_assert(\n                len(matrix.shape) == 2,\n                \"Expected convolution matrix to have 2 axis, got %d (shape %s).\"\n                % (len(matrix.shape), matrix.shape),\n            )\n            self.matrix = matrix\n            self.matrix_type = \"constant\"\n        elif isinstance(matrix, types.FunctionType):\n            self.matrix = matrix\n            self.matrix_type = \"function\"\n        else:\n            raise Exception(\n                \"Expected float, int, tuple/list with 2 entries or StochasticParameter. Got %s.\"\n                % (type(matrix),)\n            )\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        seed = random_state.randint(0, 10 ** 6, 1)[0]\n        for i, image in enumerate(images):\n            _height, _width, nb_channels = images[i].shape\n\n            input_dtype = image.dtype\n            if image.dtype.type in [np.bool_, np.float16]:\n                image = image.astype(np.float32, copy=False)\n            elif image.dtype.type == np.int8:\n                image = image.astype(np.int16, copy=False)\n\n            if self.matrix_type == \"None\":\n                matrices = [None] * nb_channels\n            elif self.matrix_type == \"constant\":\n                matrices = [self.matrix] * nb_channels\n            elif self.matrix_type == \"function\":\n                matrices = self.matrix(\n                    images[i], nb_channels, ia.new_random_state(seed + i)\n                )\n                if ia.is_np_array(matrices) and matrices.ndim == 2:\n                    matrices = np.tile(matrices[..., np.newaxis], (1, 1, nb_channels))\n                ia.do_assert(\n                    (isinstance(matrices, list) and len(matrices) == nb_channels)\n                    or (\n                        ia.is_np_array(matrices)\n                        and matrices.ndim == 3\n                        and matrices.shape[2] == nb_channels\n                    ),\n                    \"Callable provided to Convole must return either a list of 2D matrices (one per image channel) \"\n                    \"or a 2D numpy array \"\n                    \"or a 3D numpy array where the last dimension's size matches the number of image channels. \"\n                    \"Got type %s.\" % (type(matrices),),\n                )\n\n                if ia.is_np_array(matrices):\n                    # Shape of matrices is currently (H, W, C), but in the loop below we need the\n                    # first axis to be the channel index to unify handling of lists of arrays\n                    # and arrays. So we move the channel axis here to the start.\n                    matrices = matrices.transpose((2, 0, 1))\n            else:\n                raise Exception(\"Invalid matrix type\")\n\n            image_aug = image\n            for channel in sm.xrange(nb_channels):\n                if matrices[channel] is not None:\n                    # ndimage.convolve caused problems here\n                    # cv2.filter2D() always returns same output dtype as input dtype\n                    image_aug[..., channel] = cv2.filter2D(\n                        image_aug[..., channel], -1, matrices[channel]\n                    )\n\n            if input_dtype == np.bool_:\n                image_aug = image_aug > 0.5\n            elif input_dtype in [np.int8, np.float16]:\n                image_aug = iadt.restore_dtypes_(image_aug, input_dtype)\n\n            images[i] = image_aug\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        # TODO this can fail for some matrices, e.g. [[0, 0, 1]]\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        # TODO this can fail for some matrices, e.g. [[0, 0, 1]]\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.matrix, self.matrix_type]\n\n\ndef Sharpen(alpha=0, lightness=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sharpens images and overlays the result with the original image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    lightness : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the lightness/brightness of the sharped image.\n        Sane values are somewhere in the range ``(0.5, 2)``.\n        The value 0 results in an edge map. Values higher than 1 create bright\n        images. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Sharpen(alpha=(0.0, 1.0))\n\n    sharpens input images and overlays the sharpened image by a variable\n    amount over the old image.\n\n    >>> aug = Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))\n\n    sharpens input images with a variable lightness in the range\n    ``0.75 <= x <= 2.0`` and with a variable alpha.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    lightness_param = iap.handle_continuous_param(\n        lightness,\n        \"lightness\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        lightness_sample = lightness_param.draw_sample(random_state=random_state_func)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [[-1, -1, -1], [-1, 8 + lightness_sample, -1], [-1, -1, -1]],\n            dtype=np.float32,\n        )\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Emboss(alpha=0, strength=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that embosses images and overlays the result with the original\n    image.\n\n    The embossed version pronounces highlights and shadows,\n    letting the image look as if it was recreated on a metal plate (\"embossed\").\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    strength : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the strength of the embossing.\n        Sane values are somewhere in the range ``(0, 2)`` with 1 being the standard\n        embossing effect. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))\n\n    embosses an image with a variable strength in the range ``0.5 <= x <= 1.5``\n    and overlays the result with a variable alpha in the range ``0.0 <= a <= 1.0``\n    over the old image.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    strength_param = iap.handle_continuous_param(\n        strength,\n        \"strength\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        strength_sample = strength_param.draw_sample(random_state=random_state_func)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [\n                [-1 - strength_sample, 0 - strength_sample, 0],\n                [0 - strength_sample, 1, 0 + strength_sample],\n                [0, 0 + strength_sample, 1 + strength_sample],\n            ],\n            dtype=np.float32,\n        )\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\n# TODO tests\n\n\n# TODO tests\n# TODO merge EdgeDetect and DirectedEdgeDetect?\ndef DirectedEdgeDetect(\n    alpha=0, direction=(0.0, 1.0), name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that detects edges that have certain directions and marks them\n    in a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of edges to pronounce, where 0 represents 0 degrees and 1.0\n        represents 360 degrees (both clockwise, starting at the top).\n        Default value is ``(0.0, 1.0)``, i.e. pick a random angle per image.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=0)\n\n    turns input images into edge images in which edges are detected from\n    top side of the image (i.e. the top sides of horizontal edges are\n    added to the output).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=90/360)\n\n    same as before, but detecting edges from the right (right side of each\n    vertical edge).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=(0.0, 1.0))\n\n    same as before, but detecting edges from a variable direction (anything\n    between 0 and 1.0, i.e. 0 degrees and 360 degrees, starting from the\n    top and moving clockwise).\n\n    >>> aug = DirectedEdgeDetect(alpha=(0.0, 0.3), direction=0)\n\n    generates edge images (edges detected from the top) and overlays them\n    with the input images by a variable amount between 0 and 30 percent\n    (e.g. for 0.3 then ``0.7*old_image + 0.3*edge_image``).\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    direction_param = iap.handle_continuous_param(\n        direction,\n        \"direction\",\n        value_range=None,\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(_image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        direction_sample = direction_param.draw_sample(random_state=random_state_func)\n\n        deg = int(direction_sample * 360) % 360\n        rad = np.deg2rad(deg)\n        x = np.cos(rad - 0.5 * np.pi)\n        y = np.sin(rad - 0.5 * np.pi)\n        direction_vector = np.array([x, y])\n\n        matrix_effect = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=np.float32)\n        for x in [-1, 0, 1]:\n            for y in [-1, 0, 1]:\n                if (x, y) != (0, 0):\n                    cell_vector = np.array([x, y])\n                    distance_deg = np.rad2deg(\n                        ia.angle_between_vectors(cell_vector, direction_vector)\n                    )\n                    distance = distance_deg / 180\n                    similarity = (1 - distance) ** 4\n                    matrix_effect[y + 1, x + 1] = similarity\n        matrix_effect = matrix_effect / np.sum(matrix_effect)\n        matrix_effect = matrix_effect * (-1)\n        matrix_effect[1, 1] = 1\n\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n", "levels": [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "import types", "import numpy as np", "import cv2", "import six.moves as sm", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Convolve(meta.Augmenter):\n", "    def __init__(self, matrix=None, name=None, deterministic=False, random_state=None):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Sharpen(alpha=0, lightness=1, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(image, nb_channels, random_state_func):\n", "def Emboss(alpha=0, strength=1, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(image, nb_channels, random_state_func):\n", "    def create_matrices(_image, nb_channels, random_state_func):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/convolutional.py", "func_name": "DirectedEdgeDetect", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Augmenter that detects edges that have certain directions and marks them\n    in a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    direction : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Angle of edges to pronounce, where 0 represents 0 degrees and 1.0\n        represents 360 degrees (both clockwise, starting at the top).\n        Default value is ``(0.0, 1.0)``, i.e. pick a random angle per image.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=0)\n\n    turns input images into edge images in which edges are detected from\n    top side of the image (i.e. the top sides of horizontal edges are\n    added to the output).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=90/360)\n\n    same as before, but detecting edges from the right (right side of each\n    vertical edge).\n\n    >>> aug = DirectedEdgeDetect(alpha=1.0, direction=(0.0, 1.0))\n\n    same as before, but detecting edges from a variable direction (anything\n    between 0 and 1.0, i.e. 0 degrees and 360 degrees, starting from the\n    top and moving clockwise).\n\n    >>> aug = DirectedEdgeDetect(alpha=(0.0, 0.3), direction=0)\n\n    generates edge images (edges detected from the top) and overlays them\n    with the input images by a variable amount between 0 and 30 percent\n    (e.g. for 0.3 then ``0.7*old_image + 0.3*edge_image``).", "docstring_tokens": ["Augmenter", "that", "detects", "edges", "that", "have", "certain", "directions", "and", "marks", "them", "in", "a", "black", "and", "white", "image", "and", "then", "overlays", "the", "result", "with", "the", "original", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/convolutional.py#L450-L568", "partition": "valid", "up_fun_num": 13, "context": "\"\"\"\nAugmenters that apply convolutions to images.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead ::\n\n    from imgaug import augmenters as iaa\n\nand then e.g. ::\n\n    seq = iaa.Sequential([\n        iaa.Sharpen((0.0, 1.0)),\n        iaa.Emboss((0.0, 1.0))\n    ])\n\nList of augmenters:\n\n    * Convolve\n    * Sharpen\n    * Emboss\n    * EdgeDetect\n    * DirectedEdgeDetect\n\nFor MotionBlur, see ``blur.py``.\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nimport types\n\nimport numpy as np\nimport cv2\nimport six.moves as sm\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Convolve(meta.Augmenter):\n    \"\"\"\n    Apply a Convolution to input images.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: no (2)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (4)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (4)\n\n        - (1) rejected by ``cv2.filter2D()``.\n        - (2) causes error: cv2.error: OpenCV(3.4.2) (...)/filter.cpp:4487: error: (-213:The function/feature is\n              not implemented) Unsupported combination of source format (=1), and destination format (=1) in\n              function 'getLinearFilter'.\n        - (3) mapped internally to ``int16``.\n        - (4) mapped internally to ``float32``.\n\n    Parameters\n    ----------\n    matrix : None or (H, W) ndarray or imgaug.parameters.StochasticParameter or callable, optional\n        The weight matrix of the convolution kernel to apply.\n\n            * If None, the input images will not be changed.\n            * If a numpy array, that array will be used for all images and\n              channels as the kernel.\n            * If a callable, the parameter will be called for each image\n              via ``param(image, C, random_state)``. The function must either return\n              a list of ``C`` matrices (i.e. one per channel) or a 2D numpy array\n              (will be used for all channels) or a 3D ``HxWxC`` numpy array.\n              If a list is returned, each entry may be None, which will result\n              in no changes to the respective channel.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> matrix = np.array([[0, -1, 0],\n    >>>                    [-1, 4, -1],\n    >>>                    [0, -1, 0]])\n    >>> aug = iaa.Convolve(matrix=matrix)\n\n    convolves all input images with the kernel shown in the `matrix`\n    variable.\n\n    >>> def gen_matrix(image, nb_channels, random_state):\n    >>>     matrix_A = np.array([[0, -1, 0],\n    >>>                          [-1, 4, -1],\n    >>>                          [0, -1, 0]])\n    >>>     matrix_B = np.array([[0, 1, 0],\n    >>>                          [1, -4, 1],\n    >>>                          [0, 1, 0]])\n    >>>     if image.shape[0] % 2 == 0:\n    >>>         return [matrix_A] * nb_channels\n    >>>     else:\n    >>>         return [matrix_B] * nb_channels\n    >>> aug = iaa.Convolve(matrix=gen_matrix)\n\n    convolves images that have an even height with matrix A and images\n    with an odd height with matrix B.\n\n    \"\"\"\n\n    def __init__(self, matrix=None, name=None, deterministic=False, random_state=None):\n        super(Convolve, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        if matrix is None:\n            self.matrix = None\n            self.matrix_type = \"None\"\n        elif ia.is_np_array(matrix):\n            ia.do_assert(\n                len(matrix.shape) == 2,\n                \"Expected convolution matrix to have 2 axis, got %d (shape %s).\"\n                % (len(matrix.shape), matrix.shape),\n            )\n            self.matrix = matrix\n            self.matrix_type = \"constant\"\n        elif isinstance(matrix, types.FunctionType):\n            self.matrix = matrix\n            self.matrix_type = \"function\"\n        else:\n            raise Exception(\n                \"Expected float, int, tuple/list with 2 entries or StochasticParameter. Got %s.\"\n                % (type(matrix),)\n            )\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        seed = random_state.randint(0, 10 ** 6, 1)[0]\n        for i, image in enumerate(images):\n            _height, _width, nb_channels = images[i].shape\n\n            input_dtype = image.dtype\n            if image.dtype.type in [np.bool_, np.float16]:\n                image = image.astype(np.float32, copy=False)\n            elif image.dtype.type == np.int8:\n                image = image.astype(np.int16, copy=False)\n\n            if self.matrix_type == \"None\":\n                matrices = [None] * nb_channels\n            elif self.matrix_type == \"constant\":\n                matrices = [self.matrix] * nb_channels\n            elif self.matrix_type == \"function\":\n                matrices = self.matrix(\n                    images[i], nb_channels, ia.new_random_state(seed + i)\n                )\n                if ia.is_np_array(matrices) and matrices.ndim == 2:\n                    matrices = np.tile(matrices[..., np.newaxis], (1, 1, nb_channels))\n                ia.do_assert(\n                    (isinstance(matrices, list) and len(matrices) == nb_channels)\n                    or (\n                        ia.is_np_array(matrices)\n                        and matrices.ndim == 3\n                        and matrices.shape[2] == nb_channels\n                    ),\n                    \"Callable provided to Convole must return either a list of 2D matrices (one per image channel) \"\n                    \"or a 2D numpy array \"\n                    \"or a 3D numpy array where the last dimension's size matches the number of image channels. \"\n                    \"Got type %s.\" % (type(matrices),),\n                )\n\n                if ia.is_np_array(matrices):\n                    # Shape of matrices is currently (H, W, C), but in the loop below we need the\n                    # first axis to be the channel index to unify handling of lists of arrays\n                    # and arrays. So we move the channel axis here to the start.\n                    matrices = matrices.transpose((2, 0, 1))\n            else:\n                raise Exception(\"Invalid matrix type\")\n\n            image_aug = image\n            for channel in sm.xrange(nb_channels):\n                if matrices[channel] is not None:\n                    # ndimage.convolve caused problems here\n                    # cv2.filter2D() always returns same output dtype as input dtype\n                    image_aug[..., channel] = cv2.filter2D(\n                        image_aug[..., channel], -1, matrices[channel]\n                    )\n\n            if input_dtype == np.bool_:\n                image_aug = image_aug > 0.5\n            elif input_dtype in [np.int8, np.float16]:\n                image_aug = iadt.restore_dtypes_(image_aug, input_dtype)\n\n            images[i] = image_aug\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        # TODO this can fail for some matrices, e.g. [[0, 0, 1]]\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        # TODO this can fail for some matrices, e.g. [[0, 0, 1]]\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.matrix, self.matrix_type]\n\n\ndef Sharpen(alpha=0, lightness=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sharpens images and overlays the result with the original image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    lightness : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the lightness/brightness of the sharped image.\n        Sane values are somewhere in the range ``(0.5, 2)``.\n        The value 0 results in an edge map. Values higher than 1 create bright\n        images. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Sharpen(alpha=(0.0, 1.0))\n\n    sharpens input images and overlays the sharpened image by a variable\n    amount over the old image.\n\n    >>> aug = Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))\n\n    sharpens input images with a variable lightness in the range\n    ``0.75 <= x <= 2.0`` and with a variable alpha.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    lightness_param = iap.handle_continuous_param(\n        lightness,\n        \"lightness\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        lightness_sample = lightness_param.draw_sample(random_state=random_state_func)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [[-1, -1, -1], [-1, 8 + lightness_sample, -1], [-1, -1, -1]],\n            dtype=np.float32,\n        )\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Emboss(alpha=0, strength=1, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that embosses images and overlays the result with the original\n    image.\n\n    The embossed version pronounces highlights and shadows,\n    letting the image look as if it was recreated on a metal plate (\"embossed\").\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    strength : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Parameter that controls the strength of the embossing.\n        Sane values are somewhere in the range ``(0, 2)`` with 1 being the standard\n        embossing effect. Default value is 1.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))\n\n    embosses an image with a variable strength in the range ``0.5 <= x <= 1.5``\n    and overlays the result with a variable alpha in the range ``0.0 <= a <= 1.0``\n    over the old image.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n    strength_param = iap.handle_continuous_param(\n        strength,\n        \"strength\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    def create_matrices(image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        strength_sample = strength_param.draw_sample(random_state=random_state_func)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array(\n            [\n                [-1 - strength_sample, 0 - strength_sample, 0],\n                [0 - strength_sample, 1, 0 + strength_sample],\n                [0, 0 + strength_sample, 1 + strength_sample],\n            ],\n            dtype=np.float32,\n        )\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\n# TODO tests\ndef EdgeDetect(alpha=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that detects all edges in images, marks them in\n    a black and white image and then overlays the result with the original\n    image.\n\n    dtype support::\n\n        See ``imgaug.augmenters.convolutional.Convolve``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Visibility of the sharpened image. At 0, only the original image is\n        visible, at 1.0 only its sharpened version is visible.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = EdgeDetect(alpha=(0.0, 1.0))\n\n    detects edges in an image  and overlays the result with a variable alpha\n    in the range ``0.0 <= a <= 1.0`` over the old image.\n\n    \"\"\"\n    alpha_param = iap.handle_continuous_param(\n        alpha, \"alpha\", value_range=(0, 1.0), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    def create_matrices(_image, nb_channels, random_state_func):\n        alpha_sample = alpha_param.draw_sample(random_state=random_state_func)\n        ia.do_assert(0 <= alpha_sample <= 1.0)\n        matrix_nochange = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=np.float32)\n        matrix_effect = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=np.float32)\n        matrix = (1 - alpha_sample) * matrix_nochange + alpha_sample * matrix_effect\n        return [matrix] * nb_channels\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return Convolve(\n        create_matrices,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\n# TODO tests\n# TODO merge EdgeDetect and DirectedEdgeDetect?\n", "levels": [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import types", "import numpy as np", "import cv2", "import six.moves as sm", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Convolve(meta.Augmenter):\n", "    def __init__(self, matrix=None, name=None, deterministic=False, random_state=None):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Sharpen(alpha=0, lightness=1, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(image, nb_channels, random_state_func):\n", "def Emboss(alpha=0, strength=1, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(image, nb_channels, random_state_func):\n", "def EdgeDetect(alpha=0, name=None, deterministic=False, random_state=None):\n", "    def create_matrices(_image, nb_channels, random_state_func):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/utils.py", "func_name": "normalize_shape", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Normalize a shape tuple or array to a shape tuple.\n\n    Parameters\n    ----------\n    shape : tuple of int or ndarray\n        The input to normalize. May optionally be an array.\n\n    Returns\n    -------\n    tuple of int\n        Shape tuple.", "docstring_tokens": ["Normalize", "a", "shape", "tuple", "or", "array", "to", "a", "shape", "tuple", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/utils.py#L8-L27", "partition": "valid", "up_fun_num": 0, "context": "from __future__ import print_function, absolute_import, division\nimport numpy as np\nimport six.moves as sm\nimport imgaug as ia\n\n\n# TODO integrate into keypoints\n\n\n# TODO integrate into keypoints\ndef project_coords(coords, from_shape, to_shape):\n    \"\"\"\n    Project coordinates from one image shape to another.\n\n    This performs a relative projection, e.g. a point at 60% of the old\n    image width will be at 60% of the new image width after projection.\n\n    Parameters\n    ----------\n    coords : ndarray or tuple of number\n        Coordinates to project. Either a ``(N,2)`` numpy array or a tuple\n        of `(x,y)` coordinates.\n\n    from_shape : tuple of int or ndarray\n        Old image shape.\n\n    to_shape : tuple of int or ndarray\n        New image shape.\n\n    Returns\n    -------\n    ndarray\n        Projected coordinates as ``(N,2)`` ``float32`` numpy array.\n\n    \"\"\"\n    from_shape = normalize_shape(from_shape)\n    to_shape = normalize_shape(to_shape)\n    if from_shape[0:2] == to_shape[0:2]:\n        return coords\n\n    from_height, from_width = from_shape[0:2]\n    to_height, to_width = to_shape[0:2]\n    assert all([v > 0 for v in [from_height, from_width, to_height, to_width]])\n\n    # make sure to not just call np.float32(coords) here as the following lines\n    # perform in-place changes and np.float32(.) only copies if the input\n    # was *not* a float32 array\n    coords_proj = np.array(coords).astype(np.float32)\n    coords_proj[:, 0] = (coords_proj[:, 0] / from_width) * to_width\n    coords_proj[:, 1] = (coords_proj[:, 1] / from_height) * to_height\n    return coords_proj\n\n\ndef interpolate_point_pair(point_a, point_b, nb_steps):\n    if nb_steps < 1:\n        return []\n    x1, y1 = point_a\n    x2, y2 = point_b\n    vec = np.float32([x2 - x1, y2 - y1])\n    step_size = vec / (1 + nb_steps)\n    return [\n        (x1 + (i + 1) * step_size[0], y1 + (i + 1) * step_size[1])\n        for i in sm.xrange(nb_steps)\n    ]\n\n\ndef interpolate_points(points, nb_steps, closed=True):\n    if len(points) <= 1:\n        return points\n    if closed:\n        points = list(points) + [points[0]]\n    points_interp = []\n    for point_a, point_b in zip(points[:-1], points[1:]):\n        points_interp.extend(\n            [point_a] + interpolate_point_pair(point_a, point_b, nb_steps)\n        )\n    if not closed:\n        points_interp.append(points[-1])\n    # close does not have to be reverted here, as last point is not included in the extend()\n    return points_interp\n\n\ndef interpolate_points_by_max_distance(points, max_distance, closed=True):\n    ia.do_assert(\n        max_distance > 0,\n        \"max_distance must have value greater than 0, got %.8f\" % (max_distance,),\n    )\n    if len(points) <= 1:\n        return points\n    if closed:\n        points = list(points) + [points[0]]\n    points_interp = []\n    for point_a, point_b in zip(points[:-1], points[1:]):\n        dist = np.sqrt((point_a[0] - point_b[0]) ** 2 + (point_a[1] - point_b[1]) ** 2)\n        nb_steps = int((dist / max_distance) - 1)\n        points_interp.extend(\n            [point_a] + interpolate_point_pair(point_a, point_b, nb_steps)\n        )\n    if not closed:\n        points_interp.append(points[-1])\n    return points_interp\n", "levels": [0, 0, 0, 0], "package": ["from __future__ import print_function, absolute_import, division", "import numpy as np", "import six.moves as sm", "import imgaug as ia"], "function": ["def project_coords(coords, from_shape, to_shape):\n", "def interpolate_point_pair(point_a, point_b, nb_steps):\n", "def interpolate_points(points, nb_steps, closed=True):\n", "def interpolate_points_by_max_distance(points, max_distance, closed=True):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmentables/utils.py", "func_name": "project_coords", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Project coordinates from one image shape to another.\n\n    This performs a relative projection, e.g. a point at 60% of the old\n    image width will be at 60% of the new image width after projection.\n\n    Parameters\n    ----------\n    coords : ndarray or tuple of number\n        Coordinates to project. Either a ``(N,2)`` numpy array or a tuple\n        of `(x,y)` coordinates.\n\n    from_shape : tuple of int or ndarray\n        Old image shape.\n\n    to_shape : tuple of int or ndarray\n        New image shape.\n\n    Returns\n    -------\n    ndarray\n        Projected coordinates as ``(N,2)`` ``float32`` numpy array.", "docstring_tokens": ["Project", "coordinates", "from", "one", "image", "shape", "to", "another", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/utils.py#L31-L71", "partition": "valid", "up_fun_num": 1, "context": "from __future__ import print_function, absolute_import, division\nimport numpy as np\nimport six.moves as sm\nimport imgaug as ia\n\n\n# TODO integrate into keypoints\ndef normalize_shape(shape):\n    \"\"\"\n    Normalize a shape tuple or array to a shape tuple.\n\n    Parameters\n    ----------\n    shape : tuple of int or ndarray\n        The input to normalize. May optionally be an array.\n\n    Returns\n    -------\n    tuple of int\n        Shape tuple.\n\n    \"\"\"\n    if isinstance(shape, tuple):\n        return shape\n    assert ia.is_np_array(shape), \"Expected tuple of ints or array, got %s.\" % (\n        type(shape),\n    )\n    return shape.shape\n\n\n# TODO integrate into keypoints\n\n\ndef interpolate_point_pair(point_a, point_b, nb_steps):\n    if nb_steps < 1:\n        return []\n    x1, y1 = point_a\n    x2, y2 = point_b\n    vec = np.float32([x2 - x1, y2 - y1])\n    step_size = vec / (1 + nb_steps)\n    return [\n        (x1 + (i + 1) * step_size[0], y1 + (i + 1) * step_size[1])\n        for i in sm.xrange(nb_steps)\n    ]\n\n\ndef interpolate_points(points, nb_steps, closed=True):\n    if len(points) <= 1:\n        return points\n    if closed:\n        points = list(points) + [points[0]]\n    points_interp = []\n    for point_a, point_b in zip(points[:-1], points[1:]):\n        points_interp.extend(\n            [point_a] + interpolate_point_pair(point_a, point_b, nb_steps)\n        )\n    if not closed:\n        points_interp.append(points[-1])\n    # close does not have to be reverted here, as last point is not included in the extend()\n    return points_interp\n\n\ndef interpolate_points_by_max_distance(points, max_distance, closed=True):\n    ia.do_assert(\n        max_distance > 0,\n        \"max_distance must have value greater than 0, got %.8f\" % (max_distance,),\n    )\n    if len(points) <= 1:\n        return points\n    if closed:\n        points = list(points) + [points[0]]\n    points_interp = []\n    for point_a, point_b in zip(points[:-1], points[1:]):\n        dist = np.sqrt((point_a[0] - point_b[0]) ** 2 + (point_a[1] - point_b[1]) ** 2)\n        nb_steps = int((dist / max_distance) - 1)\n        points_interp.extend(\n            [point_a] + interpolate_point_pair(point_a, point_b, nb_steps)\n        )\n    if not closed:\n        points_interp.append(points[-1])\n    return points_interp\n", "levels": [0, 0, 0, 0], "package": ["from __future__ import print_function, absolute_import, division", "import numpy as np", "import six.moves as sm", "import imgaug as ia"], "function": ["def normalize_shape(shape):\n", "def interpolate_point_pair(point_a, point_b, nb_steps):\n", "def interpolate_points(points, nb_steps, closed=True):\n", "def interpolate_points_by_max_distance(points, max_distance, closed=True):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "AdditivePoissonNoise", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).", "docstring_tokens": ["Create", "an", "augmenter", "to", "add", "poisson", "noise", "to", "images", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L546-L626", "partition": "valid", "up_fun_num": 14, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "Dropout", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.", "docstring_tokens": ["Augmenter", "that", "sets", "a", "certain", "fraction", "of", "pixels", "in", "images", "to", "zero", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L977-L1059", "partition": "valid", "up_fun_num": 27, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "CoarseDropout", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.", "docstring_tokens": ["Augmenter", "that", "sets", "rectangular", "areas", "within", "images", "to", "zero", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L1062-L1201", "partition": "valid", "up_fun_num": 28, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "ImpulseNoise", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.", "docstring_tokens": ["Creates", "an", "augmenter", "to", "apply", "impulse", "noise", "to", "an", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L1360-L1371", "partition": "valid", "up_fun_num": 34, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "SaltAndPepper", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.", "docstring_tokens": ["Adds", "salt", "and", "pepper", "noise", "to", "an", "image", "i", ".", "e", ".", "some", "white", "-", "ish", "and", "black", "-", "ish", "pixels", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L1374-L1430", "partition": "valid", "up_fun_num": 35, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "Pepper", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.", "docstring_tokens": ["Adds", "pepper", "noise", "to", "an", "image", "i", ".", "e", ".", "black", "-", "ish", "pixels", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L1711-L1777", "partition": "valid", "up_fun_num": 37, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "CoarsePepper", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.", "docstring_tokens": ["Adds", "coarse", "pepper", "noise", "to", "an", "image", "i", ".", "e", ".", "rectangles", "that", "contain", "noisy", "black", "-", "ish", "pixels", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L1780-L1890", "partition": "valid", "up_fun_num": 38, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\ndef ContrastNormalization(\n    alpha=1.0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.\n\n    \"\"\"\n    # placed here to avoid cyclic dependency\n    from . import contrast as contrast_lib\n\n    return contrast_lib.LinearContrast(\n        alpha=alpha,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/augmenters/arithmetic.py", "func_name": "ContrastNormalization", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Augmenter that changes the contrast of images.\n\n    dtype support:\n\n        See ``imgaug.augmenters.contrast.LinearContrast``.\n\n    Parameters\n    ----------\n    alpha : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Strength of the contrast normalization. Higher values than 1.0\n        lead to higher contrast, lower values decrease the contrast.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value will be sampled per image from\n              the range ``a <= x <= b`` and be used as the alpha value.\n            * If a list, then a random value will be sampled per image from\n              that list.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the alpha value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> iaa.ContrastNormalization((0.5, 1.5))\n\n    Decreases oder improves contrast per image by a random factor between\n    0.5 and 1.5. The factor 0.5 means that any difference from the center value\n    (i.e. 128) will be halved, leading to less contrast.\n\n    >>> iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5)\n\n    Same as before, but for 50 percent of all images the normalization is done\n    independently per channel (i.e. factors can vary per channel for the same\n    image). In the other 50 percent of all images, the factor is the same for\n    all channels.", "docstring_tokens": ["Augmenter", "that", "changes", "the", "contrast", "of", "images", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/arithmetic.py#L2151-L2207", "partition": "valid", "up_fun_num": 48, "context": "\"\"\"\nAugmenters that perform simple arithmetic changes.\n\nDo not import directly from this file, as the categorization is not final.\nUse instead::\n\n    from imgaug import augmenters as iaa\n\nand then e.g.::\n\n    seq = iaa.Sequential([iaa.Add((-5, 5)), iaa.Multiply((0.9, 1.1))])\n\nList of augmenters:\n\n    * Add\n    * AddElementwise\n    * AdditiveGaussianNoise\n    * AdditiveLaplaceNoise\n    * AdditivePoissonNoise\n    * Multiply\n    * MultiplyElementwise\n    * Dropout\n    * CoarseDropout\n    * ReplaceElementwise\n    * ImpulseNoise\n    * SaltAndPepper\n    * CoarseSaltAndPepper\n    * Salt\n    * CoarseSalt\n    * Pepper\n    * CoarsePepper\n    * Invert\n    * ContrastNormalization\n    * JpegCompression\n\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\n\nfrom PIL import Image as PIL_Image\nimport imageio\nimport tempfile\nimport numpy as np\nimport cv2\n\nfrom . import meta\nimport imgaug as ia\nfrom .. import parameters as iap\nfrom .. import dtypes as iadt\n\n\nclass Add(meta.Augmenter):\n    \"\"\"\n    Add a value to all pixels in an image.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Value to add to all pixels.\n\n            * If a number, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then a value from the discrete range ``[a, b]``\n              will be used.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then a value will be sampled per image\n              from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Add(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.Add((-10, 10))\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=True)\n\n    adds a value from the discrete range ``[-10 .. 10]`` to all pixels of\n    the input images. The exact value is sampled per image AND channel,\n    i.e. to a red-channel it might add 5 while subtracting 7 from the\n    blue channel of the same image.\n\n    >>> aug = iaa.Add((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Add, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.value = iap.handle_continuous_param(\n            value, \"value\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        value_samples = self.value.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, value_samples, per_channel_samples, input_dtypes))\n        for i, (image, value_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly add images via image+sample (uint8 only)\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.int16)\n            #     value_samples_i = value_samples_i.astype(np.int16)\n            #     for c, value in enumerate(value_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] + value, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.int16) + value_samples_i[0].astype(np.int16), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than the else-block code (around 3-4x speedup)\n                # and is still faster than the simpler image+sample approach without LUT (about 10% at 64x64 and about\n                # 2x at 224x224 -- maybe dependent on installed BLAS libraries?)\n                value_samples_i = np.clip(np.round(value_samples_i), -255, 255).astype(\n                    np.int16\n                )\n                value_range = np.arange(0, 256, dtype=np.int16)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        + value_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range + value_samples_i[0]\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                if per_channel_samples_i > 0.5:\n                    value = value_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    value = value_samples_i[0:1].reshape((1, 1, 1))\n\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=True\n                )\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\n# TODO merge this with Add\nclass AddElementwise(meta.Augmenter):\n    \"\"\"\n    Add values to the pixels of images with possibly different values for neighbouring pixels.\n\n    While the Add Augmenter adds a constant value per image, this one can\n    add different values (sampled per pixel).\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n    Parameters\n    ----------\n    value : int or tuple of int or list of int or imgaug.parameters.StochasticParameter, optional\n        Value to add to the pixels.\n\n            * If an int, then that value will be used for all images.\n            * If a tuple ``(a, b)``, then values from the discrete range ``[a .. b]``\n              will be sampled.\n            * If a list of integers, a random value will be sampled from the list\n              per image.\n            * If a StochasticParameter, then values will be sampled per pixel\n              (and possibly channel) from that parameter.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AddElementwise(10)\n\n    always adds a value of 10 to all pixels in the image.\n\n    >>> aug = iaa.AddElementwise((-10, 10))\n\n    samples per pixel a value from the discrete range ``[-10 .. 10]`` and\n    adds that value to the pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=True)\n\n    samples per pixel *and channel* a value from the discrete\n    range ``[-10 .. 10]`` ands adds it to the pixel's value. Therefore,\n    added values may differ between channels of the same pixel.\n\n    >>> aug = iaa.AddElementwise((-10, 10), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value=0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(AddElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO open to continous, similar to Add\n        self.value = iap.handle_discrete_param(\n            value,\n            \"value\",\n            value_range=(-255, 255),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n            allow_floats=False,\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            value = self.value.draw_samples(sample_shape, random_state=rs)\n\n            if image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for smaller images).\n                #\n                # Also tested to instead compute min/max of image and value and then only convert image/value dtype\n                # if actually necessary, but that was like 20-30% slower, even for 224x224 images.\n                #\n                if value.dtype.kind == \"f\":\n                    value = np.round(value)\n\n                image = image.astype(np.int16)\n                value = np.clip(value, -255, 255).astype(np.int16)\n\n                image_aug = image + value\n                image_aug = np.clip(image_aug, 0, 255).astype(np.uint8)\n\n                images[i] = image_aug\n            else:\n                # We limit here the value range of the value parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                #\n                # We need 2* the itemsize of the image here to allow to shift the image's max value to the lowest\n                # possible value, e.g. for uint8 it must allow for -255 to 255.\n                itemsize = image.dtype.itemsize * 2\n                dtype_target = np.dtype(\"%s%d\" % (value.dtype.kind, itemsize))\n                value = iadt.clip_to_dtype_value_range_(\n                    value, dtype_target, validate=100\n                )\n\n                if value.shape[2] == 1:\n                    value = np.tile(value, (1, 1, nb_channels))\n\n                image, value = iadt.promote_array_dtypes_(\n                    [image, value],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=2,\n                )\n                image = np.add(image, value, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.value, self.per_channel]\n\n\ndef AdditiveGaussianNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add gaussian noise (aka white noise) to images.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the normal distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the normal distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=(0, 0.1*255))\n\n    adds gaussian noise from the distribution ``N(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=True)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveGaussianNoise(scale=0.1*255, per_channel=0.5)\n\n    adds gaussian noise from the distribution ``N(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Normal(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditiveLaplaceNoise(\n    loc=0, scale=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Add laplace noise to images.\n\n    The laplace distribution is similar to the gaussian distribution, but has puts weight on the long tail.\n    Hence, this noise will add more outliers (very high/low values). It is somewhere between gaussian noise and\n    salt and pepper noise.\n\n    Values of around ``255 * 0.05`` for `scale` lead to visible noise (for uint8).\n    Values of around ``255 * 0.10`` for `scale` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    loc : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Mean of the laplace distribution that generates the noise.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    scale : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Standard deviation of the laplace distribution that generates the noise.\n        Must be ``>= 0``. If 0 then only `loc` will be used.\n        Recommended to be around ``255 * 0.05``.\n\n            * If an int or float, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per\n              image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=(0, 0.1*255))\n\n    Adds laplace noise from the distribution ``Laplace(0, s)`` to images,\n    where s is sampled per image from the range ``0 <= s <= 0.1*255``.\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=True)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditiveLaplaceNoise(scale=0.1*255, per_channel=0.5)\n\n    Adds laplace noise from the distribution ``Laplace(0, 0.1*255)`` to images,\n    where the noise value is sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    loc2 = iap.handle_continuous_param(\n        loc, \"loc\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n    )\n    scale2 = iap.handle_continuous_param(\n        scale,\n        \"scale\",\n        value_range=(0, None),\n        tuple_to_uniform=True,\n        list_to_choice=True,\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.Laplace(loc=loc2, scale=scale2),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef AdditivePoissonNoise(\n    lam=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Create an augmenter to add poisson noise to images.\n\n    Poisson noise is comparable to gaussian noise as in ``AdditiveGaussianNoise``, but the values are sampled from\n    a poisson distribution instead of a gaussian distribution. As poisson distributions produce only positive numbers,\n    the sign of the sampled values are here randomly flipped.\n\n    Values of around ``10.0`` for `lam` lead to visible noise (for uint8).\n    Values of around ``20.0`` for `lam` lead to very visible noise (for uint8).\n    It is recommended to usually set `per_channel` to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.AddElementwise``.\n\n    Parameters\n    ----------\n    lam : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        Lambda parameter of the poisson distribution. Recommended values are around ``0.0`` to ``10.0``.\n\n            * If a number, exactly that value will be used.\n            * If a tuple ``(a, b)``, a random value from the range ``a <= x <= b`` will\n              be sampled per image.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, a value will be sampled from the\n              parameter per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same noise value per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0))\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images, where ``x`` is randomly sampled per image from the\n    interval ``[0.0, 10.0]``.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=5.0, per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(5.0)`` to images,\n    where the values are different per pixel *and* channel (e.g. a\n    different one for red, green and blue channels for the same pixel).\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=(0.0, 10.0), per_channel=True)\n\n    Adds poisson noise sampled from ``Poisson(x)`` to images,\n    with ``x`` being sampled from ``uniform(0.0, 10.0)`` per image, pixel and channel.\n    This is the *recommended* configuration.\n\n    >>> aug = iaa.AdditivePoissonNoise(lam=2, per_channel=0.5)\n\n    Adds poisson noise sampled from the distribution ``Poisson(2)`` to images,\n    where the values are sometimes (50 percent of all cases) the same\n    per pixel for all channels and sometimes different (other 50 percent).\n\n    \"\"\"\n    lam2 = iap.handle_continuous_param(\n        lam, \"lam\", value_range=(0, None), tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return AddElementwise(\n        iap.RandomSign(iap.Poisson(lam=lam2)),\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Multiply(meta.Augmenter):\n    \"\"\"\n    Multiply all pixels in an image with a specific value.\n\n    This augmenter can be used to make images lighter or darker.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value with which to multiply the pixel values in each image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and used for all pixels.\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image.\n\n    per_channel : bool or float, optional\n        Whether to use the same multiplier per pixel for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Multiply(2.0)\n\n    would multiply all images by a factor of 2, making the images\n    significantly brighter.\n\n    >>> aug = iaa.Multiply((0.5, 1.5))\n\n    would multiply images by a random value from the range ``0.5 <= x <= 1.5``,\n    making some images darker and others brighter.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Multiply, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        nb_channels_max = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        mul_samples = self.mul.draw_samples(\n            (nb_images, nb_channels_max), random_state=rss[0]\n        )\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        gen = enumerate(zip(images, mul_samples, per_channel_samples, input_dtypes))\n        for i, (image, mul_samples_i, per_channel_samples_i, input_dtype) in gen:\n            nb_channels = image.shape[2]\n\n            # Example code to directly multiply images via image*sample (uint8 only) -- apparently slower than LUT\n            # if per_channel_samples_i > 0.5:\n            #     result = []\n            #     image = image.astype(np.float32)\n            #     mul_samples_i = mul_samples_i.astype(np.float32)\n            #     for c, mul in enumerate(mul_samples_i[0:nb_channels]):\n            #         result.append(np.clip(image[..., c:c+1] * mul, 0, 255).astype(np.uint8))\n            #     images[i] = np.concatenate(result, axis=2)\n            # else:\n            #     images[i] = np.clip(\n            #         image.astype(np.float32) * mul_samples_i[0].astype(np.float32), 0, 255).astype(np.uint8)\n\n            if image.dtype.name == \"uint8\":\n                # Using this LUT approach is significantly faster than else-block code (more than 10x speedup)\n                # and is still faster than the simpler image*sample approach without LUT (1.5-3x speedup,\n                # maybe dependent on installed BLAS libraries?)\n                value_range = np.arange(0, 256, dtype=np.float32)\n                if per_channel_samples_i > 0.5:\n                    result = []\n                    mul_samples_i = mul_samples_i.astype(np.float32)\n                    tables = (\n                        np.tile(value_range[np.newaxis, :], (nb_channels, 1))\n                        * mul_samples_i[0:nb_channels, np.newaxis]\n                    )\n                    tables = np.clip(tables, 0, 255).astype(image.dtype)\n                    for c, table in enumerate(tables):\n                        arr_aug = cv2.LUT(image[..., c], table)\n                        result.append(arr_aug[..., np.newaxis])\n                    images[i] = np.concatenate(result, axis=2)\n                else:\n                    table = value_range * mul_samples_i[0].astype(np.float32)\n                    image_aug = cv2.LUT(\n                        image, np.clip(table, 0, 255).astype(image.dtype)\n                    )\n                    if image_aug.ndim == 2:\n                        image_aug = image_aug[..., np.newaxis]\n                    images[i] = image_aug\n            else:\n                # TODO estimate via image min/max values whether a resolution increase is necessary\n\n                if per_channel_samples_i > 0.5:\n                    mul = mul_samples_i[0:nb_channels].reshape((1, 1, nb_channels))\n                else:\n                    mul = mul_samples_i[0:1].reshape((1, 1, 1))\n\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2 not 1\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(mul, dtype_target, validate=True)\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image.dtype, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\n# TODO merge with Multiply\nclass MultiplyElementwise(meta.Augmenter):\n    \"\"\"\n    Multiply values of pixels with possibly different values for neighbouring pixels.\n\n    While the Multiply Augmenter uses a constant multiplier per image,\n    this one can use different multipliers per pixel.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        Note: tests were only conducted for rather small multipliers, around -10.0 to +10.0.\n\n        In general, the multipliers sampled from `mul` must be in a value range that corresponds to\n        the input image's dtype. E.g. if the input image has dtype uint16 and the samples generated\n        from `mul` are float64, this augmenter will still force all samples to be within the value\n        range of float16, as it has the same number of bytes (two) as uint16. This is done to\n        make overflows less likely to occur.\n\n    Parameters\n    ----------\n    mul : number or tuple of number or list of number or imgaug.parameters.StochasticParameter, optional\n        The value by which to multiply the pixel values in the image.\n\n            * If a number, then that value will always be used.\n            * If a tuple ``(a, b)``, then a value from the range ``a <= x <= b`` will\n              be sampled per image and pixel.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then that parameter will be used to\n              sample a new value per image and pixel.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> from imgaug import augmenters as iaa\n    >>> aug = iaa.MultiplyElementwise(2.0)\n\n    multiply all images by a factor of 2.0, making them significantly\n    bighter.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5))\n\n    samples per pixel a value from the range ``0.5 <= x <= 1.5`` and\n    multiplies the pixel with that value.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=True)\n\n    samples per pixel *and channel* a value from the range\n    ``0.5 <= x <= 1.5`` ands multiplies the pixel by that value. Therefore,\n    added multipliers may differ between channels of the same pixel.\n\n    >>> aug = iaa.MultiplyElementwise((0.5, 1.5), per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mul=1.0,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(MultiplyElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mul = iap.handle_continuous_param(\n            mul, \"mul\", value_range=None, tuple_to_uniform=True, list_to_choice=True\n        )\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\"bool\", \"uint8\", \"uint16\", \"int8\", \"int16\", \"float16\", \"float32\"],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float64\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        input_dtypes = iadt.copy_dtypes_for_restore(images, force_list=True)\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n        is_mul_binomial = isinstance(self.mul, iap.Binomial) or (\n            isinstance(self.mul, iap.FromLowerResolution)\n            and isinstance(self.mul.other_param, iap.Binomial)\n        )\n\n        gen = enumerate(zip(images, per_channel_samples, rss[:-1], input_dtypes))\n        for i, (image, per_channel_samples_i, rs, input_dtype) in gen:\n            height, width, nb_channels = image.shape\n            sample_shape = (\n                height,\n                width,\n                nb_channels if per_channel_samples_i > 0.5 else 1,\n            )\n            mul = self.mul.draw_samples(sample_shape, random_state=rs)\n            # TODO let Binomial return boolean mask directly instead of [0, 1] integers?\n            # hack to improve performance for Dropout and CoarseDropout\n            # converts mul samples to mask if mul is binomial\n            if mul.dtype.kind != \"b\" and is_mul_binomial:\n                mul = mul.astype(bool, copy=False)\n\n            if mul.dtype.kind == \"b\":\n                images[i] *= mul\n            elif image.dtype.name == \"uint8\":\n                # This special uint8 block is around 60-100% faster than the else-block further below (more speedup\n                # for larger images).\n                #\n                if mul.dtype.kind == \"f\":\n                    # interestingly, float32 is here significantly faster than float16\n                    # TODO is that system dependent?\n                    # TODO does that affect int8-int32 too?\n                    mul = mul.astype(np.float32, copy=False)\n                    image_aug = image.astype(np.float32)\n                else:\n                    mul = mul.astype(np.int16, copy=False)\n                    image_aug = image.astype(np.int16)\n\n                image_aug = np.multiply(image_aug, mul, casting=\"no\", out=image_aug)\n                images[i] = iadt.restore_dtypes_(image_aug, np.uint8, round=False)\n            else:\n                # TODO maybe introduce to stochastic parameters some way to get the possible min/max values,\n                # could make things faster for dropout to get 0/1 min/max from the binomial\n                mul_min = np.min(mul)\n                mul_max = np.max(mul)\n                is_not_increasing_value_range = (-1 <= mul_min <= 1) and (\n                    -1 <= mul_max <= 1\n                )\n\n                # We limit here the value range of the mul parameter to the bytes in the image's dtype.\n                # This prevents overflow problems and makes it less likely that the image has to be up-casted, which\n                # again improves performance and saves memory. Note that this also enables more dtypes for image inputs.\n                # The downside is that the mul parameter is limited in its value range.\n                itemsize = max(\n                    image.dtype.itemsize, 2 if mul.dtype.kind == \"f\" else 1\n                )  # float min itemsize is 2\n                dtype_target = np.dtype(\"%s%d\" % (mul.dtype.kind, itemsize))\n                mul = iadt.clip_to_dtype_value_range_(\n                    mul, dtype_target, validate=True, validate_values=(mul_min, mul_max)\n                )\n\n                if mul.shape[2] == 1:\n                    mul = np.tile(mul, (1, 1, nb_channels))\n\n                image, mul = iadt.promote_array_dtypes_(\n                    [image, mul],\n                    dtypes=[image, dtype_target],\n                    increase_itemsize_factor=1 if is_not_increasing_value_range else 2,\n                )\n                image = np.multiply(image, mul, out=image, casting=\"no\")\n                image = iadt.restore_dtypes_(image, input_dtype)\n                images[i] = image\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mul, self.per_channel]\n\n\ndef Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Augmenter that sets a certain fraction of pixels in images to zero.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all images. A value\n              of 1.0 would mean that all pixels will be dropped and 0.0 that\n              no pixels would be dropped. A value of 0.05 corresponds to 5\n              percent of all pixels dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n              If you instead want to provide the probability as a stochastic\n              parameter, you can usually do ``imgaug.parameters.Binomial(1-p)``\n              to convert parameter `p` to a 0/1 representation.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float p, then for p percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Dropout(0.02)\n\n    drops 2 percent of all pixels.\n\n    >>> aug = iaa.Dropout((0.0, 0.05))\n\n    drops in each image a random fraction of all pixels, where the fraction\n    is in the range ``0.0 <= x <= 0.05``.\n\n    >>> aug = iaa.Dropout(0.02, per_channel=True)\n\n    drops 2 percent of all pixels in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.Dropout(0.02, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p2,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseDropout(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Augmenter that sets rectangular areas within images to zero.\n\n    In contrast to Dropout, these areas can have larger sizes.\n    (E.g. you might end up with three large black rectangles in an image.)\n    Note that the current implementation leads to correlated sizes,\n    so when there is one large area that is dropped, there is a high likelihood\n    that all other dropped areas are also large.\n\n    This method is implemented by generating the dropout mask at a\n    lower resolution (than the image has) and then upsampling the mask\n    before dropping the pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.MultiplyElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The probability of any pixel being dropped (i.e. set to zero).\n\n            * If a float, then that value will be used for all pixels. A value\n              of 1.0 would mean, that all pixels will be dropped. A value of\n              0.0 would lead to no pixels being dropped.\n            * If a tuple ``(a, b)``, then a value p will be sampled from the\n              range ``a <= p <= b`` per image and be used as the pixel's dropout\n              probability.\n            * If a StochasticParameter, then this parameter will be used to\n              determine per pixel whether it should be dropped (sampled value\n              of 0) or shouldn't (sampled value of 1).\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the dropout\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being dropped.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5)\n\n    drops 2 percent of all pixels on an lower-resolution image that has\n    50 percent of the original image's size, leading to dropped areas that\n    have roughly 2x2 pixels size.\n\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_percent=(0.05, 0.5))\n\n    generates a dropout mask at 5 to 50 percent of image's size. In that mask,\n    0 to 5 percent of all pixels are dropped (random per image).\n\n    >>> aug = iaa.CoarseDropout((0.0, 0.05), size_px=(2, 16))\n\n    same as previous example, but the lower resolution image has 2 to 16 pixels\n    size.\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=True)\n\n    drops 2 percent of all pixels at 50 percent resolution (2x2 sizes)\n    in a channel-wise fashion, i.e. it is unlikely\n    for any pixel to have all channels set to zero (black pixels).\n\n    >>> aug = iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n\n    same as previous example, but the `per_channel` feature is only active\n    for 50 percent of all images.\n\n    \"\"\"\n    if ia.is_single_number(p):\n        p2 = iap.Binomial(1 - p)\n    elif ia.is_iterable(p):\n        ia.do_assert(len(p) == 2)\n        ia.do_assert(p[0] < p[1])\n        ia.do_assert(0 <= p[0] <= 1.0)\n        ia.do_assert(0 <= p[1] <= 1.0)\n        p2 = iap.Binomial(iap.Uniform(1 - p[1], 1 - p[0]))\n    elif isinstance(p, iap.StochasticParameter):\n        p2 = p\n    else:\n        raise Exception(\n            \"Expected p to be float or int or StochasticParameter, got %s.\" % (type(p),)\n        )\n\n    if size_px is not None:\n        p3 = iap.FromLowerResolution(other_param=p2, size_px=size_px, min_size=min_size)\n    elif size_percent is not None:\n        p3 = iap.FromLowerResolution(\n            other_param=p2, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return MultiplyElementwise(\n        p3,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass ReplaceElementwise(meta.Augmenter):\n    \"\"\"\n    Replace pixels in an image with new values.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested\n        * ``int64``: yes; tested\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no\n        * ``bool``: yes; tested\n\n        - (1) uint64 is currently not supported, because iadt.clip_to_dtype_value_range_() does not\n              support it, which again is because numpy.clip() seems to not support it.\n\n    Parameters\n    ----------\n    mask : float or tuple of float or list of float or imgaug.parameters.StochasticParameter\n        Mask that indicates the pixels that are supposed to be replaced.\n        The mask will be thresholded with 0.5. A value of 1 then indicates a\n        pixel that is supposed to be replaced.\n\n            * If this is a float, then that value will be used as the\n              probability of being a 1 per pixel.\n            * If a tuple ``(a, b)``, then the probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used to\n              sample a mask.\n\n    replacement : number or tuple of number or list of number or imgaug.parameters.StochasticParameter\n        The replacement to use at all locations that are marked as `1` in the mask.\n\n            * If this is a number, then that value will always be used as the\n              replacement.\n            * If a tuple ``(a, b)``, then the replacement will be sampled pixelwise\n              from the range ``a <= x <= b``.\n            * If a list of number, then a random value will be picked from\n              that list as the replacement per pixel.\n            * If a StochasticParameter, then this parameter will be used sample\n              pixelwise replacement values.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = ReplaceElementwise(0.05, [0, 255])\n\n    Replace 5 percent of all pixels in each image by either 0 or 255.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        mask,\n        replacement,\n        per_channel=False,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(ReplaceElementwise, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        self.mask = iap.handle_probability_param(\n            mask, \"mask\", tuple_to_uniform=True, list_to_choice=True\n        )\n        self.replacement = iap.handle_continuous_param(replacement, \"replacement\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=self,\n        )\n\n        nb_images = len(images)\n        rss = ia.derive_random_states(random_state, 2 * nb_images + 1)\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[-1]\n        )\n\n        gen = zip(images, per_channel_samples, rss[:-1:2], rss[1:-1:2])\n        for image, per_channel_i, rs_mask, rs_replacement in gen:\n            height, width, nb_channels = image.shape\n            sampling_shape = (height, width, nb_channels if per_channel_i > 0.5 else 1)\n            mask_samples = self.mask.draw_samples(sampling_shape, random_state=rs_mask)\n\n            # This is slightly faster (~20%) for masks that are True at many locations, but slower (~50%) for masks\n            # with few Trues, which is probably the more common use-case:\n            # replacement_samples = self.replacement.draw_samples(sampling_shape, random_state=rs_replacement)\n            #\n            # # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # # lead to True instead of False).\n            # if image.dtype.kind in [\"i\", \"u\", \"b\"] and replacement_samples.dtype.kind == \"f\":\n            #     replacement_samples = np.round(replacement_samples)\n            #\n            # replacement_samples = iadt.clip_to_dtype_value_range_(replacement_samples, image.dtype, validate=False)\n            # replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n            #\n            # if sampling_shape[2] == 1:\n            #     mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            #     replacement_samples = np.tile(replacement_samples, (1, 1, nb_channels))\n            # mask_thresh = mask_samples > 0.5\n            # image[mask_thresh] = replacement_samples[mask_thresh]\n\n            if sampling_shape[2] == 1:\n                mask_samples = np.tile(mask_samples, (1, 1, nb_channels))\n            mask_thresh = mask_samples > 0.5\n\n            # TODO add separate per_channels for mask and replacement\n            # TODO add test that replacement with per_channel=False is not sampled per channel\n            if per_channel_i <= 0.5:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh[:, :, 0])),), random_state=rs_replacement\n                )\n                # important here to use repeat instead of tile. repeat converts e.g. [0, 1, 2] to [0, 0, 1, 1, 2, 2],\n                # while tile leads to [0, 1, 2, 0, 1, 2]. The assignment below iterates over each channel and pixel\n                # simultaneously, *not* first over all pixels of channel 0, then all pixels in channel 1, ...\n                replacement_samples = np.repeat(\n                    replacement_samples, mask_thresh.shape[2]\n                )\n            else:\n                replacement_samples = self.replacement.draw_samples(\n                    (int(np.sum(mask_thresh)),), random_state=rs_replacement\n                )\n\n            # round, this makes 0.2 e.g. become 0 in case of boolean image (otherwise replacing values with 0.2 would\n            # lead to True instead of False).\n            if (\n                image.dtype.kind in [\"i\", \"u\", \"b\"]\n                and replacement_samples.dtype.kind == \"f\"\n            ):\n                replacement_samples = np.round(replacement_samples)\n\n            replacement_samples = iadt.clip_to_dtype_value_range_(\n                replacement_samples, image.dtype, validate=False\n            )\n            replacement_samples = replacement_samples.astype(image.dtype, copy=False)\n\n            image[mask_thresh] = replacement_samples\n\n        return images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.mask, self.replacement, self.per_channel]\n\n\ndef ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Creates an augmenter to apply impulse noise to an image.\n\n    This is identical to ``SaltAndPepper``, except that per_channel is always set to True.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.SaltAndPepper``.\n\n    \"\"\"\n    return SaltAndPepper(\n        p=p,\n        per_channel=True,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef SaltAndPepper(\n    p=0, per_channel=False, name=None, deterministic=False, random_state=None\n):\n    \"\"\"\n    Adds salt and pepper noise to an image, i.e. some white-ish and black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.SaltAndPepper(0.05)\n\n    Replaces 5 percent of all pixels with salt/pepper.\n\n    \"\"\"\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=iap.Beta(0.5, 0.5) * 255,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSaltAndPepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt and pepper noise to an image, i.e. rectangles that contain noisy white-ish and black-ish pixels.\n\n    TODO replace dtype support with uint8 only, because replacement is geared towards that value range\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt/pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt/pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and ``W`` the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW.``\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSaltAndPepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt/pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement = iap.Beta(0.5, 0.5) * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds salt noise to an image, i.e. white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Salt(0.05)\n\n    Replaces 5 percent of all pixels with salt.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = (\n        replacement01 * 255\n    )  # FIXME max replacement seems to essentially never exceed 254\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarseSalt(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse salt noise to an image, i.e. rectangles containing noisy white-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to salt noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that salt is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where H is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values M, N will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a ``1x1`` low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarseSalt(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with salt in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=True, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n    \"\"\"\n    Adds pepper noise to an image, i.e. black-ish pixels.\n\n    This is similar to dropout, but slower and the black pixels are not uniformly black.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b``.\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Pepper(0.05)\n\n    Replaces 5 percent of all pixels with pepper.\n\n    \"\"\"\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=p,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\ndef CoarsePepper(\n    p=0,\n    size_px=None,\n    size_percent=None,\n    per_channel=False,\n    min_size=4,\n    name=None,\n    deterministic=False,\n    random_state=None,\n):\n    \"\"\"\n    Adds coarse pepper noise to an image, i.e. rectangles that contain noisy black-ish pixels.\n\n    dtype support::\n\n        See ``imgaug.augmenters.arithmetic.ReplaceElementwise``.\n\n    Parameters\n    ----------\n    p : float or tuple of float or list of float or imgaug.parameters.StochasticParameter, optional\n        Probability of changing a pixel to pepper noise.\n\n            * If a float, then that value will be used for all images as the\n              probability.\n            * If a tuple ``(a, b)``, then a probability will be sampled per image\n              from the range ``a <= x <= b.``\n            * If a list, then a random value will be sampled from that list\n              per image.\n            * If a StochasticParameter, then this parameter will be used as\n              the *mask*, i.e. it is expected to contain values between\n              0.0 and 1.0, where 1.0 means that pepper is to be added\n              at that location.\n\n    size_px : int or tuple of int or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask in absolute pixel dimensions.\n\n            * If an integer, then that size will be used for both height and\n              width. E.g. a value of 3 would lead to a ``3x3`` mask, which is then\n              upsampled to ``HxW``, where ``H`` is the image size and W the image width.\n            * If a tuple ``(a, b)``, then two values ``M``, ``N`` will be sampled from the\n              range ``[a..b]`` and the mask will be generated at size ``MxN``, then\n              upsampled to ``HxW``.\n            * If a StochasticParameter, then this parameter will be used to\n              determine the sizes. It is expected to be discrete.\n\n    size_percent : float or tuple of float or imgaug.parameters.StochasticParameter, optional\n        The size of the lower resolution image from which to sample the noise\n        mask *in percent* of the input image.\n\n            * If a float, then that value will be used as the percentage of the\n              height and width (relative to the original size). E.g. for value\n              p, the mask will be sampled from ``(p*H)x(p*W)`` and later upsampled\n              to ``HxW``.\n            * If a tuple ``(a, b)``, then two values ``m``, ``n`` will be sampled from the\n              interval ``(a, b)`` and used as the percentages, i.e the mask size\n              will be ``(m*H)x(n*W)``.\n            * If a StochasticParameter, then this parameter will be used to\n              sample the percentage values. It is expected to be continuous.\n\n    per_channel : bool or float, optional\n        Whether to use the same value (is dropped / is not dropped)\n        for all channels of a pixel (False) or to sample a new value for each\n        channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_size : int, optional\n        Minimum size of the low resolution mask, both width and height. If\n        `size_percent` or `size_px` leads to a lower value than this, `min_size`\n        will be used instead. This should never have a value of less than 2,\n        otherwise one may end up with a 1x1 low resolution mask, leading easily\n        to the whole image being replaced.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.CoarsePepper(0.05, size_percent=(0.01, 0.1))\n\n    Replaces 5 percent of all pixels with pepper in an image that has\n    1 to 10 percent of the input image size, then upscales the results\n    to the input image size, leading to large rectangular areas being replaced.\n\n    \"\"\"\n    mask = iap.handle_probability_param(\n        p, \"p\", tuple_to_uniform=True, list_to_choice=True\n    )\n\n    if size_px is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_px=size_px, min_size=min_size\n        )\n    elif size_percent is not None:\n        mask_low = iap.FromLowerResolution(\n            other_param=mask, size_percent=size_percent, min_size=min_size\n        )\n    else:\n        raise Exception(\"Either size_px or size_percent must be set.\")\n\n    replacement01 = (\n        iap.ForceSign(iap.Beta(0.5, 0.5) - 0.5, positive=False, mode=\"invert\") + 0.5\n    )\n    replacement = replacement01 * 255\n\n    if name is None:\n        name = \"Unnamed%s\" % (ia.caller_name(),)\n\n    return ReplaceElementwise(\n        mask=mask_low,\n        replacement=replacement,\n        per_channel=per_channel,\n        name=name,\n        deterministic=deterministic,\n        random_state=random_state,\n    )\n\n\nclass Invert(meta.Augmenter):\n    \"\"\"\n    Augmenter that inverts all values in images.\n\n    For the standard value range of 0-255 it converts 0 to 255, 255 to 0\n    and 10 to ``(255-10)=245``.\n\n    Let ``M`` be the maximum value possible, ``m`` the minimum value possible,\n    ``v`` a value. Then the distance of ``v`` to ``m`` is ``d=abs(v-m)`` and the new value\n    is given by ``v'=M-d``.\n\n    dtype support::\n\n        if (min_value=None and max_value=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: yes; tested\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: yes; tested\n            * ``float128``: yes; tested\n            * ``bool``: yes; tested\n\n        if (min_value!=None or max_value!=None)::\n\n            * ``uint8``: yes; fully tested\n            * ``uint16``: yes; tested\n            * ``uint32``: yes; tested\n            * ``uint64``: yes; tested\n            * ``int8``: yes; tested\n            * ``int16``: yes; tested\n            * ``int32``: yes; tested\n            * ``int64``: no (1)\n            * ``float16``: yes; tested\n            * ``float32``: yes; tested\n            * ``float64``: no (1)\n            * ``float128``: no (2)\n            * ``bool``: no (3)\n\n            - (1) Not allowed as int/float have to be increased in resolution when using min/max values.\n            - (2) Not tested.\n            - (3) Makes no sense when using min/max values.\n\n    Parameters\n    ----------\n    p : float or imgaug.parameters.StochasticParameter, optional\n        The probability of an image to be inverted.\n\n            * If a float, then that probability will be used for all images.\n            * If a StochasticParameter, then that parameter will queried per\n              image and is expected to return values in the range ``[0.0, 1.0]``,\n              where values ``>0.5`` mean that the image/channel is supposed to be\n              inverted. Recommended to be some form of ``imgaug.parameters.Binomial``.\n\n    per_channel : bool or float, optional\n        Whether to use the same value for all channels (False)\n        or to sample a new value for each channel (True).\n        If this value is a float ``p``, then for ``p`` percent of all images\n        `per_channel` will be treated as True, otherwise as False.\n\n    min_value : None or number, optional\n        Minimum of the value range of input images, e.g. 0 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    max_value : int or float, optional\n        Maximum of the value range of input images, e.g. 255 for uint8 images.\n        If set to None, the value will be automatically derived from the image's dtype.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.Invert(0.1)\n\n    Inverts the colors in 10 percent of all images.\n\n    >>> aug = iaa.Invert(0.1, per_channel=0.5)\n\n    For 50 percent of all images, it inverts all channels with a probability of\n    10 percent (same as the first example). For the other 50 percent of all\n    images, it inverts each channel individually with a probability of 10\n    percent (so some channels of an image may end up inverted, others not).\n\n    \"\"\"\n\n    # when no custom min/max are chosen, all bool, uint, int and float dtypes should be invertable (float tested only\n    # up to 64bit)\n    # when chosing custom min/max:\n    # - bool makes no sense, not allowed\n    # - int and float must be increased in resolution if custom min/max values are chosen,\n    #   hence they are limited to 32 bit and below\n    # - float16 seems to not be perfectly accurate, but still ok-ish -- was off by 10 for center value of\n    #   range (float 16 min, 16), where float 16 min is around -65500\n    ALLOW_DTYPES_CUSTOM_MINMAX = [\n        np.dtype(dt)\n        for dt in [\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n            np.int8,\n            np.int16,\n            np.int32,\n            np.float16,\n            np.float32,\n        ]\n    ]\n\n    def __init__(\n        self,\n        p=0,\n        per_channel=False,\n        min_value=None,\n        max_value=None,\n        name=None,\n        deterministic=False,\n        random_state=None,\n    ):\n        super(Invert, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # TODO allow list and tuple for p\n        self.p = iap.handle_probability_param(p, \"p\")\n        self.per_channel = iap.handle_probability_param(per_channel, \"per_channel\")\n        self.min_value = min_value\n        self.max_value = max_value\n\n        self.dtype_kind_to_invert_func = {\n            \"b\": self._invert_bool,\n            \"u\": self._invert_uint,\n            \"i\": self._invert_int,\n            \"f\": self._invert_float,\n        }\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        nb_images = len(images)\n        nb_channels = meta.estimate_max_number_of_channels(images)\n        rss = ia.derive_random_states(random_state, 2)\n        p_samples = self.p.draw_samples((nb_images, nb_channels), random_state=rss[0])\n        per_channel_samples = self.per_channel.draw_samples(\n            (nb_images,), random_state=rss[1]\n        )\n\n        for image, per_channel_samples_i, p_samples_i in zip(\n            images, per_channel_samples, p_samples\n        ):\n            min_value_dt, _, max_value_dt = iadt.get_value_range_of_dtype(image.dtype)\n            min_value = min_value_dt if self.min_value is None else self.min_value\n            max_value = max_value_dt if self.max_value is None else self.max_value\n            assert min_value >= min_value_dt, (\n                \"Expected min_value to be above or equal to dtype's min value, got %s (vs. min possible %s for %s)\"\n                % (str(min_value), str(min_value_dt), image.dtype.name)\n            )\n            assert max_value <= max_value_dt, (\n                \"Expected max_value to be below or equal to dtype's max value, got %s (vs. max possible %s for %s)\"\n                % (str(max_value), str(max_value_dt), image.dtype.name)\n            )\n            assert (\n                min_value < max_value\n            ), \"Expected min_value to be below max_value, got %s and %s\" % (\n                str(min_value),\n                str(max_value),\n            )\n\n            if min_value != min_value_dt or max_value != max_value_dt:\n                ia.do_assert(\n                    image.dtype.type in self.ALLOW_DTYPES_CUSTOM_MINMAX,\n                    \"Can use custom min/max values only with the following dtypes: %s. Got: %s.\"\n                    % (\n                        \", \".join([dt.name for dt in self.ALLOW_DTYPES_CUSTOM_MINMAX]),\n                        image.dtype.name,\n                    ),\n                )\n\n            _invertfunc = self.dtype_kind_to_invert_func[image.dtype.kind]\n\n            if per_channel_samples_i > 0.5:\n                for c, p_sample in enumerate(p_samples_i):\n                    if p_sample > 0.5:\n                        image[..., c] = _invertfunc(image[..., c], min_value, max_value)\n            else:\n                if p_samples_i[0] > 0.5:\n                    image[:, :, :] = _invertfunc(image, min_value, max_value)\n\n        return images\n\n    @classmethod\n    def _invert_bool(cls, arr, min_value, max_value):\n        ia.do_assert(\n            min_value == 0, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        ia.do_assert(\n            max_value == 1, \"Cannot modify min/max value for bool arrays in Invert.\"\n        )\n        return ~arr\n\n    @classmethod\n    def _invert_uint(cls, arr, min_value, max_value):\n        if min_value == 0 and max_value == np.iinfo(arr.dtype).max:\n            return max_value - arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_int(cls, arr, min_value, max_value):\n        # note that for int dtypes the max value is\n        #   (-1) * min_value - 1\n        # e.g. -128 and 127 (min/max) for int8\n        # mapping example:\n        #  [-4, -3, -2, -1,  0,  1,  2,  3]\n        # will be mapped to\n        #  [ 3,  2,  1,  0, -1, -2, -3, -4]\n        # hence we can not simply compute the inverse as:\n        #  after = (-1) * before\n        # but instead need\n        #  after = (-1) * before - 1\n        # however, this exceeds the value range for the minimum value, e.g. for int8: -128 -> 128 -> 127,\n        # where 128 exceeds it. Hence, we must compute the inverse via a mask (extra step for the minimum)\n        # or we have to increase the resolution of the array. Here, a two-step approach is used.\n\n        if min_value == (-1) * max_value - 1:\n            mask = arr == min_value\n\n            # there is probably a one-liner here to do this, but\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value\n            # has the disadvantage of inverting min_value to max_value - 1\n            # while\n            #  ((-1) * (arr * ~mask) - 1) + mask * (max_value+1)\n            #  ((-1) * (arr * ~mask) - 1) + mask * max_value + mask\n            # both sometimes increase the dtype resolution (e.g. int32 to int64)\n            n_min = np.sum(mask)\n            if n_min > 0:\n                arr[mask] = max_value\n            if n_min < arr.size:\n                arr[~mask] = (-1) * arr[~mask] - 1\n            return arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_float(cls, arr, min_value, max_value):\n        if np.isclose(max_value, (-1) * min_value, rtol=0):\n            return (-1) * arr\n        else:\n            return cls._invert_by_distance(\n                np.clip(arr, min_value, max_value), min_value, max_value\n            )\n\n    @classmethod\n    def _invert_by_distance(cls, arr, min_value, max_value):\n        arr_modify = arr\n        if arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.increase_array_resolutions_([np.copy(arr)], 2)[0]\n        distance_from_min = np.abs(arr_modify - min_value)  # d=abs(v-min)\n        arr_modify = max_value - distance_from_min  # v'=MAX-d\n        # due to floating point inaccuracies, we might exceed the min/max values for floats here, hence clip\n        # this happens especially for values close to the float dtype's maxima\n        if arr.dtype.kind == \"f\":\n            arr_modify = np.clip(arr_modify, min_value, max_value)\n        elif arr.dtype.kind in [\"i\", \"f\"]:\n            arr_modify = iadt.restore_dtypes_(arr_modify, [arr.dtype], clip=False)\n        return arr_modify\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.p, self.per_channel, self.min_value, self.max_value]\n\n\n# TODO remove from examples and mark as deprecated\n\n\nclass JpegCompression(meta.Augmenter):\n    \"\"\"\n    Degrade image quality by applying JPEG compression to it.\n\n    During JPEG compression, high frequency components (e.g. edges) are removed.\n    With low compression (strength) only the highest frequency components are\n    removed, while very high compression (strength) will lead to only the lowest\n    frequency components \"surviving\". This lowers the image quality. For more\n    details, see https://en.wikipedia.org/wiki/Compression_artifact.\n\n    Note that this augmenter still returns images as numpy arrays (i.e. saves\n    the images with JPEG compression and then reloads them into arrays). It\n    does not return the raw JPEG file content.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    compression : number or tuple of number or list of number or \\\n                  imgaug.parameters.StochasticParameter, optional\n        Degree of compression used during jpeg compression within value range\n        ``[0, 100]``. Higher values denote stronger compression and will cause\n        low-frequency components to disappear. Note that JPEG's compression\n        strength is also often set as a *quality*, which is the inverse of this\n        parameter. Common choices for the *quality* setting are around 80 to 95,\n        depending on the image. This translates here to a *compression*\n        parameter of around 20 to 5.\n\n            * If a single number, then that value will be used for the\n              compression degree.\n            * If a tuple of two number ``(a, b)``, then the compression will be\n              a value sampled from the interval ``[a..b]``.\n            * If a list, then a random value will be sampled and used as the\n              compression per image.\n            * If a StochasticParameter, then ``N`` samples will be drawn from\n              that parameter per ``N`` input images, each representing the\n              compression for the nth image. Expected to be discrete.\n\n    name : None or str, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    deterministic : bool, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    random_state : None or int or numpy.random.RandomState, optional\n        See :func:`imgaug.augmenters.meta.Augmenter.__init__`.\n\n    Examples\n    --------\n    >>> aug = iaa.JpegCompression(compression=(80, 95))\n\n    Removes high frequency components in images based on JPEG compression with\n    a *compression strength* between 80 and 95 (randomly sampled per image).\n    This corresponds to a (very low) *quality* setting of 5 to 20.\n\n    \"\"\"\n\n    def __init__(\n        self, compression=50, name=None, deterministic=False, random_state=None\n    ):\n        super(JpegCompression, self).__init__(\n            name=name, deterministic=deterministic, random_state=random_state\n        )\n\n        # will be converted to int during augmentation, which is why we allow floats here\n        self.compression = iap.handle_continuous_param(\n            compression,\n            \"compression\",\n            value_range=(0, 100),\n            tuple_to_uniform=True,\n            list_to_choice=True,\n        )\n\n        # The value range 1 to 95 is suggested by PIL's save() documentation\n        # Values above 95 seem to not make sense (no improvement in visual quality, but large file size)\n        # A value of 100 would mostly deactivate jpeg compression\n        # A value of 0 would lead to no compression (instead of maximum compression)\n        # We use range 1 to 100 here, because this augmenter is about generating images for training\n        # and not for saving, hence we do not care about large file sizes\n        self.maximum_quality = 100\n        self.minimum_quality = 1\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        result = images\n        nb_images = len(images)\n        samples = self.compression.draw_samples((nb_images,), random_state=random_state)\n\n        for i, (image, sample) in enumerate(zip(images, samples)):\n            ia.do_assert(\n                image.dtype.name == \"uint8\",\n                \"Can apply jpeg compression only to uint8 images.\",\n            )\n            nb_channels = image.shape[-1]\n            is_single_channel = nb_channels == 1\n            if is_single_channel:\n                image = image[..., 0]\n            sample = int(sample)\n            ia.do_assert(100 >= sample >= 0)\n            image_pil = PIL_Image.fromarray(image)\n            with tempfile.NamedTemporaryFile(mode=\"wb+\", suffix=\".jpg\") as f:\n                # Map from compression to quality used by PIL\n                # We have valid compressions from 0 to 100, i.e. 101 possible values\n                quality = int(\n                    np.clip(\n                        np.round(\n                            self.minimum_quality\n                            + (self.maximum_quality - self.minimum_quality)\n                            * (1.0 - (sample / 101))\n                        ),\n                        self.minimum_quality,\n                        self.maximum_quality,\n                    )\n                )\n\n                image_pil.save(f, quality=quality)\n\n                # read back from file\n                # we dont read from f.name, because that leads to\n                # PermissionDenied errors on windows\n                # we add f.seek(0) here, because otherwise we get\n                # SyntaxError: index out of range\n                # from PIL\n                f.seek(0)\n                if nb_channels == 1:\n                    image = imageio.imread(f, pilmode=\"L\", format=\"jpeg\")\n                else:\n                    image = imageio.imread(f, pilmode=\"RGB\", format=\"jpeg\")\n            if is_single_channel:\n                image = image[..., np.newaxis]\n            result[i] = image\n        return result\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return [self.compression]\n", "levels": [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], "package": ["from __future__ import print_function, division, absolute_import", "from PIL import Image as PIL_Image", "import imageio", "import tempfile", "import numpy as np", "import cv2", "from . import meta", "import imgaug as ia", "from .. import parameters as iap", "from .. import dtypes as iadt"], "function": ["class Add(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class AddElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class Multiply(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class MultiplyElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def Dropout(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class ReplaceElementwise(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "def ImpulseNoise(p=0, name=None, deterministic=False, random_state=None):\n", "def Salt(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "def Pepper(p=0, per_channel=False, name=None, deterministic=False, random_state=None):\n", "class Invert(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _invert_bool(cls, arr, min_value, max_value):\n", "    def _invert_uint(cls, arr, min_value, max_value):\n", "    def _invert_int(cls, arr, min_value, max_value):\n", "    def _invert_float(cls, arr, min_value, max_value):\n", "    def _invert_by_distance(cls, arr, min_value, max_value):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n", "class JpegCompression(meta.Augmenter):\n", "    def _augment_images(self, images, random_state, parents, hooks):\n", "    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n", "    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "    def get_parameters(self):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "is_single_float", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.", "docstring_tokens": ["Checks", "whether", "a", "variable", "is", "a", "float", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L94-L109", "partition": "valid", "up_fun_num": 2, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "is_integer_array", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.", "docstring_tokens": ["Checks", "whether", "a", "variable", "is", "a", "numpy", "integer", "array", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L185-L200", "partition": "valid", "up_fun_num": 7, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "is_float_array", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.", "docstring_tokens": ["Checks", "whether", "a", "variable", "is", "a", "numpy", "float", "array", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L203-L218", "partition": "valid", "up_fun_num": 8, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "new_random_state", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.", "docstring_tokens": ["Returns", "a", "new", "random", "state", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L336-L363", "partition": "valid", "up_fun_num": 15, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "copy_random_state", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.", "docstring_tokens": ["Creates", "a", "copy", "of", "a", "random", "state", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L379-L405", "partition": "valid", "up_fun_num": 17, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "derive_random_states", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.", "docstring_tokens": ["Create", "N", "new", "random", "states", "based", "on", "an", "existing", "random", "state", "or", "seed", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L427-L446", "partition": "valid", "up_fun_num": 19, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "_quokka_normalize_extract", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.", "docstring_tokens": ["Generate", "a", "normalized", "rectangle", "to", "be", "extract", "from", "the", "standard", "quokka", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L464-L506", "partition": "valid", "up_fun_num": 21, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "_compute_resized_shape", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.", "docstring_tokens": ["Computes", "the", "intended", "new", "shape", "of", "an", "image", "-", "like", "array", "after", "resizing", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L509-L574", "partition": "valid", "up_fun_num": 22, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "quokka", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.", "docstring_tokens": ["Returns", "an", "image", "of", "a", "quokka", "as", "a", "numpy", "array", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L577-L614", "partition": "valid", "up_fun_num": 23, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "quokka_segmentation_map", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.", "docstring_tokens": ["Returns", "a", "segmentation", "map", "for", "the", "standard", "example", "quokka", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L678-L725", "partition": "valid", "up_fun_num": 26, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "quokka_keypoints", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.", "docstring_tokens": ["Returns", "example", "keypoints", "on", "the", "standard", "example", "quokke", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L728-L771", "partition": "valid", "up_fun_num": 27, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "quokka_bounding_boxes", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.", "docstring_tokens": ["Returns", "example", "bounding", "boxes", "on", "the", "standard", "example", "quokke", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L774-L824", "partition": "valid", "up_fun_num": 28, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "quokka_polygons", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.", "docstring_tokens": ["Returns", "example", "polygons", "on", "the", "standard", "example", "quokke", "image", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L827-L875", "partition": "valid", "up_fun_num": 29, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"\n    Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...\n\n    \"\"\"\n    l1 = np.linalg.norm(v1)\n    l2 = np.linalg.norm(v2)\n    v1_u = (v1 / l1) if l1 > 0 else np.float32(v1) * 0\n    v2_u = (v2 / l2) if l2 > 0 else np.float32(v2) * 0\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def angle_between_vectors(v1, v2):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
{"repo": "aleju/imgaug", "path": "imgaug/imgaug.py", "func_name": "angle_between_vectors", "original_string": "", "language": "python", "code": "", "code_tokens": [], "docstring": "Returns the angle in radians between vectors `v1` and `v2`.\n\n    From http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python\n\n    Parameters\n    ----------\n    v1 : (N,) ndarray\n        First vector.\n\n    v2 : (N,) ndarray\n        Second vector.\n\n    Returns\n    -------\n    out : float\n        Angle in radians.\n\n    Examples\n    --------\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([0, 1, 0]))\n    1.570796...\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([1, 0, 0]))\n    0.0\n\n    >>> angle_between_vectors(np.float32([1, 0, 0]), np.float32([-1, 0, 0]))\n    3.141592...", "docstring_tokens": ["Returns", "the", "angle", "in", "radians", "between", "vectors", "v1", "and", "v2", "."], "sha": "786be74aa855513840113ea523c5df495dc6a8af", "url": "https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L878-L913", "partition": "valid", "up_fun_num": 30, "context": "from __future__ import print_function, division, absolute_import\n\nimport math\nimport numbers\nimport sys\nimport os\nimport json\nimport types\nimport functools\n\nimport numpy as np\nimport cv2\nimport imageio\nimport six\nimport six.moves as sm\nimport skimage.draw\nimport skimage.measure\nimport collections\nfrom PIL import (\n    Image as PIL_Image,\n    ImageDraw as PIL_ImageDraw,\n    ImageFont as PIL_ImageFont,\n)\n\nALL = \"ALL\"\n\nFILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# filepath to the quokka image, its annotations and depth map\nQUOKKA_FP = os.path.join(FILE_DIR, \"quokka.jpg\")\nQUOKKA_ANNOTATIONS_FP = os.path.join(FILE_DIR, \"quokka_annotations.json\")\nQUOKKA_DEPTH_MAP_HALFRES_FP = os.path.join(FILE_DIR, \"quokka_depth_map_halfres.png\")\n\nDEFAULT_FONT_FP = os.path.join(\n    os.path.dirname(os.path.abspath(__file__)), \"DejaVuSans.ttf\"\n)\n\n# We instantiate a current/global random state here once.\n# One can also call np.random, but that is (in contrast to np.random.RandomState)\n# a module and hence cannot be copied via deepcopy. That's why we use RandomState\n# here (and in all augmenters) instead of np.random.\nCURRENT_RANDOM_STATE = np.random.RandomState(42)\nSEED_MIN_VALUE = 0\nSEED_MAX_VALUE = (\n    2 ** 31 - 1\n)  # use 2**31 instead of 2**32 here because 2**31 errored on some systems\n\n\n# to check if a dtype instance is among these dtypes, use e.g. `dtype.type in NP_FLOAT_TYPES`\n# do not just use `dtype in NP_FLOAT_TYPES` as that would fail\nNP_FLOAT_TYPES = set(np.sctypes[\"float\"])\nNP_INT_TYPES = set(np.sctypes[\"int\"])\nNP_UINT_TYPES = set(np.sctypes[\"uint\"])\n\nIMSHOW_BACKEND_DEFAULT = \"matplotlib\"\n\nIMRESIZE_VALID_INTERPOLATIONS = [\n    \"nearest\",\n    \"linear\",\n    \"area\",\n    \"cubic\",\n    cv2.INTER_NEAREST,\n    cv2.INTER_LINEAR,\n    cv2.INTER_AREA,\n    cv2.INTER_CUBIC,\n]\n\n\ndef is_np_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    out : bool\n        True if the variable is a numpy array. Otherwise False.\n\n    \"\"\"\n    # using np.generic here via isinstance(val, (np.ndarray, np.generic)) seems to also fire for scalar numpy values\n    # even though those are not arrays\n    return isinstance(val, np.ndarray)\n\n\ndef is_single_integer(val):\n    \"\"\"\n    Checks whether a variable is an integer.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an integer. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, numbers.Integral) and not isinstance(val, bool)\n\n\ndef is_single_float(val):\n    \"\"\"\n    Checks whether a variable is a float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a float. Otherwise False.\n\n    \"\"\"\n    return (\n        isinstance(val, numbers.Real)\n        and not is_single_integer(val)\n        and not isinstance(val, bool)\n    )\n\n\ndef is_single_number(val):\n    \"\"\"\n    Checks whether a variable is a number, i.e. an integer or float.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a number. Otherwise False.\n\n    \"\"\"\n    return is_single_integer(val) or is_single_float(val)\n\n\ndef is_iterable(val):\n    \"\"\"\n    Checks whether a variable is iterable.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is an iterable. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, collections.Iterable)\n\n\n# TODO convert to is_single_string() or rename is_single_integer/float/number()\ndef is_string(val):\n    \"\"\"\n    Checks whether a variable is a string.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a string. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, six.string_types)\n\n\ndef is_single_bool(val):\n    \"\"\"\n    Checks whether a variable is a boolean.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a boolean. Otherwise False.\n\n    \"\"\"\n    return type(val) == type(True)\n\n\ndef is_integer_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy integer array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy integer array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.integer)\n\n\ndef is_float_array(val):\n    \"\"\"\n    Checks whether a variable is a numpy float array.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a numpy float array. Otherwise False.\n\n    \"\"\"\n    return is_np_array(val) and issubclass(val.dtype.type, np.floating)\n\n\ndef is_callable(val):\n    \"\"\"\n    Checks whether a variable is a callable, e.g. a function.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True if the variable is a callable. Otherwise False.\n\n    \"\"\"\n    # python 3.x with x <= 2 does not support callable(), apparently\n    if sys.version_info[0] == 3 and sys.version_info[1] <= 2:\n        return hasattr(val, \"__call__\")\n    else:\n        return callable(val)\n\n\ndef is_generator(val):\n    \"\"\"\n    Checks whether a variable is a generator.\n\n    Parameters\n    ----------\n    val\n        The variable to check.\n\n    Returns\n    -------\n    bool\n        True is the variable is a generator. Otherwise False.\n\n    \"\"\"\n    return isinstance(val, types.GeneratorType)\n\n\ndef flatten(nested_iterable):\n    \"\"\"\n    Flattens arbitrarily nested lists/tuples.\n\n    Code partially taken from https://stackoverflow.com/a/10824420.\n\n    Parameters\n    ----------\n    nested_iterable\n        A list or tuple of arbitrarily nested values.\n\n    Yields\n    ------\n    any\n        Non-list and non-tuple values in `nested_iterable`.\n\n    \"\"\"\n    # don't just check if something is iterable here, because then strings\n    # and arrays will be split into their characters and components\n    if not isinstance(nested_iterable, (list, tuple)):\n        yield nested_iterable\n    else:\n        for i in nested_iterable:\n            if isinstance(i, (list, tuple)):\n                for j in flatten(i):\n                    yield j\n            else:\n                yield i\n\n\ndef caller_name():\n    \"\"\"\n    Returns the name of the caller, e.g. a function.\n\n    Returns\n    -------\n    str\n        The name of the caller as a string\n\n    \"\"\"\n    return sys._getframe(1).f_code.co_name\n\n\ndef seed(seedval):\n    \"\"\"\n    Set the seed used by the global random state and thereby all randomness\n    in the library.\n\n    This random state is by default by all augmenters. Under special\n    circumstances (e.g. when an augmenter is switched to deterministic mode),\n    the global random state is replaced by another -- local -- one.\n    The replacement is dependent on the global random state.\n\n    Parameters\n    ----------\n    seedval : int\n        The seed to use.\n\n    \"\"\"\n    CURRENT_RANDOM_STATE.seed(seedval)\n\n\ndef current_random_state():\n    \"\"\"\n    Returns the current/global random state of the library.\n\n    Returns\n    ----------\n    numpy.random.RandomState\n        The current/global random state.\n\n    \"\"\"\n    return CURRENT_RANDOM_STATE\n\n\ndef new_random_state(seed=None, fully_random=False):\n    \"\"\"\n    Returns a new random state.\n\n    Parameters\n    ----------\n    seed : None or int, optional\n        Optional seed value to use.\n        The same datatypes are allowed as for ``numpy.random.RandomState(seed)``.\n\n    fully_random : bool, optional\n        Whether to use numpy's random initialization for the\n        RandomState (used if set to True). If False, a seed is sampled from\n        the global random state, which is a bit faster and hence the default.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    if seed is None:\n        if not fully_random:\n            # sample manually a seed instead of just RandomState(),\n            # because the latter one\n            # is way slower.\n            seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return np.random.RandomState(seed)\n\n\ndef dummy_random_state():\n    \"\"\"\n    Returns a dummy random state that is always based on a seed of 1.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        The new random state.\n\n    \"\"\"\n    return np.random.RandomState(1)\n\n\ndef copy_random_state(random_state, force_copy=False):\n    \"\"\"\n    Creates a copy of a random state.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        The random state to copy.\n\n    force_copy : bool, optional\n        If True, this function will always create a copy of every random\n        state. If False, it will not copy numpy's default random state,\n        but all other random states.\n\n    Returns\n    -------\n    rs_copy : numpy.random.RandomState\n        The copied random state.\n\n    \"\"\"\n    if random_state == np.random and not force_copy:\n        return random_state\n    else:\n        rs_copy = dummy_random_state()\n        orig_state = random_state.get_state()\n        rs_copy.set_state(orig_state)\n        return rs_copy\n\n\ndef derive_random_state(random_state):\n    \"\"\"\n    Create a new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive the new random state.\n\n    Returns\n    -------\n    numpy.random.RandomState\n        Derived random state.\n\n    \"\"\"\n    return derive_random_states(random_state, n=1)[0]\n\n\n# TODO use this everywhere instead of manual seed + create\ndef derive_random_states(random_state, n=1):\n    \"\"\"\n    Create N new random states based on an existing random state or seed.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state or seed from which to derive new random states.\n\n    n : int, optional\n        Number of random states to derive.\n\n    Returns\n    -------\n    list of numpy.random.RandomState\n        Derived random states.\n\n    \"\"\"\n    seed_ = random_state.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0]\n    return [new_random_state(seed_ + i) for i in sm.xrange(n)]\n\n\ndef forward_random_state(random_state):\n    \"\"\"\n    Forward the internal state of a random state.\n\n    This makes sure that future calls to the random_state will produce new random values.\n\n    Parameters\n    ----------\n    random_state : numpy.random.RandomState\n        Random state to forward.\n\n    \"\"\"\n    random_state.uniform()\n\n\ndef _quokka_normalize_extract(extract):\n    \"\"\"\n    Generate a normalized rectangle to be extract from the standard quokka image.\n\n    Parameters\n    ----------\n    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Unnormalized representation of the image subarea to be extracted.\n\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``\n              will be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    bb : imgaug.BoundingBox\n        Normalized representation of the area to extract from the standard quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    if extract == \"square\":\n        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)\n    elif isinstance(extract, tuple) and len(extract) == 4:\n        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])\n    elif isinstance(extract, BoundingBox):\n        bb = extract\n    elif isinstance(extract, BoundingBoxesOnImage):\n        do_assert(len(extract.bounding_boxes) == 1)\n        do_assert(extract.shape[0:2] == (643, 960))\n        bb = extract.bounding_boxes[0]\n    else:\n        raise Exception(\n            \"Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage \"\n            + \"for parameter 'extract', got %s.\" % (type(extract),)\n        )\n    return bb\n\n\ndef _compute_resized_shape(from_shape, to_shape):\n    \"\"\"\n    Computes the intended new shape of an image-like array after resizing.\n\n    Parameters\n    ----------\n    from_shape : tuple or ndarray\n        Old shape of the array. Usually expected to be a tuple of form ``(H, W)`` or ``(H, W, C)`` or\n        alternatively an array with two or three dimensions.\n\n    to_shape : None or tuple of ints or tuple of floats or int or float or ndarray\n        New shape of the array.\n\n            * If None, then `from_shape` will be used as the new shape.\n            * If an int ``V``, then the new shape will be ``(V, V, [C])``, where ``C`` will be added if it\n              is part of `from_shape`.\n            * If a float ``V``, then the new shape will be ``(H*V, W*V, [C])``, where ``H`` and ``W`` are the old\n              height/width.\n            * If a tuple ``(H', W', [C'])`` of ints, then ``H'`` and ``W'`` will be used as the new height\n              and width.\n            * If a tuple ``(H', W', [C'])`` of floats (except ``C``), then ``H'`` and ``W'`` will\n              be used as the new height and width.\n            * If a numpy array, then the array's shape will be used.\n\n    Returns\n    -------\n    to_shape_computed : tuple of int\n        New shape.\n\n    \"\"\"\n    if is_np_array(from_shape):\n        from_shape = from_shape.shape\n    if is_np_array(to_shape):\n        to_shape = to_shape.shape\n\n    to_shape_computed = list(from_shape)\n\n    if to_shape is None:\n        pass\n    elif isinstance(to_shape, tuple):\n        do_assert(len(from_shape) in [2, 3])\n        do_assert(len(to_shape) in [2, 3])\n\n        if len(from_shape) == 3 and len(to_shape) == 3:\n            do_assert(from_shape[2] == to_shape[2])\n        elif len(to_shape) == 3:\n            to_shape_computed.append(to_shape[2])\n\n        do_assert(\n            all([v is None or is_single_number(v) for v in to_shape[0:2]]),\n            \"Expected the first two entries in to_shape to be None or numbers, \"\n            + \"got types %s.\" % (str([type(v) for v in to_shape[0:2]]),),\n        )\n\n        for i, from_shape_i in enumerate(from_shape[0:2]):\n            if to_shape[i] is None:\n                to_shape_computed[i] = from_shape_i\n            elif is_single_integer(to_shape[i]):\n                to_shape_computed[i] = to_shape[i]\n            else:  # float\n                to_shape_computed[i] = int(np.round(from_shape_i * to_shape[i]))\n    elif is_single_integer(to_shape) or is_single_float(to_shape):\n        to_shape_computed = _compute_resized_shape(from_shape, (to_shape, to_shape))\n    else:\n        raise Exception(\n            \"Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int \"\n            + \"or single float, got %s.\" % (type(to_shape),)\n        )\n\n    return tuple(to_shape_computed)\n\n\ndef quokka(size=None, extract=None):\n    \"\"\"\n    Returns an image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea of the quokka image to extract:\n\n            * If None, then the whole image will be used.\n            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)`` will\n              be extracted from the image.\n            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``\n              and ``y2``.\n            * If a BoundingBox, then that bounding box's area will be extracted from the image.\n            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box\n              and a shape matching the full image dimensions (i.e. ``(643, 960, *)``). Then the\n              one bounding box will be used similar to BoundingBox.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    img = imageio.imread(QUOKKA_FP, pilmode=\"RGB\")\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is not None:\n        shape_resized = _compute_resized_shape(img.shape, size)\n        img = imresize_single_image(img, shape_resized[0:2])\n    return img\n\n\ndef quokka_square(size=None):\n    \"\"\"\n    Returns an (square) image of a quokka as a numpy array.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        Size of the output image. Input into :func:`imgaug.imgaug.imresize_single_image`.\n        Usually expected to be a tuple ``(H, W)``, where ``H`` is the desired height\n        and ``W`` is the width. If None, then the image will not be resized.\n\n    Returns\n    -------\n    img : (H,W,3) ndarray\n        The image array of dtype uint8.\n\n    \"\"\"\n    return quokka(size=size, extract=\"square\")\n\n\ndef quokka_heatmap(size=None, extract=None):\n    \"\"\"\n    Returns a heatmap (here: depth map) for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.HeatmapsOnImage\n        Depth map as an heatmap object. Values close to 0.0 denote objects that are close to\n        the camera. Values close to 1.0 denote objects that are furthest away (among all shown\n        objects).\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.heatmaps import HeatmapsOnImage\n\n    img = imageio.imread(QUOKKA_DEPTH_MAP_HALFRES_FP, pilmode=\"RGB\")\n    img = imresize_single_image(img, (643, 960), interpolation=\"cubic\")\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img = bb.extract_from_image(img)\n    if size is None:\n        size = img.shape[0:2]\n\n    shape_resized = _compute_resized_shape(img.shape, size)\n    img = imresize_single_image(img, shape_resized[0:2])\n    img_0to1 = img[..., 0]  # depth map was saved as 3-channel RGB\n    img_0to1 = img_0to1.astype(np.float32) / 255.0\n    img_0to1 = 1 - img_0to1  # depth map was saved as 0 being furthest away\n\n    return HeatmapsOnImage(img_0to1, shape=img_0to1.shape[0:2] + (3,))\n\n\ndef quokka_segmentation_map(size=None, extract=None):\n    \"\"\"\n    Returns a segmentation map for the standard example quokka image.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int, optional\n        See :func:`imgaug.quokka`.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    result : imgaug.SegmentationMapOnImage\n        Segmentation map object.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.segmaps import SegmentationMapOnImage\n\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n\n    xx = []\n    yy = []\n    for kp_dict in json_dict[\"polygons\"][0][\"keypoints\"]:\n        x = kp_dict[\"x\"]\n        y = kp_dict[\"y\"]\n        xx.append(x)\n        yy.append(y)\n\n    img_seg = np.zeros((643, 960, 1), dtype=np.float32)\n    rr, cc = skimage.draw.polygon(np.array(yy), np.array(xx), shape=img_seg.shape)\n    img_seg[rr, cc] = 1.0\n\n    if extract is not None:\n        bb = _quokka_normalize_extract(extract)\n        img_seg = bb.extract_from_image(img_seg)\n\n    segmap = SegmentationMapOnImage(img_seg, shape=img_seg.shape[0:2] + (3,))\n\n    if size is not None:\n        shape_resized = _compute_resized_shape(img_seg.shape, size)\n        segmap = segmap.resize(shape_resized[0:2])\n        segmap.shape = tuple(shape_resized[0:2]) + (3,)\n\n    return segmap\n\n\ndef quokka_keypoints(size=None, extract=None):\n    \"\"\"\n    Returns example keypoints on the standard example quokke image.\n\n    The keypoints cover the eyes, ears, nose and paws.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the keypoints are placed. If None, then the keypoints\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    kpsoi : imgaug.KeypointsOnImage\n        Example keypoints on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    keypoints = []\n    for kp_dict in json_dict[\"keypoints\"]:\n        keypoints.append(Keypoint(x=kp_dict[\"x\"] - left, y=kp_dict[\"y\"] - top))\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    kpsoi = KeypointsOnImage(keypoints, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        kpsoi = kpsoi.on(shape_resized)\n    return kpsoi\n\n\ndef quokka_bounding_boxes(size=None, extract=None):\n    \"\"\"\n    Returns example bounding boxes on the standard example quokke image.\n\n    Currently only a single bounding box is returned that covers the quokka.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the BBs are placed. If None, then the BBs\n        are not projected to any new size (positions on the original image are used).\n        Floats lead to relative size changes, ints to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    bbsoi : imgaug.BoundingBoxesOnImage\n        Example BBs on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    bbs = []\n    for bb_dict in json_dict[\"bounding_boxes\"]:\n        bbs.append(\n            BoundingBox(\n                x1=bb_dict[\"x1\"] - left,\n                y1=bb_dict[\"y1\"] - top,\n                x2=bb_dict[\"x2\"] - left,\n                y2=bb_dict[\"y2\"] - top,\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    bbsoi = BoundingBoxesOnImage(bbs, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        bbsoi = bbsoi.on(shape_resized)\n    return bbsoi\n\n\ndef quokka_polygons(size=None, extract=None):\n    \"\"\"\n    Returns example polygons on the standard example quokke image.\n\n    The result contains one polygon, covering the quokka's outline.\n\n    Parameters\n    ----------\n    size : None or float or tuple of int or tuple of float, optional\n        Size of the output image on which the polygons are placed. If None,\n        then the polygons are not projected to any new size (positions on the\n        original image are used). Floats lead to relative size changes, ints\n        to absolute sizes in pixels.\n\n    extract : None or 'square' or tuple of number or imgaug.BoundingBox or \\\n              imgaug.BoundingBoxesOnImage\n        Subarea to extract from the image. See :func:`imgaug.quokka`.\n\n    Returns\n    -------\n    psoi : imgaug.PolygonsOnImage\n        Example polygons on the quokka image.\n\n    \"\"\"\n    # TODO get rid of this deferred import\n    from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n\n    left, top = 0, 0\n    if extract is not None:\n        bb_extract = _quokka_normalize_extract(extract)\n        left = bb_extract.x1\n        top = bb_extract.y1\n    with open(QUOKKA_ANNOTATIONS_FP, \"r\") as f:\n        json_dict = json.load(f)\n    polygons = []\n    for poly_json in json_dict[\"polygons\"]:\n        polygons.append(\n            Polygon(\n                [\n                    (point[\"x\"] - left, point[\"y\"] - top)\n                    for point in poly_json[\"keypoints\"]\n                ]\n            )\n        )\n    if extract is not None:\n        shape = (bb_extract.height, bb_extract.width, 3)\n    else:\n        shape = (643, 960, 3)\n    psoi = PolygonsOnImage(polygons, shape=shape)\n    if size is not None:\n        shape_resized = _compute_resized_shape(shape, size)\n        psoi = psoi.on(shape_resized)\n    return psoi\n\n\n# TODO is this used anywhere?\ndef compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n    \"\"\"\n    Compute the intersection point of two lines.\n\n    Taken from https://stackoverflow.com/a/20679579 .\n\n    Parameters\n    ----------\n    x1 : number\n        x coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    y1 : number\n        y coordinate of the first point on line 1. (The lines extends beyond this point.)\n\n    x2 : number\n        x coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    y2 : number\n        y coordinate of the second point on line 1. (The lines extends beyond this point.)\n\n    x3 : number\n        x coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    y3 : number\n        y coordinate of the first point on line 2. (The lines extends beyond this point.)\n\n    x4 : number\n        x coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    y4 : number\n        y coordinate of the second point on line 2. (The lines extends beyond this point.)\n\n    Returns\n    -------\n    tuple of number or bool\n        The coordinate of the intersection point as a tuple ``(x, y)``.\n        If the lines are parallel (no intersection point or an infinite number of them), the result is False.\n\n    \"\"\"\n\n    def _make_line(p1, p2):\n        A = p1[1] - p2[1]\n        B = p2[0] - p1[0]\n        C = p1[0] * p2[1] - p2[0] * p1[1]\n        return A, B, -C\n\n    L1 = _make_line((x1, y1), (x2, y2))\n    L2 = _make_line((x3, y3), (x4, y4))\n\n    D = L1[0] * L2[1] - L1[1] * L2[0]\n    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n    Dy = L1[0] * L2[2] - L1[2] * L2[0]\n    if D != 0:\n        x = Dx / D\n        y = Dy / D\n        return x, y\n    else:\n        return False\n\n\n# TODO replace by cv2.putText()?\ndef draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n    \"\"\"\n    Draw text on an image.\n\n    This uses by default DejaVuSans as its font, which is included in this library.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: no\n        * ``uint32``: no\n        * ``uint64``: no\n        * ``int8``: no\n        * ``int16``: no\n        * ``int32``: no\n        * ``int64``: no\n        * ``float16``: no\n        * ``float32``: yes; not tested\n        * ``float64``: no\n        * ``float128``: no\n        * ``bool``: no\n\n        TODO check if other dtypes could be enabled\n\n    Parameters\n    ----------\n    img : (H,W,3) ndarray\n        The image array to draw text on.\n        Expected to be of dtype uint8 or float32 (value range 0.0 to 255.0).\n\n    y : int\n        x-coordinate of the top left corner of the text.\n\n    x : int\n        y- coordinate of the top left corner of the text.\n\n    text : str\n        The text to draw.\n\n    color : iterable of int, optional\n        Color of the text to draw. For RGB-images this is expected to be an RGB color.\n\n    size : int, optional\n        Font size of the text to draw.\n\n    Returns\n    -------\n    img_np : (H,W,3) ndarray\n        Input image with text drawn on it.\n\n    \"\"\"\n    do_assert(img.dtype in [np.uint8, np.float32])\n\n    input_dtype = img.dtype\n    if img.dtype == np.float32:\n        img = img.astype(np.uint8)\n\n    img = PIL_Image.fromarray(img)\n    font = PIL_ImageFont.truetype(DEFAULT_FONT_FP, size)\n    context = PIL_ImageDraw.Draw(img)\n    context.text((x, y), text, fill=tuple(color), font=font)\n    img_np = np.asarray(img)\n\n    # PIL/asarray returns read only array\n    if not img_np.flags[\"WRITEABLE\"]:\n        try:\n            # this seems to no longer work with np 1.16 (or was pillow updated?)\n            img_np.setflags(write=True)\n        except ValueError as ex:\n            if \"cannot set WRITEABLE flag to True of this array\" in str(ex):\n                img_np = np.copy(img_np)\n\n    if img_np.dtype != input_dtype:\n        img_np = img_np.astype(input_dtype)\n\n    return img_np\n\n\n# TODO rename sizes to size?\ndef imresize_many_images(images, sizes=None, interpolation=None):\n    \"\"\"\n    Resize many images to a specified size.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: no (1)\n        * ``uint64``: no (2)\n        * ``int8``: yes; tested (3)\n        * ``int16``: yes; tested\n        * ``int32``: limited; tested (4)\n        * ``int64``: no (2)\n        * ``float16``: yes; tested (5)\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: no (1)\n        * ``bool``: yes; tested (6)\n\n        - (1) rejected by ``cv2.imresize``\n        - (2) results too inaccurate\n        - (3) mapped internally to ``int16`` when interpolation!=\"nearest\"\n        - (4) only supported for interpolation=\"nearest\", other interpolations lead to cv2 error\n        - (5) mapped internally to ``float32``\n        - (6) mapped internally to ``uint8``\n\n    Parameters\n    ----------\n    images : (N,H,W,[C]) ndarray or list of (H,W,[C]) ndarray\n        Array of the images to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        The new size of the images, given either as a fraction (a single float) or as\n        a ``(height, width)`` tuple of two integers or as a ``(height fraction, width fraction)``\n        tuple of two floats.\n\n    interpolation : None or str or int, optional\n        The interpolation to use during resize.\n        If int, then expected to be one of:\n\n            * ``cv2.INTER_NEAREST`` (nearest neighbour interpolation)\n            * ``cv2.INTER_LINEAR`` (linear interpolation)\n            * ``cv2.INTER_AREA`` (area interpolation)\n            * ``cv2.INTER_CUBIC`` (cubic interpolation)\n\n        If string, then expected to be one of:\n\n            * ``nearest`` (identical to ``cv2.INTER_NEAREST``)\n            * ``linear`` (identical to ``cv2.INTER_LINEAR``)\n            * ``area`` (identical to ``cv2.INTER_AREA``)\n            * ``cubic`` (identical to ``cv2.INTER_CUBIC``)\n\n        If None, the interpolation will be chosen automatically. For size\n        increases, area interpolation will be picked and for size decreases,\n        linear interpolation will be picked.\n\n    Returns\n    -------\n    result : (N,H',W',[C]) ndarray\n        Array of the resized images.\n\n    Examples\n    --------\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), 2.0)\n\n    Converts 2 RGB images of height and width 16 to images of height and width 16*2 = 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (16, 32))\n\n    Converts 2 RGB images of height and width 16 to images of height 16 and width 32.\n\n    >>> imresize_many_images(np.zeros((2, 16, 16, 3), dtype=np.uint8), (2.0, 4.0))\n\n    Converts 2 RGB images of height and width 16 to images of height 32 and width 64.\n\n    \"\"\"\n    # we just do nothing if the input contains zero images\n    # one could also argue that an exception would be appropriate here\n    if len(images) == 0:\n        return images\n\n    # verify that all input images have height/width > 0\n    do_assert(\n        all([image.shape[0] > 0 and image.shape[1] > 0 for image in images]),\n        (\n            \"Cannot resize images, because at least one image has a height and/or width of zero. \"\n            + \"Observed shapes were: %s.\"\n        )\n        % (str([image.shape for image in images]),),\n    )\n\n    # verify that sizes contains only values >0\n    if is_single_number(sizes) and sizes <= 0:\n        raise Exception(\n            \"Cannot resize to the target size %.8f, because the value is zero or lower than zero.\"\n            % (sizes,)\n        )\n    elif isinstance(sizes, tuple) and (sizes[0] <= 0 or sizes[1] <= 0):\n        sizes_str = [\n            \"int %d\" % (sizes[0],)\n            if is_single_integer(sizes[0])\n            else \"float %.8f\" % (sizes[0],),\n            \"int %d\" % (sizes[1],)\n            if is_single_integer(sizes[1])\n            else \"float %.8f\" % (sizes[1],),\n        ]\n        sizes_str = \"(%s, %s)\" % (sizes_str[0], sizes_str[1])\n        raise Exception(\n            \"Cannot resize to the target sizes %s. At least one value is zero or lower than zero.\"\n            % (sizes_str,)\n        )\n\n    # change after the validation to make the above error messages match the original input\n    if is_single_number(sizes):\n        sizes = (sizes, sizes)\n    else:\n        do_assert(\n            len(sizes) == 2,\n            \"Expected tuple with exactly two entries, got %d entries.\" % (len(sizes),),\n        )\n        do_assert(\n            all([is_single_number(val) for val in sizes]),\n            \"Expected tuple with two ints or floats, got types %s.\"\n            % (str([type(val) for val in sizes]),),\n        )\n\n    # if input is a list, call this function N times for N images\n    # but check beforehand if all images have the same shape, then just convert to a single array and de-convert\n    # afterwards\n    if isinstance(images, list):\n        nb_shapes = len(set([image.shape for image in images]))\n        if nb_shapes == 1:\n            return list(\n                imresize_many_images(\n                    np.array(images), sizes=sizes, interpolation=interpolation\n                )\n            )\n        else:\n            return [\n                imresize_many_images(\n                    image[np.newaxis, ...], sizes=sizes, interpolation=interpolation\n                )[0, ...]\n                for image in images\n            ]\n\n    shape = images.shape\n    do_assert(\n        images.ndim in [3, 4],\n        \"Expected array of shape (N, H, W, [C]), got shape %s\" % (str(shape),),\n    )\n    nb_images = shape[0]\n    im_height, im_width = shape[1], shape[2]\n    nb_channels = shape[3] if images.ndim > 3 else None\n\n    height, width = sizes[0], sizes[1]\n    height = int(np.round(im_height * height)) if is_single_float(height) else height\n    width = int(np.round(im_width * width)) if is_single_float(width) else width\n\n    if height == im_height and width == im_width:\n        return np.copy(images)\n\n    ip = interpolation\n    do_assert(ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS)\n    if ip is None:\n        if height > im_height or width > im_width:\n            ip = cv2.INTER_AREA\n        else:\n            ip = cv2.INTER_LINEAR\n    elif ip in [\"nearest\", cv2.INTER_NEAREST]:\n        ip = cv2.INTER_NEAREST\n    elif ip in [\"linear\", cv2.INTER_LINEAR]:\n        ip = cv2.INTER_LINEAR\n    elif ip in [\"area\", cv2.INTER_AREA]:\n        ip = cv2.INTER_AREA\n    else:  # if ip in [\"cubic\", cv2.INTER_CUBIC]:\n        ip = cv2.INTER_CUBIC\n\n    # TODO find more beautiful way to avoid circular imports\n    from . import dtypes as iadt\n\n    if ip == cv2.INTER_NEAREST:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n    else:\n        iadt.gate_dtypes(\n            images,\n            allowed=[\n                \"bool\",\n                \"uint8\",\n                \"uint16\",\n                \"int8\",\n                \"int16\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ],\n            disallowed=[\n                \"uint32\",\n                \"uint64\",\n                \"uint128\",\n                \"uint256\",\n                \"int32\",\n                \"int64\",\n                \"int128\",\n                \"int256\",\n                \"float96\",\n                \"float128\",\n                \"float256\",\n            ],\n            augmenter=None,\n        )\n\n    result_shape = (nb_images, height, width)\n    if nb_channels is not None:\n        result_shape = result_shape + (nb_channels,)\n    result = np.zeros(result_shape, dtype=images.dtype)\n    for i, image in enumerate(images):\n        input_dtype = image.dtype\n        if image.dtype.type == np.bool_:\n            image = image.astype(np.uint8) * 255\n        elif image.dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            image = image.astype(np.int16)\n        elif image.dtype.type == np.float16:\n            image = image.astype(np.float32)\n\n        result_img = cv2.resize(image, (width, height), interpolation=ip)\n        assert result_img.dtype == image.dtype\n\n        # cv2 removes the channel axis if input was (H, W, 1)\n        # we re-add it (but only if input was not (H, W))\n        if len(result_img.shape) == 2 and nb_channels is not None and nb_channels == 1:\n            result_img = result_img[:, :, np.newaxis]\n\n        if input_dtype.type == np.bool_:\n            result_img = result_img > 127\n        elif input_dtype.type == np.int8 and ip != cv2.INTER_NEAREST:\n            # TODO somehow better avoid circular imports here\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.int8)\n        elif input_dtype.type == np.float16:\n            # TODO see above\n            from . import dtypes as iadt\n\n            result_img = iadt.restore_dtypes_(result_img, np.float16)\n        result[i] = result_img\n    return result\n\n\ndef imresize_single_image(image, sizes, interpolation=None):\n    \"\"\"\n    Resizes a single image.\n\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Parameters\n    ----------\n    image : (H,W,C) ndarray or (H,W) ndarray\n        Array of the image to resize.\n        Usually recommended to be of dtype uint8.\n\n    sizes : float or iterable of int or iterable of float\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    interpolation : None or str or int, optional\n        See :func:`imgaug.imgaug.imresize_many_images`.\n\n    Returns\n    -------\n    out : (H',W',C) ndarray or (H',W') ndarray\n        The resized image.\n\n    \"\"\"\n    grayscale = False\n    if image.ndim == 2:\n        grayscale = True\n        image = image[:, :, np.newaxis]\n    do_assert(len(image.shape) == 3, image.shape)\n    rs = imresize_many_images(\n        image[np.newaxis, :, :, :], sizes, interpolation=interpolation\n    )\n    if grayscale:\n        return np.squeeze(rs[0, :, :, 0])\n    else:\n        return rs[0, ...]\n\n\n# TODO add crop() function too\ndef pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n    \"\"\"\n    Pad an image-like array on its top/right/bottom/left side.\n\n    This function is a wrapper around :func:`numpy.pad`.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested (1)\n        * ``uint16``: yes; fully tested (1)\n        * ``uint32``: yes; fully tested (2) (3)\n        * ``uint64``: yes; fully tested (2) (3)\n        * ``int8``: yes; fully tested (1)\n        * ``int16``: yes; fully tested (1)\n        * ``int32``: yes; fully tested (1)\n        * ``int64``: yes; fully tested (2) (3)\n        * ``float16``: yes; fully tested (2) (3)\n        * ``float32``: yes; fully tested (1)\n        * ``float64``: yes; fully tested (1)\n        * ``float128``: yes; fully tested (2) (3)\n        * ``bool``: yes; tested (2) (3)\n\n        - (1) Uses ``cv2`` if `mode` is one of: ``\"constant\"``, ``\"edge\"``, ``\"reflect\"``, ``\"symmetric\"``.\n              Otherwise uses ``numpy``.\n        - (2) Uses ``numpy``.\n        - (3) Rejected by ``cv2``.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    top : int, optional\n        Amount of pixels to add at the top side of the image. Must be 0 or greater.\n\n    right : int, optional\n        Amount of pixels to add at the right side of the image. Must be 0 or greater.\n\n    bottom : int, optional\n        Amount of pixels to add at the bottom side of the image. Must be 0 or greater.\n\n    left : int, optional\n        Amount of pixels to add at the left side of the image. Must be 0 or greater.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n        In case of mode ``constant``, the parameter `cval` will be used as the ``constant_values``\n        parameter to :func:`numpy.pad`.\n        In case of mode ``linear_ramp``, the parameter `cval` will be used as the ``end_values``\n        parameter to :func:`numpy.pad`.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n        The cval is expected to match the input array's dtype and value range.\n\n    Returns\n    -------\n    arr_pad : (H',W') ndarray or (H',W',C) ndarray\n        Padded array with height ``H'=H+top+bottom`` and width ``W'=W+left+right``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(top >= 0)\n    do_assert(right >= 0)\n    do_assert(bottom >= 0)\n    do_assert(left >= 0)\n    if top > 0 or right > 0 or bottom > 0 or left > 0:\n        mapping_mode_np_to_cv2 = {\n            \"constant\": cv2.BORDER_CONSTANT,\n            \"edge\": cv2.BORDER_REPLICATE,\n            \"linear_ramp\": None,\n            \"maximum\": None,\n            \"mean\": None,\n            \"median\": None,\n            \"minimum\": None,\n            \"reflect\": cv2.BORDER_REFLECT_101,\n            \"symmetric\": cv2.BORDER_REFLECT,\n            \"wrap\": None,\n            cv2.BORDER_CONSTANT: cv2.BORDER_CONSTANT,\n            cv2.BORDER_REPLICATE: cv2.BORDER_REPLICATE,\n            cv2.BORDER_REFLECT_101: cv2.BORDER_REFLECT_101,\n            cv2.BORDER_REFLECT: cv2.BORDER_REFLECT,\n        }\n        bad_mode_cv2 = mapping_mode_np_to_cv2.get(mode, None) is None\n\n        # these datatypes all simply generate a \"TypeError: src data type = X is not supported\" error\n        bad_datatype_cv2 = arr.dtype.name in [\n            \"uint32\",\n            \"uint64\",\n            \"int64\",\n            \"float16\",\n            \"float128\",\n            \"bool\",\n        ]\n\n        if not bad_datatype_cv2 and not bad_mode_cv2:\n            cval = (\n                float(cval) if arr.dtype.kind == \"f\" else int(cval)\n            )  # results in TypeError otherwise for np inputs\n\n            if arr.ndim == 2 or arr.shape[2] <= 4:\n                # without this, only the first channel is padded with the cval, all following channels with 0\n                if arr.ndim == 3:\n                    cval = tuple([cval] * arr.shape[2])\n\n                arr_pad = cv2.copyMakeBorder(\n                    arr,\n                    top=top,\n                    bottom=bottom,\n                    left=left,\n                    right=right,\n                    borderType=mapping_mode_np_to_cv2[mode],\n                    value=cval,\n                )\n                if arr.ndim == 3 and arr_pad.ndim == 2:\n                    arr_pad = arr_pad[..., np.newaxis]\n            else:\n                result = []\n                channel_start_idx = 0\n                while channel_start_idx < arr.shape[2]:\n                    arr_c = arr[..., channel_start_idx : channel_start_idx + 4]\n                    cval_c = tuple([cval] * arr_c.shape[2])\n                    arr_pad_c = cv2.copyMakeBorder(\n                        arr_c,\n                        top=top,\n                        bottom=bottom,\n                        left=left,\n                        right=right,\n                        borderType=mapping_mode_np_to_cv2[mode],\n                        value=cval_c,\n                    )\n                    arr_pad_c = np.atleast_3d(arr_pad_c)\n                    result.append(arr_pad_c)\n                    channel_start_idx += 4\n                arr_pad = np.concatenate(result, axis=2)\n        else:\n            paddings_np = [(top, bottom), (left, right)]  # paddings for 2d case\n            if arr.ndim == 3:\n                paddings_np.append((0, 0))  # add paddings for 3d case\n\n            if mode == \"constant\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, constant_values=cval)\n            elif mode == \"linear_ramp\":\n                arr_pad = np.pad(arr, paddings_np, mode=mode, end_values=cval)\n            else:\n                arr_pad = np.pad(arr, paddings_np, mode=mode)\n\n        return arr_pad\n    return np.copy(arr)\n\n\n# TODO allow shape as input instead of array\ndef compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n    \"\"\"\n    Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio.\n\n    The aspect ratio is given as width/height.\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array for which to compute pad amounts.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    Returns\n    -------\n    result : tuple of int\n        Required paddign amounts to reach the target aspect ratio, given as a tuple\n        of the form ``(top, right, bottom, left)``.\n\n    \"\"\"\n    do_assert(arr.ndim in [2, 3])\n    do_assert(aspect_ratio > 0)\n    height, width = arr.shape[0:2]\n    do_assert(height > 0)\n    aspect_ratio_current = width / height\n\n    pad_top = 0\n    pad_right = 0\n    pad_bottom = 0\n    pad_left = 0\n\n    if aspect_ratio_current < aspect_ratio:\n        # vertical image, height > width\n        diff = (aspect_ratio * height) - width\n        pad_right = int(np.ceil(diff / 2))\n        pad_left = int(np.floor(diff / 2))\n    elif aspect_ratio_current > aspect_ratio:\n        # horizontal image, width > height\n        diff = ((1 / aspect_ratio) * width) - height\n        pad_top = int(np.floor(diff / 2))\n        pad_bottom = int(np.ceil(diff / 2))\n\n    return pad_top, pad_right, pad_bottom, pad_left\n\n\ndef pad_to_aspect_ratio(\n    arr, aspect_ratio, mode=\"constant\", cval=0, return_pad_amounts=False\n):\n    \"\"\"\n    Pad an image-like array on its sides so that it matches a target aspect ratio.\n\n    Depending on which dimension is smaller (height or width), only the corresponding\n    sides (left/right or top/bottom) will be padded. In each case, both of the sides will\n    be padded equally.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pad`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pad.\n\n    aspect_ratio : float\n        Target aspect ratio, given as width/height. E.g. 2.0 denotes the image having twice\n        as much width as height.\n\n    mode : str, optional\n        Padding mode to use. See :func:`numpy.pad` for details.\n\n    cval : number, optional\n        Value to use for padding if `mode` is ``constant``. See :func:`numpy.pad` for details.\n\n    return_pad_amounts : bool, optional\n        If False, then only the padded image will be returned. If True, a tuple with two\n        entries will be returned, where the first entry is the padded image and the second\n        entry are the amounts by which each image side was padded. These amounts are again a\n        tuple of the form (top, right, bottom, left), with each value being an integer.\n\n    Returns\n    -------\n    arr_padded : (H',W') ndarray or (H',W',C) ndarray\n        Padded image as (H',W') or (H',W',C) ndarray, fulfulling the given aspect_ratio.\n\n    tuple of int\n        Amounts by which the image was padded on each side, given as a tuple ``(top, right, bottom, left)``.\n        This tuple is only returned if `return_pad_amounts` was set to True.\n        Otherwise only ``arr_padded`` is returned.\n\n    \"\"\"\n    pad_top, pad_right, pad_bottom, pad_left = compute_paddings_for_aspect_ratio(\n        arr, aspect_ratio\n    )\n    arr_padded = pad(\n        arr,\n        top=pad_top,\n        right=pad_right,\n        bottom=pad_bottom,\n        left=pad_left,\n        mode=mode,\n        cval=cval,\n    )\n\n    if return_pad_amounts:\n        return arr_padded, (pad_top, pad_right, pad_bottom, pad_left)\n    else:\n        return arr_padded\n\n\ndef pool(arr, block_size, func, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array by pooling values within blocks.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; tested\n        * ``uint32``: yes; tested (2)\n        * ``uint64``: no (1)\n        * ``int8``: yes; tested\n        * ``int16``: yes; tested\n        * ``int32``: yes; tested (2)\n        * ``int64``: no (1)\n        * ``float16``: yes; tested\n        * ``float32``: yes; tested\n        * ``float64``: yes; tested\n        * ``float128``: yes; tested (2)\n        * ``bool``: yes; tested\n\n        - (1) results too inaccurate (at least when using np.average as func)\n        - (2) Note that scikit-image documentation says that the wrapped pooling function converts\n              inputs to float64. Actual tests showed no indication of that happening (at least when\n              using preserve_dtype=True).\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. Ideally of datatype ``numpy.float64``.\n\n    block_size : int or tuple of int\n        Spatial size of each group of values to pool, aka kernel size.\n        If a single integer, then a symmetric block of that size along height and width will be used.\n        If a tuple of two values, it is assumed to be the block size along height and width of the image-like,\n        with pooling happening per channel.\n        If a tuple of three values, it is assumed to be the block size along height, width and channels.\n\n    func : callable\n        Function to apply to a given block in order to convert it to a single number,\n        e.g. :func:`numpy.average`, :func:`numpy.min`, :func:`numpy.max`.\n\n    cval : number, optional\n        Value to use in order to pad the array along its border if the array cannot be divided\n        by `block_size` without remainder.\n\n    preserve_dtype : bool, optional\n        Whether to convert the array back to the input datatype if it is changed away from\n        that in the pooling process.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after pooling.\n\n    \"\"\"\n    # TODO find better way to avoid circular import\n    from . import dtypes as iadt\n\n    iadt.gate_dtypes(\n        arr,\n        allowed=[\n            \"bool\",\n            \"uint8\",\n            \"uint16\",\n            \"uint32\",\n            \"int8\",\n            \"int16\",\n            \"int32\",\n            \"float16\",\n            \"float32\",\n            \"float64\",\n            \"float128\",\n        ],\n        disallowed=[\n            \"uint64\",\n            \"uint128\",\n            \"uint256\",\n            \"int64\",\n            \"int128\",\n            \"int256\",\n            \"float256\",\n        ],\n        augmenter=None,\n    )\n\n    do_assert(arr.ndim in [2, 3])\n    is_valid_int = is_single_integer(block_size) and block_size >= 1\n    is_valid_tuple = (\n        is_iterable(block_size)\n        and len(block_size) in [2, 3]\n        and [is_single_integer(val) and val >= 1 for val in block_size]\n    )\n    do_assert(is_valid_int or is_valid_tuple)\n\n    if is_single_integer(block_size):\n        block_size = [block_size, block_size]\n    if len(block_size) < arr.ndim:\n        block_size = list(block_size) + [1]\n\n    input_dtype = arr.dtype\n    arr_reduced = skimage.measure.block_reduce(arr, tuple(block_size), func, cval=cval)\n    if preserve_dtype and arr_reduced.dtype.type != input_dtype:\n        arr_reduced = arr_reduced.astype(input_dtype)\n    return arr_reduced\n\n\ndef avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using average pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See :func:`imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after average pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.average, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef max_pool(arr, block_size, cval=0, preserve_dtype=True):\n    \"\"\"\n    Resize an array using max-pooling.\n\n    dtype support::\n\n        See :func:`imgaug.imgaug.pool`.\n\n    Parameters\n    ----------\n    arr : (H,W) ndarray or (H,W,C) ndarray\n        Image-like array to pool. See :func:`imgaug.pool` for details.\n\n    block_size : int or tuple of int or tuple of int\n        Size of each block of values to pool. See `imgaug.pool` for details.\n\n    cval : number, optional\n        Padding value. See :func:`imgaug.pool` for details.\n\n    preserve_dtype : bool, optional\n        Whether to preserve the input array dtype. See :func:`imgaug.pool` for details.\n\n    Returns\n    -------\n    arr_reduced : (H',W') ndarray or (H',W',C') ndarray\n        Array after max-pooling.\n\n    \"\"\"\n    return pool(arr, block_size, np.max, cval=cval, preserve_dtype=preserve_dtype)\n\n\ndef draw_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts multiple input images into a single image showing them in a grid.\n\n    dtype support::\n\n        * ``uint8``: yes; fully tested\n        * ``uint16``: yes; fully tested\n        * ``uint32``: yes; fully tested\n        * ``uint64``: yes; fully tested\n        * ``int8``: yes; fully tested\n        * ``int16``: yes; fully tested\n        * ``int32``: yes; fully tested\n        * ``int64``: yes; fully tested\n        * ``float16``: yes; fully tested\n        * ``float32``: yes; fully tested\n        * ``float64``: yes; fully tested\n        * ``float128``: yes; fully tested\n        * ``bool``: yes; fully tested\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        The input images to convert to a grid.\n\n    rows : None or int, optional\n        The number of rows to show in the grid.\n        If None, it will be automatically derived.\n\n    cols : None or int, optional\n        The number of cols to show in the grid.\n        If None, it will be automatically derived.\n\n    Returns\n    -------\n    grid : (H',W',3) ndarray\n        Image of the generated grid.\n\n    \"\"\"\n    nb_images = len(images)\n    do_assert(nb_images > 0)\n\n    if is_np_array(images):\n        do_assert(images.ndim == 4)\n    else:\n        do_assert(\n            is_iterable(images) and is_np_array(images[0]) and images[0].ndim == 3\n        )\n        dts = [image.dtype.name for image in images]\n        nb_dtypes = len(set(dts))\n        do_assert(\n            nb_dtypes == 1,\n            (\n                \"All images provided to draw_grid() must have the same dtype, \"\n                + \"found %d dtypes (%s)\"\n            )\n            % (nb_dtypes, \", \".join(dts)),\n        )\n\n    cell_height = max([image.shape[0] for image in images])\n    cell_width = max([image.shape[1] for image in images])\n    channels = set([image.shape[2] for image in images])\n    do_assert(\n        len(channels) == 1,\n        \"All images are expected to have the same number of channels, \"\n        + \"but got channel set %s with length %d instead.\"\n        % (str(channels), len(channels)),\n    )\n    nb_channels = list(channels)[0]\n    if rows is None and cols is None:\n        rows = cols = int(math.ceil(math.sqrt(nb_images)))\n    elif rows is not None:\n        cols = int(math.ceil(nb_images / rows))\n    elif cols is not None:\n        rows = int(math.ceil(nb_images / cols))\n    do_assert(rows * cols >= nb_images)\n\n    width = cell_width * cols\n    height = cell_height * rows\n    dt = images.dtype if is_np_array(images) else images[0].dtype\n    grid = np.zeros((height, width, nb_channels), dtype=dt)\n    cell_idx = 0\n    for row_idx in sm.xrange(rows):\n        for col_idx in sm.xrange(cols):\n            if cell_idx < nb_images:\n                image = images[cell_idx]\n                cell_y1 = cell_height * row_idx\n                cell_y2 = cell_y1 + image.shape[0]\n                cell_x1 = cell_width * col_idx\n                cell_x2 = cell_x1 + image.shape[1]\n                grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image\n            cell_idx += 1\n\n    return grid\n\n\ndef show_grid(images, rows=None, cols=None):\n    \"\"\"\n    Converts the input images to a grid image and shows it in a new window.\n\n    dtype support::\n\n        minimum of (\n            :func:`imgaug.imgaug.draw_grid`,\n            :func:`imgaug.imgaug.imshow`\n        )\n\n    Parameters\n    ----------\n    images : (N,H,W,3) ndarray or iterable of (H,W,3) array\n        See :func:`imgaug.draw_grid`.\n\n    rows : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    cols : None or int, optional\n        See :func:`imgaug.draw_grid`.\n\n    \"\"\"\n    grid = draw_grid(images, rows=rows, cols=cols)\n    imshow(grid)\n\n\ndef imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n    \"\"\"\n    Shows an image in a window.\n\n    dtype support::\n\n        * ``uint8``: yes; not tested\n        * ``uint16``: ?\n        * ``uint32``: ?\n        * ``uint64``: ?\n        * ``int8``: ?\n        * ``int16``: ?\n        * ``int32``: ?\n        * ``int64``: ?\n        * ``float16``: ?\n        * ``float32``: ?\n        * ``float64``: ?\n        * ``float128``: ?\n        * ``bool``: ?\n\n    Parameters\n    ----------\n    image : (H,W,3) ndarray\n        Image to show.\n\n    backend : {'matplotlib', 'cv2'}, optional\n        Library to use to show the image. May be either matplotlib or OpenCV ('cv2').\n        OpenCV tends to be faster, but apparently causes more technical issues.\n\n    \"\"\"\n    do_assert(\n        backend in [\"matplotlib\", \"cv2\"],\n        \"Expected backend 'matplotlib' or 'cv2', got %s.\" % (backend,),\n    )\n\n    if backend == \"cv2\":\n        image_bgr = image\n        if image.ndim == 3 and image.shape[2] in [3, 4]:\n            image_bgr = image[..., 0:3][..., ::-1]\n\n        win_name = \"imgaug-default-window\"\n        cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n        cv2.imshow(win_name, image_bgr)\n        cv2.waitKey(0)\n        cv2.destroyWindow(win_name)\n    else:\n        # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225)\n        import matplotlib.pyplot as plt\n\n        dpi = 96\n        h, w = image.shape[0] / dpi, image.shape[1] / dpi\n        w = max(\n            w, 6\n        )  # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly)\n        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)\n        fig.canvas.set_window_title(\"imgaug.imshow(%s)\" % (image.shape,))\n        ax.imshow(image, cmap=\"gray\")  # cmap is only activate for grayscale images\n        plt.show()\n\n\ndef do_assert(condition, message=\"Assertion failed.\"):\n    \"\"\"\n    Function that behaves equally to an `assert` statement, but raises an\n    Exception.\n\n    This is added because `assert` statements are removed in optimized code.\n    It replaces `assert` statements throughout the library that should be\n    kept even in optimized code.\n\n    Parameters\n    ----------\n    condition : bool\n        If False, an exception is raised.\n\n    message : str, optional\n        Error message.\n\n    \"\"\"\n    if not condition:\n        raise AssertionError(str(message))\n\n\nclass HooksImages(object):\n    \"\"\"\n    Class to intervene with image augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    Parameters\n    ----------\n    activator : None or callable, optional\n        A function that gives permission to execute an augmenter.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        where ``images`` are the input images to augment, ``augmenter`` is the\n        instance of the augmenter to execute, ``parents`` are previously\n        executed augmenters and ``default`` is an expected default value to be\n        returned if the activator function does not plan to make a decision\n        for the given inputs.\n\n    propagator : None or callable, optional\n        A function that gives permission to propagate the augmentation further\n        to the children of an augmenter. This happens after the activator.\n        In theory, an augmenter may augment images itself (if allowed by the\n        activator) and then execute child augmenters afterwards (if allowed by\n        the propagator). If the activator returned False, the propagation step\n        will never be executed.\n        The expected interface is ``f(images, augmenter, parents, default)``,\n        with all arguments having identical meaning to the activator.\n\n    preprocessor : None or callable, optional\n        A function to call before an augmenter performed any augmentations.\n        The interface is ``f(images, augmenter, parents)``,\n        with all arguments having identical meaning to the activator.\n        It is expected to return the input images, optionally modified.\n\n    postprocessor : None or callable, optional\n        A function to call after an augmenter performed augmentations.\n        The interface is the same as for the preprocessor.\n\n    Examples\n    --------\n    >>> seq = iaa.Sequential([\n    >>>     iaa.GaussianBlur(3.0, name=\"blur\"),\n    >>>     iaa.Dropout(0.05, name=\"dropout\"),\n    >>>     iaa.Affine(translate_px=-5, name=\"affine\")\n    >>> ])\n    >>> images = [np.zeros((10, 10), dtype=np.uint8)]\n    >>>\n    >>> def activator(images, augmenter, parents, default):\n    >>>     return False if augmenter.name in [\"blur\", \"dropout\"] else default\n    >>>\n    >>> seq_det = seq.to_deterministic()\n    >>> images_aug = seq_det.augment_images(images)\n    >>> heatmaps = [np.random.rand(*(3, 10, 10))]\n    >>> heatmaps_aug = seq_det.augment_images(\n    >>>     heatmaps,\n    >>>     hooks=ia.HooksImages(activator=activator)\n    >>> )\n\n    This augments images and their respective heatmaps in the same way.\n    The heatmaps however are only modified by Affine, not by GaussianBlur or\n    Dropout.\n\n    \"\"\"\n\n    def __init__(\n        self, activator=None, propagator=None, preprocessor=None, postprocessor=None\n    ):\n        self.activator = activator\n        self.propagator = propagator\n        self.preprocessor = preprocessor\n        self.postprocessor = postprocessor\n\n    def is_activated(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may be executed.\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be executed. If False, it may not be executed.\n\n        \"\"\"\n        if self.activator is None:\n            return default\n        else:\n            return self.activator(images, augmenter, parents, default)\n\n    def is_propagating(self, images, augmenter, parents, default):\n        \"\"\"\n        Returns whether an augmenter may call its children to augment an\n        image. This is independent of the augmenter itself possible changing\n        the image, without calling its children. (Most (all?) augmenters with\n        children currently dont perform any changes themselves.)\n\n        Returns\n        -------\n        bool\n            If True, the augmenter may be propagate to its children. If False, it may not.\n\n        \"\"\"\n        if self.propagator is None:\n            return default\n        else:\n            return self.propagator(images, augmenter, parents, default)\n\n    def preprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called before the augmentation of images starts (per augmenter).\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.preprocessor is None:\n            return images\n        else:\n            return self.preprocessor(images, augmenter, parents)\n\n    def postprocess(self, images, augmenter, parents):\n        \"\"\"\n        A function to be called after the augmentation of images was\n        performed.\n\n        Returns\n        -------\n        (N,H,W,C) ndarray or (N,H,W) ndarray or list of (H,W,C) ndarray or list of (H,W) ndarray\n            The input images, optionally modified.\n\n        \"\"\"\n        if self.postprocessor is None:\n            return images\n        else:\n            return self.postprocessor(images, augmenter, parents)\n\n\nclass HooksHeatmaps(HooksImages):\n    \"\"\"\n    Class to intervene with heatmap augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\nclass HooksKeypoints(HooksImages):\n    \"\"\"\n    Class to intervene with keypoint augmentation runs.\n\n    This is e.g. useful to dynamically deactivate some augmenters.\n\n    This class is currently the same as the one for images. This may or may\n    not change in the future.\n\n    \"\"\"\n\n    pass\n\n\n#####################################################################\n# Helpers for deprecation\n#####################################################################\n\n\nclass DeprecationWarning(Warning):  # pylint: disable=redefined-builtin\n    \"\"\"Warning for deprecated calls.\n\n    Since python 2.7 DeprecatedWarning is silent by default. So we define\n    our own DeprecatedWarning here so that it is not silent by default.\n\n    \"\"\"\n\n    pass\n\n\ndef warn_deprecated(msg, stacklevel=2):\n    \"\"\"Generate a non-silent deprecation warning with stacktrace.\n\n    The used warning is ``imgaug.imgaug.DeprecationWarning``.\n\n    Parameters\n    ----------\n    msg : str\n        The message of the warning.\n\n    stacklevel : int, optional\n        How many steps above this function to \"jump\" in the stacktrace for\n        the displayed file and line number of the error message.\n        Usually 2.\n\n    \"\"\"\n    import warnings\n\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nclass deprecated(object):\n    \"\"\"Decorator to mark deprecated functions with warning.\n\n    Adapted from\n    <https://github.com/scikit-image/scikit-image/blob/master/skimage/_shared/utils.py>.\n\n    Parameters\n    ----------\n    alt_func : None or str, optional\n        If given, tell user what function to use instead.\n\n    behavior : {'warn', 'raise'}, optional\n        Behavior during call to deprecated function: 'warn' = warn user that\n        function is deprecated; 'raise' = raise error.\n\n    removed_version : None or str, optional\n        The package version in which the deprecated function will be removed.\n\n    comment : None or str, optional\n        An optional comment that will be appended to the warning message.\n\n    \"\"\"\n\n    def __init__(\n        self, alt_func=None, behavior=\"warn\", removed_version=None, comment=None\n    ):\n        self.alt_func = alt_func\n        self.behavior = behavior\n        self.removed_version = removed_version\n        self.comment = comment\n\n    def __call__(self, func):\n        alt_msg = None\n        if self.alt_func is not None:\n            alt_msg = \"Use ``%s`` instead.\" % (self.alt_func,)\n\n        rmv_msg = None\n        if self.removed_version is not None:\n            rmv_msg = \"It will be removed in version %s.\" % (self.removed_version,)\n\n        comment_msg = None\n        if self.comment is not None and len(self.comment) > 0:\n            comment_msg = \"%s.\" % (self.comment.rstrip(\". \"),)\n\n        addendum = \" \".join(\n            [submsg for submsg in [alt_msg, rmv_msg, comment_msg] if submsg is not None]\n        )\n\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            # TODO add class name if class method\n            import inspect\n\n            # arg_names = func.__code__.co_varnames\n\n            # getargspec() was deprecated in py3, but doesn't exist in py2\n            if hasattr(inspect, \"getfullargspec\"):\n                arg_names = inspect.getfullargspec(func)[0]\n            else:\n                arg_names = inspect.getargspec(func)[0]\n\n            if \"self\" in arg_names or \"cls\" in arg_names:\n                main_msg = \"Method ``%s.%s()`` is deprecated.\" % (\n                    args[0].__class__.__name__,\n                    func.__name__,\n                )\n            else:\n                main_msg = \"Function ``%s()`` is deprecated.\" % (func.__name__,)\n\n            msg = (main_msg + \" \" + addendum).rstrip(\" \").replace(\"``\", \"`\")\n\n            if self.behavior == \"warn\":\n                warn_deprecated(msg, stacklevel=3)\n            elif self.behavior == \"raise\":\n                raise DeprecationWarning(msg)\n            return func(*args, **kwargs)\n\n        # modify doc string to display deprecation warning\n        doc = \"**Deprecated**. \" + addendum\n        if wrapped.__doc__ is None:\n            wrapped.__doc__ = doc\n        else:\n            wrapped.__doc__ = doc + \"\\n\\n    \" + wrapped.__doc__\n\n        return wrapped\n\n\n#####################################################################\n# Create classes/functions that were moved to other files and create\n# DeprecatedWarnings when they are called.\n#####################################################################\n\n\ndef _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n    class_name_new = class_name_new if class_name_new is not None else class_name_old\n\n    def _func(*args, **kwargs):\n        import importlib\n\n        warn_deprecated(\n            \"Using imgaug.imgaug.%s is deprecated. Use %s.%s instead.\"\n            % (class_name_old, module_name_new, class_name_new)\n        )\n        module = importlib.import_module(module_name_new)\n        return getattr(module, class_name_new)(*args, **kwargs)\n\n    return _func\n\n\nMOVED = [\n    (\"Keypoint\", \"imgaug.augmentables.kps\", None),\n    (\"KeypointsOnImage\", \"imgaug.augmentables.kps\", None),\n    (\"BoundingBox\", \"imgaug.augmentables.bbs\", None),\n    (\"BoundingBoxesOnImage\", \"imgaug.augmentables.bbs\", None),\n    (\"Polygon\", \"imgaug.augmentables.polys\", None),\n    (\"PolygonsOnImage\", \"imgaug.augmentables.polys\", None),\n    (\"MultiPolygon\", \"imgaug.augmentables.polys\", None),\n    (\"_ConcavePolygonRecoverer\", \"imgaug.augmentables.polys\", None),\n    (\"HeatmapsOnImage\", \"imgaug.augmentables.heatmaps\", None),\n    (\"SegmentationMapOnImage\", \"imgaug.augmentables.segmaps\", None),\n    (\"Batch\", \"imgaug.augmentables.batches\", None),\n    (\"BatchLoader\", \"imgaug.multicore\", None),\n    (\"BackgroundAugmenter\", \"imgaug.multicore\", None),\n    (\"compute_geometric_median\", \"imgaug.augmentables.kps\", None),\n    (\"_convert_points_to_shapely_line_string\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_point_pair\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points\", \"imgaug.augmentables.polys\", None),\n    (\"_interpolate_points_by_max_distance\", \"imgaug.augmentables.polys\", None),\n]\n\nfor class_name_old, module_name_new, class_name_new in MOVED:\n    locals()[class_name_old] = _mark_moved_class_or_function(\n        class_name_old, module_name_new, class_name_new\n    )\n", "levels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1], "package": ["from __future__ import print_function, division, absolute_import", "import math", "import numbers", "import sys", "import os", "import json", "import types", "import functools", "import numpy as np", "import cv2", "import imageio", "import six", "import six.moves as sm", "import skimage.draw", "import skimage.measure", "import collections", "from PIL import (", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.heatmaps import HeatmapsOnImage", "from imgaug.augmentables.segmaps import SegmentationMapOnImage", "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage", "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage", "from imgaug.augmentables.polys import Polygon, PolygonsOnImage", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "from . import dtypes as iadt", "import matplotlib.pyplot as plt"], "function": ["def is_np_array(val):\n", "def is_single_integer(val):\n", "def is_single_float(val):\n", "def is_single_number(val):\n", "def is_iterable(val):\n", "def is_string(val):\n", "def is_single_bool(val):\n", "def is_integer_array(val):\n", "def is_float_array(val):\n", "def is_callable(val):\n", "def is_generator(val):\n", "def flatten(nested_iterable):\n", "def caller_name():\n", "def seed(seedval):\n", "def current_random_state():\n", "def new_random_state(seed=None, fully_random=False):\n", "def dummy_random_state():\n", "def copy_random_state(random_state, force_copy=False):\n", "def derive_random_state(random_state):\n", "def derive_random_states(random_state, n=1):\n", "def forward_random_state(random_state):\n", "def _quokka_normalize_extract(extract):\n", "def _compute_resized_shape(from_shape, to_shape):\n", "def quokka(size=None, extract=None):\n", "def quokka_square(size=None):\n", "def quokka_heatmap(size=None, extract=None):\n", "def quokka_segmentation_map(size=None, extract=None):\n", "def quokka_keypoints(size=None, extract=None):\n", "def quokka_bounding_boxes(size=None, extract=None):\n", "def quokka_polygons(size=None, extract=None):\n", "def compute_line_intersection_point(x1, y1, x2, y2, x3, y3, x4, y4):\n", "    def _make_line(p1, p2):\n", "def draw_text(img, y, x, text, color=(0, 255, 0), size=25):\n", "def imresize_many_images(images, sizes=None, interpolation=None):\n", "def imresize_single_image(image, sizes, interpolation=None):\n", "def pad(arr, top=0, right=0, bottom=0, left=0, mode=\"constant\", cval=0):\n", "def compute_paddings_for_aspect_ratio(arr, aspect_ratio):\n", "def pool(arr, block_size, func, cval=0, preserve_dtype=True):\n", "def avg_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def max_pool(arr, block_size, cval=0, preserve_dtype=True):\n", "def draw_grid(images, rows=None, cols=None):\n", "def show_grid(images, rows=None, cols=None):\n", "def imshow(image, backend=IMSHOW_BACKEND_DEFAULT):\n", "def do_assert(condition, message=\"Assertion failed.\"):\n", "class HooksImages(object):\n", "    def is_activated(self, images, augmenter, parents, default):\n", "    def is_propagating(self, images, augmenter, parents, default):\n", "    def preprocess(self, images, augmenter, parents):\n", "    def postprocess(self, images, augmenter, parents):\n", "class HooksHeatmaps(HooksImages):\n", "class HooksKeypoints(HooksImages):\n", "def warn_deprecated(msg, stacklevel=2):\n", "class deprecated(object):\n", "    def __call__(self, func):\n", "        def wrapped(*args, **kwargs):\n", "def _mark_moved_class_or_function(class_name_old, module_name_new, class_name_new):\n", "    def _func(*args, **kwargs):\n"]}
